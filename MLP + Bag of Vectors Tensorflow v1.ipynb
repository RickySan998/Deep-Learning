{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Question4_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMqnxmVyxaw2"
      },
      "source": [
        "# **Install and Load Libraries needed for the task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9qiCvUbBQaz",
        "outputId": "9c933fc6-49f3-43e7-c92b-cda368ede298"
      },
      "source": [
        "!pip install utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvtAl2o--Vog",
        "outputId": "23f09a5a-3f4f-4602-fd6c-4582a015f553"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import gensim\n",
        "import utils\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "from operator import itemgetter\n",
        "from copy import deepcopy\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "InVo4oTaDpox",
        "outputId": "b4358cfe-8874-42cd-cb1a-4925c4921d96"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeQi_Azu-Vok"
      },
      "source": [
        "# **Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPAcCxtl-Vok",
        "outputId": "51077193-211f-4797-c8d1-19f7ca25e1cd"
      },
      "source": [
        "# Download the dataset\n",
        "url = \"https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\"\n",
        "try:\n",
        "    local_filename, headers = urllib.request.urlretrieve(url, \"trainDevTestTrees_PTB.zip\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "print(local_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainDevTestTrees_PTB.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smO2jW5I-Vol",
        "outputId": "3fbccf33-f4c5-4394-88d7-66f9962aa421"
      },
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(local_filename, 'r') as my_zip:\n",
        "    my_zip.extractall(\"trainDevTestTrees_PTB\")\n",
        "os.listdir(\"trainDevTestTrees_PTB\") # List down the extracted data files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trees']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bpBiy-8-Vol"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "def load_sentences(path):\n",
        "    \"\"\"\n",
        "    Output would be\n",
        "    sentences = a nData x k array of sentences, where k = number of WORDS in each sentence. Each row is a sentence split\n",
        "    into strings of WORDS\n",
        "    labels = a nData array of ratings (integer) for each sentence\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    file = open(path, \"r\")\n",
        "    for line in file:\n",
        "        soup = line.split() # first split each line (of whole sentence rating along with (rating WORD) pairs) into mini-strings seperated by space. Since there's always a space between a rating and a rated WORD, the ratings would be seperated from the WORD as 2 strings\n",
        "        labels.append(int(soup[0].lstrip(\"(\"))) # the first number from the left is the rating of the sentence. Take out the leading brackets in the first string, and convert that number char to integer\n",
        "        tokens = []\n",
        "        for chunk in soup[1:]:\n",
        "            if chunk.endswith(\")\"): # every string chunks with a WORD, ends with a ')' because the original format was (rating word). Rating and word seperated due to space between as before\n",
        "                tokens.append(chunk.rstrip(\")\")) # hence remove the rightmost ')' and append to the 'tokens' array. This is just array of words\n",
        "        sentences.append(tokens) # at the end, append this array. This would be a sentence, divided into strings of words\n",
        "    return sentences, labels\n",
        "\n",
        "def get_everything_in_one_char_str(str_set,chars_needed): # unused?\n",
        "    final_str = \"\"\n",
        "    for i in range(len(str_set)):\n",
        "        final_str += \" \" + \" \".join(str_set[i]) # join all words in str_set[i] (i.e. a sentence which is array of word strings) seperating each word with a space.\n",
        "        # then join all these sentences also into a single long string.\n",
        "    final_str = final_str.lstrip() # Remove the leftmost space\n",
        "    final_str = ''.join([char for char in final_str if char in chars_needed]) # remove unwanted chars. Here, re-join each wanted character in the long string (of ALL sentences) without seperation (since we already contain spaces in this long string)\n",
        "    return final_str\n",
        "\n",
        "def erase_invalid_simbols(sentences,chars_needed):\n",
        "    \"\"\"\n",
        "    To erase invalid symbols from every word in the extracted sentences. Invalid means not alphabets and symbols declared below\n",
        "    Output:\n",
        "    A 2D array of size nData x k where k = length of each sentences.\n",
        "    \"\"\"\n",
        "    new_sentences=[]\n",
        "    for sentence in sentences:\n",
        "        token=[] # array to store the 'filtered' word\n",
        "        for word in sentence:\n",
        "            word_new=''\n",
        "            for char in word:\n",
        "                if char in chars_needed:\n",
        "                    word_new+=char # filter a word to contain only wanted characters\n",
        "            if not(word_new == ''):\n",
        "                token.append(word_new)\n",
        "        new_sentences.append(token)\n",
        "    return new_sentences\n",
        "                    \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "chars_needed = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P','Q','R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-3MQE9fWsAB",
        "outputId": "4ade15b2-0d2a-419e-bbc8-8c396103309c"
      },
      "source": [
        "# Self-picked sentences for the last part of this task\n",
        "sentences_addtest = [\"A bitter disappointment and awful ending to a brilliant trilogy .\", \"The Dark Knight Rises was a perfect sequel to a masterpiece .\", \"Christopher Nolan's second bundle of joy The Dark Knight exceeded all of my expectations !!!\",\n",
        "                     \"A lame spy spoof with no real laughs or surprises\", \"Not The Worst Brit Comedy You Will See\"]\n",
        "labels_add_test = [0, 4, 4, 0, 1]\n",
        "sentences_add_test = []\n",
        "for sentence in sentences_addtest:\n",
        "    sentences_add_test.append(sentence.split())\n",
        "\n",
        "print(sentences_add_test)\n",
        "print(len(sentences_add_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['A', 'bitter', 'disappointment', 'and', 'awful', 'ending', 'to', 'a', 'brilliant', 'trilogy', '.'], ['The', 'Dark', 'Knight', 'Rises', 'was', 'a', 'perfect', 'sequel', 'to', 'a', 'masterpiece', '.'], ['Christopher', \"Nolan's\", 'second', 'bundle', 'of', 'joy', 'The', 'Dark', 'Knight', 'exceeded', 'all', 'of', 'my', 'expectations', '!!!'], ['A', 'lame', 'spy', 'spoof', 'with', 'no', 'real', 'laughs', 'or', 'surprises'], ['Not', 'The', 'Worst', 'Brit', 'Comedy', 'You', 'Will', 'See']]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFa7FV1U-Vol",
        "outputId": "67ffdd33-4a47-49fc-d09b-3948b56ee1d1"
      },
      "source": [
        "# Extract sentences and its label\n",
        "sentences_train, labels_train = load_sentences(\"trainDevTestTrees_PTB/trees/train.txt\")\n",
        "sentences_val, labels_val = load_sentences(\"trainDevTestTrees_PTB/trees/dev.txt\")\n",
        "sentences_test, labels_test = load_sentences(\"trainDevTestTrees_PTB/trees/test.txt\")\n",
        "\n",
        "\n",
        "labels_train= np.asarray(labels_train)\n",
        "print(labels_train.shape)\n",
        "labels_val= np.asarray(labels_val)\n",
        "print(labels_val.shape)\n",
        "labels_test= np.asarray(labels_test)\n",
        "print(labels_test.shape)\n",
        "print(max(labels_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8544,)\n",
            "(1101,)\n",
            "(2210,)\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC7PZapoaRId",
        "outputId": "d9b7bf55-f15c-4add-be00-5345bb854afe"
      },
      "source": [
        "# To check that we didn't remove any sentences\n",
        "print(len(sentences_train), len(sentences_val), len(sentences_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544 1101 2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlq6H7_6-Vol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb9d713-af56-4ec9-c10a-5afa576f7a1f"
      },
      "source": [
        "# Remove invalid symbols in all sentences. We only take alphabets\n",
        "sentences_train=erase_invalid_simbols(sentences_train,chars_needed)\n",
        "sentences_val=erase_invalid_simbols(sentences_val,chars_needed)\n",
        "sentences_test=erase_invalid_simbols(sentences_test,chars_needed)\n",
        "sentences_add_test = erase_invalid_simbols(sentences_add_test, chars_needed)\n",
        "\n",
        "print(len(sentences_train), len(sentences_val), len(sentences_test), len(sentences_add_test))\n",
        "\n",
        "# As seen here, we did not remove any sentences by removing the 'invalid' characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544 1101 2210 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZawYUlJbMKq",
        "outputId": "efd73b8e-8c1b-4d89-defb-5d482d89569f"
      },
      "source": [
        "# Check if the additional sentences are loaded correctly\n",
        "sentences_add_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'bitter',\n",
              "  'disappointment',\n",
              "  'and',\n",
              "  'awful',\n",
              "  'ending',\n",
              "  'to',\n",
              "  'a',\n",
              "  'brilliant',\n",
              "  'trilogy'],\n",
              " ['The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'Rises',\n",
              "  'was',\n",
              "  'a',\n",
              "  'perfect',\n",
              "  'sequel',\n",
              "  'to',\n",
              "  'a',\n",
              "  'masterpiece'],\n",
              " ['Christopher',\n",
              "  'Nolans',\n",
              "  'second',\n",
              "  'bundle',\n",
              "  'of',\n",
              "  'joy',\n",
              "  'The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'exceeded',\n",
              "  'all',\n",
              "  'of',\n",
              "  'my',\n",
              "  'expectations'],\n",
              " ['A',\n",
              "  'lame',\n",
              "  'spy',\n",
              "  'spoof',\n",
              "  'with',\n",
              "  'no',\n",
              "  'real',\n",
              "  'laughs',\n",
              "  'or',\n",
              "  'surprises'],\n",
              " ['Not', 'The', 'Worst', 'Brit', 'Comedy', 'You', 'Will', 'See']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW9lCxLX-Vol",
        "outputId": "3b033775-f024-4db1-cd32-6789e9cb7109"
      },
      "source": [
        "# Check also if sentences in the given dataset are loaded correctly\n",
        "sentences_val[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " 's',\n",
              " 'a',\n",
              " 'lovely',\n",
              " 'film',\n",
              " 'with',\n",
              " 'lovely',\n",
              " 'performances',\n",
              " 'by',\n",
              " 'Buy',\n",
              " 'and',\n",
              " 'Accorsi']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycbp2Dz4-Vol"
      },
      "source": [
        "# **Load the pre-trained word2vec from google**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnBUfztwBzk5",
        "outputId": "dc2cecec-bc2a-4293-c34b-c499c393e954"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-11 16:17:09--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.66.14\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.66.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  77.9MB/s    in 20s     \n",
            "\n",
            "2020-12-11 16:17:29 (79.3 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50AxiIDj-Vol"
      },
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
        "# model is a dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wynV_WDqjb0",
        "outputId": "7580170c-5859-4fa6-b153-a7602d150d09"
      },
      "source": [
        "# Check similarity\n",
        "w2v.n_similarity('the', 'The')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.87517774"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaBllPVT-Vom"
      },
      "source": [
        "# Create dict to convert words (and therefore sentences) to integers and list of integers\n",
        "w2v_dict = {token: index for (index,token) in enumerate(w2v.vocab)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79b329wm-Vom",
        "outputId": "fe780626-c596-4364-926c-9a6c0295dcf4"
      },
      "source": [
        "# as follows\n",
        "w2v_dict['UNK']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_7idY5k-Vom",
        "outputId": "32e3ef4f-4917-41b5-be1b-db3b3dfce05e"
      },
      "source": [
        "# Coding every word into 300 dimensional vector\n",
        "vector = w2v['me']\n",
        "print(vector.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPa21y8L-Vom"
      },
      "source": [
        "## **Preprocessing of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXlXI2yP-Vom"
      },
      "source": [
        "# Checking for unknown words which have no embedding\n",
        "def put_unk(sentences, w2v_dict): #should we erase them?\n",
        "    \"\"\"\n",
        "    Returns the sentences as before, but now replacing words without embedding as unknowns\n",
        "    Also returns a list of words that have no embeddings (which we replace in our corpus)\n",
        "    \"\"\"\n",
        "    unknown=[]\n",
        "    new_sentences=[]\n",
        "    for sentence in sentences:\n",
        "        token=[]\n",
        "        for word in sentence:\n",
        "            if not (word in w2v_dict):\n",
        "                token.append('UNK')\n",
        "                if not(word in unknown):\n",
        "                    unknown.append(word)  \n",
        "            else:\n",
        "                token.append(word)\n",
        "        \n",
        "        new_sentences.append(token)     \n",
        "    return  new_sentences, unknown\n",
        "\n",
        "# Sentences to embeddings\n",
        "def embed(sentences,w2v_dict):\n",
        "    new_sentences=[]\n",
        "    for sentence in sentences:\n",
        "        token=[] # embedding for a sentence, would be sentence_length x 300\n",
        "        for word in sentence:\n",
        "            token.append(w2v_dict[word].reshape(1,-1)) # embed every word in the sentence to a 300 x 1 vector\n",
        "        token_np = np.concatenate(token, axis=0) # convert the sentence embedding to an np array\n",
        "        new_sentences.append(token_np) # embedding for all sentences in the corpus, would be a list of length nData. Each element would be the sentence embedding\n",
        "    return  new_sentences\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmpc17DO-Vom"
      },
      "source": [
        "# Checking for unknow words which have no embedding, and putting UNK istead of that word\n",
        "sentences_train, unkt = put_unk(sentences_train,w2v)\n",
        "sentences_val, unkv = put_unk(sentences_val,w2v)\n",
        "sentences_test, unkte = put_unk(sentences_test,w2v)\n",
        "sentences_add_test, unkadte = put_unk(sentences_add_test, w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J4IgU8mZ_-6",
        "outputId": "1e2bdabe-9a7d-454a-81a4-8f8d6105ce81"
      },
      "source": [
        "# Check if words in the additional sentences are marked correctly as unknowns\n",
        "sentences_add_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'bitter',\n",
              "  'disappointment',\n",
              "  'UNK',\n",
              "  'awful',\n",
              "  'ending',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'brilliant',\n",
              "  'trilogy'],\n",
              " ['The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'Rises',\n",
              "  'was',\n",
              "  'UNK',\n",
              "  'perfect',\n",
              "  'sequel',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'masterpiece'],\n",
              " ['Christopher',\n",
              "  'Nolans',\n",
              "  'second',\n",
              "  'bundle',\n",
              "  'UNK',\n",
              "  'joy',\n",
              "  'The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'exceeded',\n",
              "  'all',\n",
              "  'UNK',\n",
              "  'my',\n",
              "  'expectations'],\n",
              " ['A',\n",
              "  'lame',\n",
              "  'spy',\n",
              "  'spoof',\n",
              "  'with',\n",
              "  'no',\n",
              "  'real',\n",
              "  'laughs',\n",
              "  'or',\n",
              "  'surprises'],\n",
              " ['Not', 'The', 'Worst', 'Brit', 'Comedy', 'You', 'Will', 'See']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58B_GgCB-Vom"
      },
      "source": [
        "# Remove all but 1 unknowns in sentences\n",
        "def remove_all_but_one_unk(sentences,labels):\n",
        "    # Input is an array of sentences\n",
        "    sentences2 = deepcopy(sentences)\n",
        "    for j in range(len(sentences)):\n",
        "        unk_count = 0\n",
        "        for i in range(len(sentences[j])):\n",
        "            word = sentences[j][i]\n",
        "            if word == 'UNK':\n",
        "                unk_count += 1\n",
        "                if unk_count >= 2 and len(sentences2[j]) > 1:\n",
        "                    sentences2[j].remove('UNK')\n",
        "    \n",
        "    return sentences2, labels # not removing sentences, so labels should still correspond\n",
        "\n",
        "# Remove stopwords as defined below\n",
        "def remove_stopwords(sentences, words_to_remove):\n",
        "    sentences2 = deepcopy(sentences)\n",
        "    for i in range(len(sentences)):\n",
        "        for word in sentences[i]:\n",
        "            if word in words_to_remove and len(sentences2[i]) > 1:\n",
        "                sentences2[i].remove(word)\n",
        "                \n",
        "    return sentences2\n",
        "\n",
        "# Removing words that only have one letters\n",
        "def remove_one_letter_words(sentences,labels):\n",
        "    sentences2 = deepcopy(sentences)\n",
        "    for i in range(len(sentences)):\n",
        "        for word in sentences[i]:\n",
        "            if len(word)==1 and len(sentences2[i]) > 1:\n",
        "                sentences2[i].remove(word)\n",
        "                \n",
        "    return sentences2, labels # Again, not removing any sentences, so labels should still correspond\n",
        "\n",
        "def embed_val_test(sentences, train_dict, train_emb, w2v_emb): # use this to embed val or test.\n",
        "    emb_sentences = []\n",
        "    for sentence in sentences:\n",
        "        emb_sentence = []\n",
        "        for word in sentence:\n",
        "            if word not in list(train_dict.keys()):\n",
        "                emb_sentence.append(w2v_emb[word])\n",
        "            else:\n",
        "                idx = train_dict[word]\n",
        "                emb_sentence.append(train_emb[idx])\n",
        "        emb_sentences.append(emb_sentence)\n",
        "        \n",
        "    return emb_sentences\n",
        "\n",
        "\n",
        "def pad_val_test(emb_sentences, labels, edges):\n",
        "    # produce categories of padded sentences, along with their labels, and masks, and original lengths\n",
        "    numcat = len(edges) - 1\n",
        "    ori_len = []; labels_padded = []; emb_padded = []; mask_padded = []\n",
        "    for i in range(num_cat):\n",
        "        leng = []; out = []; cat = []; mask_collect = []\n",
        "        for j in range(len(emb_sentences)):\n",
        "            seq_l = len(emb_sentences[j])\n",
        "            pad = np.zeros((edges[i+1],300))\n",
        "            mask = np.zeros((edges[i+1]))\n",
        "            if seq_l > edges[i] and seq_l <= edges[i+1]:\n",
        "                pad[0:seq_l,:] = emb_sentences[j]\n",
        "                mask[0:seq_l] = 1\n",
        "                mask_collect.append(mask)\n",
        "                leng.append(seq_l)\n",
        "                cat.append(pad)\n",
        "                out.append(labels[j])\n",
        "                \n",
        "        emb_padded.append(np.array(cat))\n",
        "        mask_padded.append(np.array(mask_collect).astype(np.int64))\n",
        "        ori_len.append(leng)\n",
        "        labels_padded.append(out)\n",
        "    \n",
        "    return ori_len, labels_padded, emb_padded, mask_padded\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnuHh6rj-Vom"
      },
      "source": [
        " **Remove all but first UNK from the sentences from train/val/test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4p3SHHN-Vom"
      },
      "source": [
        "sentences_train2, labels_train2= remove_all_but_one_unk(sentences_train,labels_train)\n",
        "sentences_val2, labels_val2 = remove_all_but_one_unk(sentences_val, labels_val)\n",
        "sentences_test2, labels_test2 = remove_all_but_one_unk(sentences_test, labels_test)\n",
        "sentences_add_test2, labels_add_test2 = remove_all_but_one_unk(sentences_add_test, labels_add_test)\n",
        "# Observe that after removing all but one UNK, we are left with sentences that have only 1 UNK. This indicates that we have sentences\n",
        "# that are fully UNK. We remove them from our corpus (all train, val, and test) because intuitively, even if our model is to predict these\n",
        "# the results won't be accurate at all, as 2 different sentences (even without same length for q4) with all UNK would yield the same prediction\n",
        "# even though they have different labels. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiJFBL93cBym",
        "outputId": "89a8f0cf-ddd7-482f-afef-9b60bbcb81f5"
      },
      "source": [
        "# Check if we do not remove any sentences\n",
        "print(len(sentences_train2), len(sentences_train))\n",
        "print(len(sentences_val2), len(sentences_val))\n",
        "print(len(sentences_test2), len(sentences_test))\n",
        "print(len(sentences_add_test2), len(sentences_add_test))\n",
        "\n",
        "# Check also that we do not have empty sentences\n",
        "total_1wordsent = 0\n",
        "for sentence in sentences_train2:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_val2:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_test2:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_add_test2:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "print(total_1wordsent)\n",
        "# No empty sentences, and relatively little amount of 1-word sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544 8544\n",
            "1101 1101\n",
            "2210 2210\n",
            "5 5\n",
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWqs5QnBZW76",
        "outputId": "acdd3bc4-a55e-45de-a9e3-e471c399661c"
      },
      "source": [
        "# Check if pre-processing done so far doesn't mess up the sentences\n",
        "sentences_add_test2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'bitter',\n",
              "  'disappointment',\n",
              "  'awful',\n",
              "  'ending',\n",
              "  'UNK',\n",
              "  'brilliant',\n",
              "  'trilogy'],\n",
              " ['The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'Rises',\n",
              "  'was',\n",
              "  'perfect',\n",
              "  'sequel',\n",
              "  'UNK',\n",
              "  'masterpiece'],\n",
              " ['Christopher',\n",
              "  'Nolans',\n",
              "  'second',\n",
              "  'bundle',\n",
              "  'joy',\n",
              "  'The',\n",
              "  'Dark',\n",
              "  'Knight',\n",
              "  'exceeded',\n",
              "  'all',\n",
              "  'UNK',\n",
              "  'my',\n",
              "  'expectations'],\n",
              " ['A',\n",
              "  'lame',\n",
              "  'spy',\n",
              "  'spoof',\n",
              "  'with',\n",
              "  'no',\n",
              "  'real',\n",
              "  'laughs',\n",
              "  'or',\n",
              "  'surprises'],\n",
              " ['Not', 'The', 'Worst', 'Brit', 'Comedy', 'You', 'Will', 'See']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QegqDQnf-Von"
      },
      "source": [
        "# Deleting one letter words\n",
        "sentences_train3, labels_train3=remove_one_letter_words(sentences_train2,labels_train2)\n",
        "sentences_val3, labels_val3 =remove_one_letter_words(sentences_val2,labels_val2)\n",
        "sentences_test3, labels_test3 =remove_one_letter_words(sentences_test2, labels_test2)\n",
        "sentences_add_test3, labels_add_test3 = remove_one_letter_words(sentences_add_test2,labels_add_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFbNPDk2dKIm",
        "outputId": "6785dc7b-8cfd-48a1-9a1a-9378b936ae5f"
      },
      "source": [
        "# Check if we do not remove any sentences\n",
        "print(len(sentences_train3), len(sentences_train2))\n",
        "print(len(sentences_val3), len(sentences_val2))\n",
        "print(len(sentences_test3), len(sentences_test2))\n",
        "print(len(sentences_add_test3), len(sentences_add_test2))\n",
        "\n",
        "# Check also that we do not have empty sentences\n",
        "total_1wordsent = 0\n",
        "for sentence in sentences_train3:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_val3:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_test3:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_add_test3:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "print(total_1wordsent)\n",
        "# No empty sentences, and relatively little amount of 1-word sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544 8544\n",
            "1101 1101\n",
            "2210 2210\n",
            "5 5\n",
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKxInAtv-Von"
      },
      "source": [
        "## **Get list of words occuring in the datasets**\n",
        "The given word2vec embeddings has a size of 3 million words. This is too big to fit inside a tensor and therefore can't be declared as a variable to be trained in Tensorflow. To circumvent this, we only take words that occur in the datasets and operate on embeddings of these words for the rest of the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBmo9wyP-Von",
        "outputId": "c6bcd1c0-08ff-4c21-fe4f-097eb15d88ab"
      },
      "source": [
        "# Create vocab based on training sentences only\n",
        "word_list = []; unfiltered_word_list = []; word_count = []\n",
        "for sentences in sentences_train3:\n",
        "    for word in sentences:\n",
        "        unfiltered_word_list.append(word)\n",
        "        if word not in word_list:\n",
        "            word_list.append(word)\n",
        "            word_count.append(1)\n",
        "        else:\n",
        "            k = word_list.index(word)\n",
        "            word_count[k] += 1\n",
        "\n",
        "# Create a separate vocabulary for val and train TO READ AFTER\n",
        "word_list_val = []; unfiltered_word_list_val = []\n",
        "for sentences in sentences_val3:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_val.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val):\n",
        "             word_list_val.append(word) # contains words that are exclusively in val or val and test\n",
        "                \n",
        "word_list_test = []; unfiltered_word_list_test = []           \n",
        "for sentences in sentences_test3:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test):\n",
        "             word_list_test.append(word) # words that are exclusively in test\n",
        "\n",
        "word_list_add_test = []; unfiltered_word_list_add_test = []           \n",
        "for sentences in sentences_add_test3:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_add_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test) and (word not in word_list_add_test):\n",
        "             word_list_add_test.append(word) # words that are exclusively in add_test\n",
        "                \n",
        "print(len(word_list))\n",
        "print(len(word_list_val))\n",
        "print(len(word_list_test))\n",
        "print(len(word_list_add_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16451\n",
            "990\n",
            "1833\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeWyD2or-Von",
        "outputId": "9df98abb-5dfe-4352-f0b0-6142a38f360d"
      },
      "source": [
        "# Check if there are multiple (up,Up)  words\n",
        "double_words=[]\n",
        "for word in (word_list):\n",
        "    if not (word.lower() in double_words):\n",
        "        for word1 in (word_list):\n",
        "            if word.lower()==word1.lower() and not(word[0]==word1[0]):\n",
        "                double_words.append(word.lower())\n",
        "                break\n",
        "\n",
        "print(len(double_words))\n",
        "# Just too many unnecessary double words! none of it is a name! in any set (no Rock rock)\n",
        "# the first word is a name just a few times => lower case beggining of the sentence!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6KbK0sl-Von"
      },
      "source": [
        "**Lowercase the beggining of sentence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxPZD8hH-Von"
      },
      "source": [
        "def lowercase(sentences,w2v): # lowercase the first letter in every sentence IF the first word is not UNK AND the lowercased version\n",
        "    # of this word is in w2v. The idea is that lowercased and uppercased words (if not names) should mean the same and therefore\n",
        "    # have same embedding.\n",
        "    sentences1=[]\n",
        "    for sentence in sentences:\n",
        "        if not(sentence[0]=='UNK') and sentence[0].lower() in w2v: #dont lowercase if its not in w2v\n",
        "            new_sentence=[]\n",
        "            new_sentence.append(sentence[0].lower())\n",
        "            new_sentence+=sentence[1:]\n",
        "            sentences1.append(new_sentence)\n",
        "        else:\n",
        "            sentences1.append(sentence)\n",
        "    return sentences1\n",
        "\n",
        "sentences_train4 = lowercase(sentences_train3,w2v)\n",
        "sentences_val4 = lowercase(sentences_val3,w2v)\n",
        "sentences_test4 = lowercase(sentences_test3,w2v)\n",
        "sentences_add_test4 = lowercase(sentences_add_test3, w2v)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfCL0oZg-Von",
        "outputId": "80a3e728-371e-46cf-9a09-029d75612770"
      },
      "source": [
        "# Make vocabularies again, with the now lowercased words. Word list count will decrease\n",
        "word_list = []; unfiltered_word_list = []; word_count = []\n",
        "for sentences in sentences_train4:\n",
        "    for word in sentences:\n",
        "        unfiltered_word_list.append(word)\n",
        "        if word not in word_list:\n",
        "            word_list.append(word)\n",
        "            word_count.append(1)\n",
        "        else:\n",
        "            k = word_list.index(word)\n",
        "            word_count[k] += 1\n",
        "\n",
        "# Create a separate vocabulary for val and train TO READ AFTER only with words not in the trainable dictionary\n",
        "word_list_val = []; unfiltered_word_list_val = []\n",
        "for sentences in sentences_val4:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_val.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val):\n",
        "             word_list_val.append(word)\n",
        "                \n",
        "word_list_test = []; unfiltered_word_list_test = []           \n",
        "for sentences in sentences_test4:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test):\n",
        "             word_list_test.append(word)\n",
        "\n",
        "word_list_add_test = []; unfiltered_word_list_add_test = []           \n",
        "for sentences in sentences_add_test4:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_add_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test) and (word not in word_list_add_test):\n",
        "             word_list_add_test.append(word) # words that are exclusively in add_test\n",
        "             \n",
        "print(len(word_list))\n",
        "print(len(word_list_val))\n",
        "print(len(word_list_test))\n",
        "print(len(word_list_add_test))\n",
        "# less words better for memory, didnt use any info!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15967\n",
            "943\n",
            "1730\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXkiag1m-Von"
      },
      "source": [
        "**Removing Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GisIMcGf-Voo",
        "outputId": "5c988148-9a27-4a58-b460-a3d5238b442a"
      },
      "source": [
        "# How many of each words\n",
        "word_list_count = []\n",
        "for i in range(len(word_list)):\n",
        "    word_list_count.append((word_list[i],word_count[i]))\n",
        "\n",
        "sorted_word_list = sorted(word_list_count, key = itemgetter(1), reverse = True)\n",
        "print(sorted_word_list[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 7063), ('UNK', 7053), ('is', 2547), ('it', 2399), ('that', 1941), ('in', 1880), ('as', 1296), ('but', 1167), ('film', 1154), ('with', 1132), ('for', 1018), ('this', 987), ('an', 971), ('movie', 960), ('its', 944), ('you', 847), ('nt', 682), ('be', 669), ('on', 646), ('not', 593), ('by', 551), ('one', 549), ('has', 528), ('about', 518), ('more', 517), ('are', 512), ('at', 500), ('like', 495), ('from', 480), ('than', 474)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNJRzex3-Voo"
      },
      "source": [
        "words_to_remove = ['the', 'is','it', 'that', 'in', 'as', 'with', 'for', 'this', 'an', 'its', 'you', 'be', 'on',\n",
        "                    'are', 'has', 'at']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoYa_psc-Voo",
        "outputId": "67050305-3051-4345-8ee4-e0cc0ea375da"
      },
      "source": [
        "#Remove stopwords from train\n",
        "sentences_train5 = remove_stopwords(sentences_train4, words_to_remove)\n",
        "# Make a new vocabulary again for train\n",
        "word_list = []; unfiltered_word_list = []; word_count = []\n",
        "for sentences in sentences_train5:\n",
        "    for word in sentences:\n",
        "        unfiltered_word_list.append(word)\n",
        "        if word not in word_list:\n",
        "            word_list.append(word)\n",
        "            word_count.append(1)\n",
        "        else:\n",
        "            k = word_list.index(word)\n",
        "            word_count[k] += 1\n",
        "\n",
        "#Count the most common words again\n",
        "word_list_count = []\n",
        "for i in range(len(word_list)):\n",
        "    word_list_count.append((word_list[i],word_count[i]))\n",
        "\n",
        "sorted_word_list = sorted(word_list_count, key = itemgetter(1), reverse = True)\n",
        "print(sorted_word_list[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('UNK', 7053), ('but', 1167), ('film', 1154), ('movie', 960), ('nt', 682), ('not', 593), ('by', 551), ('one', 549), ('about', 518), ('more', 517), ('like', 495), ('from', 480), ('than', 474), ('have', 455), ('all', 444), ('his', 433), ('so', 389), ('if', 386), ('or', 374), ('LRB', 356), ('RRB', 356), ('story', 343), ('what', 327), ('there', 318), ('too', 317), ('who', 314), ('does', 305), ('into', 300), ('out', 294), ('just', 293)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STweMiPR-Voo"
      },
      "source": [
        "# Remove stopwords from test and val\n",
        "sentences_val5 = remove_stopwords(sentences_val4, words_to_remove)\n",
        "sentences_test5 = remove_stopwords(sentences_test4, words_to_remove)\n",
        "sentences_add_test5 = remove_stopwords(sentences_add_test4, words_to_remove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chUIzBNFSoFg",
        "outputId": "8efae15d-6253-4c7d-c051-d45923ca2e89"
      },
      "source": [
        "# Check if we do not remove any sentences\n",
        "# Check if we do not remove any sentences\n",
        "print(len(sentences_train5), len(sentences_train4))\n",
        "print(len(sentences_val5), len(sentences_val4))\n",
        "print(len(sentences_test5), len(sentences_test4))\n",
        "print(len(sentences_add_test5), len(sentences_add_test4))\n",
        "\n",
        "# Check also that we do not have empty sentences\n",
        "total_1wordsent = 0\n",
        "for sentence in sentences_train5:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_val5:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_test5:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "for sentence in sentences_add_test5:\n",
        "    if len(sentence) <= 1:\n",
        "      if len(sentence) == 1:\n",
        "        total_1wordsent += 1\n",
        "      else:\n",
        "        print(\"Empty Sentence\")\n",
        "\n",
        "print(total_1wordsent)\n",
        "# No empty sentences, and relatively little amount of 1-word sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544 8544\n",
            "1101 1101\n",
            "2210 2210\n",
            "5 5\n",
            "84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZQi30Py-Voo",
        "outputId": "d3835d7b-13b0-4be2-813a-28c19fa9b8d8"
      },
      "source": [
        "# Again make a new vocabulary without the stopwords for val and test\n",
        "word_list_val = []; unfiltered_word_list_val = []\n",
        "for sentences in sentences_val5:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_val.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val):\n",
        "             word_list_val.append(word)\n",
        "                \n",
        "word_list_test = []; unfiltered_word_list_test = []           \n",
        "for sentences in sentences_test5:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test):\n",
        "             word_list_test.append(word)\n",
        "\n",
        "word_list_add_test = []; unfiltered_word_list_add_test = []           \n",
        "for sentences in sentences_add_test5:\n",
        "     for word in sentences:\n",
        "        unfiltered_word_list_add_test.append(word)\n",
        "        if (word not in word_list) and (word not in word_list_val) and (word not in word_list_test) and (word not in word_list_add_test):\n",
        "             word_list_add_test.append(word) # words that are exclusively in add_test\n",
        "             \n",
        "print(len(word_list))\n",
        "print(len(word_list_val))\n",
        "print(len(word_list_test))\n",
        "print(len(word_list_add_test))\n",
        "# less words better for memory, didnt use any info!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15950\n",
            "944\n",
            "1730\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB7punCo-Voo"
      },
      "source": [
        "## **Create the Miniaturized Embedding Matrix to be Trained**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvfX25Yx-Voo",
        "outputId": "890e11f2-aabe-4874-a607-3df4d92dbc41"
      },
      "source": [
        "# Create embedding matrix\n",
        "embedding_mat = []\n",
        "for word in word_list:\n",
        "    embedding_mat.append(w2v[word])\n",
        "    \n",
        "embedding_mat=np.asarray(embedding_mat)\n",
        "print(embedding_mat.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15950, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cb5AiyBfiQP",
        "outputId": "701a7f0a-4f3d-4b3d-b22d-23905025297c"
      },
      "source": [
        "len(word_list_test + word_list_add_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1733"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_SUEQ4J-Voo",
        "outputId": "215f5998-f4cd-4dbd-aa5b-22c8488d6a73"
      },
      "source": [
        "# Do the same for val and train vocab\n",
        "# Technically, our embedding matrix is the whole given w2v matrix. However, most of the words in this matrix are not present\n",
        "# in train, test and val. Therefore, we only take those that are in train, val and test. Furthermore, only the embeddings of words in training will be trained, \n",
        "# therefore we can exclude words not in train, and feed in only the embedding matrix of words in train during training, for memory efficiency. But during prediction, we then\n",
        "# concatenate this trained embedding matrix, with the 'remaining words' i.e. exclusively in val and test.\n",
        "\n",
        "# Essentially, what we are trying to do is to feed in the smallest possible 'portion' of the given w2v embedding matrix required\n",
        "# for the operation we are doing.\n",
        "embedding_mat_val = []\n",
        "for word in word_list_val:\n",
        "    embedding_mat_val.append(w2v[word])\n",
        "embedding_mat_val=np.asarray(embedding_mat_val)\n",
        "print(embedding_mat_val.shape)\n",
        "\n",
        "embedding_mat_test = []\n",
        "for word in word_list_test + word_list_add_test:\n",
        "    embedding_mat_test.append(w2v[word])\n",
        "embedding_mat_test=np.asarray(embedding_mat_test)\n",
        "print(embedding_mat_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(944, 300)\n",
            "(1733, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3p1HCkJ-Voo"
      },
      "source": [
        "# Also create a dict to convert words to indexes of first matrix and (matrix1+matrix2) and (matrix1+matrix2+matrix3)\n",
        "integer_dict_train = {word: index for (index,word) in enumerate(word_list)}\n",
        "integer_dict_val = {word: index+embedding_mat.shape[0] for (index,word) in enumerate(word_list_val)} #because we will concatenate it\n",
        "integer_dict_test = {word: index+embedding_mat.shape[0]+embedding_mat_val.shape[0] for (index,word) in enumerate(word_list_test + word_list_add_test)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nimm2VmX-Voo",
        "outputId": "2f06fad0-2a2b-46cd-d1a1-462fec65218c"
      },
      "source": [
        "# Check if lookup will extract the correct embedding\n",
        "err = 0\n",
        "for word in integer_dict_train.keys():\n",
        "    index = integer_dict_train[word]\n",
        "    err += np.sum(w2v[word] - embedding_mat[index])\n",
        "    \n",
        "print(err)  \n",
        "\n",
        "err = 0\n",
        "for word in integer_dict_val.keys():\n",
        "    index = integer_dict_val[word]-embedding_mat.shape[0]\n",
        "    err += np.sum(w2v[word] - embedding_mat_val[index])\n",
        "    \n",
        "print(err)  \n",
        "\n",
        "err = 0\n",
        "for word in integer_dict_test.keys():\n",
        "    index = integer_dict_test[word]-(embedding_mat.shape[0]+embedding_mat_val.shape[0])\n",
        "    err += np.sum(w2v[word] - embedding_mat_test[index])\n",
        "    \n",
        "print(err)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiYbjvMw-Voo"
      },
      "source": [
        "Embedding matrix is matrix of embeddings for all words in training sentences, integer_dict[word] gives the corresponding integer (row position) of that word in the embedding matrix. Integer_dict_val[word] will give the index of the word in matrix that we will get when we concatenate embedding matrix for train and val. Similarly for test with 3 matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKmZhj70-Voo",
        "outputId": "08e7e66a-2c13-4e48-8d89-b4a5149b5bb6"
      },
      "source": [
        "# Find the maximum length of sentences among all training, val, and test\n",
        "max_length = 0; train_sentence_lengths = []; val_sentence_lengths = []; test_sentence_lengths = []; add_test_sentence_lengths = []\n",
        "for sentence in sentences_train5:\n",
        "    train_sentence_lengths.append(len(sentence))\n",
        "    if max_length < len(sentence):\n",
        "        max_length = len(sentence)\n",
        "        \n",
        "for sentence in sentences_val5:\n",
        "    val_sentence_lengths.append(len(sentence))\n",
        "    if max_length < len(sentence):\n",
        "        max_length = len(sentence)\n",
        "        \n",
        "for sentence in sentences_test5:\n",
        "    test_sentence_lengths.append(len(sentence))\n",
        "    if max_length < len(sentence):\n",
        "        max_length = len(sentence)\n",
        "\n",
        "for sentence in sentences_add_test5:\n",
        "    add_test_sentence_lengths.append(len(sentence))\n",
        "    if max_length < len(sentence):\n",
        "        max_length = len(sentence)\n",
        "        \n",
        "print(\"Maximum sentence length is\", max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sentence length is 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar6CdeWD-Vop"
      },
      "source": [
        "## **Get distribution of sentence lengths**\n",
        "\n",
        "To perform batch operations in Tensorflow, we need to have all inputs in a certain batch to be of same shape. However, sentences all have different lengths and therefore we first need to pad sentences in every batch to a common length. Details on what we do to achieve this can be found in the project report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "X2xli1fK-Vop",
        "outputId": "f31789f2-a08a-4399-f6ad-48824909dc02"
      },
      "source": [
        "# Histogram for all train, val, and test\n",
        "sent_lengths = np.concatenate((train_sentence_lengths, val_sentence_lengths, test_sentence_lengths, add_test_sentence_lengths))\n",
        "h= plt.hist(sent_lengths, bins=5)\n",
        "print(h[1])\n",
        "print(h[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.   7.8 14.6 21.4 28.2 35. ]\n",
            "[2908. 4943. 3118.  805.   86.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "-_5Klb9XhgdO",
        "outputId": "b52317c4-2f35-4d2a-f98a-c5bc978dacb5"
      },
      "source": [
        "# For training sentences only\n",
        "h= plt.hist(train_sentence_lengths, bins=[1, 8, 15, 21, 28, 35])\n",
        "print(h[1])\n",
        "print(h[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  8 15 21 28 35]\n",
            "[2119. 3540. 2051.  733.  101.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "azsss60e-Vop",
        "outputId": "da6ad885-3fa0-4305-91aa-7443bb645e92"
      },
      "source": [
        "# For all sentences\n",
        "h= plt.hist(sent_lengths, bins=[1, 8, 15, 21, 28, 35])\n",
        "print(h[1])\n",
        "print(h[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  8 15 21 28 35]\n",
            "[2908. 4943. 2860. 1030.  119.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQXklEQVR4nO3cf6xfdX3H8efLFtSosyB3DWm7XZxNDC4TSQcYjXEQocKysgQJZpuNIem2YKLJlln8B0VZYMnEmUyWbnRWo2Ljj9GIGTaAcftD4CLITxlXhNCm0GoBNUYW8L0/vp+6r/Xe3nvp7f3er5/nI7n5nvM+n+/5vs8JfX0P55zvSVUhSerDS0bdgCRp6Rj6ktQRQ1+SOmLoS1JHDH1J6sjKUTdwJCeddFJNTk6Oug1JGit33XXXD6tqYqZlyzr0JycnmZqaGnUbkjRWkjw+2zJP70hSRwx9SeqIoS9JHZlX6Cd5LMl9Se5JMtVqJybZneSR9npCqyfJJ5NMJ7k3yelD69ncxj+SZPOx2SRJ0mwWcqT/R1V1WlVtaPNbgVuqaj1wS5sHeCewvv1tAa6DwZcEcAVwJnAGcMWhLwpJ0tI4mtM7m4AdbXoHcOFQ/TM18G1gVZKTgfOA3VV1sKqeBnYDG4/i8yVJCzTf0C/gG0nuSrKl1VZX1b42/SSwuk2vAZ4Yeu+eVput/iuSbEkylWTqwIED82xPkjQf871P/61VtTfJbwO7k3xveGFVVZJFeUZzVW0DtgFs2LDB5z5L0iKa15F+Ve1tr/uBrzI4J/9UO21De93fhu8F1g29fW2rzVaXJC2ROY/0k7wCeElV/aRNnwtcCewCNgNXt9cb21t2Ae9LcgODi7bPVtW+JDcDfz908fZc4PJF3RoBMLn1plG3MDKPXX3BqFuQlrX5nN5ZDXw1yaHxn6+q/0xyJ7AzyaXA48DFbfzXgfOBaeBnwHsBqupgko8Cd7ZxV1bVwUXbEknSnOYM/ap6FHjjDPUfAefMUC/gslnWtR3YvvA2JUmLwV/kSlJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR+Yd+klWJLk7ydfa/ClJbk8yneSLSY5v9Ze2+em2fHJoHZe3+sNJzlvsjZEkHdlCjvTfDzw0NH8NcG1VvQ54Gri01S8Fnm71a9s4kpwKXAK8AdgIfCrJiqNrX5K0EPMK/SRrgQuAf2vzAc4GvtSG7AAubNOb2jxt+Tlt/Cbghqp6rqp+AEwDZyzGRkiS5me+R/qfAP4O+EWbfw3wTFU93+b3AGva9BrgCYC2/Nk2/pf1Gd7zS0m2JJlKMnXgwIEFbIokaS5zhn6SPwb2V9VdS9APVbWtqjZU1YaJiYml+EhJ6sbKeYx5C/AnSc4HXgb8FvBPwKokK9vR/Fpgbxu/F1gH7EmyEng18KOh+iHD75EkLYE5j/Sr6vKqWltVkwwuxN5aVX8G3AZc1IZtBm5s07vaPG35rVVVrX5Ju7vnFGA9cMeibYkkaU7zOdKfzQeBG5J8DLgbuL7Vrwc+m2QaOMjgi4KqeiDJTuBB4Hngsqp64Sg+X5K0QAsK/ar6JvDNNv0oM9x9U1U/B941y/uvAq5aaJOSpMXhL3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjc4Z+kpcluSPJd5M8kOQjrX5KktuTTCf5YpLjW/2lbX66LZ8cWtflrf5wkvOO1UZJkmY2nyP954Czq+qNwGnAxiRnAdcA11bV64CngUvb+EuBp1v92jaOJKcClwBvADYCn0qyYjE3RpJ0ZHOGfg38tM0e1/4KOBv4UqvvAC5s05vaPG35OUnS6jdU1XNV9QNgGjhjUbZCkjQv8zqnn2RFknuA/cBu4PvAM1X1fBuyB1jTptcATwC05c8Crxmuz/Ce4c/akmQqydSBAwcWvkWSpFnNK/Sr6oWqOg1Yy+Do/PXHqqGq2lZVG6pqw8TExLH6GEnq0oLu3qmqZ4DbgDcDq5KsbIvWAnvb9F5gHUBb/mrgR8P1Gd4jSVoC87l7ZyLJqjb9cuAdwEMMwv+iNmwzcGOb3tXmactvrapq9Uva3T2nAOuBOxZrQyRJc1s59xBOBna0O21eAuysqq8leRC4IcnHgLuB69v464HPJpkGDjK4Y4eqeiDJTuBB4Hngsqp6YXE351dNbr3pWK5eksbOnKFfVfcCb5qh/igz3H1TVT8H3jXLuq4Crlp4m5KkxTCfI31pbPym/t/dY1dfMOoW9BvCxzBIUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjJn6CdZl+S2JA8meSDJ+1v9xCS7kzzSXk9o9ST5ZJLpJPcmOX1oXZvb+EeSbD52myVJmsl8jvSfB/6mqk4FzgIuS3IqsBW4parWA7e0eYB3Auvb3xbgOhh8SQBXAGcCZwBXHPqikCQtjTlDv6r2VdV32vRPgIeANcAmYEcbtgO4sE1vAj5TA98GViU5GTgP2F1VB6vqaWA3sHFRt0aSdEQLOqefZBJ4E3A7sLqq9rVFTwKr2/Qa4Imht+1ptdnqh3/GliRTSaYOHDiwkPYkSXOYd+gneSXwZeADVfXj4WVVVUAtRkNVta2qNlTVhomJicVYpSSpmVfoJzmOQeB/rqq+0spPtdM2tNf9rb4XWDf09rWtNltdkrRE5nP3ToDrgYeq6uNDi3YBh+7A2QzcOFR/T7uL5yzg2XYa6Gbg3CQntAu457aaJGmJrJzHmLcAfwHcl+SeVvsQcDWwM8mlwOPAxW3Z14HzgWngZ8B7AarqYJKPAne2cVdW1cFF2QpJ0rzMGfpV9d9AZll8zgzjC7hslnVtB7YvpEFJ0uLxF7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSROUM/yfYk+5PcP1Q7McnuJI+01xNaPUk+mWQ6yb1JTh96z+Y2/pEkm4/N5kiSjmQ+R/qfBjYeVtsK3FJV64Fb2jzAO4H17W8LcB0MviSAK4AzgTOAKw59UUiSls7KuQZU1beSTB5W3gS8vU3vAL4JfLDVP1NVBXw7yaokJ7exu6vqIECS3Qy+SL5w1FsgdWBy602jbmEkHrv6glG38BvnxZ7TX11V+9r0k8DqNr0GeGJo3J5Wm63+a5JsSTKVZOrAgQMvsj1J0kyO+kJuO6qvRejl0Pq2VdWGqtowMTGxWKuVJPHiQ/+pdtqG9rq/1fcC64bGrW212eqSpCX0YkN/F3DoDpzNwI1D9fe0u3jOAp5tp4FuBs5NckK7gHtuq0mSltCcF3KTfIHBhdiTkuxhcBfO1cDOJJcCjwMXt+FfB84HpoGfAe8FqKqDST4K3NnGXXnooq4kaenM5+6dd8+y6JwZxhZw2Szr2Q5sX1B3kqRF5S9yJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyMpRNyBJs5ncetOoWxiZx66+4Jis1yN9SerIkod+ko1JHk4ynWTrUn++JPVsSUM/yQrgn4F3AqcC705y6lL2IEk9W+oj/TOA6ap6tKr+F7gB2LTEPUhSt5b6Qu4a4Imh+T3AmcMDkmwBtrTZnyZ5eJZ1nQT8cNE7PHbGrV+w56Uybj2PW78whj3nmqPq+XdnW7Ds7t6pqm3AtrnGJZmqqg1L0NKiGLd+wZ6Xyrj1PG79gj0PW+rTO3uBdUPza1tNkrQEljr07wTWJzklyfHAJcCuJe5Bkrq1pKd3qur5JO8DbgZWANur6oEXubo5TwEtM+PWL9jzUhm3nsetX7DnX0pVHYv1SpKWIX+RK0kdMfQlqSNjF/rj+BiHJI8luS/JPUmmRt3PTJJsT7I/yf1DtROT7E7ySHs9YZQ9Hm6Wnj+cZG/b1/ckOX+UPQ5Lsi7JbUkeTPJAkve3+rLdz0foeTnv55cluSPJd1vPH2n1U5Lc3rLji+1mkpE7Qr+fTvKDoX182qJ8YFWNzR+Di7/fB14LHA98Fzh11H3No+/HgJNG3cccPb4NOB24f6j2D8DWNr0VuGbUfc6j5w8Dfzvq3mbp92Tg9Db9KuB/GDyOZNnu5yP0vJz3c4BXtunjgNuBs4CdwCWt/i/AX4+61zn6/TRw0WJ/3rgd6fsYh2Okqr4FHDysvAnY0aZ3ABcuaVNzmKXnZauq9lXVd9r0T4CHGPxKfdnu5yP0vGzVwE/b7HHtr4CzgS+1+rLZz0fo95gYt9Cf6TEOy/o/wKaAbyS5qz1mYlysrqp9bfpJYPUom1mA9yW5t53+WTanSoYlmQTexOCobiz282E9wzLez0lWJLkH2A/sZnCG4Jmqer4NWVbZcXi/VXVoH1/V9vG1SV66GJ81bqE/rt5aVaczeLroZUneNuqGFqoG/+85Dvf3Xgf8HnAasA/4x9G28+uSvBL4MvCBqvrx8LLlup9n6HlZ7+eqeqGqTmPwq/8zgNePuKUjOrzfJL8PXM6g7z8ETgQ+uBifNW6hP5aPcaiqve11P/BVBv8RjoOnkpwM0F73j7ifOVXVU+0f0C+Af2WZ7eskxzEIz89V1VdaeVnv55l6Xu77+ZCqega4DXgzsCrJoR+kLsvsGOp3Yzu1VlX1HPDvLNI+HrfQH7vHOCR5RZJXHZoGzgXuP/K7lo1dwOY2vRm4cYS9zMuh8Gz+lGW0r5MEuB54qKo+PrRo2e7n2Xpe5vt5IsmqNv1y4B0MrkXcBlzUhi2b/TxLv98bOhAIg+sPi7KPx+4Xue3WsE/w/49xuGrELR1RktcyOLqHwWMvPr8ce07yBeDtDB5B+xRwBfAfDO54+B3gceDiqlo2F05n6fntDE45FIO7pv5y6Hz5SCV5K/BfwH3AL1r5QwzOkS/L/XyEnt/N8t3Pf8DgQu0KBge2O6vqyvZv8QYGp0ruBv68HUWP1BH6vRWYYHB3zz3AXw1d8H3xnzduoS9JevHG7fSOJOkoGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI/8HspWTlQYguDkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLvxexwW-Vop"
      },
      "source": [
        "## **Padding and Encoding**\n",
        "After we get the sentence length distribution, we pad the sentences according to which bin they belong to (and we pad to the right-end of this bin). Then we encode each word into an integer index according to its row index in the embedding matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mAL9ja0-Vop"
      },
      "source": [
        "Note: integer_dict_val and integer_dict_test would convert the words in validation and test set according to the index in the concatenated matrix. We concatenate our training embedding matrix by the embeddings of additional words exclusively in val or val and test, and then finally followed by embeddings of words exclusively in test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_snK9qR-Vop"
      },
      "source": [
        "def conv_sent_to_int(sentences, integer_dict, integer_dict_val, integer_dict_test): # convert each word in sentences to its index for embedding matrices\n",
        "    int_sentences = []\n",
        "    for sentence in sentences:\n",
        "        int_sentence = []\n",
        "        for word in sentence:\n",
        "            if word in integer_dict:\n",
        "                int_idx = integer_dict[word]\n",
        "            elif word in integer_dict_val:\n",
        "                int_idx = integer_dict_val[word]\n",
        "            else:\n",
        "                int_idx = integer_dict_test[word]\n",
        "                \n",
        "            int_sentence.append(int_idx)\n",
        "        int_sentences.append(int_sentence)\n",
        "        \n",
        "    return int_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gWtceY2-Vop"
      },
      "source": [
        "# conv training sentences to integer indexes of the embedding matrix\n",
        "int_train_sent = conv_sent_to_int(sentences_train5, integer_dict_train, integer_dict_val, integer_dict_test)\n",
        "int_val_sent = conv_sent_to_int(sentences_val5, integer_dict_train, integer_dict_val, integer_dict_test)\n",
        "int_test_sent = conv_sent_to_int(sentences_test5, integer_dict_train, integer_dict_val, integer_dict_test)\n",
        "int_add_test_sent = conv_sent_to_int(sentences_add_test5, integer_dict_train, integer_dict_val, integer_dict_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rluy-J-j-Vop",
        "outputId": "d976a250-eeba-4481-8489-bcf17a0e4b54"
      },
      "source": [
        "# Quick check to ensure sentences are converted to integers correctly\n",
        "err = 0\n",
        "for sent in int_train_sent:\n",
        "    for int_word in sent:\n",
        "        word = list(integer_dict_train.keys())[int_word]\n",
        "        err += np.sum(embedding_mat[int_word] - w2v[word])\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsvLBIok-Vop",
        "outputId": "eded1bdd-6d54-4118-f11a-f9c7216efe0a"
      },
      "source": [
        "err = 0\n",
        "for sent in int_val_sent:\n",
        "    for int_word in sent:\n",
        "        if int_word >= embedding_mat.shape[0]:\n",
        "            word = list(integer_dict_val.keys())[int_word-embedding_mat.shape[0]]\n",
        "            err += np.sum(embedding_mat_val[int_word-embedding_mat.shape[0]] - w2v[word])\n",
        "        else:\n",
        "            word = list(integer_dict_train.keys())[int_word]\n",
        "            err += np.sum(embedding_mat[int_word] - w2v[word])\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrVncAKZ-Vop",
        "outputId": "ca968f76-1f95-45a4-d466-8d8d50728eea"
      },
      "source": [
        "err = 0\n",
        "for sent in int_test_sent:\n",
        "    for int_word in sent:\n",
        "        if int_word >= embedding_mat.shape[0] + embedding_mat_val.shape[0]:\n",
        "            word = list(integer_dict_test.keys())[int_word-embedding_mat.shape[0] - embedding_mat_val.shape[0]]\n",
        "            err += np.sum(embedding_mat_test[int_word - embedding_mat.shape[0] - embedding_mat_val.shape[0]] - w2v[word])\n",
        "        elif int_word >= embedding_mat.shape[0]:\n",
        "            word = list(integer_dict_val.keys())[int_word-embedding_mat.shape[0]]\n",
        "            err += np.sum(embedding_mat_val[int_word-embedding_mat.shape[0]] - w2v[word])\n",
        "        else:\n",
        "            word = list(integer_dict_train.keys())[int_word]\n",
        "            err += np.sum(embedding_mat[int_word] - w2v[word])\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG2Ky1B6izp8",
        "outputId": "b59a8263-60a8-4274-8a3f-8b01bfd9cf8e"
      },
      "source": [
        "err = 0\n",
        "for sent in int_add_test_sent:\n",
        "    for int_word in sent:\n",
        "        if int_word >= embedding_mat.shape[0] + embedding_mat_val.shape[0]:\n",
        "            word = list(integer_dict_test.keys())[int_word-embedding_mat.shape[0] - embedding_mat_val.shape[0]]\n",
        "            err += np.sum(embedding_mat_test[int_word - embedding_mat.shape[0] - embedding_mat_val.shape[0]] - w2v[word])\n",
        "        elif int_word >= embedding_mat.shape[0]:\n",
        "            word = list(integer_dict_val.keys())[int_word-embedding_mat.shape[0]]\n",
        "            err += np.sum(embedding_mat_val[int_word-embedding_mat.shape[0]] - w2v[word])\n",
        "        else:\n",
        "            word = list(integer_dict_train.keys())[int_word]\n",
        "            err += np.sum(embedding_mat[int_word] - w2v[word])\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odld1E8P-Vop"
      },
      "source": [
        "def pad_int(int_sentences, labels, edges):\n",
        "    # produce categories of padded int_sentences, along with their labels, and masks, and original lengths\n",
        "    numcat = len(edges) - 1\n",
        "    ori_len = []; labels_padded = []; int_padded = []; mask_padded = []\n",
        "    for i in range(numcat):\n",
        "        leng = []; out = []; cat = []; mask_collect = []\n",
        "        for j in range(len(int_sentences)):\n",
        "            seq_l = len(int_sentences[j])\n",
        "            pad = np.zeros((edges[i+1]))\n",
        "            mask = np.zeros((edges[i+1]))\n",
        "            if seq_l > edges[i] and seq_l <= edges[i+1]:\n",
        "                pad[0:seq_l] = int_sentences[j]\n",
        "                mask[0:seq_l] = 1\n",
        "                mask_collect.append(mask)\n",
        "                leng.append(seq_l)\n",
        "                cat.append(pad)\n",
        "                out.append(labels[j])\n",
        "                \n",
        "        int_padded.append(np.array(cat).astype(np.int32))\n",
        "        mask_padded.append(np.array(mask_collect).astype(np.int32))\n",
        "        ori_len.append(leng)\n",
        "        labels_padded.append(np.array(out).astype(np.int32))\n",
        "    \n",
        "    return ori_len, labels_padded, int_padded, mask_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq3odSjX-Vop"
      },
      "source": [
        "labels_train5=deepcopy(labels_train3)\n",
        "labels_val5=deepcopy(labels_val3)\n",
        "labels_test5=deepcopy(labels_test3)\n",
        "labels_add_test5 = deepcopy(labels_add_test3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wsr2kd1WloA",
        "outputId": "ffe8446a-98f8-4025-8f20-ca5d73f8a5fc"
      },
      "source": [
        "print(len(labels_add_test5), len(sentences_add_test5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0dnzujK-Voq"
      },
      "source": [
        "edges = [0, 8, 15, 21, 28, 35]\n",
        "len_train, labels_train_padded, int_train_padded, mask_train_padded = pad_int(int_train_sent, labels_train5, edges)\n",
        "len_val, labels_val_padded, int_val_padded, mask_val_padded = pad_int(int_val_sent, labels_val5, edges)\n",
        "len_test, labels_test_padded, int_test_padded, mask_test_padded = pad_int(int_test_sent, labels_test5, edges)\n",
        "len_add_test, labels_add_test_padded, int_add_test_padded, mask_add_test_padded = pad_int(int_add_test_sent, labels_add_test5, edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt-anUO52O-b"
      },
      "source": [
        "## **Check after encoding each word to an integer index (of the embedding matri) and padding, that the sentences remains unchanged**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev-tGUnzjtNW",
        "outputId": "b13fb5c4-385f-4662-b5f6-f3dcc239a3d1"
      },
      "source": [
        "sent = []\n",
        "for word in int_add_test_padded[0][1]:\n",
        "    if word >= embedding_mat.shape[0] + embedding_mat_val.shape[0]:\n",
        "        sent.append(list(integer_dict_test.keys())[list(integer_dict_test.values()).index(word)]) \n",
        "    elif word >= embedding_mat.shape[0]:\n",
        "        sent.append(list(integer_dict_val.keys())[list(integer_dict_val.values()).index(word)])\n",
        "    else:\n",
        "        sent.append(list(integer_dict_train.keys())[list(integer_dict_train.values()).index(word)])\n",
        "\n",
        "print(sent, labels_add_test_padded[0][1], len(sent), mask_add_test_padded[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dark', 'Knight', 'Rises', 'was', 'perfect', 'sequel', 'UNK', 'masterpiece'] 4 8 [1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxdoR5guky7v",
        "outputId": "74472789-fac1-42b1-f21b-4d2ee4f78660"
      },
      "source": [
        "print(sentences_add_test5[1], len(sentences_add_test5[1]), labels_add_test5[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dark', 'Knight', 'Rises', 'was', 'perfect', 'sequel', 'UNK', 'masterpiece'] 8 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XpCyB5O-Voq",
        "outputId": "eea7b306-e48c-4ab7-dfa7-716531814ede"
      },
      "source": [
        "sent = []\n",
        "for word in int_train_padded[3][0]:\n",
        "    sent.append(list(integer_dict_train.keys())[list(integer_dict_train.values()).index(word)])\n",
        "print(sent, labels_train_padded[3][0], len(sent), mask_train_padded[3][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gorgeously', 'elaborate', 'continuation', 'The', 'Lord', 'Rings', 'trilogy', 'so', 'huge', 'column', 'words', 'can', 'not', 'adequately', 'describe', 'Peter', 'Jackson', 'expanded', 'vision', 'JRR', 'Tolkien', 'UNK', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock', 'Rock'] 4 28 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKqDpuE-Voq",
        "outputId": "d6f792b3-d01a-4dd0-99f3-36755ccd2fe4"
      },
      "source": [
        "print(sentences_train5[0],len(sentences_train5[0]), labels_train5[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rock', 'destined', 'st', 'Century', 'new', 'Conan', 'he', 'going', 'make', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', 'UNK', 'Van', 'Damme', 'or', 'Steven', 'Segal'] 21 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjQKgfhI-Voq",
        "outputId": "3b24e7da-c570-4f3a-bd24-b8cbb4ee7960"
      },
      "source": [
        "print(sentences_val5[0],len(sentences_val5[0]), labels_val5[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lovely', 'film', 'lovely', 'performances', 'by', 'Buy', 'UNK', 'Accorsi'] 8 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jat0Ufav-Voq",
        "outputId": "710e9075-7f22-4dca-fc43-fd9fde48662f"
      },
      "source": [
        "sent = []\n",
        "for word in int_val_padded[1][0]:\n",
        "    if word >= embedding_mat.shape[0]:\n",
        "        sent.append(list(integer_dict_val.keys())[list(integer_dict_val.values()).index(word)])\n",
        "    else:\n",
        "        sent.append(list(integer_dict_train.keys())[list(integer_dict_train.values()).index(word)])\n",
        "    \n",
        "print(sent, labels_val_padded[1][0], len(sent), mask_val_padded[1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['half', 'Submarine', 'flick', 'Half', 'Ghost', 'Story', 'All', 'one', 'criminally', 'neglected', 'film', 'Rock', 'Rock', 'Rock', 'Rock'] 2 15 [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GetL-2ja-Voq",
        "outputId": "56cc00d4-b6da-42d6-cced-bf729ad26394"
      },
      "source": [
        "print(sentences_test5[0],len(sentences_test5[0]), labels_test5[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['effective', 'but', 'UNK', 'biopic'] 4 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q729iB-q-Voq",
        "outputId": "104b2b4c-31db-41d5-e021-51cad7cfa36b"
      },
      "source": [
        "sent = []\n",
        "for word in int_test_padded[0][0]:\n",
        "    if word >= embedding_mat.shape[0] + embedding_mat_val.shape[0]:\n",
        "        sent.append(list(integer_dict_test.keys())[list(integer_dict_test.values()).index(word)]) \n",
        "    elif word >= embedding_mat.shape[0]:\n",
        "        sent.append(list(integer_dict_val.keys())[list(integer_dict_val.values()).index(word)])\n",
        "    else:\n",
        "        sent.append(list(integer_dict_train.keys())[list(integer_dict_train.values()).index(word)])\n",
        "    \n",
        "print(sent, labels_test_padded[0][0], len(sent), mask_test_padded[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['effective', 'but', 'UNK', 'biopic', 'Rock', 'Rock', 'Rock', 'Rock'] 2 8 [1 1 1 1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH2ZoSAE-Voq"
      },
      "source": [
        "## **Normalize the embedding vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEWw_Hmt-Voq",
        "outputId": "68a05080-397b-4585-f0e4-6edc82824c2a"
      },
      "source": [
        "#Normalizing matrix\n",
        "\n",
        "maxi=np.max(embedding_mat)\n",
        "mini=np.min(embedding_mat)\n",
        "embedding_mat1=2*(embedding_mat-mini)/(maxi-mini)-1\n",
        "print(np.max(embedding_mat1))\n",
        "print(np.min(embedding_mat1))\n",
        "\n",
        "maxi=np.max(embedding_mat_val)\n",
        "mini=np.min(embedding_mat_val)\n",
        "embedding_mat_val1=2*(embedding_mat_val-mini)/(maxi-mini)-1\n",
        "print(np.max(embedding_mat_val1))\n",
        "print(np.min(embedding_mat_val1))\n",
        "\n",
        "maxi=np.max(embedding_mat_test)\n",
        "mini=np.min(embedding_mat_test)\n",
        "embedding_mat_test1=2*(embedding_mat_test-mini)/(maxi-mini)-1\n",
        "print(np.max(embedding_mat_test1))\n",
        "print(np.min(embedding_mat_test1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErS4JS8q-Vor"
      },
      "source": [
        "## **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juI4qkdD-Vor"
      },
      "source": [
        "\"\"\"\n",
        "Summarize data that we work with:\n",
        "Non-padded groups, with pre-processing:\n",
        "sentences_train5, labels_train5\n",
        "\n",
        "Padded-groups, with pre-processing\n",
        "len_train, labels_train_padded, int_train_padded, mask_train_padded\n",
        "\n",
        "Same convention for val and test, simply replace train with val, test\n",
        "\n",
        "Embedding matrix for training\n",
        "embedding_mat\n",
        "\n",
        "Embedding matrix for val and test:\n",
        "concatenate embedding_mat (after training) with embedding_mat_val and embedding_mat_test\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DavA6fXt-Vor"
      },
      "source": [
        "# To convert the integer labels to one-hot vectors\n",
        "def one_hot_labels(labels):\n",
        "    max_label = 4 # there are 5 labels, 0 to 4 exclusive\n",
        "    m = labels.shape[0]\n",
        "    oh_labels = np.zeros((m,max_label+1))\n",
        "    oh_labels[np.arange(m),labels] = 1\n",
        "    return labels, oh_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B20XhFe-Vor"
      },
      "source": [
        "# Model Skeleton for this task\n",
        "class Model():\n",
        "    def __init__(self, num_classes, nodes_per_layer, learning_rate, embedding_matrix, proj_length = 300, hlactivation = 'tanh', optimizer_name = \"Adam\", \n",
        "                 reg_coeff = 0, drop_prob = 0, train_embed = True, max_pool = True, min_pool = True, av_pool = True, isProject = True):\n",
        "        \n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        num_classes = number of labels in the output\n",
        "        nodes_per_layer = array of L values, where L is number of hidden layers in the MLP\n",
        "        learning_rate = learning rate for the optimizer\n",
        "        embedding_matrix = word embedding matrix to be used for the lookup layer\n",
        "        hlactivation = activation of the MLP hidden layers. Default is tanh. Assume all layers have same activations\n",
        "        optimizer_option = see below for choices, string argument\n",
        "        reg_coeff = L2 regularization coefficient\n",
        "        drop_prop = dropout probability\n",
        "        train_embed = whether to train embedding matrix, default is true\n",
        "        max, min, av_pool = Booleans to indicate which pooling to use\n",
        "        \"\"\"\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embedding_matrix = np.array(embedding_matrix) # size is nVocab x embedding_length\n",
        "        # need to declare as np array before passing to tf as initial value, otherwise become very slow\n",
        "\n",
        "        nVocab, embedding_length = self.embedding_matrix.shape\n",
        "        \n",
        "        # Create input placeholders\n",
        "        self.sent = tf.placeholder(tf.int32, shape = [None, None]) # input sentences fit here, shape is batch_size * padded_group_length\n",
        "        self.labels = tf.placeholder(tf.int32, shape = [None, num_classes]) # one-hot encoding of the target labels\n",
        "        self.isTrain = tf.placeholder(tf.bool, shape = None) # to activate/deactivate dropout\n",
        "        self.mask = tf.placeholder(tf.int32, shape = [None, None]) # masks to indicate original sentence lengths, with ones on original words, and 0 on padded words\n",
        "        self.n_dat = tf.placeholder(tf.int32, shape = None) # to know how many times to replicate the projection layer\n",
        "        \n",
        "        # Create embedding matrix as variable, to train. put self, so we can access the trained matrix\n",
        "        self.embedder = tf.Variable(initial_value = self.embedding_matrix, name = 'Embedder', trainable = train_embed)\n",
        "        proj_weights = tf.Variable(initial_value = np.random.randn(embedding_length, proj_length).astype(np.float32), name= 'proj_weights', trainable = True)\n",
        "        \n",
        "        # Also add placeholder for the 'predict' embedding matrix. Sufficient to use placeholder for this because\n",
        "        # during predict (i.e. inference on validation and test set), we are NOT changing the embedding matrix\n",
        "        # put as self so we can feed in\n",
        "        self.embedder_infer = tf.placeholder(tf.float32, shape = [None, embedding_length])\n",
        "        \n",
        "        # Embedding_lookup layer\n",
        "        embedded = tf.cond(tf.equal(self.isTrain,tf.constant(True)), lambda: tf.nn.embedding_lookup(self.embedder, self.sent),\n",
        "                           lambda: tf.nn.embedding_lookup(self.embedder_infer, self.sent)) # i.e. if we are infering during validation\n",
        "        # and test, we use the inference embedding matrix for lookup (i.e. the concatenated one)\n",
        "        self.return_embedded = embedded\n",
        "\n",
        "        # Project the embeddings to new vectors of length proj_length\n",
        "        rep_proj_weights = tf.repeat(tf.expand_dims(proj_weights, axis = 0), [self.n_dat], axis = 0)\n",
        "\n",
        "        if isProject:\n",
        "            embedded = tf.linalg.matmul(embedded, rep_proj_weights)\n",
        "            replicate_length = proj_length\n",
        "        else:\n",
        "            replicate_length = embedding_length\n",
        "        # Pooling and masking operations\n",
        "        # First, create 3D Mask\n",
        "        mask_3D = tf.repeat(tf.expand_dims(self.mask, axis = 2), [replicate_length], axis = 2) # 2D masks are replicated along the embedding features (because discarding that word means disccarding its embedding vect)\n",
        "        if max_pool:\n",
        "            max_mask = tf.minimum(((2*tf.cast(mask_3D, tf.float32) - 1) * np.inf), embedded) # force entries where mask = 0, to be -inf\n",
        "            pooled_max = tf.reduce_max(max_mask,axis = 1) # max over all words, i.e. along padded_length\n",
        "        if min_pool:\n",
        "            min_mask = tf.maximum(((1 - 2*tf.cast(mask_3D, tf.float32))* np.inf), embedded) # force entries where mask = 0, to be inf\n",
        "            pooled_min = tf.reduce_min(min_mask, axis = 1)\n",
        "        if av_pool:\n",
        "            av_mask = tf.multiply(embedded, tf.cast(mask_3D, tf.float32)) # by element-wise multiplication, the entries where mask = 0 would be 0\n",
        "            pooled_av = tf.reduce_mean(av_mask, axis = 1)\n",
        "\n",
        "        # Concatenate pooling results\n",
        "        if max_pool and min_pool and av_pool:\n",
        "            print(\"All Pooling\")\n",
        "            embed_cat = tf.concat([pooled_av, pooled_max, pooled_min], axis = 1)\n",
        "        elif max_pool and min_pool:\n",
        "            print(\"Max and min Pooling\")\n",
        "            embed_cat = tf.concat([pooled_max, pooled_min], axis = 1)\n",
        "        elif max_pool and av_pool:\n",
        "            print(\"Max and Av Pooling\")\n",
        "            embed_cat = tf.concat([pooled_av, pooled_max], axis = 1)\n",
        "        elif min_pool and av_pool:\n",
        "            print(\"Min and av Pooling\")\n",
        "            embed_cat = tf.concat([pooled_av, pooled_min], axis = 1)\n",
        "        elif max_pool:\n",
        "            print(\"Max Pooling\")\n",
        "            embed_cat = pooled_max\n",
        "        elif min_pool:\n",
        "            print(\"Min Pooling\")\n",
        "            embed_cat = pooled_min\n",
        "        elif av_pool:\n",
        "            print(\"Av Pooling\")\n",
        "            embed_cat = pooled_av\n",
        "\n",
        "        # Define MLP Layers\n",
        "        out_lay = embed_cat\n",
        "        for layer in range(0,len(nodes_per_layer)-1):\n",
        "            out_node = nodes_per_layer[layer]\n",
        "            if hlactivation == 'tanh':\n",
        "                out_lay = tf.layers.dense(out_lay, out_node, activation = tf.nn.tanh, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                        bias_initializer = tf.initializers.glorot_uniform)\n",
        "                out_lay = tf.layers.dropout(out_lay, rate = drop_prob, training = self.isTrain)\n",
        "            elif hlactivation == 'relu':\n",
        "                out_lay = tf.layers.dense(out_lay, out_node, activation = tf.nn.relu, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                        bias_initializer = tf.initializers.glorot_uniform)\n",
        "                out_lay = tf.layers.dropout(out_lay, rate = drop_prob, training = self.isTrain)\n",
        "            else:\n",
        "                out_lay = tf.layers.dense(out_lay, out_node, activation = tf.nn.leaky_relu, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                        bias_initializer = tf.initializers.glorot_uniform)\n",
        "                out_lay = tf.layers.dropout(out_lay, rate = drop_prob, training = self.isTrain)\n",
        "\n",
        "        # Define output layer\n",
        "        out_lay = tf.layers.dense(out_lay, num_classes, kernel_initializer =  tf.initializers.glorot_uniform, bias_initializer = tf.initializers.glorot_uniform)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Define prediction operation (returns predictions in the form of non-one-hot labels)\n",
        "        self.predictions = tf.reshape(tf.argmax(out_lay,1), [-1])\n",
        "        \n",
        "        # Define loss\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits=out_lay))\n",
        "        \n",
        "        # Add L2 Regularization\n",
        "        L2norms = [tf.nn.l2_loss(weights) for weights in tf.trainable_variables()]\n",
        "        L2 = tf.reduce_sum(L2norms)\n",
        "        self.loss_L2 = self.loss + reg_coeff * L2\n",
        "        \n",
        "        # Define training operation\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "        elif optimizer_name == 'SGD':\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
        "        elif optimizer_name == 'RMSProp':\n",
        "            optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate)\n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "        self.train_op = optimizer.minimize(self.loss_L2)\n",
        "        \n",
        "        # Note: We don't define accuracy here. For evaluation on train and val after every epoch, return predictions and then\n",
        "        # evaluate accuracy outside, using numpy\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo5z1t1y-Vor"
      },
      "source": [
        "# Predict function for the created model\n",
        "def predict(model, int_sent_batch, labels_batch, mask_batch, embedding_mat, tfsession, batch_size = 1000):\n",
        "    \"\"\"\n",
        "    This takes in ONE category of data of arbitrary sentence length, as well as the corresponding labels, and returns the count\n",
        "    of correct results as well as the loss. Also takes in the CONCATENATED embedding matrix (with val and test words)\n",
        "    \n",
        "    Batch size here is how many samples to process at once. We want to be as large as possible technically, i.e. ideally to\n",
        "    process whole batch in 1 go.\n",
        "    \n",
        "    Shapes:\n",
        "    int_sent_batch = nData_in_cat x categ_len\n",
        "    labels_batch = nData_in_cat (1-Dim)\n",
        "    mask_batch = nData_in_cat x categ_len\n",
        "    \"\"\"\n",
        "    # Same approach as train, i.e. divide whole chunk into batches\n",
        "    nData, _ = int_sent_batch.shape\n",
        "    \n",
        "    # Need one-hot labels to evaluate loss\n",
        "    _, labels_batch_oh = one_hot_labels(labels_batch)\n",
        "    \n",
        "    if batch_size >= nData:\n",
        "        num_batch = 1\n",
        "    elif nData % batch_size == 0:\n",
        "        num_batch = (nData // batch_size)\n",
        "    else:\n",
        "        num_batch = (nData // batch_size) + 1\n",
        "    \n",
        "    count_correct = 0; loss_accum = 0; batch_total_length = 0\n",
        "    for batch_ind in range(num_batch): # Iterate through each batch\n",
        "        low_ind = (batch_ind) * batch_size\n",
        "        high_ind = min(low_ind + batch_size, nData)\n",
        "        \n",
        "        # Extract the batch of data\n",
        "        batch_in = int_sent_batch[low_ind:high_ind, :]\n",
        "        batch_labels_oh = labels_batch_oh[low_ind:high_ind, :]\n",
        "        batch_labels = labels_batch[low_ind:high_ind]\n",
        "        batch_masks = mask_batch[low_ind:high_ind, :]\n",
        "        \n",
        "        # Extract the batch length (not always = batch size, in case nData < batch size)\n",
        "        batch_length = batch_in.shape[0]\n",
        "        \n",
        "        # Create feed dictionary to predict\n",
        "        feed_pred = {model.sent : batch_in, model.labels : batch_labels_oh, model.isTrain : False, model.mask : batch_masks, \n",
        "                     model.embedder_infer : embedding_mat, model.n_dat : batch_length}\n",
        "        \n",
        "        # Perform predictions to get average loss for this batch, and the predictions on this batch\n",
        "        batch_pred, av_loss = tfsession.run([model.predictions, model.loss], feed_dict = feed_pred)\n",
        "        \n",
        "        # Sum up the loss and correct counts, because we want to average over ALL training data later\n",
        "        loss_accum += av_loss * batch_length\n",
        "        count_correct += np.sum(batch_pred == batch_labels)\n",
        "        # count_correct += np.sum(np.abs(batch_pred - batch_labels) <= 1)\n",
        "        batch_total_length += batch_length # sanity check, must be = nData\n",
        "        \n",
        "    \n",
        "    \n",
        "    return count_correct, loss_accum, nData\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f28T3V_-Vor"
      },
      "source": [
        "# Training function for the created model\n",
        "def train(model, int_train_padded, labels_train_padded, mask_train_padded, embedding_val, embedding_test, n_epochs, batch_size,\n",
        "         int_val_padded, labels_val_padded, mask_val_padded, tfsession):\n",
        "    \"\"\"\n",
        "    Train the model for n_epochs using the fed training data (already in integer form) and then predict on whole\n",
        "    train and val after every epoch. Returns a list of acc and loss after every epoch.\n",
        "    \n",
        "    Also takes in the embeddings from val and test words (these are NOT in train, hence never updated, but are in w2v)\n",
        "    to concatenate with the trained embeddings\n",
        "    \"\"\"\n",
        "    # with tf.Session() as sess:\n",
        "    embed_train = tfsession.run(model.embedder, feed_dict = None) # get initial training embedding matrix\n",
        "        \n",
        "    embedding_full = np.concatenate([embed_train, embedding_val, embedding_test], axis = 0)\n",
        "    acc_train_per_epoch = []; loss_train_per_epoch = []; acc_val_per_epoch = []; loss_val_per_epoch = []\n",
        "    # Start of each epoch.\n",
        "    for epoch in range(n_epochs):\n",
        "        # Shuffle the categories\n",
        "#         c = list(zip(int_train_padded, labels_train_padded, mask_train_padded))\n",
        "#         random.shuffle(c)\n",
        "#         new_int_train, new_labels_train, new_mask_train = zip(*c) # new... are shuffled versions of the original, but shuffled in triplets accordingly\n",
        "        c = list (zip(list(zip(int_train_padded, labels_train_padded)),mask_train_padded))\n",
        "        random.shuffle(c)\n",
        "        ab, new_mask_train = zip(*c)\n",
        "        new_int_train, new_labels_train =zip(*ab)\n",
        "        \n",
        "\n",
        "        for categ in range(len(new_int_train)): # iterate through the categories\n",
        "            group_int_inputs = new_int_train[categ]; group_labels = new_labels_train[categ]; group_masks = new_mask_train[categ]\n",
        "\n",
        "            # Shuffle within category\n",
        "            permute = np.random.permutation(group_int_inputs.shape[0])\n",
        "            group_int_inputs = group_int_inputs[permute,:]\n",
        "            group_labels = group_labels[permute]\n",
        "            group_masks = group_masks[permute,:]\n",
        "\n",
        "            # one-hot encode labels for training\n",
        "            _, group_labels_oh = one_hot_labels(group_labels)\n",
        "\n",
        "            # Take batches of data within categories, in case some categories contain too big of chunk\n",
        "            nData,_ = group_int_inputs.shape\n",
        "\n",
        "            if batch_size >= nData:\n",
        "                num_batch = 1\n",
        "            elif nData % batch_size == 0:\n",
        "                num_batch = (nData // batch_size)\n",
        "            else:\n",
        "                num_batch = (nData // batch_size) + 1\n",
        "\n",
        "            for batch_ind in range(num_batch): # Iterate through each batch\n",
        "                low_ind = (batch_ind) * batch_size\n",
        "                high_ind = min(low_ind + batch_size, nData)\n",
        "\n",
        "                # Take in batch, all the data needed for TRAINING only.\n",
        "                batch_in = group_int_inputs[low_ind:high_ind, :]\n",
        "                batch_labels_oh = group_labels_oh[low_ind:high_ind, :]\n",
        "                batch_masks = group_masks[low_ind:high_ind, :]\n",
        "                batch_dat = batch_in.shape[0]\n",
        "\n",
        "                # Create dictionary to feed\n",
        "                feed_train = {model.sent : batch_in, model.labels : batch_labels_oh, model.isTrain : True, model.mask : batch_masks, \n",
        "                             model.embedder_infer : embedding_full, model.n_dat : batch_dat}\n",
        "\n",
        "                # Train\n",
        "                tfsession.run(model.train_op, feed_dict = feed_train)\n",
        "                    \n",
        "        # At the end of every epoch, update the embedding matrix from training, and re-concatenate with val and test\n",
        "        embedding_train = tfsession.run(model.embedder, feed_dict = None)\n",
        "        embedding_full = np.concatenate([embedding_train, embedding_val, embedding_test], axis = 0)\n",
        "        \n",
        "        # Then use this full embedding to do predictions on train and val. Complete the predict function above. It will take\n",
        "        # the WHOLE training group from each category, and return the total loss as well as the total count of correct predict,\n",
        "        # as well as the sample count (i.e. how many processed). Use these to compute sum over all categories, and take mean over\n",
        "        # these to be our acc and loss respectively.\n",
        "        whole_train_loss = 0; whole_train_count = 0; whole_val_loss = 0; whole_val_count = 0; train_samples = 0; val_samples = 0\n",
        "        \n",
        "        # For train\n",
        "        for categ in range(len(new_int_train)): # iterate through the categories again, for inference\n",
        "            group_int_inputs = new_int_train[categ]; group_labels = new_labels_train[categ]; group_masks = new_mask_train[categ]\n",
        "            count_correct, loss_accum, batch_length = predict(model, group_int_inputs, group_labels, group_masks, embedding_full, tfsession, batch_size = 2000)\n",
        "            whole_train_loss += loss_accum; whole_train_count += count_correct; train_samples += batch_length\n",
        "            \n",
        "        acc_train_per_epoch.append(whole_train_count / train_samples)\n",
        "        loss_train_per_epoch.append(whole_train_loss / train_samples)\n",
        "        \n",
        "        # For val\n",
        "        for categ in range(len(int_val_padded)): # iterate through the categories again, for inference\n",
        "            group_int_inputs = int_val_padded[categ]; group_labels = labels_val_padded[categ]; group_masks = mask_val_padded[categ]\n",
        "            count_correct, loss_accum, batch_length = predict(model, group_int_inputs, group_labels, group_masks, embedding_full, tfsession, batch_size = 2000)\n",
        "            whole_val_loss += loss_accum; whole_val_count += count_correct; val_samples += batch_length\n",
        "            \n",
        "        acc_val_per_epoch.append(whole_val_count / val_samples)\n",
        "        loss_val_per_epoch.append(whole_val_loss / val_samples)\n",
        "        \n",
        "        print(f\"For epoch {epoch+1}, train acc and loss are {whole_train_count / train_samples} and {whole_train_loss / train_samples}\")\n",
        "        print(f\"For epoch {epoch+1}, val acc and loss are {whole_val_count / val_samples} and {whole_val_loss / val_samples}\")\n",
        "\n",
        "\n",
        "\n",
        "    return acc_train_per_epoch, loss_train_per_epoch, acc_val_per_epoch, loss_val_per_epoch, embedding_full\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlu9_SyAE-0"
      },
      "source": [
        "# Function to predict on dataset AFTER model has been trained\n",
        "def predict_test(int_test_padded, labels_test_padded, mask_test_padded, embedding_mat, tfsession):\n",
        "    whole_test_loss = 0; whole_test_count = 0; test_samples = 0\n",
        "    for categ in range(len(int_test_padded)):\n",
        "        group_int_inputs = int_test_padded[categ]; group_labels = labels_test_padded[categ]; group_masks = mask_test_padded[categ]\n",
        "        count_correct, loss_accum, batch_length = predict(model, group_int_inputs, group_labels, group_masks, embedding_mat, tfsession, batch_size = 2000)\n",
        "        whole_test_loss += loss_accum; whole_test_count += count_correct; test_samples += batch_length\n",
        "\n",
        "    test_acc = whole_test_count / test_samples; test_loss = whole_test_loss / test_samples\n",
        "\n",
        "    return test_acc, test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxLfOfyj-Vor",
        "outputId": "6ca1c246-dce3-4340-cde5-0eaf17376ecc"
      },
      "source": [
        "# Model Training Code Skeleton. We used this to explore hyperparameters by manually inputting the values we tried, and running this code\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # Create Model\n",
        "  embedding_train = embedding_mat; num_classes = 5; nodes_per_layer = [250, 125, num_classes]\n",
        "  learning_rate = 0.001; reg_coeff = 2e-03; drop_prob = 0.1; embedding_val = embedding_mat_val\n",
        "  embedding_test = embedding_mat_test; proj_length = 450\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  model = Model(num_classes, nodes_per_layer, learning_rate, embedding_train, reg_coeff= reg_coeff, drop_prob= drop_prob, proj_length = proj_length\n",
        "  ,max_pool= False, min_pool= False, av_pool= True, optimizer_name= \"Adam\", train_embed= False, hlactivation = 'relu', isProject = False)\n",
        "\n",
        "  # Input Data\n",
        "  x_cat = int_train_padded; y_cat = labels_train_padded; mask_cat = mask_train_padded;\n",
        "  x_val = int_val_padded; y_val = labels_val_padded; mask_val = mask_val_padded\n",
        "\n",
        "  init = tf.initialize_all_variables()\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(init)\n",
        "\n",
        "  # Train\n",
        "  acc_train, loss_train, acc_val, loss_val, embedding_full = train(model, x_cat, y_cat, mask_cat, embedding_val, embedding_test, 175, 1024, x_val, y_val, mask_val, sess)\n",
        "\n",
        "  # Predict on test\n",
        "  acc_test, loss_test = predict_test(int_test_padded, labels_test_padded, mask_test_padded, embedding_full, sess)\n",
        "\n",
        "  argmax_val = np.argmax(acc_val)\n",
        "  print(f\"Max validation accuracy obtained at epoch {np.argmax(acc_val)+1} with value of {np.max(acc_val)}\")\n",
        "  print(f\"Training accuracy at that epoch is {acc_train[argmax_val]}\")\n",
        "  print(f\"Accuracy and loss on test using the final trained model is {acc_test} and {loss_test} respectively\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Av Pooling\n",
            "For epoch 1, train acc and loss are 0.6125936329588015 and 1.5615642019983536\n",
            "For epoch 1, val acc and loss are 0.6112624886466849 and 1.569592283705383\n",
            "For epoch 2, train acc and loss are 0.7907303370786517 and 1.5394513485360235\n",
            "For epoch 2, val acc and loss are 0.7983651226158038 and 1.5463853758319088\n",
            "For epoch 3, train acc and loss are 0.767439138576779 and 1.5019375828935413\n",
            "For epoch 3, val acc and loss are 0.773841961852861 and 1.5088932010718197\n",
            "For epoch 4, train acc and loss are 0.8092228464419475 and 1.4494196102860268\n",
            "For epoch 4, val acc and loss are 0.8192552225249773 and 1.4505763415528037\n",
            "For epoch 5, train acc and loss are 0.8233848314606742 and 1.3891128782420123\n",
            "For epoch 5, val acc and loss are 0.8256130790190735 and 1.3906553760644633\n",
            "For epoch 6, train acc and loss are 0.8375468164794008 and 1.3402443308285559\n",
            "For epoch 6, val acc and loss are 0.8374205267938238 and 1.3507794853127295\n",
            "For epoch 7, train acc and loss are 0.8419943820224719 and 1.3097348779942213\n",
            "For epoch 7, val acc and loss are 0.8374205267938238 and 1.3252323518332083\n",
            "For epoch 8, train acc and loss are 0.8422284644194756 and 1.2942168673549252\n",
            "For epoch 8, val acc and loss are 0.8392370572207084 and 1.3121410624532674\n",
            "For epoch 9, train acc and loss are 0.8436329588014981 and 1.2830049895615168\n",
            "For epoch 9, val acc and loss are 0.8419618528610354 and 1.3014553351146756\n",
            "For epoch 10, train acc and loss are 0.8491338951310862 and 1.2743641791049014\n",
            "For epoch 10, val acc and loss are 0.8510445049954587 and 1.300717610212806\n",
            "For epoch 11, train acc and loss are 0.851123595505618 and 1.2679560159364442\n",
            "For epoch 11, val acc and loss are 0.849227974568574 and 1.2955924620095651\n",
            "For epoch 12, train acc and loss are 0.8505383895131086 and 1.2657129324498249\n",
            "For epoch 12, val acc and loss are 0.8401453224341507 and 1.2963434694898226\n",
            "For epoch 13, train acc and loss are 0.8500702247191011 and 1.2647465512100677\n",
            "For epoch 13, val acc and loss are 0.8483197093551317 and 1.3064089911726797\n",
            "For epoch 14, train acc and loss are 0.8520599250936329 and 1.2570675913983962\n",
            "For epoch 14, val acc and loss are 0.8401453224341507 and 1.2906415058633178\n",
            "For epoch 15, train acc and loss are 0.8513576779026217 and 1.2549415222658646\n",
            "For epoch 15, val acc and loss are 0.849227974568574 and 1.300182291965502\n",
            "For epoch 16, train acc and loss are 0.8552200374531835 and 1.2503020865249723\n",
            "For epoch 16, val acc and loss are 0.8455949137148048 and 1.2924983765622033\n",
            "For epoch 17, train acc and loss are 0.8542837078651685 and 1.2493895295909727\n",
            "For epoch 17, val acc and loss are 0.846503178928247 and 1.2957535085626128\n",
            "For epoch 18, train acc and loss are 0.8463249063670412 and 1.2519656529698926\n",
            "For epoch 18, val acc and loss are 0.8365122615803815 and 1.3051218646531968\n",
            "For epoch 19, train acc and loss are 0.8569756554307116 and 1.2452116797814208\n",
            "For epoch 19, val acc and loss are 0.849227974568574 and 1.2908992549703946\n",
            "For epoch 20, train acc and loss are 0.851123595505618 and 1.247335783997725\n",
            "For epoch 20, val acc and loss are 0.8310626702997275 and 1.2942412805817107\n",
            "For epoch 21, train acc and loss are 0.8460908239700374 and 1.2467221238472488\n",
            "For epoch 21, val acc and loss are 0.8265213442325159 and 1.2981427732540411\n",
            "For epoch 22, train acc and loss are 0.8540496254681648 and 1.2457015394383155\n",
            "For epoch 22, val acc and loss are 0.8474114441416893 and 1.298048669592453\n",
            "For epoch 23, train acc and loss are 0.8496020599250936 and 1.2393972759836176\n",
            "For epoch 23, val acc and loss are 0.8365122615803815 and 1.2924248453273652\n",
            "For epoch 24, train acc and loss are 0.855688202247191 and 1.2410058403171405\n",
            "For epoch 24, val acc and loss are 0.8446866485013624 and 1.297417151397407\n",
            "For epoch 25, train acc and loss are 0.849250936329588 and 1.23798430228278\n",
            "For epoch 25, val acc and loss are 0.8283378746594006 and 1.29414313365718\n",
            "For epoch 26, train acc and loss are 0.8579119850187266 and 1.243005674709095\n",
            "For epoch 26, val acc and loss are 0.8410535876475931 and 1.2933425419337092\n",
            "For epoch 27, train acc and loss are 0.8510065543071161 and 1.2353515823682149\n",
            "For epoch 27, val acc and loss are 0.8201634877384196 and 1.289792994712289\n",
            "For epoch 28, train acc and loss are 0.8409410112359551 and 1.2401761276724186\n",
            "For epoch 28, val acc and loss are 0.8219800181653043 and 1.3048150559970186\n",
            "For epoch 29, train acc and loss are 0.8547518726591761 and 1.234264503871457\n",
            "For epoch 29, val acc and loss are 0.8419618528610354 and 1.2976010560339737\n",
            "For epoch 30, train acc and loss are 0.8547518726591761 and 1.2302492211541434\n",
            "For epoch 30, val acc and loss are 0.8437783832879201 and 1.2906048110568664\n",
            "For epoch 31, train acc and loss are 0.8491338951310862 and 1.2293783444701956\n",
            "For epoch 31, val acc and loss are 0.8319709355131698 and 1.2929348928293891\n",
            "For epoch 32, train acc and loss are 0.8506554307116105 and 1.2289243613680203\n",
            "For epoch 32, val acc and loss are 0.8374205267938238 and 1.2930438823855865\n",
            "For epoch 33, train acc and loss are 0.8451544943820225 and 1.2348487453904937\n",
            "For epoch 33, val acc and loss are 0.8365122615803815 and 1.3087061129300623\n",
            "For epoch 34, train acc and loss are 0.8520599250936329 and 1.2259624402286409\n",
            "For epoch 34, val acc and loss are 0.8265213442325159 and 1.2877202253792093\n",
            "For epoch 35, train acc and loss are 0.8478464419475655 and 1.2307302009188728\n",
            "For epoch 35, val acc and loss are 0.815622161671208 and 1.2912773806652516\n",
            "For epoch 36, train acc and loss are 0.8498361423220974 and 1.2263030482365398\n",
            "For epoch 36, val acc and loss are 0.8337874659400545 and 1.2995533060746016\n",
            "For epoch 37, train acc and loss are 0.851123595505618 and 1.2242138243737293\n",
            "For epoch 37, val acc and loss are 0.8374205267938238 and 1.293887108374898\n",
            "For epoch 38, train acc and loss are 0.8549859550561798 and 1.2223114127690873\n",
            "For epoch 38, val acc and loss are 0.8356039963669392 and 1.2868552412583978\n",
            "For epoch 39, train acc and loss are 0.8477294007490637 and 1.2230758096618153\n",
            "For epoch 39, val acc and loss are 0.8283378746594006 and 1.2930232649169977\n",
            "For epoch 40, train acc and loss are 0.8467930711610487 and 1.2274444147330545\n",
            "For epoch 40, val acc and loss are 0.8328792007266121 and 1.3088894699185032\n",
            "For epoch 41, train acc and loss are 0.8551029962546817 and 1.2189031211233765\n",
            "For epoch 41, val acc and loss are 0.8337874659400545 and 1.287261152678896\n",
            "For epoch 42, train acc and loss are 0.8425795880149812 and 1.229447913415423\n",
            "For epoch 42, val acc and loss are 0.8283378746594006 and 1.310946899040735\n",
            "For epoch 43, train acc and loss are 0.8528792134831461 and 1.2194407044836644\n",
            "For epoch 43, val acc and loss are 0.8410535876475931 and 1.2960057547696604\n",
            "For epoch 44, train acc and loss are 0.8594335205992509 and 1.2249778242928258\n",
            "For epoch 44, val acc and loss are 0.8419618528610354 and 1.2881259991838974\n",
            "For epoch 45, train acc and loss are 0.8481975655430711 and 1.2211128553647674\n",
            "For epoch 45, val acc and loss are 0.8228882833787466 and 1.2902465937464591\n",
            "For epoch 46, train acc and loss are 0.8526451310861424 and 1.217356782671664\n",
            "For epoch 46, val acc and loss are 0.8401453224341507 and 1.293496277525033\n",
            "For epoch 47, train acc and loss are 0.8481975655430711 and 1.2208763333630472\n",
            "For epoch 47, val acc and loss are 0.8247048138056312 and 1.289959210899069\n",
            "For epoch 48, train acc and loss are 0.860369850187266 and 1.223785802749882\n",
            "For epoch 48, val acc and loss are 0.8419618528610354 and 1.2949618470333144\n",
            "For epoch 49, train acc and loss are 0.8441011235955056 and 1.222291024018093\n",
            "For epoch 49, val acc and loss are 0.8310626702997275 and 1.305771806909213\n",
            "For epoch 50, train acc and loss are 0.8552200374531835 and 1.216190900109457\n",
            "For epoch 50, val acc and loss are 0.8328792007266121 and 1.2843839646901574\n",
            "For epoch 51, train acc and loss are 0.8525280898876404 and 1.2129269766338755\n",
            "For epoch 51, val acc and loss are 0.8283378746594006 and 1.2849316707423988\n",
            "For epoch 52, train acc and loss are 0.8562734082397003 and 1.2111842820651075\n",
            "For epoch 52, val acc and loss are 0.8356039963669392 and 1.2920105118409382\n",
            "For epoch 53, train acc and loss are 0.8607209737827716 and 1.2127110580268423\n",
            "For epoch 53, val acc and loss are 0.8410535876475931 and 1.2918584283972523\n",
            "For epoch 54, train acc and loss are 0.8504213483146067 and 1.2146391003682149\n",
            "For epoch 54, val acc and loss are 0.8328792007266121 and 1.3025278320754257\n",
            "For epoch 55, train acc and loss are 0.856624531835206 and 1.2118526684880704\n",
            "For epoch 55, val acc and loss are 0.8337874659400545 and 1.289018243470049\n",
            "For epoch 56, train acc and loss are 0.8586142322097379 and 1.2154678968705712\n",
            "For epoch 56, val acc and loss are 0.8328792007266121 and 1.2842964728673298\n",
            "For epoch 57, train acc and loss are 0.8527621722846442 and 1.2073314273206706\n",
            "For epoch 57, val acc and loss are 0.8346957311534968 and 1.2950694771055522\n",
            "For epoch 58, train acc and loss are 0.8577949438202247 and 1.2040701078509124\n",
            "For epoch 58, val acc and loss are 0.8319709355131698 and 1.2837612728548526\n",
            "For epoch 59, train acc and loss are 0.8500702247191011 and 1.210318102400178\n",
            "For epoch 59, val acc and loss are 0.8201634877384196 and 1.2931753077580646\n",
            "For epoch 60, train acc and loss are 0.8536985018726592 and 1.206357766752051\n",
            "For epoch 60, val acc and loss are 0.8328792007266121 and 1.2944013723777057\n",
            "For epoch 61, train acc and loss are 0.8594335205992509 and 1.2056486693242292\n",
            "For epoch 61, val acc and loss are 0.8301544050862852 and 1.2841662519309436\n",
            "For epoch 62, train acc and loss are 0.8544007490636704 and 1.2107262622411554\n",
            "For epoch 62, val acc and loss are 0.8346957311534968 and 1.30453470346605\n",
            "For epoch 63, train acc and loss are 0.8477294007490637 and 1.207362534206235\n",
            "For epoch 63, val acc and loss are 0.8265213442325159 and 1.2993627524614118\n",
            "For epoch 64, train acc and loss are 0.8601357677902621 and 1.2012726966193998\n",
            "For epoch 64, val acc and loss are 0.8346957311534968 and 1.2869914963936178\n",
            "For epoch 65, train acc and loss are 0.8618913857677902 and 1.1984422736372171\n",
            "For epoch 65, val acc and loss are 0.8428701180744778 and 1.2871223255031008\n",
            "For epoch 66, train acc and loss are 0.8520599250936329 and 1.2017098175647776\n",
            "For epoch 66, val acc and loss are 0.8328792007266121 and 1.2983072340001203\n",
            "For epoch 67, train acc and loss are 0.858380149812734 and 1.1979031406817364\n",
            "For epoch 67, val acc and loss are 0.8301544050862852 and 1.2837635464932462\n",
            "For epoch 68, train acc and loss are 0.857560861423221 and 1.1970285953263218\n",
            "For epoch 68, val acc and loss are 0.8328792007266121 and 1.287169822879968\n",
            "For epoch 69, train acc and loss are 0.8625936329588015 and 1.1947990905646975\n",
            "For epoch 69, val acc and loss are 0.8392370572207084 and 1.283379509490149\n",
            "For epoch 70, train acc and loss are 0.8604868913857678 and 1.1934281696708462\n",
            "For epoch 70, val acc and loss are 0.8410535876475931 and 1.289831831388101\n",
            "For epoch 71, train acc and loss are 0.8632958801498127 and 1.1922787056768431\n",
            "For epoch 71, val acc and loss are 0.8428701180744778 and 1.2907830906607258\n",
            "For epoch 72, train acc and loss are 0.8632958801498127 and 1.191250022417039\n",
            "For epoch 72, val acc and loss are 0.8392370572207084 and 1.2845494008735567\n",
            "For epoch 73, train acc and loss are 0.8591994382022472 and 1.194777383744605\n",
            "For epoch 73, val acc and loss are 0.8392370572207084 and 1.2844932228732824\n",
            "For epoch 74, train acc and loss are 0.8591994382022472 and 1.1906142571473612\n",
            "For epoch 74, val acc and loss are 0.8401453224341507 and 1.2891959248619878\n",
            "For epoch 75, train acc and loss are 0.8651685393258427 and 1.190741516538104\n",
            "For epoch 75, val acc and loss are 0.8419618528610354 and 1.2851011932816536\n",
            "For epoch 76, train acc and loss are 0.8604868913857678 and 1.1881020349425992\n",
            "For epoch 76, val acc and loss are 0.8356039963669392 and 1.287710782076206\n",
            "For epoch 77, train acc and loss are 0.8599016853932584 and 1.1885148860263022\n",
            "For epoch 77, val acc and loss are 0.8337874659400545 and 1.286349400512529\n",
            "For epoch 78, train acc and loss are 0.8601357677902621 and 1.1885346910647685\n",
            "For epoch 78, val acc and loss are 0.8365122615803815 and 1.2956970112633424\n",
            "For epoch 79, train acc and loss are 0.8541666666666666 and 1.1918804989604468\n",
            "For epoch 79, val acc and loss are 0.8310626702997275 and 1.3049668281539584\n",
            "For epoch 80, train acc and loss are 0.8579119850187266 and 1.1868217017934117\n",
            "For epoch 80, val acc and loss are 0.8319709355131698 and 1.290779857392965\n",
            "For epoch 81, train acc and loss are 0.8595505617977528 and 1.1894195345317604\n",
            "For epoch 81, val acc and loss are 0.8328792007266121 and 1.2821942169378282\n",
            "For epoch 82, train acc and loss are 0.8655196629213483 and 1.1843141534048296\n",
            "For epoch 82, val acc and loss are 0.8392370572207084 and 1.2893776369571253\n",
            "For epoch 83, train acc and loss are 0.8644662921348315 and 1.1907490553955238\n",
            "For epoch 83, val acc and loss are 0.8319709355131698 and 1.3045525354866978\n",
            "For epoch 84, train acc and loss are 0.860369850187266 and 1.1865364054029577\n",
            "For epoch 84, val acc and loss are 0.8365122615803815 and 1.2862382035814124\n",
            "For epoch 85, train acc and loss are 0.8687968164794008 and 1.183811862105399\n",
            "For epoch 85, val acc and loss are 0.8346957311534968 and 1.2938795178506073\n",
            "For epoch 86, train acc and loss are 0.8573267790262172 and 1.1854605253799801\n",
            "For epoch 86, val acc and loss are 0.8346957311534968 and 1.3014674066522791\n",
            "For epoch 87, train acc and loss are 0.8658707865168539 and 1.1842114933616437\n",
            "For epoch 87, val acc and loss are 0.8383287920072662 and 1.2787753239639448\n",
            "For epoch 88, train acc and loss are 0.8542837078651685 and 1.1881185165337855\n",
            "For epoch 88, val acc and loss are 0.8310626702997275 and 1.286437114188067\n",
            "For epoch 89, train acc and loss are 0.8678604868913857 and 1.1920731112723717\n",
            "For epoch 89, val acc and loss are 0.8410535876475931 and 1.2862178928520764\n",
            "For epoch 90, train acc and loss are 0.8680945692883895 and 1.1928384802007719\n",
            "For epoch 90, val acc and loss are 0.8410535876475931 and 1.302506415763841\n",
            "For epoch 91, train acc and loss are 0.8586142322097379 and 1.1822987454716631\n",
            "For epoch 91, val acc and loss are 0.8346957311534968 and 1.2992900841675705\n",
            "For epoch 92, train acc and loss are 0.8526451310861424 and 1.1876461834747916\n",
            "For epoch 92, val acc and loss are 0.8228882833787466 and 1.307006222567268\n",
            "For epoch 93, train acc and loss are 0.8654026217228464 and 1.1756958094763845\n",
            "For epoch 93, val acc and loss are 0.8374205267938238 and 1.2860010933811072\n",
            "For epoch 94, train acc and loss are 0.8713717228464419 and 1.1739987282131272\n",
            "For epoch 94, val acc and loss are 0.8410535876475931 and 1.287752471437896\n",
            "For epoch 95, train acc and loss are 0.868562734082397 and 1.1702165861478013\n",
            "For epoch 95, val acc and loss are 0.8437783832879201 and 1.2907386056733716\n",
            "For epoch 96, train acc and loss are 0.8677434456928839 and 1.1688243463384302\n",
            "For epoch 96, val acc and loss are 0.8365122615803815 and 1.2893392311021699\n",
            "For epoch 97, train acc and loss are 0.872308052434457 and 1.1695865955943248\n",
            "For epoch 97, val acc and loss are 0.8437783832879201 and 1.2894093936189102\n",
            "For epoch 98, train acc and loss are 0.8746488764044944 and 1.1690424601041645\n",
            "For epoch 98, val acc and loss are 0.8392370572207084 and 1.289916313419983\n",
            "For epoch 99, train acc and loss are 0.8730102996254682 and 1.1691317140553774\n",
            "For epoch 99, val acc and loss are 0.8410535876475931 and 1.2866910638644629\n",
            "For epoch 100, train acc and loss are 0.8691479400749064 and 1.1646250620177399\n",
            "For epoch 100, val acc and loss are 0.8383287920072662 and 1.2879945548635305\n",
            "For epoch 101, train acc and loss are 0.8617743445692884 and 1.1699153751079063\n",
            "For epoch 101, val acc and loss are 0.8337874659400545 and 1.3029167302406235\n",
            "For epoch 102, train acc and loss are 0.868562734082397 and 1.1639220338831011\n",
            "For epoch 102, val acc and loss are 0.8365122615803815 and 1.2970786395666276\n",
            "For epoch 103, train acc and loss are 0.8682116104868914 and 1.1623595281179702\n",
            "For epoch 103, val acc and loss are 0.8401453224341507 and 1.2811909361821106\n",
            "For epoch 104, train acc and loss are 0.8718398876404494 and 1.1598814939347546\n",
            "For epoch 104, val acc and loss are 0.8401453224341507 and 1.2880576690904235\n",
            "For epoch 105, train acc and loss are 0.8739466292134831 and 1.160883025995243\n",
            "For epoch 105, val acc and loss are 0.8356039963669392 and 1.2905571251755732\n",
            "For epoch 106, train acc and loss are 0.8719569288389513 and 1.1577704314854038\n",
            "For epoch 106, val acc and loss are 0.8410535876475931 and 1.289144664759207\n",
            "For epoch 107, train acc and loss are 0.8762874531835206 and 1.1562207288593611\n",
            "For epoch 107, val acc and loss are 0.8455949137148048 and 1.2838999497468204\n",
            "For epoch 108, train acc and loss are 0.8754681647940075 and 1.1545265356308958\n",
            "For epoch 108, val acc and loss are 0.8410535876475931 and 1.2917159421134277\n",
            "For epoch 109, train acc and loss are 0.8645833333333334 and 1.1582408209195298\n",
            "For epoch 109, val acc and loss are 0.8346957311534968 and 1.2944645247169237\n",
            "For epoch 110, train acc and loss are 0.8669241573033708 and 1.1608814655450845\n",
            "For epoch 110, val acc and loss are 0.8346957311534968 and 1.2857374094704084\n",
            "For epoch 111, train acc and loss are 0.8663389513108615 and 1.1585470396425395\n",
            "For epoch 111, val acc and loss are 0.8310626702997275 and 1.3057120293514173\n",
            "For epoch 112, train acc and loss are 0.8710205992509363 and 1.1518328502802144\n",
            "For epoch 112, val acc and loss are 0.8401453224341507 and 1.2845455137412187\n",
            "For epoch 113, train acc and loss are 0.8747659176029963 and 1.151358147462209\n",
            "For epoch 113, val acc and loss are 0.8310626702997275 and 1.295882105719058\n",
            "For epoch 114, train acc and loss are 0.8807350187265918 and 1.1507463384042964\n",
            "For epoch 114, val acc and loss are 0.8428701180744778 and 1.2948015462691733\n",
            "For epoch 115, train acc and loss are 0.867626404494382 and 1.1502283237009459\n",
            "For epoch 115, val acc and loss are 0.8392370572207084 and 1.2856660794388912\n",
            "For epoch 116, train acc and loss are 0.8774578651685393 and 1.1467997850466534\n",
            "For epoch 116, val acc and loss are 0.8356039963669392 and 1.291614472378827\n",
            "For epoch 117, train acc and loss are 0.8789794007490637 and 1.1510579298330603\n",
            "For epoch 117, val acc and loss are 0.8401453224341507 and 1.2893296218156598\n",
            "For epoch 118, train acc and loss are 0.8813202247191011 and 1.1451152248608039\n",
            "For epoch 118, val acc and loss are 0.8474114441416893 and 1.2871750048568962\n",
            "For epoch 119, train acc and loss are 0.8815543071161048 and 1.1455408913198482\n",
            "For epoch 119, val acc and loss are 0.8392370572207084 and 1.2939096986976784\n",
            "For epoch 120, train acc and loss are 0.8786282771535581 and 1.1411646113553073\n",
            "For epoch 120, val acc and loss are 0.8310626702997275 and 1.2944009764209647\n",
            "For epoch 121, train acc and loss are 0.8772237827715356 and 1.1391973771880406\n",
            "For epoch 121, val acc and loss are 0.8410535876475931 and 1.288394330957605\n",
            "For epoch 122, train acc and loss are 0.8835440074906367 and 1.1373702900881848\n",
            "For epoch 122, val acc and loss are 0.8455949137148048 and 1.2864429468896366\n",
            "For epoch 123, train acc and loss are 0.8775749063670412 and 1.1390158910067116\n",
            "For epoch 123, val acc and loss are 0.8410535876475931 and 1.2934711046591334\n",
            "For epoch 124, train acc and loss are 0.8673923220973783 and 1.1482339071228487\n",
            "For epoch 124, val acc and loss are 0.8147138964577657 and 1.3146449757315266\n",
            "For epoch 125, train acc and loss are 0.8874063670411985 and 1.1374825783464568\n",
            "For epoch 125, val acc and loss are 0.8401453224341507 and 1.2883457200945128\n",
            "For epoch 126, train acc and loss are 0.8813202247191011 and 1.1395976978322764\n",
            "For epoch 126, val acc and loss are 0.8474114441416893 and 1.2914079713561555\n",
            "For epoch 127, train acc and loss are 0.8840121722846442 and 1.1344673249326396\n",
            "For epoch 127, val acc and loss are 0.8446866485013624 and 1.2871590709816207\n",
            "For epoch 128, train acc and loss are 0.881437265917603 and 1.134078215793724\n",
            "For epoch 128, val acc and loss are 0.8346957311534968 and 1.2984997799567588\n",
            "For epoch 129, train acc and loss are 0.8803838951310862 and 1.1311011831328441\n",
            "For epoch 129, val acc and loss are 0.8401453224341507 and 1.2894311642018803\n",
            "For epoch 130, train acc and loss are 0.8813202247191011 and 1.1291215252898605\n",
            "For epoch 130, val acc and loss are 0.8374205267938238 and 1.2957721215611042\n",
            "For epoch 131, train acc and loss are 0.8809691011235955 and 1.1282192520127539\n",
            "For epoch 131, val acc and loss are 0.8428701180744778 and 1.2930749457711852\n",
            "For epoch 132, train acc and loss are 0.8744147940074907 and 1.130658731604002\n",
            "For epoch 132, val acc and loss are 0.8401453224341507 and 1.29453288512702\n",
            "For epoch 133, train acc and loss are 0.8769897003745318 and 1.1327989775031693\n",
            "For epoch 133, val acc and loss are 0.8201634877384196 and 1.3087491828890738\n",
            "For epoch 134, train acc and loss are 0.8822565543071161 and 1.1284161189829143\n",
            "For epoch 134, val acc and loss are 0.8437783832879201 and 1.2881392758072343\n",
            "For epoch 135, train acc and loss are 0.8871722846441947 and 1.1311003113172473\n",
            "For epoch 135, val acc and loss are 0.846503178928247 and 1.2939122124870293\n",
            "For epoch 136, train acc and loss are 0.8904494382022472 and 1.129826870652732\n",
            "For epoch 136, val acc and loss are 0.8410535876475931 and 1.3059994731568727\n",
            "For epoch 137, train acc and loss are 0.8857677902621723 and 1.1274628577412067\n",
            "For epoch 137, val acc and loss are 0.8265213442325159 and 1.309193420258573\n",
            "For epoch 138, train acc and loss are 0.8780430711610487 and 1.1227534903764502\n",
            "For epoch 138, val acc and loss are 0.8346957311534968 and 1.297025814584772\n",
            "For epoch 139, train acc and loss are 0.8851825842696629 and 1.1193417675430408\n",
            "For epoch 139, val acc and loss are 0.8410535876475931 and 1.2905953237947607\n",
            "For epoch 140, train acc and loss are 0.8806179775280899 and 1.1201771720546239\n",
            "For epoch 140, val acc and loss are 0.8356039963669392 and 1.2951295933433276\n",
            "For epoch 141, train acc and loss are 0.8890449438202247 and 1.1182514575732112\n",
            "For epoch 141, val acc and loss are 0.8337874659400545 and 1.3007813102001498\n",
            "For epoch 142, train acc and loss are 0.8889279026217228 and 1.1133414114291749\n",
            "For epoch 142, val acc and loss are 0.8446866485013624 and 1.2889450312093862\n",
            "For epoch 143, train acc and loss are 0.8863529962546817 and 1.1119829842660311\n",
            "For epoch 143, val acc and loss are 0.8374205267938238 and 1.2967585316579198\n",
            "For epoch 144, train acc and loss are 0.8735955056179775 and 1.1419243465564894\n",
            "For epoch 144, val acc and loss are 0.8401453224341507 and 1.3028898508737132\n",
            "For epoch 145, train acc and loss are 0.8783941947565543 and 1.128091458873021\n",
            "For epoch 145, val acc and loss are 0.8237965485921889 and 1.3242870056012022\n",
            "For epoch 146, train acc and loss are 0.8865870786516854 and 1.118606699623642\n",
            "For epoch 146, val acc and loss are 0.8310626702997275 and 1.3120367829091542\n",
            "For epoch 147, train acc and loss are 0.8815543071161048 and 1.119861190713077\n",
            "For epoch 147, val acc and loss are 0.8437783832879201 and 1.2919139668250712\n",
            "For epoch 148, train acc and loss are 0.8892790262172284 and 1.1106973173522323\n",
            "For epoch 148, val acc and loss are 0.8356039963669392 and 1.298147265744361\n",
            "For epoch 149, train acc and loss are 0.8906835205992509 and 1.1079329832383755\n",
            "For epoch 149, val acc and loss are 0.8337874659400545 and 1.2978925348735744\n",
            "For epoch 150, train acc and loss are 0.8919709737827716 and 1.104304597074731\n",
            "For epoch 150, val acc and loss are 0.8383287920072662 and 1.294642509491632\n",
            "For epoch 151, train acc and loss are 0.8927902621722846 and 1.1073864153289839\n",
            "For epoch 151, val acc and loss are 0.8501362397820164 and 1.2888136504456522\n",
            "For epoch 152, train acc and loss are 0.8909176029962547 and 1.1032907713469717\n",
            "For epoch 152, val acc and loss are 0.8446866485013624 and 1.3040651286547018\n",
            "For epoch 153, train acc and loss are 0.8900983146067416 and 1.098864854554112\n",
            "For epoch 153, val acc and loss are 0.8365122615803815 and 1.2931739073466648\n",
            "For epoch 154, train acc and loss are 0.8911516853932584 and 1.0975550237471021\n",
            "For epoch 154, val acc and loss are 0.8383287920072662 and 1.2926010120792024\n",
            "For epoch 155, train acc and loss are 0.8910346441947565 and 1.09778587866142\n",
            "For epoch 155, val acc and loss are 0.8392370572207084 and 1.2934043633732115\n",
            "For epoch 156, train acc and loss are 0.8937265917602997 and 1.0943663366390077\n",
            "For epoch 156, val acc and loss are 0.8383287920072662 and 1.3001169661410172\n",
            "For epoch 157, train acc and loss are 0.892439138576779 and 1.0931572022704819\n",
            "For epoch 157, val acc and loss are 0.8383287920072662 and 1.2948212156936756\n",
            "For epoch 158, train acc and loss are 0.8876404494382022 and 1.0927788668106335\n",
            "For epoch 158, val acc and loss are 0.8401453224341507 and 1.2978300856634446\n",
            "For epoch 159, train acc and loss are 0.8996956928838952 and 1.0934563944681306\n",
            "For epoch 159, val acc and loss are 0.8419618528610354 and 1.2915125723647378\n",
            "For epoch 160, train acc and loss are 0.892439138576779 and 1.092004027719913\n",
            "For epoch 160, val acc and loss are 0.8474114441416893 and 1.2931575505545094\n",
            "For epoch 161, train acc and loss are 0.8931413857677902 and 1.0879922735082745\n",
            "For epoch 161, val acc and loss are 0.8383287920072662 and 1.304335302704578\n",
            "For epoch 162, train acc and loss are 0.8900983146067416 and 1.0946543302093998\n",
            "For epoch 162, val acc and loss are 0.821071752951862 and 1.3164983211486152\n",
            "For epoch 163, train acc and loss are 0.8927902621722846 and 1.0856508781596292\n",
            "For epoch 163, val acc and loss are 0.8346957311534968 and 1.2928819096597295\n",
            "For epoch 164, train acc and loss are 0.8902153558052435 and 1.0869181044260214\n",
            "For epoch 164, val acc and loss are 0.8319709355131698 and 1.3102408652950914\n",
            "For epoch 165, train acc and loss are 0.8890449438202247 and 1.0870605503882138\n",
            "For epoch 165, val acc and loss are 0.8319709355131698 and 1.2984335851279527\n",
            "For epoch 166, train acc and loss are 0.8900983146067416 and 1.0841546885613422\n",
            "For epoch 166, val acc and loss are 0.8365122615803815 and 1.3053083273630377\n",
            "For epoch 167, train acc and loss are 0.8987593632958801 and 1.0806077197612671\n",
            "For epoch 167, val acc and loss are 0.8437783832879201 and 1.2943049509020743\n",
            "For epoch 168, train acc and loss are 0.8905664794007491 and 1.080115724020125\n",
            "For epoch 168, val acc and loss are 0.8410535876475931 and 1.2957846242443851\n",
            "For epoch 169, train acc and loss are 0.8938436329588015 and 1.0771443946670727\n",
            "For epoch 169, val acc and loss are 0.8365122615803815 and 1.2998626447611783\n",
            "For epoch 170, train acc and loss are 0.8951310861423221 and 1.074659025819784\n",
            "For epoch 170, val acc and loss are 0.8337874659400545 and 1.2991538580496889\n",
            "For epoch 171, train acc and loss are 0.9014513108614233 and 1.076118447891112\n",
            "For epoch 171, val acc and loss are 0.8401453224341507 and 1.3018457389332618\n",
            "For epoch 172, train acc and loss are 0.900749063670412 and 1.073238600059395\n",
            "For epoch 172, val acc and loss are 0.8346957311534968 and 1.3014017191938008\n",
            "For epoch 173, train acc and loss are 0.8891619850187266 and 1.0973571691982977\n",
            "For epoch 173, val acc and loss are 0.8410535876475931 and 1.3116028043597963\n",
            "For epoch 174, train acc and loss are 0.8968867041198502 and 1.0759778349531277\n",
            "For epoch 174, val acc and loss are 0.8328792007266121 and 1.3074709329683059\n",
            "For epoch 175, train acc and loss are 0.8968867041198502 and 1.0693087436342508\n",
            "For epoch 175, val acc and loss are 0.8374205267938238 and 1.2986171474032355\n",
            "Max validation accuracy obtained at epoch 10 with value of 0.8510445049954587\n",
            "Training accuracy at that epoch is 0.8491338951310862\n",
            "Accuracy and loss on test using the final trained model is 0.8497737556561086 and 1.2596616870677309 respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "mHeGL_3Dn_5f",
        "outputId": "feceb09e-edf3-44ba-bdb6-d287af55d268"
      },
      "source": [
        "# Grid Search Skeleton for the case where we do NOT train the embeddings, but use a projection matrix.\n",
        "with tf.device('/device:GPU:0'):\n",
        "  num_classes = 5\n",
        "  reg_val = [2e-03, 2e-06]; nodes = [[10, 5, num_classes], [50, 25, num_classes], [250, 125, num_classes]]; drops = [0, 0.1]; plengths = [150, 300, 450]\n",
        "  val_accs2 = np.zeros((2,3,2,3))\n",
        "  for i in range(len(reg_val)):\n",
        "    for j in range(len(nodes)):\n",
        "      for k in range(len(drops)):\n",
        "        for pl in range(len(plengths)):\n",
        "          # Create Model\n",
        "          embedding_train = embedding_mat; nodes_per_layer = nodes[j]\n",
        "          learning_rate = 0.001; reg_coeff = reg_val[i]; drop_prob = drops[k]; embedding_val = embedding_mat_val\n",
        "          embedding_test = embedding_mat_test; proj_length = plengths[pl]\n",
        "\n",
        "          tf.reset_default_graph()\n",
        "\n",
        "          model = Model(num_classes, nodes_per_layer, learning_rate, embedding_train, reg_coeff= reg_coeff, drop_prob= drop_prob, proj_length = proj_length\n",
        "          ,max_pool= False, min_pool= True, av_pool= True, optimizer_name= \"Adam\", train_embed= False, hlactivation = 'relu', isProject = True)\n",
        "\n",
        "          # Input Data\n",
        "          x_cat = int_train_padded; y_cat = labels_train_padded; mask_cat = mask_train_padded;\n",
        "          x_val = int_val_padded; y_val = labels_val_padded; mask_val = mask_val_padded\n",
        "\n",
        "          init = tf.initialize_all_variables()\n",
        "\n",
        "          sess = tf.Session()\n",
        "          sess.run(init)\n",
        "\n",
        "          # Train\n",
        "          acc_train, loss_train, acc_val, loss_val, embedding_full = train(model, x_cat, y_cat, mask_cat, embedding_val, embedding_test, 10, 1024, x_val, y_val, mask_val, sess)\n",
        "\n",
        "          # Predict on test\n",
        "          acc_test, loss_test = predict_test(int_test_padded, labels_test_padded, mask_test_padded, embedding_full, sess)\n",
        "\n",
        "          argmax_val = np.argmax(acc_val); best_val = np.max(acc_val)\n",
        "          val_accs2[i,j,k,pl] = best_val\n",
        "          print(f\"Drop prob = {drop_prob}, Reg Coeff = {reg_coeff}, Nodes per Layer = {nodes_per_layer}, Projection Length = {proj_length}\")\n",
        "          print(f\"Max validation accuracy obtained at epoch {np.argmax(acc_val)+1} with value of {np.max(acc_val)}\")\n",
        "          print(f\"Training accuracy at that epoch is {acc_train[argmax_val]}\")\n",
        "          print(f\"Accuracy and loss on test using the final trained model is {acc_test} and {loss_test} respectively\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min and av Pooling\n",
            "For epoch 1, train acc and loss are 0.12804307116104868 and 1.7102140562978576\n",
            "For epoch 1, val acc and loss are 0.12897366030881016 and 1.7035370922218551\n",
            "For epoch 2, train acc and loss are 0.12804307116104868 and 1.7044640784239056\n",
            "For epoch 2, val acc and loss are 0.1262488646684832 and 1.6991276237555355\n",
            "For epoch 3, train acc and loss are 0.12804307116104868 and 1.6995242531882244\n",
            "For epoch 3, val acc and loss are 0.1262488646684832 and 1.6947457151560217\n",
            "For epoch 4, train acc and loss are 0.12804307116104868 and 1.6948241316591308\n",
            "For epoch 4, val acc and loss are 0.1262488646684832 and 1.6905140372214806\n",
            "For epoch 5, train acc and loss are 0.12804307116104868 and 1.690474561426077\n",
            "For epoch 5, val acc and loss are 0.1262488646684832 and 1.6864618731021448\n",
            "For epoch 6, train acc and loss are 0.12816011235955055 and 1.6862127085805831\n",
            "For epoch 6, val acc and loss are 0.1262488646684832 and 1.6825476003494402\n",
            "For epoch 7, train acc and loss are 0.1283941947565543 and 1.6821059572339505\n",
            "For epoch 7, val acc and loss are 0.1262488646684832 and 1.6787692774002603\n",
            "For epoch 8, train acc and loss are 0.12851123595505617 and 1.6781198100539183\n",
            "For epoch 8, val acc and loss are 0.1262488646684832 and 1.675163941855435\n",
            "For epoch 9, train acc and loss are 0.12851123595505617 and 1.6743671671114164\n",
            "For epoch 9, val acc and loss are 0.1262488646684832 and 1.671619275177099\n",
            "For epoch 10, train acc and loss are 0.12851123595505617 and 1.6707728255107608\n",
            "For epoch 10, val acc and loss are 0.1262488646684832 and 1.6681686902890738\n",
            "Drop prob = 0, Reg Coeff = 0.002, Nodes per Layer = [10, 5, 5], Projection Length = 150\n",
            "Max validation accuracy obtained at epoch 1 with value of 0.12897366030881016\n",
            "Training accuracy at that epoch is 0.12804307116104868\n",
            "Accuracy and loss on test using the final trained model is 0.1262443438914027 and 1.6633694654136761 respectively\n",
            "Min and av Pooling\n",
            "For epoch 1, train acc and loss are 0.150749063670412 and 1.718487433670612\n",
            "For epoch 1, val acc and loss are 0.14986376021798364 and 1.728405327714648\n",
            "For epoch 2, train acc and loss are 0.150749063670412 and 1.698023375081882\n",
            "For epoch 2, val acc and loss are 0.14986376021798364 and 1.7082454910286982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-d8cf26ef2fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0;31m# Predict on test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-619fd1df6bd6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, int_train_padded, labels_train_padded, mask_train_padded, embedding_val, embedding_test, n_epochs, batch_size, int_val_padded, labels_val_padded, mask_val_padded, tfsession)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcateg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_int_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate through the categories again, for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mgroup_int_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_int_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcateg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgroup_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcateg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgroup_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mask_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcateg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mcount_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_accum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_int_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfsession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mwhole_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_accum\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mwhole_train_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount_correct\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrain_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-25c7e6be6ee7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, int_sent_batch, labels_batch, mask_batch, embedding_mat, tfsession, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Perform predictions to get average loss for this batch, and the predictions on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mbatch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mav_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfsession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Sum up the loss and correct counts, because we want to average over ALL training data later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgNZdKuFKVDD"
      },
      "source": [
        "# Grid Search Skeleton Code, for the case where we do not use projection vector, and do NOT train the embeddings\n",
        "with tf.device('/device:GPU:0'):\n",
        "  reg_val = [2e-03, 2e-06]; nodes = [[10, 5, num_classes], [50, 25, num_classes], [250, 125, num_classes]]; drops = [0, 0.1]\n",
        "  val_accs = np.zeros((2,3,2))\n",
        "  for i in range(len(reg_val)):\n",
        "    for j in range(len(nodes)):\n",
        "      for k in range(len(drops)):\n",
        "        # Create Model\n",
        "        embedding_train = embedding_mat; num_classes = 5; nodes_per_layer = nodes[j]\n",
        "        learning_rate = 0.001; reg_coeff = reg_val[i]; drop_prob = drops[k]; embedding_val = embedding_mat_val\n",
        "        embedding_test = embedding_mat_test; proj_length = 300\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        model = Model(num_classes, nodes_per_layer, learning_rate, embedding_train, reg_coeff= reg_coeff, drop_prob= drop_prob, proj_length = proj_length\n",
        "        ,max_pool= False, min_pool= False, av_pool= True, optimizer_name= \"Adam\", train_embed= False, hlactivation = 'relu', isProject = False)\n",
        "\n",
        "        # Input Data\n",
        "        x_cat = int_train_padded; y_cat = labels_train_padded; mask_cat = mask_train_padded;\n",
        "        x_val = int_val_padded; y_val = labels_val_padded; mask_val = mask_val_padded\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        sess = tf.Session()\n",
        "        sess.run(init)\n",
        "\n",
        "        # Train\n",
        "        acc_train, loss_train, acc_val, loss_val, embedding_full = train(model, x_cat, y_cat, mask_cat, embedding_val, embedding_test, 500, 1024, x_val, y_val, mask_val, sess)\n",
        "\n",
        "        # Predict on test\n",
        "        acc_test, loss_test = predict_test(int_test_padded, labels_test_padded, mask_test_padded, embedding_full, sess)\n",
        "\n",
        "        argmax_val = np.argmax(acc_val); best_val = np.max(acc_val)\n",
        "        val_accs[i,j,k] = best_val\n",
        "        print(f\"Drop prob = {drop_prob}, Reg Coeff = {reg_coeff}, Nodes per Layer = {nodes_per_layer}\")\n",
        "        print(f\"Max validation accuracy obtained at epoch {np.argmax(acc_val)+1} with value of {np.max(acc_val)}\")\n",
        "        print(f\"Training accuracy at that epoch is {acc_train[argmax_val]}\")\n",
        "        print(f\"Accuracy and loss on test using the final trained model is {acc_test} and {loss_test} respectively\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ9JCeWXzDp8"
      },
      "source": [
        "# Grid Search code skeleton, for the case where we update the embeddings during training, and therefore do not use projections\n",
        "with tf.device('/device:GPU:0'):\n",
        "  reg_val = [2e-03, 2e-06]; nodes = [[8, num_classes], [32, num_classes], [128, num_classes]]; drops = [0, 0.1]\n",
        "  val_accs3 = np.zeros((2,3,2))\n",
        "  for i in range(len(reg_val)):\n",
        "    for j in range(len(nodes)):\n",
        "      for k in range(len(drops)):\n",
        "        # Create Model\n",
        "        embedding_train = embedding_mat; num_classes = 5; nodes_per_layer = nodes[j]\n",
        "        learning_rate = 0.001; reg_coeff = reg_val[i]; drop_prob = drops[k]; embedding_val = embedding_mat_val\n",
        "        embedding_test = embedding_mat_test; proj_length = 300\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        model = Model(num_classes, nodes_per_layer, learning_rate, embedding_train, reg_coeff= reg_coeff, drop_prob= drop_prob, proj_length = proj_length\n",
        "        ,max_pool= False, min_pool= False, av_pool= True, optimizer_name= \"Adam\", train_embed= True, hlactivation = 'relu', isProject = False)\n",
        "\n",
        "        # Input Data\n",
        "        x_cat = int_train_padded; y_cat = labels_train_padded; mask_cat = mask_train_padded;\n",
        "        x_val = int_val_padded; y_val = labels_val_padded; mask_val = mask_val_padded\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        sess = tf.Session()\n",
        "        sess.run(init)\n",
        "\n",
        "        # Train\n",
        "        acc_train, loss_train, acc_val, loss_val, embedding_full = train(model, x_cat, y_cat, mask_cat, embedding_val, embedding_test, 150, 1024, x_val, y_val, mask_val, sess)\n",
        "\n",
        "        # Predict on test\n",
        "        acc_test, loss_test = predict_test(int_test_padded, labels_test_padded, mask_test_padded, embedding_full, sess)\n",
        "\n",
        "        argmax_val = np.argmax(acc_val); best_val = np.max(acc_val)\n",
        "        val_accs3[i,j,k] = best_val\n",
        "        print(f\"Drop prob = {drop_prob}, Reg Coeff = {reg_coeff}, Nodes per Layer = {nodes_per_layer}\")\n",
        "        print(f\"Max validation accuracy obtained at epoch {np.argmax(acc_val)+1} with value of {np.max(acc_val)}\")\n",
        "        print(f\"Training accuracy at that epoch is {acc_train[argmax_val]}\")\n",
        "        print(f\"Accuracy and loss on test using the final trained model is {acc_test} and {loss_test} respectively\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42_YBzBpPuMA"
      },
      "source": [
        "# Print the result of the grid search, for the case where we do not train the word embeddings, and do not use projection matrix\n",
        "res = np.argwhere(val_accs == np.max(val_accs))\n",
        "print(res) # index of hyperparameters that achieves the highest maximum validation accuracy\n",
        "print(val_accs) # maximum validation accuracies of every combination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaH_5Zs1xedZ",
        "outputId": "38e97467-3ac9-4623-adab-b85b5fd78248"
      },
      "source": [
        "# Print the result of the grid search, for the case where we do not train the word embeddings, and use projection matrix\n",
        "res2 = np.argwhere(val_accs2 == np.max(val_accs2))\n",
        "print(res2) # index of hyperparameters that achieves the highest maximum validation accuracy\n",
        "print(val_accs2) # maximum validation accuracies of every combination"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 1]\n",
            " [0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Tqgiyh3kBI"
      },
      "source": [
        "# Print the result of the grid search, for the case where we do train the word embeddings, and do not use projection matrix\n",
        "res3 = np.argwhere(val_accs3 == np.max(val_accs3))\n",
        "print(res3) # index of hyperparameters that achieves the highest maximum validation accuracy\n",
        "print(val_accs3) # maximum validation accuracies of every combination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D4wGg96q38N",
        "outputId": "21080608-dc2c-4f75-e749-70757fc9c0fb"
      },
      "source": [
        "# Print the additional sentences (after they have been grouped due to padding lengths)\n",
        "full_sentences = []\n",
        "for group in int_add_test_padded:\n",
        "  for sentence in group:\n",
        "    sent = []\n",
        "    for word in sentence:\n",
        "      if word >= embedding_mat.shape[0] + embedding_mat_val.shape[0]:\n",
        "        sent.append(list(integer_dict_test.keys())[list(integer_dict_test.values()).index(word)]) \n",
        "      elif word >= embedding_mat.shape[0]:\n",
        "        sent.append(list(integer_dict_val.keys())[list(integer_dict_val.values()).index(word)])\n",
        "      else:\n",
        "        sent.append(list(integer_dict_train.keys())[list(integer_dict_train.values()).index(word)])\n",
        "    full_sentences.append(sent)\n",
        "\n",
        "for sentences in full_sentences:\n",
        "  print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bitter', 'disappointment', 'awful', 'ending', 'UNK', 'brilliant', 'trilogy', 'Rock']\n",
            "['Dark', 'Knight', 'Rises', 'was', 'perfect', 'sequel', 'UNK', 'masterpiece']\n",
            "['lame', 'spy', 'spoof', 'no', 'real', 'laughs', 'or', 'surprises']\n",
            "['not', 'The', 'Worst', 'Brit', 'Comedy', 'You', 'Will', 'See']\n",
            "['christopher', 'Nolans', 'second', 'bundle', 'joy', 'The', 'Dark', 'Knight', 'exceeded', 'all', 'UNK', 'my', 'expectations', 'Rock', 'Rock']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI5z0vZftS9K",
        "outputId": "569472c1-54ba-4d25-8d5e-9ada2d2f4d74"
      },
      "source": [
        "# Predict using the final trained model, and print the predicted ratings of the above additional sentences\n",
        "add_predictions = []\n",
        "for group_id in range(len(int_add_test_padded)):\n",
        "  if len(int_add_test_padded[group_id]) > 0:\n",
        "    feed_pred = {model.sent : int_add_test_padded[group_id],  model.isTrain : False, model.mask : mask_add_test_padded[group_id], \n",
        "                        model.embedder_infer : embedding_full, model.n_dat : len(int_add_test_padded[group_id])}\n",
        "    preds = sess.run(model.predictions, feed_dict = feed_pred)\n",
        "    add_predictions.append(preds)\n",
        "\n",
        "\n",
        "\n",
        "print(add_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0, 4, 0, 1]), array([4])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPvGIwHA-Vos"
      },
      "source": [
        "# Plot auxillary function for the trained model\n",
        "def plot_acc_loss(acc_train, loss_train, acc_val, loss_val):\n",
        "    fig,ax = plt.subplots(2,2, figsize = (15,10))\n",
        "    ax[0][0].plot(range(len(acc_train)), acc_train)\n",
        "    ax[0][0].set_title('Training accuracy')\n",
        "    ax[0][0].set_xlabel('Epoch')\n",
        "    ax[0][0].set_ylabel('Accuracy')\n",
        "    ax[0][0].grid()\n",
        "    \n",
        "    ax[0][1].plot(range(len(loss_train)), loss_train)\n",
        "    ax[0][1].set_title('Training Loss')\n",
        "    ax[0][1].set_xlabel('Epoch')\n",
        "    ax[0][1].set_ylabel('Accuracy')\n",
        "    ax[0][1].grid()\n",
        "    \n",
        "    ax[1][0].plot(range(len(acc_val)), acc_val)\n",
        "    ax[1][0].set_title('Validation accuracy')\n",
        "    ax[1][0].set_xlabel('Epoch')\n",
        "    ax[1][0].set_ylabel('Accuracy')\n",
        "    ax[1][0].grid()\n",
        "    \n",
        "    ax[1][1].plot(range(len(loss_val)), loss_val)\n",
        "    ax[1][1].set_title('Validation loss')\n",
        "    ax[1][1].set_xlabel('Epoch')\n",
        "    ax[1][1].set_ylabel('Accuracy')\n",
        "    ax[1][1].grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "c4k-nhQS-Vos",
        "outputId": "ee3fc1b0-925a-4ab8-b00a-fb47c276bd03"
      },
      "source": [
        "# Template to print the training results of the trained model\n",
        "plot_acc_loss(acc_train, loss_train, acc_val, loss_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}