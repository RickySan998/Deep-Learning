{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Python Tensor 1 v4.2] Task 3 Sequence Prediction",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ5n8l6VpykE",
        "outputId": "0e7ef994-5ae1-4ab3-d46c-563cba26daa1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsRESGOODxOm",
        "outputId": "40f56911-43eb-4cd7-b097-62752b306138"
      },
      "source": [
        "cd 'gdrive/MyDrive/MSc DSML//UCL/Main/COMP0090 CW + Lab/Assignment 2/Task 3 checkpoints/'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/MSc DSML/UCL/Main/COMP0090 CW + Lab/Assignment 2/Task 3 checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZyiZ7d9qD7O"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsEmkpS__Szm",
        "outputId": "c8410d4f-3176-46c3-e6b6-48c2b882711c"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "print(tf.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0fYuen5sa6EA",
        "outputId": "cc51fb12-2da1-4e2b-dbb2-8803166bbc69"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf8U7GavAaxT"
      },
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrbTxeqaAye-",
        "outputId": "0b2e0c50-c288-4187-e028-6da8394b3f2e"
      },
      "source": [
        "# Download the dataset\n",
        "url = \"https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\"\n",
        "try:\n",
        "    local_filename, headers = urllib.request.urlretrieve(url, \"trainDevTestTrees_PTB.zip\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "print(local_filename)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainDevTestTrees_PTB.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHrYJi9cBRG2",
        "outputId": "51f62db5-7486-4500-c39f-56911f08ef2d"
      },
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(local_filename, 'r') as my_zip:\n",
        "    my_zip.extractall(\"trainDevTestTrees_PTB\")\n",
        "os.listdir(\"trainDevTestTrees_PTB\") # List down the extracted data files"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trees']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1q47OjNCjHd"
      },
      "source": [
        "# Load Data\n",
        "def loadsst(path):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  file = open(path, \"r\")\n",
        "  for line in file:\n",
        "    soup = line.split()\n",
        "    ys.append(soup[0].lstrip(\"(\"))\n",
        "    tokens = []\n",
        "    for chunk in soup[1:]:\n",
        "      if chunk.endswith(\")\"): \n",
        "        tokens.append(chunk.rstrip(\")\"))\n",
        "    tokens.append(\"\\n\") # Retain enter character at the end of each line\n",
        "    xs.append(tokens)\n",
        "  return (xs, ys)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPiXdFvxFMcb"
      },
      "source": [
        "ssttrainxs, _ = loadsst(\"trainDevTestTrees_PTB/trees/train.txt\")\n",
        "sstvalidxs, _ = loadsst(\"trainDevTestTrees_PTB/trees/dev.txt\")\n",
        "ssttestxs, _ = loadsst(\"trainDevTestTrees_PTB/trees/test.txt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guxaZUl8AMY-"
      },
      "source": [
        "def get_everything_in_one_char_str(str_set,chars_needed):\n",
        "  final_str = \"\"\n",
        "  for i in range(len(str_set)):\n",
        "    final_str += \" \" + \" \".join(str_set[i])\n",
        "  final_str = final_str.lstrip() # Remove the leftmost space\n",
        "  final_str = ''.join([char for char in final_str if char in chars_needed])\n",
        "  return final_str"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk-c4mUc7GYY"
      },
      "source": [
        "chars_needed = ['\\n', ' ', '!', '\\'', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmHNkJvKTebz"
      },
      "source": [
        "end_char = '_'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcFmQQnAk1a"
      },
      "source": [
        "text_train = get_everything_in_one_char_str(ssttrainxs,chars_needed)\n",
        "text_val = get_everything_in_one_char_str(sstvalidxs,chars_needed)\n",
        "text_test = get_everything_in_one_char_str(ssttestxs,chars_needed)\n",
        "text_total = text_train + text_val + text_test + end_char"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfCgBboVC18r"
      },
      "source": [
        "# Get a set of unique characters\n",
        "unique_char_list = list(set(text_total))\n",
        "unique_char_list.sort()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR3gmhY77PNG"
      },
      "source": [
        "# Hyper-parameters\n",
        "batch_size = 1024\n",
        "seq_len = 32\n",
        "learning_rate = 0.01\n",
        "lambd = 0.05\n",
        "keep_prob = 0.9\n",
        "temperature = 0.5\n",
        "convergence_threshold = 0.0001\n",
        "max_epoch = 100\n",
        "class_count = len(unique_char_list)\n",
        "##\n",
        "RNN_layer_option = \"LSTM\" # \"Vanilla\",\"LSTM\",\"GRU\"\n",
        "optimizer_option = \"Adam\" # \"Adam\",\"SGD\",\"RMSProp\",\"Adagrad\"\n",
        "##\n",
        "rnn_layers = 2\n",
        "rnn_units = [128,128]\n",
        "assert rnn_layers == len(rnn_units)\n",
        "##\n",
        "depths = 2\n",
        "layer_sizes = [128,class_count]\n",
        "assert depths == len(layer_sizes)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfePC4gpkSEL"
      },
      "source": [
        "def prepare_data(text,seq_len):\n",
        "  seqs = []\n",
        "\n",
        "  idx = []\n",
        "  for i in range(0, len(text)-seq_len-1, 1):\n",
        "    idx.append(i)\n",
        "  # random.shuffle(idx)\n",
        "  for i in idx:\n",
        "    seq = text[i:i+seq_len+1]\n",
        "    seqs.append(seq)\n",
        "  return seqs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHsQCDyF4Rv8"
      },
      "source": [
        "train_seq = prepare_data(text_train,seq_len)\n",
        "val_seq = prepare_data(text_val,seq_len)\n",
        "test_seq = prepare_data(text_test,seq_len)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRsTWycS7UMS",
        "outputId": "6202fa37-5e8f-4bcc-e9cb-0b60b3d6a5bc"
      },
      "source": [
        "print(len(train_seq),len(val_seq),len(test_seq))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "899602 116853 232862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9FpjmapYn4n"
      },
      "source": [
        "def trunc(seqs,batch_size):\n",
        "  amount_to_retain = (len(seqs)//batch_size)*batch_size\n",
        "  return seqs[0:amount_to_retain]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUpN9ypSuH0r"
      },
      "source": [
        "# Ensure size of datasets can be divided exactly by batch_size\n",
        "train_seq = trunc(train_seq,batch_size)\n",
        "val_seq = trunc(val_seq,batch_size)\n",
        "test_seq = trunc(test_seq,batch_size)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_ObrJgqqqdZ",
        "outputId": "d0369c16-f524-412f-b204-7b11fd359e29"
      },
      "source": [
        "print(len(train_seq),len(val_seq),len(test_seq))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "899072 116736 232448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGeA_p2X7W-H"
      },
      "source": [
        "def onehot(x,y,seqs,unique_char_list):\n",
        "\n",
        "  for i,seq in enumerate(seqs):\n",
        "    input = seq[:-1]\n",
        "    output = seq[-1]\n",
        "    for j,char in enumerate(input):\n",
        "      x[i,j,unique_char_list.index(char)] = 1 # Turn the encoding of a character at the found position (in the unique list) to 1 \n",
        "    y[i] = unique_char_list.index(output)\n",
        "\n",
        "  return x,y"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWRX7AvtfD1Q"
      },
      "source": [
        "def onehot_for_sampling(x,y,seqs,unique_char_list):\n",
        "\n",
        "  for i,seq in enumerate(seqs):\n",
        "    input = seq\n",
        "    output = seq[-1]\n",
        "    for j,char in enumerate(input):\n",
        "      x[i,j,unique_char_list.index(char)] = 1 # Turn the encoding of a character at the found position (in the unique list) to 1 \n",
        "    y[i] = unique_char_list.index(output)\n",
        "\n",
        "  return x,y"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjw4pBtQ91NI"
      },
      "source": [
        "def build_model(rnn_units, class_count, rnn_layers, learning_rate, RNN_layer_option\n",
        "                , batch_size, seq_len, optimizer_option, lambd, keep_prob, depths, layer_sizes):  \n",
        "    \n",
        "  # Placeholders\n",
        "  x = tf.placeholder(tf.float32, [batch_size, seq_len, class_count], name='placeholder_x')\n",
        "  y = tf.placeholder(tf.int64, [batch_size], name='placeholder_y')\n",
        "  seq_size = tf.placeholder(tf.int32, [batch_size], name='placeholder_seq_size')\n",
        "  decayed_lr = tf.placeholder(tf.float64, name='placeholder_decayed_lr')\n",
        "  is_training = tf.placeholder(tf.bool, name='placeholder_is_training')\n",
        "  is_sampling = tf.placeholder(tf.bool, name='placeholder_is_sampling')\n",
        "  sampling_index = tf.placeholder(tf.int32, name='placeholder_sampling_index')\n",
        "\n",
        "  # RNN Cells\n",
        "  layers = []\n",
        "  for size in rnn_units:\n",
        "    if RNN_layer_option == \"Vanilla\":\n",
        "      layer = tf.nn.rnn_cell.BasicRNNCell(size)\n",
        "    elif RNN_layer_option == \"LSTM\":\n",
        "      layer = tf.nn.rnn_cell.LSTMCell(size)\n",
        "    elif RNN_layer_option == \"GRU\":\n",
        "      layer = tf.nn.rnn_cell.GRUCell(size)   \n",
        "    keep_prob = tf.cond(is_training, lambda:keep_prob, lambda:1.0)\n",
        "    layers.append(tf.nn.rnn_cell.DropoutWrapper(cell=layer,output_keep_prob=keep_prob))\n",
        "  cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
        "  output, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=x, sequence_length=seq_size, dtype=tf.float32)\n",
        "  index = tf.cond(is_sampling, lambda:sampling_index, lambda:tf.constant(-1))\n",
        "  output = tf.reshape(output[:,index,:],[batch_size,rnn_units[-1]])\n",
        "\n",
        "  # Hidden Layers\n",
        "  for i in range(depths-1):\n",
        "    output = tf.compat.v1.layers.dense(output, layer_sizes[i], activation = 'relu')      \n",
        " \n",
        "  # Output (Softmax) Layer\n",
        "  pred = tf.compat.v1.layers.dense(output, layer_sizes[-1])\n",
        "\n",
        "  # Loss\n",
        "  loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
        "  # loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
        "  \n",
        "  # Apply L2 regularization (only to training time)\n",
        "  l2_norms = [tf.nn.l2_loss(v) for v in tf.trainable_variables()]\n",
        "  l2_norm = tf.reduce_sum(l2_norms)\n",
        "  loss = (loss + lambd*l2_norm)/batch_size\n",
        "\n",
        "  # Select optimizer\n",
        "  if optimizer_option == \"Adam\":\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=decayed_lr)\n",
        "  elif optimizer_option == \"SGD\":\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=decayed_lr)\n",
        "  elif optimizer_option == \"RMSProp\":\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=decayed_lr)\n",
        "  elif optimizer_option == \"Adagrad\":\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate=decayed_lr)\n",
        "    \n",
        "  # Gradient clipping\n",
        "  # gradients_variables = optimizer.compute_gradients(loss)\n",
        "  # clipped_gradients_variables = [(tf.clip_by_value(gradients, -1.0, 1.0), variables) for gradients, variables in gradients_variables]\n",
        "  # train_op = optimizer.apply_gradients(clipped_gradients_variables)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "\n",
        "  # Evaluation\n",
        "  prediction = tf.reshape(tf.argmax(pred, 1),[batch_size])\n",
        "  expectation = y\n",
        "  matches = tf.equal(prediction, expectation)\n",
        "  accuracy = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
        "  total_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
        "  total_matches = tf.reduce_sum(tf.cast(matches, tf.float32))\n",
        "\n",
        "  return {\n",
        "        \"x\": x\n",
        "        , \"y\": y\n",
        "        , \"final_state\": final_state\n",
        "        , \"loss\": loss\n",
        "        , \"accuracy\": accuracy\n",
        "        , \"total_loss\": total_loss\n",
        "        , \"total_matches\": total_matches\n",
        "        , \"optimizer\": train_op\n",
        "        , \"sequence_size\": seq_size\n",
        "        , \"prediction\": prediction\n",
        "        , \"expectation\": expectation\n",
        "        , \"posterior_pmf\": pred\n",
        "        , \"decayed_lr\":decayed_lr\n",
        "        , \"output\": output\n",
        "        , \"is_training\": is_training\n",
        "        , \"is_sampling\": is_sampling\n",
        "        , \"sampling_index\": sampling_index\n",
        "      }"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTKKJj4iDy4H"
      },
      "source": [
        "def train_model(session, model, train_seq, seq_len, batch_size, step, learning_rate, decay_rate=1):\n",
        "\n",
        "  decayed_lr = learning_rate*((decay_rate)**(step/10)) # Learning rate decay\n",
        "  batch = 1\n",
        "  total_loss = 0.0\n",
        "  total_matches = 0.0\n",
        "  idx = []\n",
        "  for i in range(0,len(train_seq),batch_size):\n",
        "    idx.append(i)\n",
        "  random.shuffle(idx)\n",
        "  for i in idx:\n",
        "    batch_train_seq = train_seq[i:i+batch_size]\n",
        "    trainx_zeros = np.zeros((batch_size,seq_len,class_count))\n",
        "    trainy_zeros = np.zeros((batch_size))\n",
        "    x, y = onehot(trainx_zeros,trainy_zeros,batch_train_seq,unique_char_list)\n",
        "    input_dict = {\n",
        "              model['x']: x\n",
        "              , model['y']: y\n",
        "              , model['sequence_size']: np.array([seq_len]*batch_size, dtype=np.int32)\n",
        "              , model['decayed_lr']: decayed_lr\n",
        "              , model['is_training']: True\n",
        "              , model['is_sampling']: False\n",
        "              , model['sampling_index']: -1\n",
        "            }    \n",
        "    optimizer, prediction, expectation, loss, matches = session.run(fetches=[model[\"optimizer\"],model[\"prediction\"],model[\"expectation\"],model[\"total_loss\"],model[\"total_matches\"]], feed_dict=input_dict)   \n",
        "    total_loss += loss\n",
        "    total_matches += matches\n",
        "    # print(\"Batch {}/{}\".format(batch,len(idx))+\": Prediction: \"+str(prediction)+\"| Expectation: \"+str(expectation), end=\"\\r\")\n",
        "    # print(\"Batch {}/{}\".format(batch,len(idx)), end=\"\\r\")\n",
        "    # print(\"Batch {}/{}\".format(batch,len(idx)))\n",
        "    batch += 1\n",
        "\n",
        "  total_samples = len(train_seq)\n",
        "\n",
        "  return total_loss/total_samples, total_matches/total_samples"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCyDwpfQwsk3"
      },
      "source": [
        "def run_model(session, model, seq, seq_len, batch_size):\n",
        "\n",
        "  total_loss = 0.0\n",
        "  total_matches = 0.0\n",
        "  \n",
        "  for i in range(0,len(seq),batch_size):\n",
        "    batch_seq = seq[i:i+batch_size]\n",
        "    x_zeros = np.zeros((batch_size,seq_len,class_count))\n",
        "    y_zeros = np.zeros((batch_size))\n",
        "    x, y = onehot(x_zeros,y_zeros,batch_seq,unique_char_list)\n",
        "    input_dict = {\n",
        "              model['x']: x\n",
        "              , model['y']: y\n",
        "              , model['sequence_size']: np.array([seq_len]*batch_size, dtype=np.int32)\n",
        "              , model['is_training']: False\n",
        "              , model['is_sampling']: False\n",
        "              , model['sampling_index']: -1\n",
        "            }  \n",
        "\n",
        "    loss, matches, prediction = session.run(fetches=[model[\"total_loss\"],model[\"total_matches\"],model[\"prediction\"]], feed_dict=input_dict)    \n",
        "    total_loss += loss\n",
        "    total_matches += matches\n",
        "    \n",
        "  total_samples = len(seq)\n",
        "    \n",
        "  return total_loss/total_samples, total_matches/total_samples"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VH023pBbSqU"
      },
      "source": [
        "def predict_posterior(session, model, x, y, seq_len, batch_size, sampling_index):\n",
        "\n",
        "  input_dict = {\n",
        "            model['x']: x\n",
        "            , model['y']: y\n",
        "            , model['sequence_size']: np.array([seq_len]*batch_size, dtype=np.int32)\n",
        "            , model['is_training']: False\n",
        "            , model['is_sampling']: True\n",
        "            , model['sampling_index']: sampling_index\n",
        "          }  \n",
        "\n",
        "  posterior_pmf = session.run(fetches=[model[\"posterior_pmf\"]], feed_dict=input_dict)\n",
        "    \n",
        "  return posterior_pmf[0][0]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKg_nHZSVoWf"
      },
      "source": [
        "def sampling(prediction_pmf,temperature,unique_char_list):\n",
        "\n",
        "  # Randomly generate a character based on the posterior distribution of the previous character\n",
        "  if temperature == 1: # Control the scale of probabilities of different categories\n",
        "    prediction = np.random.choice(unique_char_list, p=prediction_pmf)\n",
        "  else:\n",
        "    prediction_pmf = np.log(prediction_pmf)/temperature\n",
        "    prediction_pmf = np.exp(prediction_pmf)\n",
        "    prediction_pmf = prediction_pmf/np.sum(prediction_pmf) # Normalize\n",
        "    prediction = np.random.choice(unique_char_list, p=prediction_pmf)\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBBwIGLwCte3"
      },
      "source": [
        "def predict_setences(seq_to_pred,model,session,unique_char_list,seq_len,batch_size):\n",
        "\n",
        "  sampling_index = -1\n",
        "  if len(seq_to_pred) < seq_len:\n",
        "    sentence_generate = seq_to_pred + (seq_len-len(seq_to_pred))*end_char # Padding\n",
        "    sampling_index = len(seq_to_pred) - 1\n",
        "  else:\n",
        "    sentence_generate = seq_to_pred\n",
        "    sampling_index = -1\n",
        "  sentence_generate = sentence_generate[-1-seq_len:]\n",
        "  result = seq_to_pred\n",
        "\n",
        "  while True: \n",
        "    x_zeros = np.zeros((batch_size,seq_len,class_count)) # Generate one-hot enconding matrix of the character (with batch padding if size < batch_size)\n",
        "    y_zeros = np.zeros((batch_size))\n",
        "    for i in range(seq_len):\n",
        "      x_zeros[:,i,unique_char_list.index(sentence_generate[i])] = 1\n",
        "    x = x_zeros\n",
        "    dummy_y = y_zeros\n",
        "    prediction_pmf = predict_posterior(session, model, x, dummy_y, seq_len, batch_size, sampling_index) # Generate the posterior distribution  \n",
        "    prediction_pmf = np.exp(prediction_pmf)/sum(np.exp(prediction_pmf))\n",
        "    next_char = sampling(prediction_pmf,temperature,unique_char_list)\n",
        "    # next_char = unique_char_list[np.argmax(prediction_pmf)]\n",
        "    result += next_char\n",
        "    if len(result) < seq_len:\n",
        "      sentence_generate = result + (seq_len-len(result))*end_char # Padding\n",
        "      sampling_index = len(result) - 1\n",
        "    else:\n",
        "      sentence_generate = result\n",
        "      sampling_index = -1\n",
        "    sentence_generate = sentence_generate[-1-seq_len:]\n",
        "    if next_char == \".\":\n",
        "      break\n",
        "\n",
        "  return result"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P8zXuxLVqUe"
      },
      "source": [
        "seq_to_pred = \"I have a pen, I have an app \""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNQLgjr0ZiH"
      },
      "source": [
        "restart = True\n",
        "\n",
        "if restart:\n",
        "  model_prefix = 'task3_model_v4.2_checkpoint_'\n",
        "  latest_epoch = 0\n",
        "  starting_epoch = 0\n",
        "else:\n",
        "  # Get the latest checkpoint\n",
        "  model_prefix = 'task3_model_v4.2_checkpoint_'\n",
        "  files = os.listdir()\n",
        "  matched_files = [file for file in files if model_prefix in file]\n",
        "  latest_epoch = max([int(matched_file.replace(model_prefix, \"\")[0]) for matched_file in matched_files])\n",
        "  starting_epoch = latest_epoch"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykrqgm3EK731",
        "outputId": "9ca1ae8e-df22-4620-9166-ee368bae1242"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trainDevTestTrees_PTB.zip', 'trainDevTestTrees_PTB']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am90q5pCCtNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbfb2507-2eb6-4981-d91f-f5b4fc6d7265"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    # Prepare performance measures\n",
        "    train_acc = []\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "    prev_train_loss = 100.0\n",
        "    best_val_loss = 100\n",
        "    sentences_generate = []\n",
        "\n",
        "    # Start new session\n",
        "    tf.reset_default_graph()\n",
        "    model = build_model(rnn_units, class_count, rnn_layers, learning_rate, RNN_layer_option\n",
        "                        , batch_size, seq_len, optimizer_option, lambd, keep_prob, depths, layer_sizes)\n",
        "    init = tf.global_variables_initializer()\n",
        "    session = tf.Session()\n",
        "    session.run(init)\n",
        "    saver = tf.train.Saver(tf.trainable_variables(),max_to_keep=3000)\n",
        "\n",
        "    if not restart:\n",
        "      # Load Existing Model\n",
        "      saver.restore(session,model_prefix+str(latest_epoch)) \n",
        "      print(\"Loading weights success\")  \n",
        "\n",
        "    # Training\n",
        "    for epoch in range(starting_epoch,max_epoch):\n",
        "      print(\"#============= Epoch {} =============#\".format(epoch+1))\n",
        "      start_time = time.time()\n",
        "      checkpoint_name = model_prefix+str(epoch+1)\n",
        "\n",
        "      current_train_loss, current_train_acc = train_model(session, model, train_seq, seq_len, batch_size, epoch+1, learning_rate)\n",
        "      current_val_loss, current_val_acc = run_model(session, model, val_seq, seq_len, batch_size)\n",
        "      train_acc.append(current_train_acc)\n",
        "      train_loss.append(current_train_loss)\n",
        "      val_acc.append(current_val_acc)\n",
        "      val_loss.append(current_val_loss)\n",
        "      print(\"Train Loss: {} Train Accuracy: {} \".format(current_train_loss, current_train_acc))\n",
        "      print(\"Validation Loss: {} Validation Accuracy: {} \".format(current_val_loss, current_val_acc))\n",
        "\n",
        "      end_time = time.time()\n",
        "      print(\"--- %s seconds ---\" % (end_time - start_time))\n",
        "\n",
        "      # Generate text\n",
        "      # if current_val_acc >= 0.5:\n",
        "      #   sentence_generated = predict_setences(seq_to_pred,model,session,unique_char_list,seq_len,batch_size)\n",
        "      #   sentences_generate.append(sentence_generated)\n",
        "      #   print(\"Sentences Generated:\")\n",
        "      #   print(sentence_generated)\n",
        "\n",
        "      # Convergence rule\n",
        "      # if (abs(current_train_loss-prev_train_loss) < convergence_threshold) or ((current_val_loss-best_val_loss)/best_val_loss > 0.1):\n",
        "      if abs(current_train_loss-prev_train_loss) < convergence_threshold:\n",
        "        break\n",
        "      else:\n",
        "        prev_train_loss = current_train_loss\n",
        "        if current_val_loss < best_val_loss:\n",
        "          best_val_loss = current_val_loss  \n",
        "      \n",
        "      saver.save(session, checkpoint_name)\n",
        "      print('saved '+checkpoint_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#============= Epoch 1 =============#\n",
            "Train Loss: 2.174068037768431 Train Accuracy: 0.3706955616457859 \n",
            "Validation Loss: 1.7836087387904787 Validation Accuracy: 0.4658460115131579 \n",
            "--- 83.15674352645874 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_1\n",
            "#============= Epoch 2 =============#\n",
            "Train Loss: 1.7762447569799316 Train Accuracy: 0.4693695276907745 \n",
            "Validation Loss: 1.6630679599025793 Validation Accuracy: 0.49935752467105265 \n",
            "--- 81.4774718284607 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_2\n",
            "#============= Epoch 3 =============#\n",
            "Train Loss: 1.6985327628587537 Train Accuracy: 0.49008866920558086 \n",
            "Validation Loss: 1.6077603718690705 Validation Accuracy: 0.5158391584429824 \n",
            "--- 81.00032997131348 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_3\n",
            "#============= Epoch 4 =============#\n",
            "Train Loss: 1.6671913689795823 Train Accuracy: 0.4998086916287016 \n",
            "Validation Loss: 1.5856042577509295 Validation Accuracy: 0.5230263157894737 \n",
            "--- 81.14301776885986 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_4\n",
            "#============= Epoch 5 =============#\n",
            "Train Loss: 1.6472315925400458 Train Accuracy: 0.5052787763382688 \n",
            "Validation Loss: 1.5721461762461746 Validation Accuracy: 0.528234649122807 \n",
            "--- 81.3080370426178 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_5\n",
            "#============= Epoch 6 =============#\n",
            "Train Loss: 1.6356142221659353 Train Accuracy: 0.5085844070330297 \n",
            "Validation Loss: 1.5595037550256963 Validation Accuracy: 0.5302820038377193 \n",
            "--- 80.46806597709656 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_6\n",
            "#============= Epoch 7 =============#\n",
            "Train Loss: 1.6266143419748016 Train Accuracy: 0.5107043707289294 \n",
            "Validation Loss: 1.5557669662592704 Validation Accuracy: 0.5327748081140351 \n",
            "--- 81.73823189735413 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_7\n",
            "#============= Epoch 8 =============#\n",
            "Train Loss: 1.6210157986653964 Train Accuracy: 0.5122871138240319 \n",
            "Validation Loss: 1.54321845581657 Validation Accuracy: 0.536047149122807 \n",
            "--- 81.63467192649841 seconds ---\n",
            "saved task3_model_v4.2_checkpoint_8\n",
            "#============= Epoch 9 =============#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJp2Qw96FLV9"
      },
      "source": [
        "print(train_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QpNuBfnFPZN"
      },
      "source": [
        "print(train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd2fXB-OFQ3W"
      },
      "source": [
        "print(val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7GtpM--FR_c"
      },
      "source": [
        "print(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aKrGeJuf-fi"
      },
      "source": [
        "seqs_to_pred = [\n",
        "    \"This is a nice mov\"\n",
        "    , \"I love trave\"\n",
        "    , \"Life is tou\"\n",
        "    , \"There is nothing more intere\"\n",
        "    , \"Wish you happ\"\n",
        "]\n",
        "\n",
        "# Generate text based on the final model\n",
        "for seq in seqs_to_pred:\n",
        "  print(\"By Human: \"+seq)\n",
        "  print(\"By Robot: \"+predict_setences(seq,model,session,unique_char_list,seq_len,batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5CBKPrPY8VQ"
      },
      "source": [
        "# Evaluation\n",
        "label = 'Train'\n",
        "loss, acc = run_model(session, model, train_seq, seq_len, batch_size)\n",
        "print('{} Loss:'.format(label), loss)\n",
        "print('{} Accuracy:'.format(label), acc)  \n",
        "\n",
        "label = 'Validation'\n",
        "loss, acc = run_model(session, model, val_seq, seq_len, batch_size)\n",
        "print('{} Loss:'.format(label), loss)\n",
        "print('{} Accuracy:'.format(label), acc) \n",
        "\n",
        "label = 'Test'\n",
        "loss, acc = run_model(session, model, test_seq, seq_len, batch_size)\n",
        "print('{} Loss:'.format(label), loss)\n",
        "print('{} Accuracy:'.format(label), acc) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slDJdnAroYDU"
      },
      "source": [
        "# Loss Plots\n",
        "x = list(range(len(train_loss)))\n",
        "plt.plot(x, train_loss, label = \"Train Loss\") \n",
        "plt.plot(x, val_loss, label = \"Validation Loss\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"assignment_2_task_3_loss_result_v2.jpg\")\n",
        "files.download(\"assignment_2_task_3_loss_result_v2.jpg\") \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSzNUfeqoZ8u"
      },
      "source": [
        "# Accuracy Plots\n",
        "x = list(range(len(train_acc)))\n",
        "plt.plot(x, train_acc, label = \"Train Accuracy\") \n",
        "plt.plot(x, val_acc, label = \"Validation Accuracy\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"assignment_2_task_3_acc_result_v2.jpg\")\n",
        "files.download(\"assignment_2_task_3_acc_result_v2.jpg\") \n",
        "plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9XWJP86ZFsx"
      },
      "source": [
        "# End session\n",
        "session.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}