{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V6e5ysOElNZb",
        "SBc2HzA3mw3C",
        "Nant_Oj-ncAi",
        "ZnwYQ-9Wn54B",
        "u8lJiCXIoWti",
        "5sFlwlctopvL",
        "_4TYvvtMr1qg",
        "Vlo-JxT2mn3b",
        "ggY602k9QYtQ",
        "VKNybY3XQ7kI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7H9jreHW8gy"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import seaborn as sb\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQM3QeuNbSoH"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtUrXXOWbjcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb274f8-c683-4e0b-f2e6-c6c462ce6a9f"
      },
      "source": [
        "((x_dat, y_dat), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE745PTuXlph"
      },
      "source": [
        "def process_labels_SL(data_samples, data_labels):\n",
        "\n",
        "    m = data_samples.shape[0]\n",
        "    # normalize train and test images\n",
        "    mean_dat = np.mean(data_samples, axis = 0).reshape(1,-1); \n",
        "    sd_data = np.std(data_samples, axis = 0).reshape(1,-1)\n",
        "    new_train = (data_samples - mean_dat) / sd_data\n",
        "\n",
        "    # one-hot encode the labels\n",
        "    max_label = 9 # there are 10 labels, 0 to 9 inclusive\n",
        "    y_onehot_train = np.zeros((m,max_label+1))\n",
        "\n",
        "    y_onehot_train[np.arange(m),data_labels.astype(int)] = 1\n",
        "\n",
        "    # also return the non-one-hot encoded labels, to compare accuracy.\n",
        "    return data_samples, data_labels, new_train, y_onehot_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7_i3iahYm9j"
      },
      "source": [
        "data_samples, data_labels, new_samples, new_labels = process_labels_SL(data_samples, data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6qqerdjY1BF",
        "outputId": "7b5f9429-51aa-4062-b2f4-8dc42b3ff5e9"
      },
      "source": [
        "new_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9298, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ppZFNLt6IIe"
      },
      "source": [
        "# Get class 0 data\n",
        "class0_loc = np.argwhere(y_dat == 0)[:5].reshape(-1)\n",
        "xdat0 = x_dat[class0_loc,:,:]\n",
        "\n",
        "# Get class 6 data\n",
        "class6_loc = np.argwhere(y_dat == 6)[:5].reshape(-1)\n",
        "xdat6 = x_dat[class6_loc,:,:]\n",
        "\n",
        "# Get class 4 data\n",
        "class4_loc = np.argwhere(y_dat == 4)[:5].reshape(-1)\n",
        "xdat4 = x_dat[class4_loc,:,:]\n",
        "\n",
        "# Get class 2 data\n",
        "class2_loc = np.argwhere(y_dat == 2)[:5].reshape(-1)\n",
        "xdat2 = x_dat[class2_loc,:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "fmTtaUxP8CDk",
        "outputId": "1b7f31a8-94d8-47b0-9cb3-24719a40cb85"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat0[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "tx4Eyp8tCOof",
        "outputId": "2edf864e-08b8-44fe-8235-11c8def3dc48"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat2[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "CHFHbfrDCXjY",
        "outputId": "532e47db-fb2e-46bd-fa37-a62e200e1594"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat4[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "gF5DHByKCaR-",
        "outputId": "366c6e3d-b152-42bf-9c28-f88368cd6826"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat6[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRivMUPrbjn6"
      },
      "source": [
        "def reshape_data(data):\n",
        "    # reshape image data to columns\n",
        "    (m,d1,d2) = data.shape\n",
        "    temp = np.zeros((d1*d2,m))\n",
        "    for i in range(m):\n",
        "        temp[:,i] = data[i,:,:].flatten('F')\n",
        "    return temp\n",
        "\n",
        "def process_data():\n",
        "    ((x_dat, y_dat), (x_test, y_test)) = fashion_mnist.load_data()\n",
        "    \n",
        "    # split to training and val\n",
        "    x_train = x_dat[0:50000]; x_val = x_dat[50000:];\n",
        "    y_train = y_dat[0:50000]; y_val = y_dat[50000:];\n",
        "    \n",
        "    # reshape images to vectors\n",
        "    x_train = reshape_data(x_train).T # data shape is now nData x nDim\n",
        "    (m,dim) = x_train.shape\n",
        "    x_val = reshape_data(x_val).T\n",
        "    (m3,dim) = x_val.shape\n",
        "    x_test = reshape_data(x_test).T\n",
        "    (m2,dim) = x_test.shape\n",
        "\n",
        "    # normalize train and test images\n",
        "    mean_dat = np.mean(x_train, axis = 0).reshape(1,-1); \n",
        "    sd_data = np.std(x_train, axis = 0).reshape(1,-1)\n",
        "    x_train = (x_train - mean_dat) / sd_data\n",
        "    x_test = (x_test - mean_dat) / sd_data\n",
        "    x_val = (x_val - mean_dat) / sd_data\n",
        "\n",
        "    # one-hot encode the labels\n",
        "    max_label = 9 # there are 10 labels, 0 to 9 inclusive\n",
        "    y_onehot_train = np.zeros((m,max_label+1))\n",
        "    y_onehot_test = np.zeros((m2,max_label+1))\n",
        "    y_onehot_val = np.zeros((m3,max_label+1))\n",
        "\n",
        "    y_onehot_train[np.arange(m),y_train] = 1\n",
        "    y_onehot_test[np.arange(m2),y_test] = 1\n",
        "    y_onehot_val[np.arange(m3),y_val] = 1\n",
        "\n",
        "    # also return the non-one-hot encoded labels, to compare accuracy.\n",
        "    return x_train, x_test, x_val, y_onehot_train, y_onehot_test, y_onehot_val, y_train, y_test, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXV-Nqhv6CNv"
      },
      "source": [
        "# Reshape x_test to 3 dimensions\n",
        "x_test = np.reshape(x_test, (-1,28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ2-eJyD40m8"
      },
      "source": [
        "# Get class 0 test data\n",
        "class0_loc = np.argwhere(y_test == 0)[:5].reshape(-1)\n",
        "xdat0 = x_test[class0_loc,:,:]\n",
        "\n",
        "# Get class 6 test data\n",
        "class6_loc = np.argwhere(y_test == 6)[:5].reshape(-1)\n",
        "xdat6 = x_test[class6_loc,:,:]\n",
        "\n",
        "# Get class 4 test data\n",
        "class4_loc = np.argwhere(y_test == 4)[:5].reshape(-1)\n",
        "xdat4 = x_test[class4_loc,:,:]\n",
        "\n",
        "# Get class 2 test data\n",
        "class2_loc = np.argwhere(y_test == 2)[:5].reshape(-1)\n",
        "xdat2 = x_test[class2_loc,:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "WiYZnnKi40nH",
        "outputId": "ab95d689-04a4-45d1-a7ca-72eaa5725302"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat0[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "07J8ygpv40nI",
        "outputId": "bd7e3558-230a-4a40-f4f9-455440efd5f0"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat2[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "ET5pEzhU40nJ",
        "outputId": "cef0e537-35cb-4ac3-cee0-41b952205808"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat4[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "sUtNNLfz40nK",
        "outputId": "67fccf54-95b7-47f2-c09a-764140d091cc"
      },
      "source": [
        "fig,ax = plt.subplots(1,5, figsize = (15,8))\n",
        "for i in range(5):\n",
        "  ax[i].imshow(xdat6[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJe35wslmoV"
      },
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
        "\n",
        "def build_model2(layers, nodes_per_layer, learn_rate, filter_bank, num_labels = 10, threshold = 0.5, hlactivation = \"tanh\", optimizer_name = \"Adam\"\n",
        ", reg_coeff = 0, drop_prob = 0):\n",
        "    \"\"\"\n",
        "    Constructs the model and return training, predict, loss evaluation, given the fed data and labels. This fed data and labels could\n",
        "    be training or test labels\n",
        "    inputs:\n",
        "        layers : (1x1) number of layers in the MLP\n",
        "        nodes_per_layer : (1 x L+1) array specifiying #nodes in each layer\n",
        "        learn_rate : (1x1) learning rate\n",
        "        num_labels: (1x1) number of classes in the problem, defaults to 10 as specificied\n",
        "        threshold: (1x1) threshold posterior to consider as class 1\n",
        "        hlactivation = (string) argument on the activation of hidden layers. Either \"tanh\", \"relu\" or \"sigmoid\". Assume all layers activation\n",
        "        are identical except the output layer which will always be softmax\n",
        "        optimizer_name : Name of the optimizer to use. Default is Adam Optimizer\n",
        "        reg_coeff : L2 Regularization coeff, default is 0 i.e. no regularization\n",
        "        drop_prob : Probability of drop out, defaults to 0 i.e. no dropout\n",
        "    \"\"\"\n",
        "    # Construct Model\n",
        "    # Here, we don't fix the batch size so that later, we can flexibly use the loss operator here to predict for whole data set\n",
        "    # x = tf.placeholder(tf.float32, shape = [None, nodes_per_layer[0]]) # to hold the fed in data\n",
        "    x = tf.placeholder(tf.float32, shape = [None, 16, 16, 1])\n",
        "    y = tf.placeholder(tf.int64, shape = [None, num_labels]) # to hold one-hot encodings of the labels\n",
        "    isTrain = tf.placeholder(tf.bool, shape = None) # to enable/ disable dropout\n",
        "\n",
        "    # Define Hidden Layers\n",
        "    # Convolutional layers\n",
        "    wc1 = tf.get_variable('W0', shape=(3,3,1,filter_bank), initializer=tf.initializers.glorot_uniform())\n",
        "    bc1 = tf.get_variable('B0', shape=(filter_bank), initializer=tf.initializers.glorot_uniform())\n",
        "\n",
        "    wc2 = tf.get_variable('W1', shape=(3,3,filter_bank,2*filter_bank), initializer=tf.initializers.glorot_uniform())\n",
        "    bc2 = tf.get_variable('B1', shape=(2*filter_bank), initializer=tf.initializers.glorot_uniform())\n",
        "    # wc1 = tf.Variable(initial_value = np.random.normal(size = (3,3,1,32)).astype(np.float32))\n",
        "    # bc1 = tf.Variable(initial_value = np.random.normal(size = (32)).astype(np.float32))\n",
        "    conv1 = conv2d(x, wc1, bc1)\n",
        "    conv1 = maxpool2d(conv1, k = 2)\n",
        "    conv2 = conv2d(conv1, wc2, bc2)\n",
        "    out = maxpool2d(conv2, k = 2)\n",
        "    out_shape = out.shape; out_flatten = out_shape[1] * out_shape[2] * out_shape[3]\n",
        "    # Reshape\n",
        "    out = tf.reshape(out, shape = [-1, out_flatten])\n",
        "\n",
        "    # Fully connected layers\n",
        "    for layer in range(1,len(nodes_per_layer)-1):\n",
        "        out_node = nodes_per_layer[layer]\n",
        "        if hlactivation == 'tanh':\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.tanh, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "        elif hlactivation == 'relu':\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.relu, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "        else:\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.sigmoid, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "    \n",
        "    # Define output layer\n",
        "    out = tf.layers.dense(out, num_labels, kernel_initializer =  tf.initializers.glorot_uniform, bias_initializer = tf.initializers.glorot_uniform) # create linear layer\n",
        "    out_softmax = tf.nn.softmax(out)\n",
        "\n",
        "    # Define loss\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out))\n",
        "\n",
        "    # Add L2 Regularization\n",
        "    L2norms = [tf.nn.l2_loss(weights) for weights in tf.trainable_variables()]\n",
        "    L2 = tf.reduce_sum(L2norms)\n",
        "    loss_L2 = loss + reg_coeff * L2\n",
        "\n",
        "    # Add Predictions\n",
        "    predictions = tf.reshape(tf.argmax(out,1), [-1]) # for predictions, can argmax over un-normalized probabilities, since softmax doesnt change max position\n",
        "\n",
        "    # Add Training Optimizer and Operation\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'SGD':\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'RMSProp':\n",
        "        optimizer = tf.train.RMSPropOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        optimizer = tf.train.AdagradOptimizer(learning_rate = learn_rate)\n",
        "\n",
        "    train_op = optimizer.minimize(loss_L2)\n",
        "\n",
        "    # return the model as a dictionary, containing the loss evaluator (without L2), loss (with L2), predictions (in terms of labels)\n",
        "    # train operation, and last layer output evaluator. ALso return the placeholders so we can feed later\n",
        "    model_dict = {'loss': loss\n",
        "                 ,'loss_L2': loss_L2\n",
        "                 ,'predictions': predictions\n",
        "                 ,'train_op': train_op\n",
        "                 ,'out': out_softmax\n",
        "                 , 'batch_size': batch_size\n",
        "                 , 'input': x\n",
        "                 , 'targets': y\n",
        "                 , 'isTrain': isTrain}\n",
        "    \n",
        "    return model_dict\n",
        "\n",
        "\n",
        "def train_model2(model, n_epochs, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh):\n",
        "    \"\"\"\n",
        "    Performs training for n_epochs, using the given model, training data. Also evaluates training and validation acc and loss\n",
        "    after every epoch.\n",
        "    \"\"\"\n",
        "    # create arrays to store loss and accuracy at the end of every epoch\n",
        "    train_acc_arr = []; train_loss_arr = []; val_acc_arr = []; val_loss_arr = []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Processing Epoch\", epoch + 1)\n",
        "        nData = x_train.shape[0]\n",
        "        \n",
        "        # Shuffle training data\n",
        "        permute = np.random.permutation(nData);\n",
        "        x_train = x_train[permute,:]\n",
        "        y_train = y_train[permute]\n",
        "        y_train_oh = y_train_oh[permute,:]\n",
        "        \n",
        "        # split to batches\n",
        "        if batch_size >= nData:\n",
        "            num_batch = 1\n",
        "        else:\n",
        "            num_batch = (nData // batch_size) + 1\n",
        "        \n",
        "        # train on each batch\n",
        "        for batch_ind in range(num_batch):\n",
        "            low_ind = (batch_ind) * batch_size\n",
        "            high_ind = min(low_ind + batch_size, nData)\n",
        "        \n",
        "            batch = x_train[low_ind:high_ind, :]\n",
        "            batch_lab = y_train_oh[low_ind:high_ind, :]\n",
        "\n",
        "            # create feed dictionary to predict\n",
        "            feed_train = {model['input']: batch,\n",
        "                         model['targets']: batch_lab,\n",
        "                         model['isTrain']: True}\n",
        "\n",
        "            # train\n",
        "            sess.run(model['train_op'], feed_dict = feed_train)\n",
        "            \n",
        "        # At the end of each epoch, evaluate acc and loss, append to array\n",
        "        feed_alltrain = {model['input']: x_train,\n",
        "                        model['targets']: y_train_oh,\n",
        "                        model['isTrain']: False}\n",
        "        \n",
        "        feed_val = {model['input']: x_val,\n",
        "                    model['targets']: y_val_oh,\n",
        "                   model['isTrain']: False}\n",
        "        \n",
        "        # Note: Change here to print L2 included loss as well\n",
        "        train_pred, train_loss = sess.run([model['predictions'], model['loss']], feed_dict = feed_alltrain)\n",
        "        train_acc = np.mean(train_pred == y_train)\n",
        "        \n",
        "        val_pred, val_loss = sess.run([model['predictions'], model['loss']], feed_dict = feed_val)\n",
        "        val_acc = np.mean(val_pred == y_val)\n",
        "        \n",
        "        print(\"Training acc and loss are\",train_acc, \"and\", train_loss) \n",
        "        print(\"Val acc and loss are\", val_acc,\"and\", val_loss) # for debugging\n",
        "        \n",
        "        # Append these information\n",
        "        train_acc_arr.append(train_acc); train_loss_arr.append(train_loss)\n",
        "        val_acc_arr.append(val_acc); val_loss_arr.append(val_loss)\n",
        "    \n",
        "    return train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ji0-sLUbjqj"
      },
      "source": [
        "def build_model(layers, nodes_per_layer, learn_rate, num_labels = 10, threshold = 0.5, hlactivation = \"tanh\", optimizer_name = \"Adam\"\n",
        ", reg_coeff = 0, drop_prob = 0):\n",
        "    \"\"\"\n",
        "    Constructs the model and return training, predict, loss evaluation, given the fed data and labels. This fed data and labels could\n",
        "    be training or test labels\n",
        "    inputs:\n",
        "        layers : (1x1) number of layers in the MLP\n",
        "        nodes_per_layer : (1 x L+1) array specifiying #nodes in each layer\n",
        "        learn_rate : (1x1) learning rate\n",
        "        num_labels: (1x1) number of classes in the problem, defaults to 10 as specificied\n",
        "        threshold: (1x1) threshold posterior to consider as class 1\n",
        "        hlactivation = (string) argument on the activation of hidden layers. Either \"tanh\", \"relu\" or \"sigmoid\". Assume all layers activation\n",
        "        are identical except the output layer which will always be softmax\n",
        "        optimizer_name : Name of the optimizer to use. Default is Adam Optimizer\n",
        "        reg_coeff : L2 Regularization coeff, default is 0 i.e. no regularization\n",
        "        drop_prob : Probability of drop out, defaults to 0 i.e. no dropout\n",
        "    \"\"\"\n",
        "    # Construct Model\n",
        "    # Here, we don't fix the batch size so that later, we can flexibly use the loss operator here to predict for whole data set\n",
        "    x = tf.placeholder(tf.float32, shape = [None, nodes_per_layer[0]]) # to hold the fed in data\n",
        "    y = tf.placeholder(tf.int64, shape = [None, num_labels]) # to hold one-hot encodings of the labels\n",
        "    isTrain = tf.placeholder(tf.bool, shape = None) # to enable/ disable dropout\n",
        "\n",
        "    # Define Hidden Layers\n",
        "    out = x\n",
        "    for layer in range(1,len(nodes_per_layer)-1):\n",
        "        out_node = nodes_per_layer[layer]\n",
        "        if hlactivation == 'tanh':\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.tanh, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "        elif hlactivation == 'relu':\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.relu, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "        else:\n",
        "            out = tf.layers.dense(out, out_node, activation = tf.nn.sigmoid, kernel_initializer =  tf.initializers.glorot_uniform,\n",
        "                                    bias_initializer = tf.initializers.glorot_uniform)\n",
        "            out = tf.layers.dropout(out, rate = drop_prob, training = isTrain)\n",
        "    \n",
        "    # Define output layer\n",
        "    out = tf.layers.dense(out, num_labels, kernel_initializer =  tf.initializers.glorot_uniform, bias_initializer = tf.initializers.glorot_uniform) # create linear layer\n",
        "    out_softmax = tf.nn.softmax(out)\n",
        "\n",
        "    # Define loss\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out))\n",
        "\n",
        "    # Add L2 Regularization\n",
        "    L2norms = [tf.nn.l2_loss(weights) for weights in tf.trainable_variables()]\n",
        "    L2 = tf.reduce_sum(L2norms)\n",
        "    loss_L2 = loss + reg_coeff * L2\n",
        "\n",
        "    # Add Predictions\n",
        "    predictions = tf.reshape(tf.argmax(out,1), [-1]) # for predictions, can argmax over un-normalized probabilities, since softmax doesnt change max position\n",
        "\n",
        "    # Add Training Optimizer and Operation\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'SGD':\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'RMSProp':\n",
        "        optimizer = tf.train.RMSPropOptimizer(learning_rate = learn_rate)\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        optimizer = tf.train.AdagradOptimizer(learning_rate = learn_rate)\n",
        "\n",
        "    train_op = optimizer.minimize(loss_L2)\n",
        "\n",
        "    # return the model as a dictionary, containing the loss evaluator (without L2), loss (with L2), predictions (in terms of labels)\n",
        "    # train operation, and last layer output evaluator. ALso return the placeholders so we can feed later\n",
        "    model_dict = {'loss': loss\n",
        "                 ,'loss_L2': loss_L2\n",
        "                 ,'predictions': predictions\n",
        "                 ,'train_op': train_op\n",
        "                 ,'out': out_softmax\n",
        "                 , 'batch_size': batch_size\n",
        "                 , 'input': x\n",
        "                 , 'targets': y\n",
        "                 , 'isTrain': isTrain}\n",
        "    \n",
        "    return model_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqTc5-O6bjtp"
      },
      "source": [
        "x_train, x_test, x_val, y_train_oh, y_test_oh, y_val_oh, y_train, y_test, y_val = process_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4hfZM5zbzTY",
        "outputId": "222319c9-7678-4bcf-d6ba-47c4ec141769"
      },
      "source": [
        "# Check shapes\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)\n",
        "print(y_train_oh.shape)\n",
        "print(y_test_oh.shape)\n",
        "print(y_val_oh.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n",
            "(10000, 784)\n",
            "(50000, 10)\n",
            "(10000, 10)\n",
            "(10000, 10)\n",
            "(50000,)\n",
            "(10000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5eYvpMW5z5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNPFcA6bzYs"
      },
      "source": [
        "def train_model(model, n_epochs, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh):\n",
        "    \"\"\"\n",
        "    Performs training for n_epochs, using the given model, training data. Also evaluates training and validation acc and loss\n",
        "    after every epoch.\n",
        "    \"\"\"\n",
        "    # create arrays to store loss and accuracy at the end of every epoch\n",
        "    train_acc_arr = []; train_loss_arr = []; val_acc_arr = []; val_loss_arr = []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Processing Epoch\", epoch + 1)\n",
        "        nData, nDim = x_train.shape\n",
        "        \n",
        "        # Shuffle training data\n",
        "        permute = np.random.permutation(nData);\n",
        "        x_train = x_train[permute,:]\n",
        "        y_train = y_train[permute]\n",
        "        y_train_oh = y_train_oh[permute,:]\n",
        "        \n",
        "        # split to batches\n",
        "        if batch_size >= nData:\n",
        "            num_batch = 1\n",
        "        else:\n",
        "            num_batch = (nData // batch_size) + 1\n",
        "        \n",
        "        # train on each batch\n",
        "        for batch_ind in range(num_batch):\n",
        "            low_ind = (batch_ind) * batch_size\n",
        "            high_ind = min(low_ind + batch_size, nData)\n",
        "        \n",
        "            batch = x_train[low_ind:high_ind, :]\n",
        "            batch_lab = y_train_oh[low_ind:high_ind, :]\n",
        "\n",
        "            # create feed dictionary to predict\n",
        "            feed_train = {model['input']: batch,\n",
        "                         model['targets']: batch_lab,\n",
        "                         model['isTrain']: True}\n",
        "\n",
        "            # train\n",
        "            sess.run(model['train_op'], feed_dict = feed_train)\n",
        "            \n",
        "        # At the end of each epoch, evaluate acc and loss, append to array\n",
        "        feed_alltrain = {model['input']: x_train,\n",
        "                        model['targets']: y_train_oh,\n",
        "                        model['isTrain']: False}\n",
        "        \n",
        "        feed_val = {model['input']: x_val,\n",
        "                    model['targets']: y_val_oh,\n",
        "                   model['isTrain']: False}\n",
        "        \n",
        "        # Note: Change here to print L2 included loss as well\n",
        "        train_pred, train_loss = sess.run([model['predictions'], model['loss']], feed_dict = feed_alltrain)\n",
        "        train_acc = np.mean(train_pred == y_train)\n",
        "        \n",
        "        val_pred, val_loss = sess.run([model['predictions'], model['loss']], feed_dict = feed_val)\n",
        "        val_acc = np.mean(val_pred == y_val)\n",
        "        \n",
        "        print(\"Training acc and loss are\",train_acc, \"and\", train_loss) \n",
        "        print(\"Val acc and loss are\", val_acc,\"and\", val_loss) # for debugging\n",
        "        \n",
        "        # Append these information\n",
        "        train_acc_arr.append(train_acc); train_loss_arr.append(train_loss)\n",
        "        val_acc_arr.append(val_acc); val_loss_arr.append(val_loss)\n",
        "    \n",
        "    return train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w645RZmbefWG"
      },
      "source": [
        "# Plots the loss and accuracy evolution during training \n",
        "def plot_loss_acc(loss, acc, title):\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.2))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.plot(np.arange(len(loss)), loss,'r')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    ax1 = fig.add_subplot(1, 2, 2)\n",
        "    ax1.plot(np.arange(len(acc)), acc)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    best_epoch=np.argmax(acc)\n",
        "    best_accuracy = max(acc)\n",
        "    print('best_accuracy:',best_accuracy,'achieved at epoch:',best_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuAuB2k3ahTD"
      },
      "source": [
        "def split_train_test(data_samples, data_labels, train_prop, test_prop, seed = 88):\n",
        "    \"\"\"\n",
        "    Split to 80% train and 20% test as required, randomly by first shuffling\n",
        "    \"\"\"\n",
        "    n_tot = data_samples.shape[0]; n_train = round(train_prop * n_tot)\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    permute = np.random.permutation(n_tot)\n",
        "    \n",
        "    data_shuffled = data_samples[permute,:]\n",
        "    data_shuffled_labels = data_labels[permute]\n",
        "    \n",
        "    train_samples = data_shuffled[:n_train, :]; train_labels = data_shuffled_labels[:n_train]\n",
        "    test_samples = data_shuffled[n_train:, :]; test_labels = data_shuffled_labels[n_train:]\n",
        "    \n",
        "    return train_samples, train_labels, test_samples, test_labels\n",
        "\n",
        "def process_labels_SL(train_samples, train_labels, test_samples, test_labels):\n",
        "\n",
        "    m = train_samples.shape[0]; m2 = test_samples.shape[0]\n",
        "    # normalize train and test images\n",
        "    mean_dat = np.mean(train_samples, axis = 0).reshape(1,-1); \n",
        "    sd_data = np.std(train_samples, axis = 0).reshape(1,-1)\n",
        "    new_train = (train_samples - mean_dat) / sd_data\n",
        "    new_test = (test_samples - mean_dat) / sd_data\n",
        "\n",
        "\n",
        "    # one-hot encode the labels\n",
        "    max_label = 9 # there are 10 labels, 0 to 9 inclusive\n",
        "    y_onehot_train = np.zeros((m,max_label+1))\n",
        "    y_onehot_test = np.zeros((m2, max_label + 1))\n",
        "\n",
        "    y_onehot_train[np.arange(m),train_labels] = 1\n",
        "    y_onehot_test[np.arange(m2), test_labels] = 1\n",
        "\n",
        "    # also return the non-one-hot encoded labels, to compare accuracy.\n",
        "    return train_samples, train_labels, new_train, y_onehot_train, test_samples, test_labels, new_test, y_onehot_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcWy9e_na-Bv"
      },
      "source": [
        "path_real_data = \"/content/drive/My Drive/zipcombo.dat\"\n",
        "data = np.loadtxt(path_real_data)\n",
        "data_samples = data[:, 1:]; data_labels = data[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Fm1BElarKM"
      },
      "source": [
        "train_samples, train_labels, test_samples, test_labels = split_train_test(data_samples, data_labels, 0.8, 0.2, seed = 98)\n",
        "train_labels = train_labels.astype(int)\n",
        "test_labels = test_labels.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrq1ARbKbApW"
      },
      "source": [
        "train_samples, train_labels, new_train, train_oh, test_samples, test_labels, new_test, test_oh = process_labels_SL(train_samples, train_labels, test_samples, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCusUqaWyhFY"
      },
      "source": [
        "Parameters to cross validate: filter_bank size, layer size. Fix # layers, epochs, and learning rate, and regularization coefficient. Test outside for one configuration, and show why they are fixed. Values to try\n",
        "\n",
        "1. filter bank size = 4, 8, 16, 32\n",
        "2. layer size = 10, 100, 1000\n",
        "\n",
        "For other values, reg_coeff = we saw no signs of overfitting, even for the largest model that we try, so there's no need for regularizing. For other parameters, they are chosen as such because we require the speed for training. Hence they are chosen in consideration of the training speed, and not validation performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrC49cyHrG-l",
        "outputId": "f91d978e-dc3d-4eb7-c02a-f8e630138e1c"
      },
      "source": [
        "new_train2 = new_train.reshape(-1, 16, 16, 1)\n",
        "new_test2 = new_test.reshape(-1, 16, 16, 1)\n",
        "\n",
        "nData, dim = new_samples.shape; num_labels = 10\n",
        "layers = 2; nodes_per_layer = [1000, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 0; drop_prob = 0.0; filter_bank = 32\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model2(layers, nodes_per_layer, learn_rate, filter_bank, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob, hlactivation = 'relu', optimizer_name = \"Adam\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qLRLCJsNqq",
        "outputId": "4ae8cbd0-8997-43cd-92f1-9ff72d94800d"
      },
      "source": [
        "init = tf.initialize_all_variables()\n",
        "\n",
        "sess.run(init)\n",
        "start = time.time()\n",
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model2(MLP, 200, batch_size, new_train2, train_labels, train_oh, new_test2, test_labels, test_oh)\n",
        "\n",
        "print(f\"Final validation error rate is {1 - val_acc_arr[-1]}\")\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.10473245496101102 and 2.2659364\n",
            "Val acc and loss are 0.10268817204301076 and 2.2660036\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.30586179080397957 and 2.162788\n",
            "Val acc and loss are 0.3263440860215054 and 2.1589096\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.3008873353051896 and 2.0906246\n",
            "Val acc and loss are 0.3053763440860215 and 2.083682\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.29712288249529445 and 2.0326555\n",
            "Val acc and loss are 0.3032258064516129 and 2.0238488\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.3053240118311374 and 1.9755288\n",
            "Val acc and loss are 0.3161290322580645 and 1.9648796\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.31876848615219144 and 1.9101739\n",
            "Val acc and loss are 0.3263440860215054 and 1.897739\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.34686743748319443 and 1.8345075\n",
            "Val acc and loss are 0.3532258064516129 and 1.8204536\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.4050820112933584 and 1.7513119\n",
            "Val acc and loss are 0.41451612903225804 and 1.7356261\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.5038988975531057 and 1.6643853\n",
            "Val acc and loss are 0.5086021505376344 and 1.6469892\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.5970691045980102 and 1.5770159\n",
            "Val acc and loss are 0.5951612903225807 and 1.5581496\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.681097069104598 and 1.4904292\n",
            "Val acc and loss are 0.6844086021505377 and 1.4701208\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.7415972035493412 and 1.4050789\n",
            "Val acc and loss are 0.7451612903225806 and 1.3833609\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.7602850228556064 and 1.3204737\n",
            "Val acc and loss are 0.774731182795699 and 1.2975243\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.7676794837321861 and 1.2362953\n",
            "Val acc and loss are 0.7833333333333333 and 1.2122855\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.7727883839741866 and 1.1529905\n",
            "Val acc and loss are 0.7860215053763441 and 1.127963\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.7750739446087658 and 1.0714707\n",
            "Val acc and loss are 0.7865591397849462 and 1.0457447\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.7855606345791879 and 0.99318415\n",
            "Val acc and loss are 0.7946236559139785 and 0.96732664\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.7934928744286098 and 0.91954434\n",
            "Val acc and loss are 0.8102150537634408 and 0.8941564\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.8051895670879269 and 0.8509827\n",
            "Val acc and loss are 0.8182795698924731 and 0.8266447\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.8160795912879807 and 0.7870168\n",
            "Val acc and loss are 0.8279569892473119 and 0.7642032\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.8322129604732454 and 0.7274453\n",
            "Val acc and loss are 0.8370967741935483 and 0.70650995\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.8426996504436677 and 0.6733323\n",
            "Val acc and loss are 0.853225806451613 and 0.65429896\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.8514385587523527 and 0.62537175\n",
            "Val acc and loss are 0.8553763440860215 and 0.6081577\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.8592363538585641 and 0.5824631\n",
            "Val acc and loss are 0.863978494623656 and 0.56696504\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.8646141435869857 and 0.5435484\n",
            "Val acc and loss are 0.8655913978494624 and 0.5296471\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.8698574885721968 and 0.5087587\n",
            "Val acc and loss are 0.8650537634408603 and 0.4963135\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.875369723043829 and 0.4781224\n",
            "Val acc and loss are 0.871505376344086 and 0.46706685\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.8808819575154612 and 0.45109487\n",
            "Val acc and loss are 0.8806451612903226 and 0.44140747\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.8850497445549879 and 0.42729914\n",
            "Val acc and loss are 0.8844086021505376 and 0.41898003\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.888545307878462 and 0.40628603\n",
            "Val acc and loss are 0.8913978494623656 and 0.39934424\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.8924442054315677 and 0.38749468\n",
            "Val acc and loss are 0.8946236559139785 and 0.38193324\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.897284216187147 and 0.37079912\n",
            "Val acc and loss are 0.8956989247311828 and 0.36646396\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.9005108900242 and 0.35601544\n",
            "Val acc and loss are 0.8978494623655914 and 0.3524969\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.903737563861253 and 0.34256527\n",
            "Val acc and loss are 0.9059139784946236 and 0.3393396\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.9096531325625168 and 0.33012834\n",
            "Val acc and loss are 0.9091397849462366 and 0.32682514\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.9120731379403065 and 0.31880236\n",
            "Val acc and loss are 0.9134408602150538 and 0.31531796\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.9143586985748857 and 0.30857202\n",
            "Val acc and loss are 0.9155913978494624 and 0.30508718\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.9162409249798333 and 0.29907495\n",
            "Val acc and loss are 0.9161290322580645 and 0.2958849\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.9190642645872547 and 0.28996795\n",
            "Val acc and loss are 0.9166666666666666 and 0.2873094\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.9217531594514654 and 0.28130895\n",
            "Val acc and loss are 0.9198924731182796 and 0.2792958\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.9239042753428341 and 0.2733388\n",
            "Val acc and loss are 0.9231182795698925 and 0.2719929\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.9261898359774133 and 0.266019\n",
            "Val acc and loss are 0.9241935483870968 and 0.2652826\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.9294165098144662 and 0.25909704\n",
            "Val acc and loss are 0.9263440860215054 and 0.25881347\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.9312987362194138 and 0.25243694\n",
            "Val acc and loss are 0.928494623655914 and 0.2524018\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.9327776283947298 and 0.24609306\n",
            "Val acc and loss are 0.9311827956989247 and 0.24614558\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.9338531863404141 and 0.24015652\n",
            "Val acc and loss are 0.9344086021505377 and 0.24024826\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.9349287442860984 and 0.2345912\n",
            "Val acc and loss are 0.9365591397849462 and 0.23482722\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.9365420812046249 and 0.22924548\n",
            "Val acc and loss are 0.939247311827957 and 0.22984497\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.9396343102984673 and 0.22407366\n",
            "Val acc and loss are 0.9403225806451613 and 0.22529018\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.941785426189836 and 0.21915147\n",
            "Val acc and loss are 0.9424731182795699 and 0.22112668\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.9443398763108363 and 0.21447231\n",
            "Val acc and loss are 0.9440860215053763 and 0.21719043\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.9448776552836784 and 0.20996256\n",
            "Val acc and loss are 0.946236559139785 and 0.2132991\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.9456843237429416 and 0.20559631\n",
            "Val acc and loss are 0.9467741935483871 and 0.20943707\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.9472976606614681 and 0.20136136\n",
            "Val acc and loss are 0.9467741935483871 and 0.20564573\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.9490454423232052 and 0.1972605\n",
            "Val acc and loss are 0.9483870967741935 and 0.20199378\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.9503898897553106 and 0.19330831\n",
            "Val acc and loss are 0.95 and 0.19852543\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.9510621134713633 and 0.189474\n",
            "Val acc and loss are 0.9516129032258065 and 0.19522026\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.9514654477009948 and 0.18573964\n",
            "Val acc and loss are 0.9521505376344086 and 0.19207521\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.9522721161602581 and 0.18213238\n",
            "Val acc and loss are 0.9521505376344086 and 0.18913127\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.9529443398763109 and 0.17865674\n",
            "Val acc and loss are 0.953763440860215 and 0.18637413\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.953751008335574 and 0.17528604\n",
            "Val acc and loss are 0.9553763440860215 and 0.18372332\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.9542887873084163 and 0.17201293\n",
            "Val acc and loss are 0.9564516129032258 and 0.18111832\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.9556332347405216 and 0.16885297\n",
            "Val acc and loss are 0.9575268817204301 and 0.17856386\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.9561710137133638 and 0.16580617\n",
            "Val acc and loss are 0.9575268817204301 and 0.17609182\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.9564399031997849 and 0.16285916\n",
            "Val acc and loss are 0.9575268817204301 and 0.17373738\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.9575154611454693 and 0.1600031\n",
            "Val acc and loss are 0.9586021505376344 and 0.17151797\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.958456574347943 and 0.1572293\n",
            "Val acc and loss are 0.9602150537634409 and 0.16940457\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.9588599085775746 and 0.15453318\n",
            "Val acc and loss are 0.960752688172043 and 0.16734973\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.9593976875504168 and 0.15191437\n",
            "Val acc and loss are 0.9602150537634409 and 0.1653275\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.9599354665232589 and 0.14936568\n",
            "Val acc and loss are 0.9612903225806452 and 0.1633309\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.9604732454961011 and 0.14687996\n",
            "Val acc and loss are 0.9618279569892473 and 0.16136028\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.9610110244689433 and 0.14445844\n",
            "Val acc and loss are 0.9618279569892473 and 0.15942527\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.9614143586985748 and 0.14209768\n",
            "Val acc and loss are 0.9623655913978495 and 0.15752548\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.9618176929282065 and 0.13979031\n",
            "Val acc and loss are 0.9634408602150538 and 0.15564667\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.9623554719010486 and 0.13753258\n",
            "Val acc and loss are 0.964516129032258 and 0.15379015\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.9631621403603119 and 0.13532983\n",
            "Val acc and loss are 0.964516129032258 and 0.15197521\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.963431029846733 and 0.13318372\n",
            "Val acc and loss are 0.9650537634408602 and 0.15020709\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.9639688088195751 and 0.13109067\n",
            "Val acc and loss are 0.9655913978494624 and 0.14847438\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.9646410325356278 and 0.12904586\n",
            "Val acc and loss are 0.9672043010752688 and 0.14676082\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.9653132562516805 and 0.1270475\n",
            "Val acc and loss are 0.9682795698924731 and 0.14505166\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.9658510352245228 and 0.12509537\n",
            "Val acc and loss are 0.9682795698924731 and 0.1433479\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.966926593170207 and 0.12319002\n",
            "Val acc and loss are 0.9682795698924731 and 0.14166309\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.9677332616294703 and 0.12132573\n",
            "Val acc and loss are 0.9693548387096774 and 0.14002064\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.968405485345523 and 0.119501166\n",
            "Val acc and loss are 0.9698924731182795 and 0.13843589\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.9697499327776284 and 0.11771582\n",
            "Val acc and loss are 0.9693548387096774 and 0.13690229\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.9700188222640495 and 0.1159688\n",
            "Val acc and loss are 0.9693548387096774 and 0.13540123\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.9704221564936811 and 0.11425617\n",
            "Val acc and loss are 0.9698924731182795 and 0.1339144\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.9706910459801021 and 0.11257456\n",
            "Val acc and loss are 0.9698924731182795 and 0.13244164\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.9710943802097338 and 0.11092555\n",
            "Val acc and loss are 0.9704301075268817 and 0.13099602\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.971632159182576 and 0.10930768\n",
            "Val acc and loss are 0.9709677419354839 and 0.12959498\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.9719010486689971 and 0.10771842\n",
            "Val acc and loss are 0.9715053763440861 and 0.12824395\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.9719010486689971 and 0.106156945\n",
            "Val acc and loss are 0.9720430107526882 and 0.1269316\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.9725732723850498 and 0.10462371\n",
            "Val acc and loss are 0.9736559139784946 and 0.12564348\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.9729766066146813 and 0.10311654\n",
            "Val acc and loss are 0.9736559139784946 and 0.1243749\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.973648830330734 and 0.10163591\n",
            "Val acc and loss are 0.9741935483870968 and 0.123129025\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.9747243882764184 and 0.10018168\n",
            "Val acc and loss are 0.9741935483870968 and 0.12191748\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.9751277225060501 and 0.09875306\n",
            "Val acc and loss are 0.9747311827956989 and 0.1207484\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.9753966119924711 and 0.09735109\n",
            "Val acc and loss are 0.975268817204301 and 0.11961968\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.9755310567356816 and 0.09597519\n",
            "Val acc and loss are 0.9758064516129032 and 0.118521534\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.9759343909653132 and 0.09462387\n",
            "Val acc and loss are 0.9763440860215054 and 0.11744086\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.9764721699381554 and 0.09329481\n",
            "Val acc and loss are 0.9763440860215054 and 0.11637523\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.9770099489109976 and 0.0919883\n",
            "Val acc and loss are 0.9763440860215054 and 0.11533123\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.9775477278838397 and 0.09070487\n",
            "Val acc and loss are 0.9763440860215054 and 0.11431301\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.9778166173702608 and 0.08944367\n",
            "Val acc and loss are 0.9763440860215054 and 0.11331861\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.9779510621134714 and 0.08820336\n",
            "Val acc and loss are 0.9763440860215054 and 0.11234176\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.9779510621134714 and 0.08698349\n",
            "Val acc and loss are 0.9763440860215054 and 0.11137958\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.9780855068566819 and 0.085783064\n",
            "Val acc and loss are 0.9768817204301076 and 0.11043449\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.9784888410863135 and 0.08460141\n",
            "Val acc and loss are 0.9768817204301076 and 0.10950946\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.9787577305727346 and 0.083439805\n",
            "Val acc and loss are 0.9768817204301076 and 0.108607605\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.9794299542887873 and 0.08229718\n",
            "Val acc and loss are 0.9768817204301076 and 0.1077264\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.9796988437752084 and 0.08117351\n",
            "Val acc and loss are 0.9768817204301076 and 0.106861785\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.9799677332616294 and 0.080067344\n",
            "Val acc and loss are 0.9774193548387097 and 0.106011346\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.9802366227480506 and 0.07897865\n",
            "Val acc and loss are 0.9779569892473118 and 0.10517618\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.9806399569776821 and 0.0779075\n",
            "Val acc and loss are 0.9779569892473118 and 0.1043602\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.9809088464641033 and 0.07685375\n",
            "Val acc and loss are 0.9779569892473118 and 0.10356644\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.9817155149233665 and 0.0758165\n",
            "Val acc and loss are 0.9779569892473118 and 0.1027944\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.9822532938962086 and 0.074795686\n",
            "Val acc and loss are 0.9779569892473118 and 0.10204182\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.9822532938962086 and 0.073791124\n",
            "Val acc and loss are 0.9779569892473118 and 0.101304024\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.9822532938962086 and 0.072802044\n",
            "Val acc and loss are 0.9779569892473118 and 0.10057995\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.9822532938962086 and 0.07182777\n",
            "Val acc and loss are 0.9779569892473118 and 0.09986946\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.9823877386394192 and 0.07086877\n",
            "Val acc and loss are 0.9774193548387097 and 0.09917793\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.9827910728690509 and 0.06992497\n",
            "Val acc and loss are 0.9779569892473118 and 0.09850695\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.9827910728690509 and 0.068995684\n",
            "Val acc and loss are 0.9779569892473118 and 0.09785651\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.9827910728690509 and 0.06808097\n",
            "Val acc and loss are 0.9779569892473118 and 0.097222015\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.9830599623554719 and 0.06718109\n",
            "Val acc and loss are 0.978494623655914 and 0.09660046\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.9830599623554719 and 0.06629595\n",
            "Val acc and loss are 0.978494623655914 and 0.0959919\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.983597741328314 and 0.065425366\n",
            "Val acc and loss are 0.978494623655914 and 0.095398486\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.9838666308147351 and 0.064568795\n",
            "Val acc and loss are 0.978494623655914 and 0.09482307\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.9840010755579457 and 0.063726544\n",
            "Val acc and loss are 0.9790322580645161 and 0.0942666\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.9844044097875773 and 0.0628981\n",
            "Val acc and loss are 0.9790322580645161 and 0.0937263\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.9844044097875773 and 0.06208329\n",
            "Val acc and loss are 0.9790322580645161 and 0.09319688\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.9844044097875773 and 0.06128217\n",
            "Val acc and loss are 0.9790322580645161 and 0.09267806\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.9848077440172089 and 0.06049453\n",
            "Val acc and loss are 0.9790322580645161 and 0.09217015\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.9848077440172089 and 0.05971943\n",
            "Val acc and loss are 0.9790322580645161 and 0.09167625\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.9856144124764722 and 0.058956858\n",
            "Val acc and loss are 0.9795698924731183 and 0.091197096\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.9857488572196828 and 0.058206458\n",
            "Val acc and loss are 0.9795698924731183 and 0.09073127\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.9862866361925249 and 0.057467982\n",
            "Val acc and loss are 0.9795698924731183 and 0.09027649\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.9862866361925249 and 0.056741405\n",
            "Val acc and loss are 0.9795698924731183 and 0.08983118\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.986555525678946 and 0.056026082\n",
            "Val acc and loss are 0.9795698924731183 and 0.08939664\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.9866899704221564 and 0.055321906\n",
            "Val acc and loss are 0.9795698924731183 and 0.088972434\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.9866899704221564 and 0.05462869\n",
            "Val acc and loss are 0.9801075268817204 and 0.08856146\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.9869588599085776 and 0.053946655\n",
            "Val acc and loss are 0.9806451612903225 and 0.088163346\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.9869588599085776 and 0.0532747\n",
            "Val acc and loss are 0.9806451612903225 and 0.08777737\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.9873621941382092 and 0.052612428\n",
            "Val acc and loss are 0.9806451612903225 and 0.08740098\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.9873621941382092 and 0.051959977\n",
            "Val acc and loss are 0.9806451612903225 and 0.08703206\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.9873621941382092 and 0.05131728\n",
            "Val acc and loss are 0.9806451612903225 and 0.08667197\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.9877655283678408 and 0.05068469\n",
            "Val acc and loss are 0.9806451612903225 and 0.086323306\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.9880344178542619 and 0.050061703\n",
            "Val acc and loss are 0.9806451612903225 and 0.08598485\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.988303307340683 and 0.049447864\n",
            "Val acc and loss are 0.9806451612903225 and 0.08565585\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.988303307340683 and 0.04884338\n",
            "Val acc and loss are 0.9806451612903225 and 0.08533615\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.9888410863135252 and 0.04824816\n",
            "Val acc and loss are 0.9806451612903225 and 0.08502665\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.9888410863135252 and 0.04766141\n",
            "Val acc and loss are 0.9806451612903225 and 0.08472679\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.9893788652863673 and 0.047082696\n",
            "Val acc and loss are 0.9806451612903225 and 0.08443457\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.9896477547727884 and 0.046512313\n",
            "Val acc and loss are 0.9806451612903225 and 0.08415006\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.9897821995159989 and 0.04594999\n",
            "Val acc and loss are 0.9806451612903225 and 0.083874\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.99005108900242 and 0.04539552\n",
            "Val acc and loss are 0.9806451612903225 and 0.08360653\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.9901855337456306 and 0.044848915\n",
            "Val acc and loss are 0.9806451612903225 and 0.08334823\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.9903199784888411 and 0.044309825\n",
            "Val acc and loss are 0.9806451612903225 and 0.08309848\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.9903199784888411 and 0.04377826\n",
            "Val acc and loss are 0.9806451612903225 and 0.08285514\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.9904544232320516 and 0.043254036\n",
            "Val acc and loss are 0.9801075268817204 and 0.08261747\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.9904544232320516 and 0.04273641\n",
            "Val acc and loss are 0.9801075268817204 and 0.08238647\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.9904544232320516 and 0.04222515\n",
            "Val acc and loss are 0.9801075268817204 and 0.0821629\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.9905888679752621 and 0.04172054\n",
            "Val acc and loss are 0.9801075268817204 and 0.081946336\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.9905888679752621 and 0.041222498\n",
            "Val acc and loss are 0.9801075268817204 and 0.081735566\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.9907233127184727 and 0.04073045\n",
            "Val acc and loss are 0.9801075268817204 and 0.08153015\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.9908577574616833 and 0.040244203\n",
            "Val acc and loss are 0.9801075268817204 and 0.081329755\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.9908577574616833 and 0.039764088\n",
            "Val acc and loss are 0.9801075268817204 and 0.08113658\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.9911266469481044 and 0.03929021\n",
            "Val acc and loss are 0.9801075268817204 and 0.080950394\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.9911266469481044 and 0.03882212\n",
            "Val acc and loss are 0.9801075268817204 and 0.080769\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.9913955364345254 and 0.0383598\n",
            "Val acc and loss are 0.9801075268817204 and 0.08059223\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.9913955364345254 and 0.037903387\n",
            "Val acc and loss are 0.9801075268817204 and 0.08042038\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.9913955364345254 and 0.037452567\n",
            "Val acc and loss are 0.9801075268817204 and 0.08025391\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.9915299811777359 and 0.037007574\n",
            "Val acc and loss are 0.9801075268817204 and 0.08009276\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.9915299811777359 and 0.036568247\n",
            "Val acc and loss are 0.9795698924731183 and 0.079937965\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.9915299811777359 and 0.03613485\n",
            "Val acc and loss are 0.9795698924731183 and 0.07979032\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.9916644259209465 and 0.035707228\n",
            "Val acc and loss are 0.9795698924731183 and 0.07964909\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.9919333154073676 and 0.035285745\n",
            "Val acc and loss are 0.9795698924731183 and 0.07951378\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.9920677601505781 and 0.03486981\n",
            "Val acc and loss are 0.9795698924731183 and 0.07938467\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.9920677601505781 and 0.034459237\n",
            "Val acc and loss are 0.9795698924731183 and 0.07926153\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.9922022048937886 and 0.034054235\n",
            "Val acc and loss are 0.9795698924731183 and 0.07914525\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.9922022048937886 and 0.033654645\n",
            "Val acc and loss are 0.9795698924731183 and 0.079033755\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.9923366496369992 and 0.033260267\n",
            "Val acc and loss are 0.9795698924731183 and 0.078927144\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.9923366496369992 and 0.03287114\n",
            "Val acc and loss are 0.9795698924731183 and 0.07882522\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.9923366496369992 and 0.032487445\n",
            "Val acc and loss are 0.9790322580645161 and 0.07872749\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.9924710943802098 and 0.032109097\n",
            "Val acc and loss are 0.9790322580645161 and 0.078635134\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.9924710943802098 and 0.031736173\n",
            "Val acc and loss are 0.9790322580645161 and 0.07854718\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.9926055391234203 and 0.031368457\n",
            "Val acc and loss are 0.9795698924731183 and 0.078462705\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.9927399838666308 and 0.031005697\n",
            "Val acc and loss are 0.9795698924731183 and 0.07838011\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.9928744286098413 and 0.03064772\n",
            "Val acc and loss are 0.9795698924731183 and 0.078300424\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.9928744286098413 and 0.030294681\n",
            "Val acc and loss are 0.9795698924731183 and 0.07822523\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.9930088733530519 and 0.029946467\n",
            "Val acc and loss are 0.9795698924731183 and 0.078155525\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.993277762839473 and 0.029603014\n",
            "Val acc and loss are 0.9795698924731183 and 0.078091495\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.993277762839473 and 0.029264322\n",
            "Val acc and loss are 0.9795698924731183 and 0.07803116\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.9936810970691046 and 0.028930018\n",
            "Val acc and loss are 0.9795698924731183 and 0.07797303\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.9939499865555257 and 0.028600078\n",
            "Val acc and loss are 0.9801075268817204 and 0.0779159\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.9940844312987362 and 0.028274508\n",
            "Val acc and loss are 0.9801075268817204 and 0.07786095\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.9944877655283678 and 0.027953474\n",
            "Val acc and loss are 0.9801075268817204 and 0.0778102\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.9947566550147889 and 0.0276369\n",
            "Val acc and loss are 0.9801075268817204 and 0.077764295\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.9947566550147889 and 0.027324684\n",
            "Val acc and loss are 0.9801075268817204 and 0.07772252\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.9947566550147889 and 0.027016656\n",
            "Val acc and loss are 0.9801075268817204 and 0.07768331\n",
            "Final validation error rate is 0.019892473118279574\n",
            "15.318074941635132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgxxy4gQWo8R",
        "outputId": "8bf684b8-5213-49c9-ec25-10b751cf87ed"
      },
      "source": [
        "nData, dim = new_samples.shape; num_labels = 10\n",
        "layers = 4; nodes_per_layer = [dim, 1000, num_labels]; learn_rate = 0.01; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.0\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob, hlactivation = 'relu')\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSnCj9I2ZT3d",
        "outputId": "2c9ba20d-ea21-41c9-8e2e-28cbc0591e63"
      },
      "source": [
        "start = time.time()\n",
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 200, batch_size, new_train, train_labels, train_oh, new_test, test_labels, test_oh)\n",
        "\n",
        "print(f\"Final validation error rate is {1 - val_acc_arr[-1]}\")\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.7144393654208121 and 0.7579296\n",
            "Val acc and loss are 0.7118279569892473 and 0.78257793\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.7165904813121807 and 0.7377834\n",
            "Val acc and loss are 0.717741935483871 and 0.7487457\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.7235816079591288 and 0.72729474\n",
            "Val acc and loss are 0.7129032258064516 and 0.752306\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.7299005108900242 and 0.70060545\n",
            "Val acc and loss are 0.7387096774193549 and 0.7230561\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.7499327776283947 and 0.67413163\n",
            "Val acc and loss are 0.7564516129032258 and 0.6919871\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.7588061306802903 and 0.6528041\n",
            "Val acc and loss are 0.7623655913978494 and 0.6733262\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.7577305727346061 and 0.6325849\n",
            "Val acc and loss are 0.7537634408602151 and 0.66366965\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.7768217262705028 and 0.5998481\n",
            "Val acc and loss are 0.774731182795699 and 0.62912244\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.7761495025544501 and 0.5900284\n",
            "Val acc and loss are 0.7725806451612903 and 0.62930286\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.7834095186878193 and 0.6045395\n",
            "Val acc and loss are 0.782258064516129 and 0.65376\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.7928206507125571 and 0.5672623\n",
            "Val acc and loss are 0.7870967741935484 and 0.6106914\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.802231782737295 and 0.53759426\n",
            "Val acc and loss are 0.8026881720430108 and 0.5843448\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.8042484538854531 and 0.5328459\n",
            "Val acc and loss are 0.8010752688172043 and 0.59099567\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.8068029040064534 and 0.5109819\n",
            "Val acc and loss are 0.8080645161290323 and 0.57053435\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.8150040333422963 and 0.49942124\n",
            "Val acc and loss are 0.8198924731182796 and 0.5581267\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.8281796181769293 and 0.47653112\n",
            "Val acc and loss are 0.8145161290322581 and 0.5462089\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.8451196558214574 and 0.4572939\n",
            "Val acc and loss are 0.8360215053763441 and 0.53506947\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.8492874428609841 and 0.44850206\n",
            "Val acc and loss are 0.8413978494623656 and 0.5419907\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.8421618714708254 and 0.4513679\n",
            "Val acc and loss are 0.8333333333333334 and 0.56607944\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.8295240656090347 and 0.49405187\n",
            "Val acc and loss are 0.8193548387096774 and 0.5780326\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.8289862866361926 and 0.49291694\n",
            "Val acc and loss are 0.8204301075268817 and 0.57259107\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.6786770637268082 and 1.1134666\n",
            "Val acc and loss are 0.646236559139785 and 1.2645775\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.8027695617101371 and 0.693798\n",
            "Val acc and loss are 0.8010752688172043 and 0.7551188\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.7120193600430224 and 1.0225201\n",
            "Val acc and loss are 0.6994623655913978 and 1.0944027\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.7480505512234472 and 1.3246993\n",
            "Val acc and loss are 0.7430107526881721 and 1.3430314\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.6253025006722237 and 1.9437886\n",
            "Val acc and loss are 0.628494623655914 and 2.016385\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.742538316751815 and 1.0662856\n",
            "Val acc and loss are 0.7526881720430108 and 1.09683\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.7394460876579726 and 0.9131061\n",
            "Val acc and loss are 0.7478494623655914 and 0.9383592\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.5994891099758 and 1.2707366\n",
            "Val acc and loss are 0.5983870967741935 and 1.2701715\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.7698305996235547 and 0.7863581\n",
            "Val acc and loss are 0.7618279569892473 and 0.7979956\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.7535627856950793 and 0.91071767\n",
            "Val acc and loss are 0.7521505376344086 and 0.97279173\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.7610916913148695 and 0.87952936\n",
            "Val acc and loss are 0.760752688172043 and 0.9345376\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.7616294702877118 and 0.79919976\n",
            "Val acc and loss are 0.7618279569892473 and 0.83068347\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.7584027964506588 and 0.7627885\n",
            "Val acc and loss are 0.7559139784946236 and 0.7914531\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.7834095186878193 and 0.66203886\n",
            "Val acc and loss are 0.785483870967742 and 0.69050956\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.8000806668459263 and 0.6227005\n",
            "Val acc and loss are 0.7946236559139785 and 0.655378\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.8065340145200323 and 0.6015688\n",
            "Val acc and loss are 0.8005376344086022 and 0.63129\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.8373218607152461 and 0.5777509\n",
            "Val acc and loss are 0.828494623655914 and 0.6060277\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.8328851841892982 and 0.5489404\n",
            "Val acc and loss are 0.832258064516129 and 0.57636493\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.8261629470287711 and 0.5408602\n",
            "Val acc and loss are 0.8134408602150538 and 0.5859708\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.8207851573003495 and 0.5497285\n",
            "Val acc and loss are 0.8209677419354838 and 0.58738405\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.8500941113202474 and 0.47565505\n",
            "Val acc and loss are 0.8424731182795699 and 0.5305193\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.8379940844312987 and 0.55526793\n",
            "Val acc and loss are 0.8204301075268817 and 0.6382801\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.8514385587523527 and 0.44704053\n",
            "Val acc and loss are 0.8370967741935483 and 0.5363094\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.8414896477547728 and 0.4616223\n",
            "Val acc and loss are 0.8295698924731183 and 0.5565892\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.8514385587523527 and 0.43171424\n",
            "Val acc and loss are 0.8403225806451613 and 0.53830755\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.8826297391771982 and 0.4053683\n",
            "Val acc and loss are 0.8618279569892473 and 0.525111\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.861925248722775 and 0.39975107\n",
            "Val acc and loss are 0.8440860215053764 and 0.51210004\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.8901586447969885 and 0.36778888\n",
            "Val acc and loss are 0.8763440860215054 and 0.46879503\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.8749663888141974 and 0.36997545\n",
            "Val acc and loss are 0.860752688172043 and 0.46571976\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.9046786770637268 and 0.34142512\n",
            "Val acc and loss are 0.886021505376344 and 0.4491363\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.8892175315945147 and 0.33847785\n",
            "Val acc and loss are 0.8650537634408603 and 0.45600203\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.9092497983328852 and 0.31862146\n",
            "Val acc and loss are 0.8903225806451613 and 0.43423846\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.8982253293896209 and 0.3159619\n",
            "Val acc and loss are 0.886021505376344 and 0.42693463\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.911669803710675 and 0.301215\n",
            "Val acc and loss are 0.8935483870967742 and 0.4137689\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.9138209196020436 and 0.29157987\n",
            "Val acc and loss are 0.8956989247311828 and 0.41399023\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.9167787039526755 and 0.2839984\n",
            "Val acc and loss are 0.8919354838709678 and 0.41930285\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.9190642645872547 and 0.27158025\n",
            "Val acc and loss are 0.9026881720430108 and 0.39459088\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.9222909384243076 and 0.26119554\n",
            "Val acc and loss are 0.9080645161290323 and 0.38621554\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.9261898359774133 and 0.25189763\n",
            "Val acc and loss are 0.9048387096774193 and 0.39798972\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.9296853993008873 and 0.24312358\n",
            "Val acc and loss are 0.9043010752688172 and 0.39727592\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.928609841355203 and 0.23785329\n",
            "Val acc and loss are 0.9112903225806451 and 0.3915423\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.9342565205700457 and 0.22801158\n",
            "Val acc and loss are 0.9086021505376344 and 0.388733\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.9345254100564668 and 0.22257051\n",
            "Val acc and loss are 0.9112903225806451 and 0.38582054\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.9349287442860984 and 0.21415727\n",
            "Val acc and loss are 0.9161290322580645 and 0.37208578\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.9365420812046249 and 0.20952465\n",
            "Val acc and loss are 0.9123655913978495 and 0.37140837\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.9373487496638881 and 0.2063507\n",
            "Val acc and loss are 0.910752688172043 and 0.38179806\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.9386931970959935 and 0.19867061\n",
            "Val acc and loss are 0.9118279569892473 and 0.37636757\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.943533207851573 and 0.18881923\n",
            "Val acc and loss are 0.9155913978494624 and 0.37265208\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.9451465447700995 and 0.18264158\n",
            "Val acc and loss are 0.9145161290322581 and 0.37545833\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.9448776552836784 and 0.17806478\n",
            "Val acc and loss are 0.9134408602150538 and 0.37633172\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.9494487765528368 and 0.1724071\n",
            "Val acc and loss are 0.9150537634408602 and 0.37174302\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.9486421080935735 and 0.16779384\n",
            "Val acc and loss are 0.9188172043010753 and 0.36952674\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.948507663350363 and 0.16252312\n",
            "Val acc and loss are 0.9150537634408602 and 0.37506604\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.9520032266738371 and 0.15725082\n",
            "Val acc and loss are 0.9188172043010753 and 0.37917525\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.9522721161602581 and 0.1548259\n",
            "Val acc and loss are 0.9182795698924732 and 0.38117358\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.9538854530787846 and 0.14883144\n",
            "Val acc and loss are 0.917741935483871 and 0.37953326\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.9550954557676795 and 0.14576626\n",
            "Val acc and loss are 0.9198924731182796 and 0.38162142\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.9538854530787846 and 0.14367585\n",
            "Val acc and loss are 0.9209677419354839 and 0.39206806\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.9571121269158376 and 0.13818353\n",
            "Val acc and loss are 0.9231182795698925 and 0.40631974\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.9585910190911535 and 0.13338234\n",
            "Val acc and loss are 0.9220430107526881 and 0.39943063\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.9595321322936273 and 0.13154684\n",
            "Val acc and loss are 0.9236559139784947 and 0.39807147\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.9599354665232589 and 0.12752914\n",
            "Val acc and loss are 0.9204301075268817 and 0.40251425\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.9611454692121538 and 0.123396575\n",
            "Val acc and loss are 0.9220430107526881 and 0.39408678\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.9627588061306803 and 0.120517686\n",
            "Val acc and loss are 0.9220430107526881 and 0.3989573\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.963431029846733 and 0.117317185\n",
            "Val acc and loss are 0.921505376344086 and 0.41721722\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.964909922022049 and 0.114093214\n",
            "Val acc and loss are 0.9198924731182796 and 0.41640574\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.9658510352245228 and 0.11149466\n",
            "Val acc and loss are 0.921505376344086 and 0.42010477\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.9671954826566281 and 0.10876286\n",
            "Val acc and loss are 0.9231182795698925 and 0.43510503\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.9680021511158914 and 0.105679624\n",
            "Val acc and loss are 0.9225806451612903 and 0.43527803\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.9689432643183652 and 0.1032063\n",
            "Val acc and loss are 0.9231182795698925 and 0.43543053\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.9696154880344179 and 0.10068974\n",
            "Val acc and loss are 0.9220430107526881 and 0.44174063\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.9713632696961548 and 0.09843719\n",
            "Val acc and loss are 0.921505376344086 and 0.44812253\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.9720354934122076 and 0.09579582\n",
            "Val acc and loss are 0.9220430107526881 and 0.44835818\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.9723043828986286 and 0.09370485\n",
            "Val acc and loss are 0.9220430107526881 and 0.45582047\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.9728421618714709 and 0.09163348\n",
            "Val acc and loss are 0.9188172043010753 and 0.4627122\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.9728421618714709 and 0.09015725\n",
            "Val acc and loss are 0.9236559139784947 and 0.46581742\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.9724388276418392 and 0.09039806\n",
            "Val acc and loss are 0.9247311827956989 and 0.45270735\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.9710943802097338 and 0.09181041\n",
            "Val acc and loss are 0.9220430107526881 and 0.4998375\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.9685399300887335 and 0.10126348\n",
            "Val acc and loss are 0.9198924731182796 and 0.470108\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.9599354665232589 and 0.1230417\n",
            "Val acc and loss are 0.9118279569892473 and 0.5498712\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.9390965313256252 and 0.21706179\n",
            "Val acc and loss are 0.9053763440860215 and 0.55177474\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.9550954557676795 and 0.14523701\n",
            "Val acc and loss are 0.9016129032258065 and 0.5656415\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.9252487227749395 and 0.2775931\n",
            "Val acc and loss are 0.9005376344086021 and 0.5473302\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.9304920677601506 and 0.25003338\n",
            "Val acc and loss are 0.896774193548387 and 0.62590885\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.9331809626243613 and 0.28122953\n",
            "Val acc and loss are 0.9005376344086021 and 0.6654401\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.891771981715515 and 0.58668363\n",
            "Val acc and loss are 0.8688172043010752 and 0.8128792\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.8980908846464103 and 0.4446517\n",
            "Val acc and loss are 0.8731182795698925 and 0.6869928\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.9103253562785695 and 0.35895577\n",
            "Val acc and loss are 0.8779569892473118 and 0.65605056\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.9034686743748319 and 0.3572393\n",
            "Val acc and loss are 0.8672043010752688 and 0.682197\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.9265931702070449 and 0.28803894\n",
            "Val acc and loss are 0.9016129032258065 and 0.6129281\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.9249798332885184 and 0.283761\n",
            "Val acc and loss are 0.8956989247311828 and 0.52142155\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.9247109438020973 and 0.26511848\n",
            "Val acc and loss are 0.8956989247311828 and 0.47936553\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.9351976337725195 and 0.23764595\n",
            "Val acc and loss are 0.9134408602150538 and 0.42745745\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.9228287173971498 and 0.29730532\n",
            "Val acc and loss are 0.8989247311827957 and 0.5217609\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.9432643183651519 and 0.1936647\n",
            "Val acc and loss are 0.9204301075268817 and 0.3654567\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.9362731917182038 and 0.22814117\n",
            "Val acc and loss are 0.9123655913978495 and 0.38931447\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.9337187415972036 and 0.24155954\n",
            "Val acc and loss are 0.9096774193548387 and 0.4002352\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.9365420812046249 and 0.2212366\n",
            "Val acc and loss are 0.9123655913978495 and 0.40995634\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.941785426189836 and 0.2079303\n",
            "Val acc and loss are 0.9096774193548387 and 0.41653916\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.9474321054046787 and 0.19314809\n",
            "Val acc and loss are 0.9112903225806451 and 0.4049302\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.9568432374294165 and 0.16011582\n",
            "Val acc and loss are 0.9198924731182796 and 0.36299714\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.9556332347405216 and 0.15181132\n",
            "Val acc and loss are 0.9204301075268817 and 0.34325317\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.9532132293627319 and 0.15442902\n",
            "Val acc and loss are 0.9247311827956989 and 0.34489226\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.9538854530787846 and 0.14912295\n",
            "Val acc and loss are 0.9274193548387096 and 0.34844485\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.9591287980639956 and 0.13842133\n",
            "Val acc and loss are 0.928494623655914 and 0.35248604\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.96020435600968 and 0.13561113\n",
            "Val acc and loss are 0.9279569892473118 and 0.36020058\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.9635654745899436 and 0.12509823\n",
            "Val acc and loss are 0.928494623655914 and 0.3506295\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.9639688088195751 and 0.12034491\n",
            "Val acc and loss are 0.9279569892473118 and 0.34309018\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.9673299273998387 and 0.115470804\n",
            "Val acc and loss are 0.9279569892473118 and 0.340369\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.9675988168862597 and 0.11196748\n",
            "Val acc and loss are 0.9241935483870968 and 0.33934158\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.9678677063726808 and 0.106123425\n",
            "Val acc and loss are 0.9263440860215054 and 0.3444279\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.9706910459801021 and 0.10127117\n",
            "Val acc and loss are 0.928494623655914 and 0.35438985\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.9719010486689971 and 0.098472714\n",
            "Val acc and loss are 0.928494623655914 and 0.36372927\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.9719010486689971 and 0.09501116\n",
            "Val acc and loss are 0.9279569892473118 and 0.36391094\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.9732454961011024 and 0.091229066\n",
            "Val acc and loss are 0.9290322580645162 and 0.36058506\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.9747243882764184 and 0.08779974\n",
            "Val acc and loss are 0.9311827956989247 and 0.3574151\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.9760688357085238 and 0.08551332\n",
            "Val acc and loss are 0.9306451612903226 and 0.35694537\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.9763377251949449 and 0.08140686\n",
            "Val acc and loss are 0.9306451612903226 and 0.35588014\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.9764721699381554 and 0.07919375\n",
            "Val acc and loss are 0.9327956989247311 and 0.3607942\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.9779510621134714 and 0.07628585\n",
            "Val acc and loss are 0.9301075268817204 and 0.36540744\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.9786232858295241 and 0.07370068\n",
            "Val acc and loss are 0.9317204301075269 and 0.3647894\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.9792955095455768 and 0.07150365\n",
            "Val acc and loss are 0.932258064516129 and 0.36540744\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.9799677332616294 and 0.069357954\n",
            "Val acc and loss are 0.9338709677419355 and 0.3717365\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.9805055122344716 and 0.06706738\n",
            "Val acc and loss are 0.9354838709677419 and 0.3798053\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.9809088464641033 and 0.065018505\n",
            "Val acc and loss are 0.9333333333333333 and 0.3915055\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.9821188491529981 and 0.06344229\n",
            "Val acc and loss are 0.932258064516129 and 0.40237775\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.9823877386394192 and 0.061489105\n",
            "Val acc and loss are 0.9338709677419355 and 0.4088676\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.9830599623554719 and 0.059710726\n",
            "Val acc and loss are 0.9327956989247311 and 0.4159495\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.9831944070986824 and 0.057756055\n",
            "Val acc and loss are 0.9344086021505377 and 0.42598227\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.983328851841893 and 0.05609298\n",
            "Val acc and loss are 0.932258064516129 and 0.44046494\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.9845388545307878 and 0.054556396\n",
            "Val acc and loss are 0.9306451612903226 and 0.4515231\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.9854799677332616 and 0.053102285\n",
            "Val acc and loss are 0.9306451612903226 and 0.45616353\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.9858833019628932 and 0.051827643\n",
            "Val acc and loss are 0.9306451612903226 and 0.46130368\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.9862866361925249 and 0.050630055\n",
            "Val acc and loss are 0.9311827956989247 and 0.46987653\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.9864210809357354 and 0.04935617\n",
            "Val acc and loss are 0.9301075268817204 and 0.4803145\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.9872277493949987 and 0.04826993\n",
            "Val acc and loss are 0.9295698924731183 and 0.49644467\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.9876310836246303 and 0.047307454\n",
            "Val acc and loss are 0.9306451612903226 and 0.5037799\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.9878999731110514 and 0.046247907\n",
            "Val acc and loss are 0.9301075268817204 and 0.5052084\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.9878999731110514 and 0.045183655\n",
            "Val acc and loss are 0.9301075268817204 and 0.5092058\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.988303307340683 and 0.04420249\n",
            "Val acc and loss are 0.9311827956989247 and 0.5144678\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.9885721968271041 and 0.043338053\n",
            "Val acc and loss are 0.9311827956989247 and 0.52231944\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.9892444205431568 and 0.04240199\n",
            "Val acc and loss are 0.9301075268817204 and 0.5327387\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.9896477547727884 and 0.041504085\n",
            "Val acc and loss are 0.9279569892473118 and 0.541648\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.9896477547727884 and 0.040654358\n",
            "Val acc and loss are 0.9279569892473118 and 0.55144423\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.99005108900242 and 0.039823137\n",
            "Val acc and loss are 0.9274193548387096 and 0.5599812\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.9904544232320516 and 0.039070893\n",
            "Val acc and loss are 0.9268817204301075 and 0.5669016\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.9903199784888411 and 0.03829806\n",
            "Val acc and loss are 0.9274193548387096 and 0.5746548\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.9907233127184727 and 0.037548613\n",
            "Val acc and loss are 0.9258064516129032 and 0.58580166\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.9911266469481044 and 0.036905468\n",
            "Val acc and loss are 0.9263440860215054 and 0.5931749\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.9912610916913148 and 0.036240228\n",
            "Val acc and loss are 0.9268817204301075 and 0.60249496\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.9916644259209465 and 0.035590768\n",
            "Val acc and loss are 0.9263440860215054 and 0.6138771\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.9919333154073676 and 0.03472438\n",
            "Val acc and loss are 0.9258064516129032 and 0.6260808\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.9923366496369992 and 0.03460773\n",
            "Val acc and loss are 0.9247311827956989 and 0.6405821\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.9927399838666308 and 0.034446202\n",
            "Val acc and loss are 0.9247311827956989 and 0.6512948\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.9927399838666308 and 0.034649994\n",
            "Val acc and loss are 0.9268817204301075 and 0.6610361\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.9931433180962624 and 0.034102157\n",
            "Val acc and loss are 0.9247311827956989 and 0.6668157\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.9934122075826836 and 0.03300549\n",
            "Val acc and loss are 0.9247311827956989 and 0.6711131\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.993277762839473 and 0.031579465\n",
            "Val acc and loss are 0.9241935483870968 and 0.68248594\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.9938155418123151 and 0.03285885\n",
            "Val acc and loss are 0.9231182795698925 and 0.7010772\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.9936810970691046 and 0.031875268\n",
            "Val acc and loss are 0.9231182795698925 and 0.7093663\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.9934122075826836 and 0.033373028\n",
            "Val acc and loss are 0.9258064516129032 and 0.7083911\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.993546652325894 and 0.03409599\n",
            "Val acc and loss are 0.9247311827956989 and 0.7139041\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.993546652325894 and 0.034324706\n",
            "Val acc and loss are 0.9247311827956989 and 0.71876085\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.9938155418123151 and 0.0342705\n",
            "Val acc and loss are 0.9252688172043011 and 0.72842515\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.9939499865555257 and 0.033721067\n",
            "Val acc and loss are 0.9247311827956989 and 0.7345827\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.9938155418123151 and 0.033126757\n",
            "Val acc and loss are 0.9258064516129032 and 0.73618644\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.9939499865555257 and 0.032335583\n",
            "Val acc and loss are 0.9268817204301075 and 0.73728997\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.9942188760419468 and 0.031056322\n",
            "Val acc and loss are 0.9268817204301075 and 0.74008954\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.9940844312987362 and 0.029860744\n",
            "Val acc and loss are 0.9258064516129032 and 0.7408777\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.9943533207851573 and 0.02818457\n",
            "Val acc and loss are 0.9268817204301075 and 0.74571085\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.9944877655283678 and 0.026374137\n",
            "Val acc and loss are 0.9258064516129032 and 0.74949265\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.9901855337456306 and 0.08288115\n",
            "Val acc and loss are 0.921505376344086 and 0.80607104\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.9827910728690509 and 0.07167567\n",
            "Val acc and loss are 0.9198924731182796 and 0.7967754\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.9458187684861522 and 0.30045927\n",
            "Val acc and loss are 0.8940860215053763 and 0.9721155\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.8968808819575155 and 1.1986798\n",
            "Val acc and loss are 0.8467741935483871 and 2.0226824\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.8141973648830331 and 1.2682729\n",
            "Val acc and loss are 0.785483870967742 and 1.8580052\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.7801828448507664 and 1.3661007\n",
            "Val acc and loss are 0.7440860215053764 and 2.0424957\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.7249260553912342 and 2.0290396\n",
            "Val acc and loss are 0.7091397849462365 and 2.4644134\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.691045980102178 and 1.4473534\n",
            "Val acc and loss are 0.6876344086021505 and 1.6403522\n",
            "Final validation error rate is 0.3123655913978495\n",
            "37.06568479537964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "KUrlhxONddOD",
        "outputId": "2f26f61e-8170-45fd-ebdb-fe84cf695672"
      },
      "source": [
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7MAAADrCAYAAAC7K8FoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxcdX3/8ddntrvmZiU3OwkQlrCHGFkUw2ILiGCtItpabG2prdrijhtarX3UqlhpsYo/F2wRERVNEQFFLhQFZCckAQlhSULIvt3cbZbP749zJpl7M3fN3Dlz7n0/H495zJwzZ868c+7kfu9nvud8v+buiIiIiIiIiMRJIuoAIiIiIiIiIsOlYlZERERERERiR8WsiIiIiIiIxI6KWREREREREYkdFbMiIiIiIiISOypmRUREREREJHZUzIpUkZm5mR0RPv6GmX16KNuO4H3+zMzuHGlOERGR8UJts0h8qZgVGQYzu93MPldm/cVm9oqZpYa6L3d/j7t/vgKZ5oeN6773dvcb3P2PDnbfZd5rmZkVzKw9vK03sx+Z2auGsY/Pmtn/VDqbiIiMT2qb1TbL+KViVmR4rgf+3Mysz/p3Aje4ey6CTNX2srs3AxOAU4Gngf8zs3OijSUiIuOU2ma1zTJOqZgVGZ6fAVOB1xZXmNlk4ELg+2a21MzuN7OdZrbRzP7TzDLldmRm3zOzfy5Z/kj4mpfN7K/6bPsGM3vMzHab2Toz+2zJ0/eG9zvDb2RPM7N3mdl9Ja8/3cweMrNd4f3pJc+1mdnnzey3ZrbHzO40s2mDHQgPrHf3q4D/B3yxZJ9fC3PuNrNHzOy14frzgE8AbwuzPhGu/0szWx2+/1oz+9vB3l9ERCSktjmktlnGGxWzIsPg7p3Aj4C/KFl9CfC0uz8B5IEPANOA04BzgL8fbL9hQ/Jh4PXAQuDcPpvsDd9zEvAG4O/M7E3hc2eG95Pcvdnd7++z7ynAL4BrCBr7q4FfmNnUks3eAfwlMB3IhFmG46fAYjNrCpcfAk4CpgA/AG42s3p3vx34F+CmMOuJ4fabCf7oaAlzfNXMFg8zg4iIjENqm/ultlnGPBWzIsN3PfAWM6sPl/8iXIe7P+LuD7h7zt1fAL4JvG4I+7wE+K67P+Xue4HPlj7p7m3uvsLdC+7+JHDjEPcLQQP7rLv/d5jrRoLTj95Yss133f0PJX8QnDTEfRe9DBhBg467/4+7bwvf7ytAHXBUfy9291+4+3PhN8r3AHdS8g27iIjIINQ2H0hts4x5KmZFhsnd7wO2Am8ys8OBpQTfcGJmR5rZrRYMOLGb4JvOQU8LAmYB60qWXyx90sxebWZ3m9kWM9sFvGeI+y3u+8U+614EZpcsv1LyuANoHuK+i2YDDuwM8344PDVpl5ntBCYOlNfMzjezB8xse7j9BQNtLyIiUkptc1lqm2XMUzErMjLfJ/jW98+BO9x9U7j+vwi+WV3o7i0E16D0HZCinI3A3JLleX2e/wGwHJjr7hOBb5Ts1wfZ98vAoX3WzQM2DCHXUP0J8Ki77w2vwfkowTfak919ErCrv7xmVgf8BPgy0BpufxtDO24iIiJFapt7U9ssY56KWZGR+T7BtTN/Q3gaU2gCsBtoN7Ojgb8b4v5+BLzLzBaZWSPwmT7PTwC2u3uXmS0luI6maAtQAA7rZ9+3AUea2TvMLGVmbwMWAbcOMVtZFphtZp8B/prgj4Ni1lyYK2VmVxFcb1O0CZhvZsXfPxmCU522ADkzOx+o+NQFIiIy5qltVtss44yKWZERCK+5+R3QRPCtbNGHCRqzPcC3gJuGuL9fAv8O/AZYE96X+nvgc2a2B7iKoIEtvrYD+ALwWwtGajy1z763EQzg8CFgG8E3sxe6+9ahZCtjlpm1A+0Eg0kcDyxz9+JE8HcAtwN/IDhlqovep2ndHN5vM7NH3X0P8A/hv2kHwfErPaYiIiKDUtustlnGH3Mf7CwIERERERERkdqinlkRERERERGJHRWzIiIiIiIiEjujVsya2XfMbLOZPVWy7ktm9rSZPWlmt5jZpJLnPm5ma8zsGTP749HKJSIiIiIiIvE3mj2z3wPO67PuV8Bx7n4CwQXoHwcws0XApcCx4Wu+bmbJUcwmIiIiIiIiMTZqxay73wts77PuTnfPhYsPAHPCxxcDP3T3bnd/nmDEuKWjlU1ERERERETiLRXhe/8V+4dGn01Q3BatD9cNaNq0aT5//vyKhNm7dy9NTU0V2Vc1KXf1xTW7cldfXLOP59yPPPLIVnc/pEKRxiW1zcodhbhmV+7qi2v28Zx7oLY5kmLWzD5JMHHzDSN47eXA5QCtra18+ctfrkim9vZ2mpubK7KvalLu6otrduWuvrhmH8+5zzrrrBcrFGfcmj9/Pg8//HBF9tXW1sayZcsqsq9qUu7qi2t25a6+uGYfz7nNrN+2uerFrJm9i2CS6HN8/yS3G4C5JZvNCdcdwN2vA64DWLJkiVfqhzqePyBRiGtuiG925a6+uGZXbhEREYmDqk7NY2bnAR8FLnL3jpKnlgOXmlmdmS0AFgK/r2Y2ERERERERiY9R65k1sxuBZcA0M1sPfIZg9OI64FdmBvCAu7/H3Vea2Y+AVQSnH7/X3fOjlU1ERERERETibdSKWXd/e5nV3x5g+y8AXxitPCIiIiIiIjJ2VPU045rlDoVC1ClERERERERkiKKcmqc2rF8P8+Yx80MfgrPPjjqNiIiIyJhQKDgd2TwdPTnq00maMimSCdv3fHcuT3tXjvbuHHu6cuztzrG3J4eZUZdMkEklSCcTOFBwxx3cPVgu+L71OKzcmif57BYKxW0cHKdQgFzB6ckX6MkFt+CVvaUSxvSWeiY3ZnB3Cu4UPHiffLi/0nUFd7J5Z293jp58gXSYN5NMkCr5Nw5mxeYcuVWbmFCfYnpLPZt3d/Hitg6yJZ0sDekkx8+eSHN9iifW7WLb3u4h739qU4ZXL5hK3p0H127nld1d4THP0t6do707T3tX8Bjg8EOaOWRCHT25At25Atl8gQXTmnj1gqm8sruLx9ft4Il1u3hmQwezV/6WhnSSTXu62Bu+vq+JDWmmNdftu01tzjCxIY0NcoiSZkxrrqMxk2Tt1r1s2XPgv9nd2ba3hx0dPSyeN5njZ0/kV6s28eDz2+nK5unJF8jmCjjs+9lkUgk62jv56srfkjBImJEwsOI9RiIRrC+uS5iRThqZVDLchwH9/QOc7Xt72BzmTScT1KWCW1NdinQywd7uHAV3muvSTKhP0VSXpLkuTXN9iua6JOlkghe3dbBhZyde8lHduaWHzU3rSCSMTbu7KBR83/+R+nSSyY1pJjdlmNqUoTtX4PF1O3llV9f+z36+QHNdismNwTZTmzMsmNZEU12K+5/bxqqNu9m4q4uOMj/LRMI4bFoTi2a1cMzMFqY0ZVixfhdrNrfzyu4uOnp6Xx06pSnNjJZ6lh01feAfdAWomK2rA3espyfqJCIiIjIGbdrdxY2/f4mnN+5h465OmupSTGxI7781BvcT6tO8tG0vK1/eTVf2wKFDMqkErS31tLbUM6Olnub6FO7O+h2dvLBtL509BQru5Av7b7mw8Jo9qYE/WTybY2a00JXN05nN05XN05Ut0J0L3uu5nXkmvrQDAAe2t/ewZks7aza38/zWvQA0ZpLkC77vj+OeXGHf++QKBXL54HFnT569Pblef4wDNGWSpFMJOrqDYqOiHo7p2KGPVmYqq/6YccDPoSGdDIun/bd8wfnVqk3s6OjZV/wlE8aOjuy+1yUTxtEzJnDYxATpuhQdPTmOap1AS/2BBWrBnZ0dWba2d/P4up1sbe8+oOg5WJMbgyLwthWvAJBOGksXTGHO5IZ9/wag15cZW7J7mdSQ3vcFSaHkywv3Ap6nZDn4MiOb815F4UAmNaRpbanHDHpyBdq7c2xtL9DRk6MnV6CpLkXCYG93nj1dWfb25MkXDvyCZUpTpteXPzvas/zy+SeHdXwStr+QTycTtHfn6M6Vz1+XSjBjYj3NdakDfpbZnHPPH7bQU+a1xc9SkbuzoyNLvuB8912N/Zb9laJitr4egEQ2O8iGIiIiIkNTKDgrNuzi1idf5r8feJGeXIH505qYPamBzp48z25uZ1dnll2d2QP+QFwwrYkJ9Qf+idaVzXP/c9vY3XVgz8nEhjTNdSkSCUglEiQsvE8YyQT87rmt/PcDQ5hG+YHfHbBq+oQ6DjukiWTC2NOVI5kwMsmgpynoGTKSiQTphJFMGKmk0ZAOepma61M0pJN05wrsCXthe3KFAwqpproUE+pTNGaSOOwrPLL5AgkzCHvIjGKvWVCkGUHv2eOPP84pi08O1/feLpkw6lIJMskk6ZSRLNMt2J0rsHlPN7s6ezALtunba5dM9O6tSyWNCXVp0ikLi5083bnCsK5ce+SRh1l8yins6syyeXc3U5szHDatmfr0/isBd3ZmeWLdTvZ25zhh7iRmT2oYcoGwbkcHv12zjWTCOP3wqRx2SDNNmSSp5NCvNNyws5NHXtzBrIn1HDtrIg2ZZDgV2quH/g8NdfTkaC/z+e0rW3C27Olmb3eO+dOamNlST2KAHu+XtnWwYsMuTjt8KlOaMgPuO8i+dNjZR4u705UthD3lObpzeeZMbqS5rvfvgN/cfTeHHR/knjGxnmTC9v0f6czm2bE3y46OHrbt7SFhcOKcScyZ3ID1+bx39OT29R4/t7md3V05ls6fwqJZLb2K576y+QJrt+xl1cZdbGvv4dhZEzlm5oSwp7336/IFZ1t7NxPq0zz4SoUOVD9UzNbVAZBQz6yIiIgM0d7uHL97bhvppHHIhDqOntFCvhD0Xly/opsP33cXW9u7SRi88cRZfOj1RzFvamPZfXVl8/sK29YJ9UxsTA/43h09OTbt7qajJygKZrTUM7W5bsDX7OnKcsfKTWzZ0019OkFDOkl9Okl9OkFdKgkGK558kuNPOGHfayY2pDn8kGYmNgycpxZ0vpRkyfwpB7WPuVPK/3xG07Y1SU6YM2nAbaa31HNk64QR7X96Sz2nHHpwx2X2pAZmT2o4qH0UNWZSNGaGVn4M5z3nTW3s9/9XrTMzGjJJGjJJDpnQ///jhBnzpzX1WpcOv5SYBMycOLTjVfwZzJncyOJ5k4ecM51McNSMCRw1Y/DPYjI8bb8aVMym02CmYlZEREQGtXlPF7c9uZH/vPs5trbvv45vUmOahBnb9/bQmIJzjp3O2UcfwuuOnD5oT1F9WFi2DvGPv8ZMigXThvcn3IT6NG85Zc6A29jGVFWucRMRqRQVs2ZQX69iVkRERHpp786xvb2H+nSCX63exA0PvMSqjbsBWLpgCldfciJNdSle2r6X367ZRjZf4KITZ+EbV3Hu2SdHnF5EZOxTMQtQV6diVkRERABYt72Db977HD99dEOvAWsWzWzhY+cdzWsXTuPYWS37rhM75dDJ/MnJ+3s92zatrnpmEZHxSMUsqGdWREREcHduemgdn7t1FbmCc9GJs3j1gil09OQ5dlYLpxw6+YCBTkREJDoqZqF8MfuTn8CPfgQ33RRNJhEREakad+dTP3uKGx58idMOm8qXLzmxYoPeiIjI6FAxC0Ex23dqnnvvhZ/9LJo8IiIiUjWFgvPZ/13JDQ++xN+eeRgfO+/oAacBERGR2qBiFspfM9vVBdlsMMu0TikSEREZc9yd5U+8zLV3r+EPm9r52zMP48rzj9apxCIiMaFiFsqfZtzVFRSy+TykdJhERETGkl0dWT7y4ye4c9Umjmxt5muXnsRFJ85SISsiEiOq0iAoZnfu7L2uO5w7rqdHxayIiMgY0vbMZj55y1Ns3tPFp95wDH91xgKdViwiEkOq0qD/nlkIitnGxupnEhERkYq5c+Ur/Pzxl9myp5vfv7Cdww9p4ub3nM5JcydFHU1EREZIxSz0f80sBMWsiIiIxNa67R28/8bHaGlIM2tiPVecu5C/W3Y4dalk1NFEROQgqJiFwXtmRUREJLY+u3wlyYSx/H1nMHOiptsRERkrElEHqAn19VjfqXmKxWzf9SIiIhIbd63exF1Pb+aKcxeqkBURGWNUzIJ6ZkVERMaoXzy5kWnNGf7yjAVRRxERkQpTMQvlr5ktHc1YREREYmndjg4OO6SZdFJ/8oiIjDX6zQ7qmRURERmj1m3vZO5kzUogIjIWjVoxa2bfMbPNZvZUybopZvYrM3s2vJ8crjczu8bM1pjZk2a2eLRylVVfTyKbBff961TMioiIxFp3Ls+mPV3MmaxrZUVExqLR7Jn9HnBen3VXAne5+0LgrnAZ4HxgYXi7HPivUcx1oPp6zL33YE8qZkVERGLt5Z1duMPcKeqZFREZi0atmHX3e4HtfVZfDFwfPr4eeFPJ+u974AFgkpnNHK1sB6irC+6LBWzpY41mLCIiEkvrd3QAqGdWRGSMqvY1s63uvjF8/ArQGj6eDawr2W59uK466uuD+2IB666eWRERkZhbt70TUM+siMhYlYrqjd3dzcwH37I3M7uc4FRkWltbaWtrO+gsM154gaOB+9va6J4+HctmeV343FOPPsrWxtptBNvb2ytyDKotrrkhvtmVu/riml25ZaxYt6ODVMKY0VIfdRQRERkF1S5mN5nZTHffGJ5GvDlcvwGYW7LdnHDdAdz9OuA6gCVLlviyZcsOPtWG4K1OO/lkWLgQdu/e99RxRx4JlXiPUdLW1kZFjkGVxTU3xDe7cldfXLMrt4wV63d0MmtSA8mERR1FRERGQbVPM14OXBY+vgz4ecn6vwhHNT4V2FVyOvLo63vNbOm1szrNWERExigzO8/MnglnE7iyzPPzzOxuM3ssnG3ggihyDtfW9mCu+HXbO5g7RdfLioiMVaM5Nc+NwP3AUWa23szeDfwr8HozexY4N1wGuA1YC6wBvgX8/WjlKqvvNbOlxawGgBIRkTHIzJLAtQQzCiwC3m5mi/ps9ingR+5+MnAp8PXqphy+/3t2C6/6wq/5/fPbWb+jgzmTavdSIREROTijdpqxu7+9n6fOKbOtA+8drSyDKhaz3cE3ueqZFRGRcWApsMbd1wKY2Q8JZhdYVbKNAy3h44nAy1VNOAJ3rHwFd7jmrmfZ2t6jnlkRkTEssgGgaspAPbMqZkVEZGwqN5PAq/ts81ngTjN7P9BEcFZVzXJ32p7ZQiph3LdmKwBzJqtnVkRkrFIxC7pmVkREpLy3A99z96+Y2WnAf5vZce5eKN1oNGYagOGPUP1ye4H1Ozp50xFpbn0uS85hywtP07br2YrkGaq4jqwd19wQ3+zKXX1xza7c5amYhQN7ZounG4OKWRERGauGMpPAu4HzANz9fjOrB6axfzYCwucqP9MAwx+h+tv3PQ+s4kNvfg2Z3zzLjx5ez8XnnsH0CdWdmieuI2vHNTfEN7tyV19csyt3edUezbg26ZpZEREZfx4CFprZAjPLEAzwtLzPNi8RjnVhZscA9cCWqqYchrZnNnP4IU3MndLIx88/hmvfsbjqhayIiFSPilnQaMYiIjLuuHsOeB9wB7CaYNTilWb2OTO7KNzsQ8DfmNkTwI3Au8JBG2tOZ0+eB5/fzuuOnA7A5KYMbzhhZsSpRERkNOk0Y9A1syIiMi65+20E0+OVrruq5PEq4Ixq5xqJZzfvoSdXYOmCyVFHERGRKlHPLOg0YxERkZjbsKMT0OjFIiLjiYpZ0NQ8IiIiMbdhZ7GY1byyIiLjhYpZgHQaNztwNOO6OhWzIiIiMbB+RydNmSQTG9JRRxERkSpRMQtgRiGdPrBntqVFxayIiEgMrN/RyezJDZhZ1FFERKRKVMyGCpnMgdfMtrRoNGMREZEY2LCzU9fLioiMMypmQ4VMpnfPbDIJDQ3qmRUREYmBDTs6mD1J18uKiIwnKmZDBxSzdXWQyaiYFRERqXF7urLs7soxW4M/iYiMKypmQ973mtn6ehWzIiIiMVAcyVg9syIi44uK2VCva2a7u1XMioiIxERxjln1zIqIjC8qZkMHnGasYlZERCQW1ofF7Bz1zIqIjCsqZkNli9l0WqMZi4iI1LgNOzvJpBJMa66LOoqIiFSRitnQAfPMqmdWREQkFjbs6GT2pAYSCc0xKyIynqiYDR0wz6xGMxYREYmF9Ts7NfiTiMg4pGI2pGtmRURE4qnYMysiIuOLitlQr9OMNZqxiIhILGzY2cnW9m4WHNIUdRQREamySIpZM/uAma00s6fM7EYzqzezBWb2oJmtMbObzCxTzUzqmRUREYmfnz22AYALjpsZcRIREam2qhezZjYb+AdgibsfBySBS4EvAl919yOAHcC7q5nrgGtmNZqxiIhITXrkxe385ulNuDs/eXQ9S+dPYd7UxqhjiYhIlUV1mnEKaDCzFNAIbATOBn4cPn898KZqBlLPrIiISDxcc9ca/vr6h/naXc+ydste3rx4dtSRREQkAqlqv6G7bzCzLwMvAZ3AncAjwE53z4WbrQfKtkxmdjlwOUBrayttbW0VyTUL8K4u7rn7bk7fs4ct27aR6+5mbnc391boPUZDe3t7xY5BNcU1N8Q3u3JXX1yzK7fUus6ePAWHf//1s2RSCS44QacYi4iMR1UvZs1sMnAxsADYCdwMnDfU17v7dcB1AEuWLPFly5ZVJNfa//kfzJ1lr3kN5PPMPuwwaGmBbJZlr3sdWG3OXdfW1kaljkE1xTU3xDe7cldfXLMrt9S6rlyeY2e1sLMjy2sXTqOlPh11JBERiUDVi1ngXOB5d98CYGY/Bc4AJplZKuydnQNsqGaoQiYcb6qrq/doxgC5XHD9rIiIiESuK5tnwbQmfvbeM6jNr5pFRKQaorhm9iXgVDNrNDMDzgFWAXcDbwm3uQz4eTVDFYrFakdHcJ1saTGr62ZFRERqRle2QH06STqZIJXULIMiIuNV1VsAd3+QYKCnR4EVYYbrgI8BHzSzNcBU4NtVzVUsXHftCu6LoxmDRjQWERGJUDZf4A+b9uxb7s7lqU8lI0wkIiK1IJKvM939M+5+tLsf5+7vdPdud1/r7kvd/Qh3f6u7d1czU6FcMaueWRERkcj98KF1XPC1/2NXZ/DlctAzqx5ZEZHxTi1BaF8xu3NncF9Xp2JWRERqnpm90czGdHv++Es7yRWc3fuK2Tz1afXMioiMd2O68RuOA4pZ9cyKiEg8vA141sz+zcyOjjrMaFi9cTcAndk87k53rkCdilkRkXFPxWxo3wBQKmZFRCRG3P3PgZOB54Dvmdn9Zna5mU2IOFpFZPMF1mxuB4L5ZbtzBQCdZiwiIipmi7pmzAgePPpocK9iVkREYsLddxMMrvhDYCbwJ8CjZvb+SINVwHNb2unJBwVsZzZPVzYPoAGgRERExWxR5+zZMGsW/PKXwQqNZiwiIjFgZheZ2S1AG5AGlrr7+cCJwIcGee15ZvaMma0xsyv72eYSM1tlZivN7AeVzj+Y4inGUCxmiz2zKmZFRMa7VNQBaoYZnHUW3HBDsFxfH6wD9cyKiEgt+1Pgq+5+b+lKd+8ws3f39yIzSwLXAq8H1gMPmdlyd19Vss1C4OPAGe6+w8ymj8q/YACrN+6fkqerp6RnVqcZi4iMe2oJSp111v7HGs1YRETi4bPA74sLZtZgZvMB3P2uAV63FFgTTo3XQ3CK8sV9tvkb4Fp33xHub3PlYg/N6o27aakPvnvv6MnTlSsWs+qZFREZ79QzW6q0mK2vh0JwKpOKWRERqWE3A6eXLOfDda8a5HWzgXUly+uBV/fZ5kgAM/stkAQ+6+63992RmV0OXA7Q2tpKW1vbMOL3r729nSde7ODQlgQruuCJlavZtS74Hv4Pq1fSuO2ZirxPpbW3t1fsGFRTXHNDfLMrd/XFNbtyl6dittSCBTBvHrz0UlDM5nLBehWzIiJSu1JhzyoA7t5jZplK7RtYCCwD5gD3mtnx7r6zdCN3vw64DmDJkiW+bNmyirz5z++4m909HZx70uGs+PUfmLfgcBbNnggPPMDSxSdx+hHTKvI+ldbW1kaljkE1xTU3xDe7cldfXLMrd3k6zbhU8bpZ0GjGIiISF1vM7KLigpldDGwdwus2AHNLlueE60qtB5a7e9bdnwf+QFDcVsXL7cEZUifOnQgEU/MUr5nVPLMiIqJitq9LLoEZM+CQQzSasYiIxMF7gE+Y2Utmtg74GPC3Q3jdQ8BCM1sQ9uReCizvs83PCHplMbNpBKcdr61U8MF05x2ASY0Z0knrM5qx/oQRERnvdJpxXxdcABs3Bo/VMysiIjXO3Z8DTjWz5nC5fYivy5nZ+4A7CK6H/Y67rzSzzwEPu/vy8Lk/MrNVBNfifsTdt43KP6SMsJYllTDq00k6s3m6NQCUiIiEhlTMmlkT0OnuBTM7Ejga+KW7j+0uSxWzIiISA2b2BuBYoN7CaeXc/XODvc7dbwNu67PuqpLHDnwwvFVdsZhNJxM0pJO9TjNWMSsiIkM9R+deggZyNnAn8E7ge6MVqmaomBURkRpnZt8A3ga8HzDgrcChkYaqkFw4qUA6aTRkkr1PM07pNGMRkfFuqC2BuXsH8Gbg6+7+VoJvgMc2FbMiIlL7Tnf3vwB2uPs/AacRTqkTd/lC0DWrnlkRESlnyMWsmZ0G/Bnwi3Dd2G9FVMyKiEjt6wrvO8xsFpAFZkaYp2L2XTNb0jPbnSsOADX2/wwREZGBDXUAqCuAjwO3hINDHAbcPXqxaoRGMxYRkdr3v2Y2CfgS8CjgwLeijVQZ+weACnpmu7JBz2w6aSQTFm04ERGJ3JCKWXe/B7gHwMwSwFZ3/4fRDFYTisWsemZFRKQGhW3yXe6+E/iJmd0K1Lv7roijVUTxmtlMeJrx7q4sXdkCdSn1yoqIyBBPMzazH5hZSziq8VPAKjP7yOhGqwGJBKRSQTH7x38MH/hA1IlERET2cfcCcG3JcvdYKWQB8h50zaaSRn0mSUdPnq5cXnPMiogIMPRrZhe5+27gTcAvgQUEIxqPfZkMdHXB//0f3D32z6wWEZHYucvM/tSKc/KMIfmwZzaVtOA043AAKPXMiogIDL2YTZtZmqCYXR7OL+sjfVMzm2RmPzazp81stRubKlUAABxKSURBVJmdZmZTzOxXZvZseD95pPuvqEwGnn8eOjvh6achn486kYiISKm/BW4Gus1st5ntMbPdUYeqhH3zzIbXzHZm83RnC+qZFRERYOjF7DeBF4Am4F4zOxQ4mIbya8Dt7n40cCKwGriS4LqfhcBd4XL0MhlYuTJ43N0dFLYiIiI1wt0nuHvC3TPu3hIut0SdqxJyBUgmjETCaNw3z2xeIxmLiAgw9AGgrgGuKVn1opmdNZI3NLOJwJnAu8J99wA9ZnYxsCzc7HqgDfjYSN6jotJpWLNm//LKlXDEEdHlERERKWFmZ5Zb7+73VjtLpeUdUuGoxfXpJF3ZAp0qZkVEJDSkYjYsQD9DUIRCMLLx54CRDDKxANgCfNfMTgQeAf4RaHX3jeE2rwCtI9h35WUyUChAXV3QM7tqFVx8cdSpREREikoHZKwHlhK0rWdHE6dy8gUnnQxOImvIBAXszo4sk5vSUcYSEZEaMdR5Zr9DMIrxJeHyO4HvAm8e4XsuBt7v7g+a2dfoc0qxu7uZlb0m18wuBy4HaG1tpa2tbQQRDtTe3l52X0tzORqB9jlzSO3Zw67f/IbVp51WkfeshP5y17q45ob4Zlfu6otrduWOF3d/Y+mymc0F/j2iOBWV92DwJ4CGdLGY7WHmxPooY4mISI0YajF7uLv/acnyP5nZ4yN8z/XAend/MFz+MUExu8nMZrr7RjObCWwu92J3vw64DmDJkiW+bNmyEcbora2tjbL7mjQJ1q2jefFi2L2b+s2baa3Qe1ZCv7lrXFxzQ3yzK3f1xTW7csfeeuCYqENUQq7A/p7ZsJjd3tGj04xFRAQYejHbaWavcff7AMzsDKBzJG/o7q+Y2TozO8rdnwHOAVaFt8uAfw3vfz6S/VdcJhPcH3kkdHTAPfcEIxon1ZCKiEj0zOw/2D/DQAI4CXg0ukSVk3dIF6+ZDU8z7soWqNNoxiIiwtCL2fcA3w+vnQXYQVBwjtT7gRvMLAOsBf6SoAH+kZm9G3iR/ac0R6u0mO3pCeacfeEFOPzwSGOJiIiEHi55nANudPffRhWmkvIFJxX2zDaW9MaqZ1ZERGDooxk/AZxoZi3h8m4zuwJ4ciRv6u6PA0vKPHXOSPY3qtLhIBNHHgkefvG9apWKWRERqRU/BrrcPQ9gZkkza3T3johzHbRe18xmSorZlIpZEREZ+jyzQFDEuntxftkPjkKe2lPaM3tMeAlScd5ZERGR6N0FNJQsNwC/jihLReUKkAl7Zut79czqNGMRERn6acblWMVS1LJMBqZMCW4QDAi1fn20mURERPard/f24oK7t5tZY5SBKqXcaMag04xFRCRwMMVs2alzxpzTT4c5c/YvT58Om8sOtCwiIhKFvWa22N0fBTCzUxjhII21Jl+AVKL3PLOgnlkREQkMWMya2R7KF61G71Oaxq5PfrL3cmsrbNoUTRYREZEDXQHcbGYvE7TPM4C3RRupMvLuNKhnVkRE+jFgMevuE6oVJDZaW+Gpp6JOISIiAoC7P2RmRwNHhauecfdslJkqpdc8sxoASkRE+tB5OsM1fbp6ZkVEpGaY2XuBJnd/yt2fAprN7O+jzlUJwTWzYTFb0hureWZFRARUzA5fayvs2BHMOSsiIhK9v3H3ncUFd98B/E2EeSom75BOBKcZp5NGMnys04xFRARUzA7f9OnB/ZYt0eYQEREJJM1s3wwDZpYEMhHmqZh8wfeNZmxm+3pnVcyKiAiomB2+1tbgXiMai4hIbbgduMnMzjGzc4AbgV9GnKkiSq+Zhf1FbH1Kf76IiMjBTc0zPhV7ZnXdrIiI1IaPAZcD7wmXnyQY0Tj28t67mG3IBI/VMysiIqCe2eFTz6yIiNQQdy8ADwIvAEuBs4HVUWaqlLxDKrHvDGqdZiwiIr2oZ3a41DMrIiI1wMyOBN4e3rYCNwG4+1lR5qqkXGH/aMYADZngz5Z6jWYsIiKoZ3b4JkyA+nr1zIqISNSeJuiFvdDdX+Pu/wHkh7MDMzvPzJ4xszVmduUA2/2pmbmZLTnIzMOSdyeTLO2Z1WnGIiKyn4rZ4TILemdVzIqISLTeDGwE7jazb4WDP9kgr9knHPX4WuB8YBHwdjNbVGa7CcA/EpzKXFX5vj2z+waAUjErIiIqZkemtVWnGYuISKTc/WfufilwNHA3cAUw3cz+y8z+aAi7WAqscfe17t4D/BC4uMx2nwe+CHRVKPqQ5Zx9U/MANGSCIrZOpxmLiAi6ZnZkpk+Hl1+OOoWIiAjuvhf4AfADM5sMvJVghOM7B3npbGBdyfJ64NWlG5jZYmCuu//CzD7S347M7HKCEZVpbW2lra1tuP+MsvIF5+V162hrC75A3rWtGwPuv+9eSqbWrTnt7e0VOwbVFNfcEN/syl19cc2u3OWpmB2J1lZ47LGoU4iIiPTi7juA68LbQTGzBHA18K4hvO++91yyZIkvW7bsYN+efMHx22/jiMMWsGzZQgB+tWMFj2xZz1ln1fYYV21tbVTiGFRbXHNDfLMrd/XFNbtyl6didiSK18y6B9fQioiIxM8GYG7J8pxwXdEE4DigLewFnQEsN7OL3P3h0Q6XzReA3qcZn3b4VNq7c6P91iIiEhMqZkeitRVyOdixA6ZMiTqNiIjISDwELDSzBQRF7KXAO4pPuvsuYFpx2czagA9Xo5AFyBUcgHRJMXvhCbO48IRZ1Xh7ERGJAY2gMBLFuWY1orGIiMSUu+eA9wF3AKuBH7n7SjP7nJldFG06yBV7ZhP6U0VERMpTz+xItLYG95s2wdFHR5tFRERkhNz9NuC2Puuu6mfbZdXIVNQTFrPplIpZEREpL7IWwsySZvaYmd0aLi8wswfDidtvMrNMVNkGVeyZ1fQ8IiIioyKXD08zTmhsChERKS/Krzv/keC0pqIvAl919yOAHcC7I0k1FLPC63U0PY+IiMioKBazqaR6ZkVEpLxIWggzmwO8Afh/4bIBZwM/Dje5HnhTFNmGZMoUaGyEl16KOomIiMiYlC2Epxkn1TMrIiLlRXXN7L8DHyUY9h9gKrAzHIwCgonbZ5d74WhNzD7cCX1fNW0aHY88wsqIJy/WBMrVF9fsyl19cc2u3FILilPzpNUzKyIi/ah6MWtmFwKb3f0RM1s23NePxsTsMIIJfY8+mqYdOyKfvFgTKFdfXLMrd/XFNbtySy3Yd5qxrpkVEZF+RNEzewZwkZldANQDLcDXgElmlgp7Z/tO3F575s2DJ56IOoWIiMiYpJ5ZEREZTNVbCHf/uLvPcff5BBO0/8bd/wy4G3hLuNllwM+rnW1Y5s0LRjPu6oo6iYiIyJiTKxQHgFLPrIiIlFdLX3d+DPigma0huIb22xHnGdi8ecH9+vXR5hARERmDsjn1zIqIyMCiGgAKAHdvA9rCx2uBpVHmGZZiMbtuHRxxRLRZRERExphs2DOr0YxFRKQ/+rpzpIrFrKbnERERqbhceM1sKqE/VUREpDy1ECM1Z05wr2JWRESk4rJ5XTMrIiIDUzE7UnV1MGOGilkREZFRUBzNOKNrZkVEpB9qIQ7GvHkqZkVEREZBrhCeZqxiVkRE+qEW4mComBURERkV+04zTug0YxERKU/F7MEoFrPPPgvf+Q64R51IRERkTMjli6MZ608VEREpL9KpeWJv3jzo6IBXvQp27YIzz9Q0PSIiIhVQvGZWU/OIiEh/9HXnwShOz5MKvxNYuTK6LCIiImNIsZjVNbMiItIftRAH49xz4fOfh4ceCpZXrYo2j4iIyBiRKxRPM1bPrIiIlKfTjA/GhAnwqU8Fj+fOVc+siIhIheSKPbMJfe8uIiLlqYWolGOPVTErIiJSIT159cyKiMjAVMxWyqJF8PTTkM9HnURERCT2cvkCSQMzFbMiIlKeitlKOfZY6OqC55+POomIiEjs5QqOOmVFRGQgKmYrZdGi4F6nGouIiBy0bL6ABjIWEZGBqJmolGIxWxzRePt2+Od/hquvhm3bosslIiISQ9l8gZR6ZkVEZAAqZiulpSUY0fjJJ+HLX4YFC+DTn4YPfQhmz4Zrr406oYiISGzk8k4yoWpWRET6p2K2khYtgh/+ED7yETjzTFixIihuzzkH3vc++PrXo04oIiISC9m8rpkVEZGBqZitpLPOgilT4IYbYPlyOO44OP54uOUWeOMb4b3vhVtvjTqliIhIzcsVdM2siIgMTM1EJX30o7B1K7zjHVA6lUAmAzffHIx4/A//AJ2d0WUUEREJmdl5ZvaMma0xsyvLPP9BM1tlZk+a2V1mdmi1sumaWRERGYyK2Uoy613Elqqrg2uuCabu+cpXqptLRESkDzNLAtcC5wOLgLeb2aI+mz0GLHH3E4AfA/9WrXxZXTMrIiKDqHoxa2Zzzezu8JvelWb2j+H6KWb2KzN7NryfXO1so+7ss+Etb4F/+RdYuzbqNCIiMr4tBda4+1p37wF+CFxcuoG73+3uHeHiA8CcaoXL5Qu6ZlZERAYURc9sDviQuy8CTgXeG34TfCVwl7svBO4Kl8eeq68OTjt+xzsgm406jYiIjF+zgXUly+vDdf15N/DLUU1UIlfQAFAiIjKwVLXf0N03AhvDx3vMbDVB43kxsCzc7HqgDfhYtfONurlz4VvfgksugSuvhM9/Hhobo04lIiLSLzP7c2AJ8Lp+nr8cuBygtbWVtra2g37PzVs7Mc9XZF/V1t7ertxVFtfsyl19cc2u3OVVvZgtZWbzgZOBB4HWsNAFeAVojSjW6HvrW+Gv/zropb3mmmBO2kwGTj01mKN20qSoE4qIyNi3AZhbsjwnXNeLmZ0LfBJ4nbt3l9uRu18HXAewZMkSX7Zs2UGH+4/Vv6OzfReV2Fe1tbW1KXeVxTW7cldfXLMrd3mRFbNm1gz8BLjC3XdbycBJ7u5m5v28ruLf/kIE33ZceilTDj+ciStW0PDyyyR6epj63e/SfeutrLzqKvYs6jsGR3n6lqb64ppduasvrtmVe9x4CFhoZgsIithLgXeUbmBmJwPfBM5z983VDBdcM6vzjEVEpH+RFLNmliYoZG9w95+GqzeZ2Ux332hmM4GyjeZofPsLEX3bcc45vZd//3vqL72UUz7zGXj4YTh08BkQ9C1N9cU1u3JXX1yzK/f44O45M3sfcAeQBL7j7ivN7HPAw+6+HPgS0AzcHH7p/JK7X1SNfNm8k1EtKyIiA6h6MWtBa/htYLW7X13y1HLgMuBfw/ufVztb5JYuhdtvh1e9Ct78ZrjvPmhoiDqViIiMUe5+G3Bbn3VXlTw+t+qhQrlCgUZNICgiIgOIopk4A3gncLaZPR7eLiAoYl9vZs8C54bL48+RR8INN8Bjj8G73w1e9mxrERGRMS2b12jGIiIysChGM74P6K95Oqef9ePLhRfCF74An/gEHHMMfPrTUScSERGpqmy+QFLnGYuIyAAiHc1YBnDllbB6NVx1FRx1VDCVj4iIyDiRU8+siIgMQlej1CqzYD7a00+Hyy6Dhx6KOpGIiEjV5AoFUvorRUREBqBmopbV1cEtt8CMGXDxxbB+fdSJREREqqInV1DPrIiIDEjFbK2bPh3+939hz56goO3oiDqRiIjIqMsVnKT+ShERkQGomYiD446DG28MRjh+5zuhuzvqRCIiIqMquGZWXbMiItI/FbNxceGFcPXV8NOfwqmnwtNPR51IRERkVLg7WV0zKyIig1AzESdXXAHLl8O6dUFv7dvexpT774fNm6NOJiIiUjH5guOOrpkVEZEBqZiNmze+EVasgA98AO64gxM+8QlobYVjjw2m8/nd7yCfjzqliIjIiOUKDqBrZkVEZECaZzaOZs6EL30J/umfeOy66zg5m4Xbb4evfAW++EWYNg0WL4ajj4ampmBU5NJbYyM0NwdF8Pz5MGsWJJNR/6tEREQAyOYLALpmVkREBqRiNs4aG9l10kmwbBl85COwc2dQ1N5xBzzxRNBL29k5eE9tKgVz5sBhhx14mzMHpk6FTKYq/yQREZFcPuiZ1TWzIiIyEBWzY8mkSXDppcGtVD4PPT3BKMjd3UGBu3s3bNwIL74IL7wQ3J5/PpgGaNOmA/c9YUJQ1DY0QDodFMDpdNDL29gY9AA3NEAiEfTyDnK/YN06+M1v9q8v3lKp4FZ8nEj0f+u7376Py3Hv//gVewDMet/6rJv0+OODbsPB9CaM9LWDvG7C6tXBz6pS73cwrx3G65qfeSb4/B3M+2Uy0NIS3CZMqO6ZCO7BrVAof19U+jnqez+S50RibH/PbMRBRESkpqmYHQ+SyaDQbGjovf6EE8pv394eFLdr18KGDbBt2/5bVxdks5DLBQVyRwe8/HJw39kZ/IFeKAQF9AD383K5/X/kx8xJUQcYoVOiDjBCS0Zjp8lkUPSVfjlSXC73BQXs/6Klo2P/fM+l25cpVpdF/fkeqAgu3pdZ99pCYf8xGsbrhrXNwe677/3Xvw719QMfD4mNrK6ZFRGRIVAxKwdqbg5GSz7uuFF7i3va2li2bNn+P/5zuaDQLb0vFrzFArlvoTzY4/56qMqtLxYdxQK7tNDus/z4Y49x0okn9r/NwRQwI33tEF735JNPckLfLzBqNGupFStWcPzxxx9c1q4u2LMnOCNh9+7gC5nS4rN4Ky4Xc5b+fIufyYaG4EyEYgFbfF2ZoviFl15i/oIFBxbLpUWwWe/36Xs/Gs8Nss2GF19k3ty5o7LviuQudz91Kuzdi4wNdakEbzppFq2Z7VFHERGRGqZiVqJltv8U45jYCcF1yjGzvbExlrm3TZgQy9wAL7S1MT+G2de2tTEvhrlpa4s6gVTItOY6/v3Sk2nTz1RERAagE3hEREREREQkdlTMioiIiIiISOyomBUREREREZHYUTErIiIiIiIisaNiVkRERERERGJHxayIiIiIiIjEjvnBzN0YMTPbArxYod1NA7ZWaF/VpNzVF9fsyl19cc0+nnMf6u6HVCLMeKW2GVDuKMQ1u3JXX1yzj+fc/bbNsS5mK8nMHnb3JVHnGC7lrr64Zlfu6otrduWWWhHXn6lyV19csyt39cU1u3KXp9OMRUREREREJHZUzIqIiIiIiEjsqJjd77qoA4yQcldfXLMrd/XFNbtyS62I689UuasvrtmVu/riml25y9A1syIiIiIiIhI76pkVERERERGR2Bn3xayZnWdmz5jZGjO7Muo8/TGzuWZ2t5mtMrOVZvaP4frPmtkGM3s8vF0QddZyzOwFM1sRZnw4XDfFzH5lZs+G95OjzlnKzI4qOa6Pm9luM7uiVo+5mX3HzDab2VMl68oeYwtcE37unzSzxTWW+0tm9nSY7RYzmxSun29mnSXH/hs1lrvfz4aZfTw83s+Y2R9Hk3pflnLZbyrJ/YKZPR6ur6Vj3t/vwZr/nMvwqG2uDrXNo09tc03kVts8urmjbZvdfdzegCTwHHAYkAGeABZFnaufrDOBxeHjCcAfgEXAZ4EPR51vCPlfAKb1WfdvwJXh4yuBL0adc5DPyivAobV6zIEzgcXAU4MdY+AC4JeAAacCD9ZY7j8CUuHjL5bknl+6XQ0e77KfjfD/6hNAHbAg/L2TrKXsfZ7/CnBVDR7z/n4P1vznXLdh/ZzVNlcvv9rm0c+otjn63GqbRzd3pG3zeO+ZXQqscfe17t4D/BC4OOJMZbn7Rnd/NHy8B1gNzI421UG7GLg+fHw98KYIswzmHOA5d38x6iD9cfd7ge19Vvd3jC8Gvu+BB4BJZjazOkl7K5fb3e9091y4+AAwp+rBBtHP8e7PxcAP3b3b3Z8H1hD8/onEQNnNzIBLgBurGmoIBvg9WPOfcxkWtc3RUttcQWqbq0ttc/VF3TaP92J2NrCuZHk9MWiEzGw+cDLwYLjqfWE3/Xdq7XSgEg7caWaPmNnl4bpWd98YPn4FaI0m2pBcSu9fIHE45tD/MY7TZ/+vCL7BK1pgZo+Z2T1m9tqoQg2g3GcjTsf7tcAmd3+2ZF3NHfM+vwfHwudc9ovlz01tcyTUNkdHbXN1qW3ux3gvZmPHzJqBnwBXuPtu4L+Aw4GTgI0EpyDUote4+2LgfOC9ZnZm6ZMenHdQk0Nrm1kGuAi4OVwVl2PeSy0f4/6Y2SeBHHBDuGojMM/dTwY+CPzAzFqiyldGLD8bfbyd3n8c1twxL/N7cJ84fs4l/tQ2V5/a5uiobY6E2uZ+jPdidgMwt2R5TriuJplZmuBDcoO7/xTA3Te5e97dC8C3iPD0iIG4+4bwfjNwC0HOTcXTCsL7zdElHND5wKPuvgnic8xD/R3jmv/sm9m7gAuBPwt/CRKeCrQtfPwIwfUtR0YWso8BPhs1f7wBzCwFvBm4qbiu1o55ud+DxPhzLmXF6uemtjkyapsjoLa5+tQ2D2y8F7MPAQvNbEH4Dd+lwPKIM5UVniv/bWC1u19dsr70HPM/AZ7q+9qomVmTmU0oPiYYQOApgmN9WbjZZcDPo0k4qF7fhsXhmJfo7xgvB/4iHFHuVGBXyakgkTOz84CPAhe5e0fJ+kPMLBk+PgxYCKyNJuWBBvhsLAcuNbM6M1tAkPv31c43BOcCT7v7+uKKWjrm/f0eJKafc+mX2uYqUNscqVj+zlLbHBm1zQPxGhgFK8obwYhafyD4RuOTUecZIOdrCLrnnwQeD28XAP8NrAjXLwdmRp21TPbDCEaLewJYWTzOwFTgLuBZ4NfAlKizlsneBGwDJpasq8ljTtCobwSyBNcfvLu/Y0wwgty14ed+BbCkxnKvIbieovhZ/0a47Z+Gn6HHgUeBN9ZY7n4/G8Anw+P9DHB+rX1WwvXfA97TZ9taOub9/R6s+c+5bsP+WattHv3sapurk1Vtc/S51TaPbu5I22YLdyoiIiIiIiISG+P9NGMRERERERGJIRWzIiIiIiIiEjsqZkVERERERCR2VMyKiIiIiIhI7KiYFRERERERkdhRMSsimNkyM7s16hwiIiISUNssMjgVsyIiIiIiIhI7KmZFYsTM/tzMfm9mj5vZN80saWbtZvZVM1tpZneZ2SHhtieZ2QNm9qSZ3WJmk8P1R5jZr83sCTN71MwOD3ffbGY/NrOnzewGM7PI/qEiIiIxobZZJDoqZkViwsyOAd4GnOHuJwF54M+AJuBhdz8WuAf4TPiS7wMfc/cTgBUl628ArnX3E4HTgY3h+pOBK4BFwGHAGaP+jxIREYkxtc0i0UpFHUBEhuwc4BTgofCL2QZgM1AAbgq3+R/gp2Y2EZjk7veE668HbjazCcBsd78FwN27AML9/d7d14fLjwPzgftG/58lIiISW2qbRSKkYlYkPgy43t0/3mul2af7bOcj3H93yeM8+v0gIiIyGLXNIhHSacYi8XEX8BYzmw5gZlPM7FCC/8dvCbd5B3Cfu+8CdpjZa8P17wTucfc9wHoze1O4jzoza6zqv0JERGTsUNssEiF9uyMSE+6+ysw+BdxpZgkgC7wX2AssDZ/bTHDtDsBlwDfCBnEt8Jfh+ncC3zSzz4X7eGsV/xkiIiJjhtpmkWiZ+0jPehCRWmBm7e7eHHUOERERCahtFqkOnWYsIiIiIiIisaOeWREREREREYkd9cyKiIiIiIhI7KiYFRERERERkdhRMSsiIiIiIiKxo2JWREREREREYkfFrIiIiIiIiMSOilkRERERERGJnf8Pe+VDzzrRm1EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.9564516129032258 achieved at epoch: 129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VciQvn2_pQaD"
      },
      "source": [
        "# **Test 0** *(epoch = 1000, nodes_per_layer = [dim, 200, 100, 10]; activation = tanh; optimiser = Adam; learn_rate = 0.001; reg_coeff = 0.0; drop_prob = 0.0)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbEiyWHXbzWN",
        "outputId": "9c74bac7-d2b6-46e1-ea8c-2358bd1a8183"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 200, 100, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 0.0; drop_prob = 0.0\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-4114d2b8d98b>:30: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-10-4114d2b8d98b>:31: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAcLxnLsbzbg",
        "outputId": "1f31e102-048d-4e73-eeef-23b166ffaecd"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.40728 and 1.7635088\n",
            "Val acc and loss are 0.4104 and 1.7584035\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.59 and 1.3106408\n",
            "Val acc and loss are 0.5986 and 1.301252\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.65342 and 1.0842649\n",
            "Val acc and loss are 0.6572 and 1.0726311\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.6902 and 0.9477736\n",
            "Val acc and loss are 0.6926 and 0.9366108\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.71168 and 0.85361814\n",
            "Val acc and loss are 0.7111 and 0.84477323\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.72942 and 0.78730935\n",
            "Val acc and loss are 0.7276 and 0.78140694\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.74586 and 0.74076486\n",
            "Val acc and loss are 0.7403 and 0.73762417\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.75724 and 0.7065513\n",
            "Val acc and loss are 0.7536 and 0.7056287\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.76782 and 0.6782181\n",
            "Val acc and loss are 0.7642 and 0.67880136\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.776 and 0.6534251\n",
            "Val acc and loss are 0.7707 and 0.654841\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.78222 and 0.6325733\n",
            "Val acc and loss are 0.7771 and 0.6343754\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.78706 and 0.61554945\n",
            "Val acc and loss are 0.7829 and 0.6175547\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.7917 and 0.6008959\n",
            "Val acc and loss are 0.7867 and 0.60309273\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.79544 and 0.587193\n",
            "Val acc and loss are 0.7912 and 0.5896362\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.79894 and 0.574086\n",
            "Val acc and loss are 0.7953 and 0.57685596\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.803 and 0.5619117\n",
            "Val acc and loss are 0.7993 and 0.56511384\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.80754 and 0.55095136\n",
            "Val acc and loss are 0.8031 and 0.55470437\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.81044 and 0.5411927\n",
            "Val acc and loss are 0.8067 and 0.5455951\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.8143 and 0.5324456\n",
            "Val acc and loss are 0.8104 and 0.5375397\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.81714 and 0.52442884\n",
            "Val acc and loss are 0.8115 and 0.5301859\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.81988 and 0.5168347\n",
            "Val acc and loss are 0.8146 and 0.5231739\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.82286 and 0.50947833\n",
            "Val acc and loss are 0.8166 and 0.5163111\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.8251 and 0.5024246\n",
            "Val acc and loss are 0.8199 and 0.50969803\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.82782 and 0.49588153\n",
            "Val acc and loss are 0.8212 and 0.5036005\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.82988 and 0.4899143\n",
            "Val acc and loss are 0.8226 and 0.49813583\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.83168 and 0.48432246\n",
            "Val acc and loss are 0.8239 and 0.49313322\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.8334 and 0.47884914\n",
            "Val acc and loss are 0.8258 and 0.48833925\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.8358 and 0.47344914\n",
            "Val acc and loss are 0.8268 and 0.48368633\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.83802 and 0.4682786\n",
            "Val acc and loss are 0.8294 and 0.47928125\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.84012 and 0.46345487\n",
            "Val acc and loss are 0.8306 and 0.475173\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.8418 and 0.45891538\n",
            "Val acc and loss are 0.8317 and 0.47123447\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.84346 and 0.45452663\n",
            "Val acc and loss are 0.8328 and 0.46730292\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.84482 and 0.45025477\n",
            "Val acc and loss are 0.8342 and 0.4633663\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.84576 and 0.446171\n",
            "Val acc and loss are 0.8352 and 0.45955566\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.84674 and 0.44233382\n",
            "Val acc and loss are 0.8361 and 0.45599765\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.84796 and 0.43871835\n",
            "Val acc and loss are 0.838 and 0.45271942\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.8489 and 0.4352399\n",
            "Val acc and loss are 0.8391 and 0.44966212\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.84994 and 0.43181425\n",
            "Val acc and loss are 0.8392 and 0.44674376\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.85088 and 0.42842266\n",
            "Val acc and loss are 0.8404 and 0.443931\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.85258 and 0.42513317\n",
            "Val acc and loss are 0.8407 and 0.44126734\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.85334 and 0.42202142\n",
            "Val acc and loss are 0.8416 and 0.43879458\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.85432 and 0.4190612\n",
            "Val acc and loss are 0.8429 and 0.4364471\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.85528 and 0.41614798\n",
            "Val acc and loss are 0.8437 and 0.43408805\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.85596 and 0.41323808\n",
            "Val acc and loss are 0.8442 and 0.43166396\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.85702 and 0.41039547\n",
            "Val acc and loss are 0.8442 and 0.42925468\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.8579 and 0.4076793\n",
            "Val acc and loss are 0.8462 and 0.42694756\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.8588 and 0.40505558\n",
            "Val acc and loss are 0.8472 and 0.42473415\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.85982 and 0.4024614\n",
            "Val acc and loss are 0.8485 and 0.42256767\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.86118 and 0.39990243\n",
            "Val acc and loss are 0.8492 and 0.42045987\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.86246 and 0.3974318\n",
            "Val acc and loss are 0.8504 and 0.41845924\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.86338 and 0.39506266\n",
            "Val acc and loss are 0.8512 and 0.41656515\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.86432 and 0.39275542\n",
            "Val acc and loss are 0.8522 and 0.41472262\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.8651 and 0.39048305\n",
            "Val acc and loss are 0.852 and 0.41289762\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.86584 and 0.3882588\n",
            "Val acc and loss are 0.8531 and 0.41110957\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.86682 and 0.38609487\n",
            "Val acc and loss are 0.8535 and 0.40938437\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.86752 and 0.38397726\n",
            "Val acc and loss are 0.8539 and 0.40771952\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.86838 and 0.38189453\n",
            "Val acc and loss are 0.8548 and 0.4061063\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.86916 and 0.37985805\n",
            "Val acc and loss are 0.8553 and 0.40454912\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.8699 and 0.37787992\n",
            "Val acc and loss are 0.8556 and 0.40304747\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.87076 and 0.3759507\n",
            "Val acc and loss are 0.8565 and 0.40157965\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.8715 and 0.37405044\n",
            "Val acc and loss are 0.857 and 0.4001192\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.872 and 0.3721725\n",
            "Val acc and loss are 0.8584 and 0.39866045\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.87272 and 0.37032515\n",
            "Val acc and loss are 0.8591 and 0.39721715\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.87344 and 0.3685143\n",
            "Val acc and loss are 0.8601 and 0.39580193\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.8742 and 0.36673597\n",
            "Val acc and loss are 0.8611 and 0.39441657\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.87464 and 0.364985\n",
            "Val acc and loss are 0.8612 and 0.3930608\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.8756 and 0.36326203\n",
            "Val acc and loss are 0.8616 and 0.39174\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.8765 and 0.36156774\n",
            "Val acc and loss are 0.8623 and 0.39045978\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.87708 and 0.35989833\n",
            "Val acc and loss are 0.8623 and 0.3892205\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.87756 and 0.3582522\n",
            "Val acc and loss are 0.8626 and 0.38802096\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.87816 and 0.3566328\n",
            "Val acc and loss are 0.8631 and 0.38685897\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.87866 and 0.3550392\n",
            "Val acc and loss are 0.8635 and 0.38572383\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.87904 and 0.35346445\n",
            "Val acc and loss are 0.8636 and 0.38459978\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.87978 and 0.35190725\n",
            "Val acc and loss are 0.864 and 0.38348305\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.88064 and 0.35037276\n",
            "Val acc and loss are 0.8643 and 0.3823837\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.88078 and 0.34885985\n",
            "Val acc and loss are 0.865 and 0.38131005\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.88118 and 0.3473625\n",
            "Val acc and loss are 0.8651 and 0.38026342\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.8816 and 0.34588155\n",
            "Val acc and loss are 0.8656 and 0.379245\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.88184 and 0.34442034\n",
            "Val acc and loss are 0.8658 and 0.37825263\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.8825 and 0.34297585\n",
            "Val acc and loss are 0.8656 and 0.37727696\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.8829 and 0.3415447\n",
            "Val acc and loss are 0.8657 and 0.3763131\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.8832 and 0.34012926\n",
            "Val acc and loss are 0.8657 and 0.37536818\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.88364 and 0.3387304\n",
            "Val acc and loss are 0.8662 and 0.37445045\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.88434 and 0.3373449\n",
            "Val acc and loss are 0.8672 and 0.37355983\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.8848 and 0.3359725\n",
            "Val acc and loss are 0.8676 and 0.372691\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.88524 and 0.33461547\n",
            "Val acc and loss are 0.8674 and 0.37183526\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.88574 and 0.33327246\n",
            "Val acc and loss are 0.8676 and 0.3709814\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.88626 and 0.33194077\n",
            "Val acc and loss are 0.8683 and 0.37012416\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.8867 and 0.3306208\n",
            "Val acc and loss are 0.8689 and 0.36927006\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.88718 and 0.32931337\n",
            "Val acc and loss are 0.869 and 0.36843032\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.8876 and 0.32801738\n",
            "Val acc and loss are 0.8698 and 0.3676117\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.88818 and 0.3267318\n",
            "Val acc and loss are 0.8703 and 0.3668144\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.8886 and 0.325457\n",
            "Val acc and loss are 0.8704 and 0.3660344\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.88894 and 0.3241928\n",
            "Val acc and loss are 0.8709 and 0.36526614\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.88936 and 0.32293838\n",
            "Val acc and loss are 0.8704 and 0.36450678\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.88988 and 0.32169375\n",
            "Val acc and loss are 0.8712 and 0.36375752\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.89036 and 0.32045898\n",
            "Val acc and loss are 0.8713 and 0.3630209\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.89058 and 0.31923336\n",
            "Val acc and loss are 0.8718 and 0.3622972\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.8909 and 0.31801638\n",
            "Val acc and loss are 0.8715 and 0.3615847\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.89128 and 0.31680804\n",
            "Val acc and loss are 0.8715 and 0.36088148\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.89168 and 0.31560835\n",
            "Val acc and loss are 0.8716 and 0.36018613\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.89216 and 0.3144169\n",
            "Val acc and loss are 0.8718 and 0.35949835\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.89278 and 0.31323364\n",
            "Val acc and loss are 0.8722 and 0.3588188\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.8932 and 0.31205845\n",
            "Val acc and loss are 0.8722 and 0.3581481\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.89372 and 0.31089076\n",
            "Val acc and loss are 0.8724 and 0.3574868\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.89412 and 0.30973056\n",
            "Val acc and loss are 0.8727 and 0.35683638\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.89444 and 0.30857766\n",
            "Val acc and loss are 0.873 and 0.3561977\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.8949 and 0.30743167\n",
            "Val acc and loss are 0.8734 and 0.35557038\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.8956 and 0.3062925\n",
            "Val acc and loss are 0.8736 and 0.3549527\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.89572 and 0.30515996\n",
            "Val acc and loss are 0.874 and 0.35434267\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.89632 and 0.30403367\n",
            "Val acc and loss are 0.874 and 0.35373875\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.8968 and 0.30291343\n",
            "Val acc and loss are 0.874 and 0.3531414\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.8973 and 0.30179927\n",
            "Val acc and loss are 0.8742 and 0.35255152\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.89766 and 0.30069086\n",
            "Val acc and loss are 0.874 and 0.35196903\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.89812 and 0.29958805\n",
            "Val acc and loss are 0.8741 and 0.35139325\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.89856 and 0.2984908\n",
            "Val acc and loss are 0.8744 and 0.35082385\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.89904 and 0.2973989\n",
            "Val acc and loss are 0.8749 and 0.35026145\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.8993 and 0.29631215\n",
            "Val acc and loss are 0.8749 and 0.3497072\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.89982 and 0.2952306\n",
            "Val acc and loss are 0.875 and 0.3491614\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.90016 and 0.29415402\n",
            "Val acc and loss are 0.8749 and 0.34862256\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.9004 and 0.29308224\n",
            "Val acc and loss are 0.8753 and 0.34808874\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.90072 and 0.29201505\n",
            "Val acc and loss are 0.8755 and 0.34755954\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.90112 and 0.2909524\n",
            "Val acc and loss are 0.876 and 0.34703654\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.90148 and 0.2898941\n",
            "Val acc and loss are 0.8763 and 0.3465216\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.9018 and 0.2888401\n",
            "Val acc and loss are 0.8767 and 0.3460147\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.9023 and 0.28779024\n",
            "Val acc and loss are 0.8765 and 0.3455143\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.90268 and 0.28674442\n",
            "Val acc and loss are 0.8771 and 0.34501895\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.90312 and 0.28570256\n",
            "Val acc and loss are 0.877 and 0.34452885\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.90348 and 0.28466457\n",
            "Val acc and loss are 0.8774 and 0.3440455\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.90382 and 0.2836303\n",
            "Val acc and loss are 0.8776 and 0.3435691\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.9041 and 0.28259975\n",
            "Val acc and loss are 0.8777 and 0.34309804\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.90444 and 0.28157276\n",
            "Val acc and loss are 0.8779 and 0.34263068\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.90468 and 0.2805493\n",
            "Val acc and loss are 0.8779 and 0.34216744\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.9053 and 0.27952918\n",
            "Val acc and loss are 0.878 and 0.3417108\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.90558 and 0.27851236\n",
            "Val acc and loss are 0.8782 and 0.341263\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.9059 and 0.27749872\n",
            "Val acc and loss are 0.8783 and 0.3408233\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.90626 and 0.27648824\n",
            "Val acc and loss are 0.8784 and 0.340389\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.90684 and 0.27548072\n",
            "Val acc and loss are 0.8785 and 0.33995807\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.90722 and 0.27447608\n",
            "Val acc and loss are 0.8789 and 0.3395312\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.90772 and 0.27347425\n",
            "Val acc and loss are 0.8793 and 0.33911073\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.90792 and 0.27247506\n",
            "Val acc and loss are 0.8795 and 0.3386978\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.90836 and 0.27147844\n",
            "Val acc and loss are 0.8793 and 0.3382912\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.90872 and 0.27048433\n",
            "Val acc and loss are 0.8795 and 0.33788893\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.90906 and 0.26949263\n",
            "Val acc and loss are 0.88 and 0.33749044\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.90932 and 0.2685033\n",
            "Val acc and loss are 0.8802 and 0.33709672\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.90968 and 0.26751626\n",
            "Val acc and loss are 0.8801 and 0.33670893\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.91012 and 0.26653144\n",
            "Val acc and loss are 0.88 and 0.3363268\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.91064 and 0.26554883\n",
            "Val acc and loss are 0.8805 and 0.33594927\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.9109 and 0.26456833\n",
            "Val acc and loss are 0.8805 and 0.33557555\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.91116 and 0.26358983\n",
            "Val acc and loss are 0.8807 and 0.33520582\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.91156 and 0.26261333\n",
            "Val acc and loss are 0.8807 and 0.33484071\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.91202 and 0.2616387\n",
            "Val acc and loss are 0.8805 and 0.33448014\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.91248 and 0.26066598\n",
            "Val acc and loss are 0.8809 and 0.33412352\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.91284 and 0.25969505\n",
            "Val acc and loss are 0.8811 and 0.33377045\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.91312 and 0.25872588\n",
            "Val acc and loss are 0.8814 and 0.33342117\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.91346 and 0.25775844\n",
            "Val acc and loss are 0.8813 and 0.3330762\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.91384 and 0.25679266\n",
            "Val acc and loss are 0.8814 and 0.33273575\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.91426 and 0.2558285\n",
            "Val acc and loss are 0.8817 and 0.33239943\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.9146 and 0.25486588\n",
            "Val acc and loss are 0.8817 and 0.3320668\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.91492 and 0.25390482\n",
            "Val acc and loss are 0.882 and 0.33173776\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.91516 and 0.25294524\n",
            "Val acc and loss are 0.8819 and 0.3314126\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.91538 and 0.25198707\n",
            "Val acc and loss are 0.8822 and 0.33109188\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.91574 and 0.25103027\n",
            "Val acc and loss are 0.8822 and 0.33077568\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.91616 and 0.25007486\n",
            "Val acc and loss are 0.8823 and 0.33046383\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.91646 and 0.24912082\n",
            "Val acc and loss are 0.8823 and 0.33015636\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.91678 and 0.24816813\n",
            "Val acc and loss are 0.8821 and 0.32985356\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.917 and 0.24721679\n",
            "Val acc and loss are 0.8822 and 0.3295556\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.91744 and 0.2462668\n",
            "Val acc and loss are 0.8823 and 0.32926208\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.91774 and 0.24531808\n",
            "Val acc and loss are 0.8826 and 0.32897252\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.91816 and 0.24437062\n",
            "Val acc and loss are 0.8827 and 0.32868707\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.91852 and 0.24342442\n",
            "Val acc and loss are 0.8827 and 0.3284062\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.9189 and 0.24247937\n",
            "Val acc and loss are 0.8831 and 0.32813\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.91914 and 0.24153548\n",
            "Val acc and loss are 0.8835 and 0.32785782\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.9194 and 0.24059273\n",
            "Val acc and loss are 0.8838 and 0.32758915\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.91972 and 0.23965105\n",
            "Val acc and loss are 0.8837 and 0.32732436\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.91998 and 0.23871046\n",
            "Val acc and loss are 0.8837 and 0.32706407\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.9204 and 0.23777096\n",
            "Val acc and loss are 0.884 and 0.3268082\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.9209 and 0.23683242\n",
            "Val acc and loss are 0.884 and 0.32655612\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.92134 and 0.23589489\n",
            "Val acc and loss are 0.884 and 0.3263074\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.9216 and 0.23495837\n",
            "Val acc and loss are 0.8844 and 0.32606244\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.92186 and 0.23402274\n",
            "Val acc and loss are 0.8846 and 0.32582188\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.92202 and 0.23308806\n",
            "Val acc and loss are 0.8844 and 0.32558554\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.92252 and 0.2321543\n",
            "Val acc and loss are 0.8848 and 0.3253531\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.9228 and 0.23122142\n",
            "Val acc and loss are 0.8848 and 0.32512462\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.92314 and 0.23028941\n",
            "Val acc and loss are 0.8848 and 0.32490024\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.92356 and 0.22935832\n",
            "Val acc and loss are 0.885 and 0.32467994\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.92396 and 0.22842808\n",
            "Val acc and loss are 0.885 and 0.32446346\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.92422 and 0.22749867\n",
            "Val acc and loss are 0.8849 and 0.32425088\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.92468 and 0.22657016\n",
            "Val acc and loss are 0.8853 and 0.32404238\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.92498 and 0.22564244\n",
            "Val acc and loss are 0.8856 and 0.32383806\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.92536 and 0.22471559\n",
            "Val acc and loss are 0.8857 and 0.32363763\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.9257 and 0.22378957\n",
            "Val acc and loss are 0.8856 and 0.32344103\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.92604 and 0.22286439\n",
            "Val acc and loss are 0.8853 and 0.32324833\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.92646 and 0.22194006\n",
            "Val acc and loss are 0.8852 and 0.32305977\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.92688 and 0.22101653\n",
            "Val acc and loss are 0.8849 and 0.32287514\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.9274 and 0.22009383\n",
            "Val acc and loss are 0.885 and 0.32269424\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.92794 and 0.21917196\n",
            "Val acc and loss are 0.8851 and 0.3225171\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.92848 and 0.21825092\n",
            "Val acc and loss are 0.8853 and 0.32234398\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.92886 and 0.21733068\n",
            "Val acc and loss are 0.8852 and 0.3221749\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.92918 and 0.21641125\n",
            "Val acc and loss are 0.8851 and 0.32200968\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.92954 and 0.21549262\n",
            "Val acc and loss are 0.8852 and 0.3218483\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.92998 and 0.21457478\n",
            "Val acc and loss are 0.8853 and 0.32169098\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.93018 and 0.21365777\n",
            "Val acc and loss are 0.8857 and 0.32153785\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.93076 and 0.21274151\n",
            "Val acc and loss are 0.886 and 0.32138866\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.93116 and 0.21182607\n",
            "Val acc and loss are 0.8859 and 0.3212434\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.93152 and 0.21091141\n",
            "Val acc and loss are 0.8859 and 0.32110235\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.93192 and 0.20999753\n",
            "Val acc and loss are 0.8859 and 0.32096553\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.93224 and 0.20908442\n",
            "Val acc and loss are 0.8858 and 0.32083282\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.93258 and 0.20817207\n",
            "Val acc and loss are 0.8858 and 0.3207042\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.93302 and 0.2072605\n",
            "Val acc and loss are 0.8856 and 0.32057974\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.93336 and 0.20634969\n",
            "Val acc and loss are 0.8852 and 0.32045943\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.93368 and 0.20543967\n",
            "Val acc and loss are 0.8851 and 0.32034317\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.93406 and 0.20453036\n",
            "Val acc and loss are 0.8853 and 0.3202309\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.93442 and 0.20362183\n",
            "Val acc and loss are 0.8856 and 0.32012275\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.93478 and 0.20271406\n",
            "Val acc and loss are 0.8856 and 0.3200186\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.9352 and 0.20180704\n",
            "Val acc and loss are 0.8859 and 0.3199183\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.93558 and 0.20090075\n",
            "Val acc and loss are 0.8861 and 0.31982198\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.93596 and 0.19999522\n",
            "Val acc and loss are 0.8862 and 0.3197296\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.93642 and 0.19909044\n",
            "Val acc and loss are 0.8863 and 0.31964108\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.9368 and 0.19818641\n",
            "Val acc and loss are 0.8861 and 0.31955642\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.93718 and 0.19728312\n",
            "Val acc and loss are 0.8863 and 0.3194755\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.93752 and 0.19638059\n",
            "Val acc and loss are 0.886 and 0.31939864\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.9378 and 0.19547881\n",
            "Val acc and loss are 0.8859 and 0.31932572\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.93808 and 0.19457777\n",
            "Val acc and loss are 0.8858 and 0.3192567\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.93858 and 0.19367746\n",
            "Val acc and loss are 0.8859 and 0.3191917\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.93896 and 0.19277792\n",
            "Val acc and loss are 0.8859 and 0.31913075\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.93948 and 0.19187914\n",
            "Val acc and loss are 0.8859 and 0.3190739\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.93974 and 0.19098109\n",
            "Val acc and loss are 0.886 and 0.31902108\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.94004 and 0.19008383\n",
            "Val acc and loss are 0.886 and 0.3189725\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.94046 and 0.18918735\n",
            "Val acc and loss are 0.886 and 0.31892806\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.94074 and 0.1882916\n",
            "Val acc and loss are 0.8859 and 0.31888783\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.94108 and 0.18739665\n",
            "Val acc and loss are 0.886 and 0.31885186\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.9414 and 0.18650246\n",
            "Val acc and loss are 0.886 and 0.31882015\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.94184 and 0.18560906\n",
            "Val acc and loss are 0.8861 and 0.3187926\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.94212 and 0.18471645\n",
            "Val acc and loss are 0.8864 and 0.31876928\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.94254 and 0.18382464\n",
            "Val acc and loss are 0.8865 and 0.31875023\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.94294 and 0.18293363\n",
            "Val acc and loss are 0.8867 and 0.31873545\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.94324 and 0.18204343\n",
            "Val acc and loss are 0.8867 and 0.31872496\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.94354 and 0.18115404\n",
            "Val acc and loss are 0.8869 and 0.31871864\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.9439 and 0.18026547\n",
            "Val acc and loss are 0.887 and 0.31871665\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.94422 and 0.17937773\n",
            "Val acc and loss are 0.8871 and 0.31871888\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.94464 and 0.1784908\n",
            "Val acc and loss are 0.8872 and 0.31872544\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.94494 and 0.17760472\n",
            "Val acc and loss are 0.8868 and 0.31873628\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.9452 and 0.17671949\n",
            "Val acc and loss are 0.8865 and 0.31875148\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.94542 and 0.17583512\n",
            "Val acc and loss are 0.8867 and 0.31877103\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.94578 and 0.1749516\n",
            "Val acc and loss are 0.8869 and 0.31879506\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.94604 and 0.17406896\n",
            "Val acc and loss are 0.8867 and 0.31882343\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.9462 and 0.17318721\n",
            "Val acc and loss are 0.8869 and 0.3188562\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.94652 and 0.17230637\n",
            "Val acc and loss are 0.8868 and 0.3188935\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.94696 and 0.1714264\n",
            "Val acc and loss are 0.8869 and 0.31893536\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.94728 and 0.17054734\n",
            "Val acc and loss are 0.887 and 0.31898168\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.94768 and 0.16966923\n",
            "Val acc and loss are 0.8872 and 0.31903255\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.94804 and 0.16879202\n",
            "Val acc and loss are 0.8872 and 0.319088\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.94838 and 0.16791576\n",
            "Val acc and loss are 0.887 and 0.319148\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.94868 and 0.16704042\n",
            "Val acc and loss are 0.8871 and 0.3192126\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.9491 and 0.1661661\n",
            "Val acc and loss are 0.8874 and 0.31928167\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.9495 and 0.1652927\n",
            "Val acc and loss are 0.8871 and 0.3193553\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.94986 and 0.16442028\n",
            "Val acc and loss are 0.8871 and 0.31943354\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.9502 and 0.16354884\n",
            "Val acc and loss are 0.8872 and 0.3195163\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.95076 and 0.1626784\n",
            "Val acc and loss are 0.8875 and 0.3196035\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.95104 and 0.16180898\n",
            "Val acc and loss are 0.8876 and 0.31969535\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.95136 and 0.16094057\n",
            "Val acc and loss are 0.8875 and 0.31979162\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.95176 and 0.1600732\n",
            "Val acc and loss are 0.8875 and 0.31989253\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.95232 and 0.15920685\n",
            "Val acc and loss are 0.8876 and 0.31999764\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.95252 and 0.15834159\n",
            "Val acc and loss are 0.8877 and 0.32010776\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.95288 and 0.15747736\n",
            "Val acc and loss are 0.8878 and 0.32022142\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.95312 and 0.15661424\n",
            "Val acc and loss are 0.8879 and 0.32034132\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.95342 and 0.15575226\n",
            "Val acc and loss are 0.8878 and 0.32046235\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.95382 and 0.15489158\n",
            "Val acc and loss are 0.8879 and 0.32059467\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.954 and 0.15403298\n",
            "Val acc and loss are 0.8878 and 0.32071918\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.95432 and 0.15317982\n",
            "Val acc and loss are 0.8879 and 0.32088038\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.95454 and 0.15234847\n",
            "Val acc and loss are 0.8883 and 0.32100922\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.95502 and 0.15162046\n",
            "Val acc and loss are 0.8875 and 0.32137948\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.95424 and 0.1514196\n",
            "Val acc and loss are 0.8881 and 0.322029\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.95356 and 0.15372957\n",
            "Val acc and loss are 0.8882 and 0.3260084\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.94652 and 0.16557899\n",
            "Val acc and loss are 0.8837 and 0.33844757\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.93682 and 0.1826322\n",
            "Val acc and loss are 0.8786 and 0.35949934\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.94482 and 0.16779739\n",
            "Val acc and loss are 0.884 and 0.34196585\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.95408 and 0.14948586\n",
            "Val acc and loss are 0.8884 and 0.3245167\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.94282 and 0.1704746\n",
            "Val acc and loss are 0.8811 and 0.34957525\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.95268 and 0.1510064\n",
            "Val acc and loss are 0.8876 and 0.32721382\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.94912 and 0.15793811\n",
            "Val acc and loss are 0.8869 and 0.33405572\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.95128 and 0.15419677\n",
            "Val acc and loss are 0.8871 and 0.33370906\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.9543 and 0.14863428\n",
            "Val acc and loss are 0.8873 and 0.32876468\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.95104 and 0.15411708\n",
            "Val acc and loss are 0.8854 and 0.3330007\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.9566 and 0.14384438\n",
            "Val acc and loss are 0.8894 and 0.32283148\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.95192 and 0.15214495\n",
            "Val acc and loss are 0.8867 and 0.33426425\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.95782 and 0.14149937\n",
            "Val acc and loss are 0.8889 and 0.32320616\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.95336 and 0.14885634\n",
            "Val acc and loss are 0.8861 and 0.33086446\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.95776 and 0.14050372\n",
            "Val acc and loss are 0.8881 and 0.32350415\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.95558 and 0.14546588\n",
            "Val acc and loss are 0.8878 and 0.3301476\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.95744 and 0.139915\n",
            "Val acc and loss are 0.8889 and 0.3243078\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.95656 and 0.14202307\n",
            "Val acc and loss are 0.8877 and 0.3277772\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.95862 and 0.1391712\n",
            "Val acc and loss are 0.8882 and 0.32619205\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.9588 and 0.13908876\n",
            "Val acc and loss are 0.8887 and 0.32643592\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.95852 and 0.13840432\n",
            "Val acc and loss are 0.8886 and 0.32599992\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.95958 and 0.1364806\n",
            "Val acc and loss are 0.8883 and 0.32556686\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.95914 and 0.1373789\n",
            "Val acc and loss are 0.8875 and 0.32789943\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.96064 and 0.13424592\n",
            "Val acc and loss are 0.8891 and 0.32493135\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.95924 and 0.13590321\n",
            "Val acc and loss are 0.8886 and 0.32743254\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.96102 and 0.13262957\n",
            "Val acc and loss are 0.8889 and 0.32512623\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.96052 and 0.13423347\n",
            "Val acc and loss are 0.8878 and 0.32788506\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.96186 and 0.13150571\n",
            "Val acc and loss are 0.8888 and 0.3254201\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.96082 and 0.13215494\n",
            "Val acc and loss are 0.8892 and 0.3271489\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.9621 and 0.13052794\n",
            "Val acc and loss are 0.8878 and 0.32689947\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.96218 and 0.13009073\n",
            "Val acc and loss are 0.8875 and 0.32728294\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.96196 and 0.12960549\n",
            "Val acc and loss are 0.889 and 0.32697418\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.96304 and 0.12825346\n",
            "Val acc and loss are 0.8889 and 0.32655585\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.9628 and 0.12847693\n",
            "Val acc and loss are 0.8875 and 0.32850578\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.96354 and 0.12671533\n",
            "Val acc and loss are 0.8883 and 0.32730606\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.96326 and 0.12699355\n",
            "Val acc and loss are 0.8895 and 0.32800353\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.96396 and 0.12555763\n",
            "Val acc and loss are 0.8877 and 0.32776502\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.96404 and 0.12529\n",
            "Val acc and loss are 0.8876 and 0.32875326\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9645 and 0.12456855\n",
            "Val acc and loss are 0.8886 and 0.3285752\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.9644 and 0.123623185\n",
            "Val acc and loss are 0.8887 and 0.3284154\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.96484 and 0.12342424\n",
            "Val acc and loss are 0.888 and 0.329327\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.96526 and 0.122220956\n",
            "Val acc and loss are 0.8881 and 0.32886034\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.96522 and 0.12200125\n",
            "Val acc and loss are 0.8886 and 0.32952398\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.96596 and 0.12109883\n",
            "Val acc and loss are 0.8881 and 0.32954702\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.96626 and 0.12045203\n",
            "Val acc and loss are 0.8886 and 0.32975343\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.96588 and 0.12000414\n",
            "Val acc and loss are 0.8885 and 0.33017892\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.96662 and 0.11903858\n",
            "Val acc and loss are 0.8887 and 0.33008003\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.9668 and 0.118693575\n",
            "Val acc and loss are 0.8885 and 0.33067018\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.96664 and 0.11788699\n",
            "Val acc and loss are 0.8889 and 0.33075416\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.967 and 0.11723326\n",
            "Val acc and loss are 0.8888 and 0.3310599\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.96744 and 0.11675305\n",
            "Val acc and loss are 0.8888 and 0.33154127\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.96758 and 0.11591371\n",
            "Val acc and loss are 0.889 and 0.33143947\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.96764 and 0.11542412\n",
            "Val acc and loss are 0.8892 and 0.33187413\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.9682 and 0.11477379\n",
            "Val acc and loss are 0.8893 and 0.3323734\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.9686 and 0.11404839\n",
            "Val acc and loss are 0.8894 and 0.33241487\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.96824 and 0.1135642\n",
            "Val acc and loss are 0.8896 and 0.33273646\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.96918 and 0.112846136\n",
            "Val acc and loss are 0.8892 and 0.33318493\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.9695 and 0.1122218\n",
            "Val acc and loss are 0.8894 and 0.3334265\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.96916 and 0.11168188\n",
            "Val acc and loss are 0.8889 and 0.33364096\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.96986 and 0.11097412\n",
            "Val acc and loss are 0.8889 and 0.3339791\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.97008 and 0.11039785\n",
            "Val acc and loss are 0.8887 and 0.3343871\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.97004 and 0.10981837\n",
            "Val acc and loss are 0.8885 and 0.3346762\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.97068 and 0.10913787\n",
            "Val acc and loss are 0.8887 and 0.33491144\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.97078 and 0.10856837\n",
            "Val acc and loss are 0.8885 and 0.33533525\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.97078 and 0.107979804\n",
            "Val acc and loss are 0.8886 and 0.33562046\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.97146 and 0.10732228\n",
            "Val acc and loss are 0.8888 and 0.33582523\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.97152 and 0.106750235\n",
            "Val acc and loss are 0.8886 and 0.3362859\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.97176 and 0.1061615\n",
            "Val acc and loss are 0.8885 and 0.33660045\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.9721 and 0.10552079\n",
            "Val acc and loss are 0.8882 and 0.3369002\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.9723 and 0.10494317\n",
            "Val acc and loss are 0.8882 and 0.33726767\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.9726 and 0.1043631\n",
            "Val acc and loss are 0.8883 and 0.33760303\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.97272 and 0.10373914\n",
            "Val acc and loss are 0.8882 and 0.33800712\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.97302 and 0.10315131\n",
            "Val acc and loss are 0.8884 and 0.33827108\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.9731 and 0.10257945\n",
            "Val acc and loss are 0.8883 and 0.33863598\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.97324 and 0.10197201\n",
            "Val acc and loss are 0.8885 and 0.3390259\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.97354 and 0.101376384\n",
            "Val acc and loss are 0.8884 and 0.33933786\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.97374 and 0.10080798\n",
            "Val acc and loss are 0.8889 and 0.3396675\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.97398 and 0.100220665\n",
            "Val acc and loss are 0.8887 and 0.3400754\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.97412 and 0.099624395\n",
            "Val acc and loss are 0.8889 and 0.34040204\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.9746 and 0.09905098\n",
            "Val acc and loss are 0.889 and 0.3407265\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.97462 and 0.09848042\n",
            "Val acc and loss are 0.8891 and 0.34120908\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.97506 and 0.097894765\n",
            "Val acc and loss are 0.8891 and 0.341496\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.97514 and 0.09731483\n",
            "Val acc and loss are 0.8891 and 0.3418912\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.97554 and 0.09675\n",
            "Val acc and loss are 0.8889 and 0.34225264\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.97568 and 0.096182995\n",
            "Val acc and loss are 0.8889 and 0.34270555\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.9761 and 0.09560992\n",
            "Val acc and loss are 0.8891 and 0.34298334\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.97614 and 0.09504811\n",
            "Val acc and loss are 0.8891 and 0.34349057\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.97648 and 0.09450583\n",
            "Val acc and loss are 0.8895 and 0.34371036\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.97656 and 0.093985885\n",
            "Val acc and loss are 0.8885 and 0.34443405\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.97694 and 0.093518294\n",
            "Val acc and loss are 0.8897 and 0.34449404\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.97654 and 0.09320319\n",
            "Val acc and loss are 0.888 and 0.34580138\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.97682 and 0.09323355\n",
            "Val acc and loss are 0.8887 and 0.34589523\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.97538 and 0.09407117\n",
            "Val acc and loss are 0.8865 and 0.34921122\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.97516 and 0.09599984\n",
            "Val acc and loss are 0.8873 and 0.3503293\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.97228 and 0.098878555\n",
            "Val acc and loss are 0.8838 and 0.35703212\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.97388 and 0.097876\n",
            "Val acc and loss are 0.8856 and 0.35415295\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.97546 and 0.09274212\n",
            "Val acc and loss are 0.8862 and 0.3517301\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.9781 and 0.089192286\n",
            "Val acc and loss are 0.8882 and 0.34800303\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.9767 and 0.09190804\n",
            "Val acc and loss are 0.8879 and 0.35084674\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.97524 and 0.092805594\n",
            "Val acc and loss are 0.8858 and 0.35469472\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.97852 and 0.08849558\n",
            "Val acc and loss are 0.8881 and 0.34931195\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.97888 and 0.08787486\n",
            "Val acc and loss are 0.8884 and 0.34953925\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.9768 and 0.09000114\n",
            "Val acc and loss are 0.8865 and 0.3540896\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.97862 and 0.08781006\n",
            "Val acc and loss are 0.8882 and 0.35109064\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.98006 and 0.085602336\n",
            "Val acc and loss are 0.8882 and 0.35003576\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.97866 and 0.08681865\n",
            "Val acc and loss are 0.8865 and 0.3530347\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.9791 and 0.08655345\n",
            "Val acc and loss are 0.8876 and 0.35249004\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.98004 and 0.084326915\n",
            "Val acc and loss are 0.8875 and 0.3518265\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.98004 and 0.084028184\n",
            "Val acc and loss are 0.8874 and 0.35255325\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.97992 and 0.084663264\n",
            "Val acc and loss are 0.888 and 0.353388\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.98012 and 0.083478674\n",
            "Val acc and loss are 0.8866 and 0.35406503\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.98116 and 0.0820974\n",
            "Val acc and loss are 0.888 and 0.35313565\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.98116 and 0.0823698\n",
            "Val acc and loss are 0.8881 and 0.35408917\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.98036 and 0.082296796\n",
            "Val acc and loss are 0.8871 and 0.3559336\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.98168 and 0.08093712\n",
            "Val acc and loss are 0.8878 and 0.35470656\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.9819 and 0.080255225\n",
            "Val acc and loss are 0.8879 and 0.3550627\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.98128 and 0.080431305\n",
            "Val acc and loss are 0.887 and 0.3567125\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.98196 and 0.079877004\n",
            "Val acc and loss are 0.8883 and 0.35631686\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.98244 and 0.07881097\n",
            "Val acc and loss are 0.8876 and 0.35656285\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.98252 and 0.078396745\n",
            "Val acc and loss are 0.8874 and 0.35705626\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.98254 and 0.07834366\n",
            "Val acc and loss are 0.8879 and 0.35745075\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.98252 and 0.07775804\n",
            "Val acc and loss are 0.8863 and 0.35830817\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.98318 and 0.076894715\n",
            "Val acc and loss are 0.8884 and 0.35789633\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.9834 and 0.07649182\n",
            "Val acc and loss are 0.8882 and 0.35834408\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.98306 and 0.07631792\n",
            "Val acc and loss are 0.8865 and 0.35952142\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.98364 and 0.07578476\n",
            "Val acc and loss are 0.888 and 0.35937884\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.98362 and 0.07505582\n",
            "Val acc and loss are 0.8881 and 0.35988697\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.98382 and 0.07458209\n",
            "Val acc and loss are 0.8881 and 0.3603229\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.98416 and 0.07431932\n",
            "Val acc and loss are 0.8879 and 0.3607161\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.98386 and 0.073908575\n",
            "Val acc and loss are 0.8869 and 0.36166567\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.9845 and 0.07328744\n",
            "Val acc and loss are 0.8881 and 0.36158243\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.98472 and 0.07273496\n",
            "Val acc and loss are 0.8879 and 0.36209008\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.98456 and 0.0723693\n",
            "Val acc and loss are 0.8876 and 0.36278114\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.98502 and 0.07202755\n",
            "Val acc and loss are 0.8879 and 0.36308748\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.98474 and 0.07155724\n",
            "Val acc and loss are 0.8871 and 0.36388382\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.98532 and 0.07100308\n",
            "Val acc and loss are 0.8874 and 0.36399412\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.98546 and 0.07051218\n",
            "Val acc and loss are 0.8876 and 0.36449745\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.98546 and 0.07012439\n",
            "Val acc and loss are 0.8873 and 0.36514154\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.9858 and 0.069754906\n",
            "Val acc and loss are 0.8877 and 0.36547124\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.98552 and 0.06932367\n",
            "Val acc and loss are 0.8869 and 0.366242\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.98612 and 0.06883181\n",
            "Val acc and loss are 0.8875 and 0.3664123\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.98614 and 0.06834656\n",
            "Val acc and loss are 0.8872 and 0.36697572\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.98626 and 0.067913376\n",
            "Val acc and loss are 0.8875 and 0.3674292\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.98648 and 0.06752275\n",
            "Val acc and loss are 0.8871 and 0.36783963\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.98618 and 0.06713329\n",
            "Val acc and loss are 0.8868 and 0.36852935\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.9867 and 0.06671416\n",
            "Val acc and loss are 0.8873 and 0.36878878\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.98656 and 0.06626824\n",
            "Val acc and loss are 0.8869 and 0.3694338\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.98696 and 0.06581731\n",
            "Val acc and loss are 0.8872 and 0.3697431\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.987 and 0.06538511\n",
            "Val acc and loss are 0.887 and 0.3702505\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.9872 and 0.06498308\n",
            "Val acc and loss are 0.887 and 0.37079808\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.98734 and 0.0646143\n",
            "Val acc and loss are 0.8869 and 0.37119028\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.9873 and 0.064291194\n",
            "Val acc and loss are 0.8869 and 0.3720188\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.9874 and 0.06405714\n",
            "Val acc and loss are 0.8866 and 0.37240946\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.9872 and 0.06406105\n",
            "Val acc and loss are 0.8857 and 0.37384468\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.98646 and 0.06467148\n",
            "Val acc and loss are 0.8857 and 0.3750979\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.98506 and 0.06698771\n",
            "Val acc and loss are 0.8853 and 0.37982985\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.9797 and 0.07456827\n",
            "Val acc and loss are 0.8836 and 0.38915443\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.96724 and 0.09852741\n",
            "Val acc and loss are 0.8768 and 0.42097193\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.94218 and 0.17510058\n",
            "Val acc and loss are 0.8659 and 0.5003301\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.9357 and 0.1959454\n",
            "Val acc and loss are 0.8589 and 0.53010714\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.9777 and 0.07776955\n",
            "Val acc and loss are 0.8865 and 0.3904445\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.94856 and 0.15682733\n",
            "Val acc and loss are 0.8704 and 0.4827144\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.97144 and 0.09211329\n",
            "Val acc and loss are 0.8833 and 0.41290963\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.95296 and 0.1432295\n",
            "Val acc and loss are 0.8716 and 0.47131827\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.97308 and 0.086435296\n",
            "Val acc and loss are 0.8843 and 0.39826113\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.95312 and 0.14270468\n",
            "Val acc and loss are 0.8708 and 0.46644834\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.9788 and 0.07476521\n",
            "Val acc and loss are 0.8846 and 0.39031368\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.95648 and 0.12833089\n",
            "Val acc and loss are 0.8736 and 0.45017764\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.97478 and 0.08151208\n",
            "Val acc and loss are 0.885 and 0.3905545\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.96426 and 0.105040476\n",
            "Val acc and loss are 0.8779 and 0.4205163\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.9741 and 0.085916914\n",
            "Val acc and loss are 0.8819 and 0.4050207\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.9745 and 0.08263111\n",
            "Val acc and loss are 0.8819 and 0.3992545\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.97134 and 0.09050157\n",
            "Val acc and loss are 0.8807 and 0.40351203\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.97846 and 0.07415096\n",
            "Val acc and loss are 0.8866 and 0.3824991\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.97398 and 0.08484745\n",
            "Val acc and loss are 0.8822 and 0.4001264\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.97938 and 0.074767746\n",
            "Val acc and loss are 0.8853 and 0.39297044\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.98104 and 0.07233807\n",
            "Val acc and loss are 0.8836 and 0.3874534\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.97708 and 0.077206455\n",
            "Val acc and loss are 0.8831 and 0.39008978\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.9828 and 0.066923\n",
            "Val acc and loss are 0.8856 and 0.3784319\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.97978 and 0.07262683\n",
            "Val acc and loss are 0.8832 and 0.38920993\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.98284 and 0.06741107\n",
            "Val acc and loss are 0.885 and 0.3870323\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.98444 and 0.065832965\n",
            "Val acc and loss are 0.8845 and 0.38502058\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.98254 and 0.06838319\n",
            "Val acc and loss are 0.8845 and 0.3865068\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.98588 and 0.06151627\n",
            "Val acc and loss are 0.8871 and 0.37866417\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.9835 and 0.065972894\n",
            "Val acc and loss are 0.8835 and 0.38625282\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.98536 and 0.06262232\n",
            "Val acc and loss are 0.8855 and 0.3835835\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.98728 and 0.059640244\n",
            "Val acc and loss are 0.887 and 0.380843\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.98496 and 0.06405872\n",
            "Val acc and loss are 0.8848 and 0.38733542\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.98774 and 0.058463477\n",
            "Val acc and loss are 0.887 and 0.38070765\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.98714 and 0.059024308\n",
            "Val acc and loss are 0.8852 and 0.38149282\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.98606 and 0.06088086\n",
            "Val acc and loss are 0.8846 and 0.38424772\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.98898 and 0.056267355\n",
            "Val acc and loss are 0.8875 and 0.37911102\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.988 and 0.05833871\n",
            "Val acc and loss are 0.8863 and 0.38371566\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.98802 and 0.05770381\n",
            "Val acc and loss are 0.8868 and 0.38475302\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.98932 and 0.05522854\n",
            "Val acc and loss are 0.8869 and 0.38170958\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.98808 and 0.056914747\n",
            "Val acc and loss are 0.8853 and 0.38314742\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.98912 and 0.055285\n",
            "Val acc and loss are 0.886 and 0.3811524\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.98962 and 0.05471368\n",
            "Val acc and loss are 0.887 and 0.38209042\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.98972 and 0.054904737\n",
            "Val acc and loss are 0.8862 and 0.3845533\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.98988 and 0.05392044\n",
            "Val acc and loss are 0.8864 and 0.38461244\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.98966 and 0.053862207\n",
            "Val acc and loss are 0.886 and 0.38468686\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.99 and 0.053191055\n",
            "Val acc and loss are 0.8865 and 0.38329226\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.99012 and 0.053044472\n",
            "Val acc and loss are 0.8871 and 0.38333008\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.99034 and 0.052505363\n",
            "Val acc and loss are 0.8875 and 0.38401684\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.99048 and 0.052131314\n",
            "Val acc and loss are 0.8875 and 0.38557413\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.99038 and 0.05199777\n",
            "Val acc and loss are 0.8869 and 0.38669905\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.99066 and 0.051362745\n",
            "Val acc and loss are 0.8873 and 0.3858193\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.99058 and 0.051272277\n",
            "Val acc and loss are 0.887 and 0.3853898\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.99108 and 0.05081849\n",
            "Val acc and loss are 0.8873 and 0.38550788\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.99116 and 0.05050131\n",
            "Val acc and loss are 0.8861 and 0.38705385\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.99112 and 0.050324555\n",
            "Val acc and loss are 0.8868 and 0.38845703\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.99126 and 0.04978936\n",
            "Val acc and loss are 0.8867 and 0.3879527\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.99118 and 0.04975749\n",
            "Val acc and loss are 0.8871 and 0.3877444\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.99148 and 0.049281046\n",
            "Val acc and loss are 0.887 and 0.38785663\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.99178 and 0.04901272\n",
            "Val acc and loss are 0.8862 and 0.38915527\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.99172 and 0.048893776\n",
            "Val acc and loss are 0.8861 and 0.3903306\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.99188 and 0.04835757\n",
            "Val acc and loss are 0.8868 and 0.38982984\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.99172 and 0.048321642\n",
            "Val acc and loss are 0.8872 and 0.38980198\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.99176 and 0.047944337\n",
            "Val acc and loss are 0.8867 and 0.39020756\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.99214 and 0.047639307\n",
            "Val acc and loss are 0.8854 and 0.39128014\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.99224 and 0.047500644\n",
            "Val acc and loss are 0.8854 and 0.39203072\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.99226 and 0.047122907\n",
            "Val acc and loss are 0.8863 and 0.39181\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.99242 and 0.04692749\n",
            "Val acc and loss are 0.8873 and 0.39205277\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.99232 and 0.046667457\n",
            "Val acc and loss are 0.8873 and 0.39281476\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.99246 and 0.04640893\n",
            "Val acc and loss are 0.8861 and 0.39372334\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.99268 and 0.046156984\n",
            "Val acc and loss are 0.8855 and 0.39415327\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.9929 and 0.045930427\n",
            "Val acc and loss are 0.8861 and 0.3942581\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.99296 and 0.045671612\n",
            "Val acc and loss are 0.8866 and 0.39460108\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.9928 and 0.045425043\n",
            "Val acc and loss are 0.8863 and 0.3953056\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.99282 and 0.04521805\n",
            "Val acc and loss are 0.8862 and 0.39596555\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.99298 and 0.044935156\n",
            "Val acc and loss are 0.8856 and 0.39622703\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.9932 and 0.04473963\n",
            "Val acc and loss are 0.8858 and 0.39654845\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.99332 and 0.044488408\n",
            "Val acc and loss are 0.8858 and 0.39697602\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.99334 and 0.044249587\n",
            "Val acc and loss are 0.8861 and 0.39757246\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.99326 and 0.044044774\n",
            "Val acc and loss are 0.8857 and 0.3981773\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.99346 and 0.043783844\n",
            "Val acc and loss are 0.8858 and 0.39855763\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.9936 and 0.043584086\n",
            "Val acc and loss are 0.8858 and 0.39897954\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.99372 and 0.043341987\n",
            "Val acc and loss are 0.8855 and 0.39938873\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.99366 and 0.043123737\n",
            "Val acc and loss are 0.8858 and 0.39986768\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.9937 and 0.042902\n",
            "Val acc and loss are 0.8862 and 0.40036437\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.99396 and 0.04267483\n",
            "Val acc and loss are 0.8857 and 0.4008581\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.99396 and 0.04246274\n",
            "Val acc and loss are 0.8858 and 0.40133443\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.99412 and 0.042234972\n",
            "Val acc and loss are 0.8857 and 0.40175012\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.99424 and 0.04202499\n",
            "Val acc and loss are 0.8859 and 0.4021373\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.99418 and 0.04180209\n",
            "Val acc and loss are 0.8857 and 0.402478\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.99426 and 0.041591123\n",
            "Val acc and loss are 0.8858 and 0.4029318\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.99428 and 0.041374493\n",
            "Val acc and loss are 0.8855 and 0.40349525\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.99436 and 0.04116212\n",
            "Val acc and loss are 0.8858 and 0.40404737\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.99442 and 0.040950492\n",
            "Val acc and loss are 0.8856 and 0.40444836\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.99442 and 0.040738575\n",
            "Val acc and loss are 0.8856 and 0.40474665\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.99446 and 0.040530477\n",
            "Val acc and loss are 0.8857 and 0.40516698\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.99448 and 0.04031936\n",
            "Val acc and loss are 0.8855 and 0.40577236\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.99456 and 0.04011489\n",
            "Val acc and loss are 0.8858 and 0.40634513\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.99464 and 0.03990444\n",
            "Val acc and loss are 0.8857 and 0.40668008\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.99468 and 0.039702475\n",
            "Val acc and loss are 0.8856 and 0.40695605\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.99466 and 0.03949561\n",
            "Val acc and loss are 0.8856 and 0.40740183\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.99468 and 0.03929213\n",
            "Val acc and loss are 0.8856 and 0.40800455\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.99472 and 0.039091457\n",
            "Val acc and loss are 0.8856 and 0.40851533\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.99472 and 0.03888683\n",
            "Val acc and loss are 0.8856 and 0.4088331\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.99476 and 0.03868914\n",
            "Val acc and loss are 0.8857 and 0.40918055\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.9948 and 0.03848741\n",
            "Val acc and loss are 0.8855 and 0.40969342\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.99488 and 0.038289662\n",
            "Val acc and loss are 0.8856 and 0.41025624\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.99486 and 0.038091578\n",
            "Val acc and loss are 0.8857 and 0.41070274\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.99494 and 0.037894808\n",
            "Val acc and loss are 0.8857 and 0.41107315\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.99506 and 0.037698746\n",
            "Val acc and loss are 0.8854 and 0.41150102\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.99512 and 0.03750404\n",
            "Val acc and loss are 0.8856 and 0.41200656\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.9952 and 0.037309706\n",
            "Val acc and loss are 0.8856 and 0.41248852\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.99516 and 0.03711661\n",
            "Val acc and loss are 0.8857 and 0.412917\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.99522 and 0.03692459\n",
            "Val acc and loss are 0.8858 and 0.413356\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.99532 and 0.03673284\n",
            "Val acc and loss are 0.8856 and 0.41381934\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.9955 and 0.036542896\n",
            "Val acc and loss are 0.8857 and 0.41427016\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.9955 and 0.036352865\n",
            "Val acc and loss are 0.8857 and 0.4146998\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.9955 and 0.036164723\n",
            "Val acc and loss are 0.8854 and 0.41514727\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.99562 and 0.03597644\n",
            "Val acc and loss are 0.8855 and 0.4156245\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.99562 and 0.03579001\n",
            "Val acc and loss are 0.8853 and 0.4160924\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.99564 and 0.035603765\n",
            "Val acc and loss are 0.8852 and 0.4165234\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.99568 and 0.0354186\n",
            "Val acc and loss are 0.8851 and 0.41695586\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.9957 and 0.035234652\n",
            "Val acc and loss are 0.8851 and 0.4174169\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.99572 and 0.03505083\n",
            "Val acc and loss are 0.8852 and 0.4178853\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.99574 and 0.034868687\n",
            "Val acc and loss are 0.885 and 0.41833618\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.99578 and 0.034686796\n",
            "Val acc and loss are 0.8852 and 0.4187709\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.99578 and 0.03450606\n",
            "Val acc and loss are 0.8853 and 0.4192186\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.9958 and 0.03432614\n",
            "Val acc and loss are 0.8854 and 0.41968086\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.99582 and 0.034147028\n",
            "Val acc and loss are 0.8853 and 0.42013234\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.99586 and 0.033968713\n",
            "Val acc and loss are 0.8854 and 0.4205681\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.9959 and 0.033791445\n",
            "Val acc and loss are 0.8854 and 0.4210087\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.99598 and 0.033614725\n",
            "Val acc and loss are 0.8854 and 0.42146826\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.99602 and 0.033439126\n",
            "Val acc and loss are 0.8852 and 0.42193267\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.99606 and 0.033264194\n",
            "Val acc and loss are 0.8851 and 0.4223747\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.9961 and 0.033090137\n",
            "Val acc and loss are 0.8852 and 0.42280424\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.99614 and 0.032916974\n",
            "Val acc and loss are 0.8852 and 0.42325327\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.9962 and 0.032744527\n",
            "Val acc and loss are 0.8851 and 0.42372304\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.99628 and 0.032573026\n",
            "Val acc and loss are 0.8851 and 0.42418286\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.9963 and 0.03240224\n",
            "Val acc and loss are 0.8851 and 0.4246149\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.99632 and 0.032232396\n",
            "Val acc and loss are 0.8853 and 0.42504755\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.99638 and 0.032063253\n",
            "Val acc and loss are 0.8853 and 0.42550898\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.99642 and 0.03189503\n",
            "Val acc and loss are 0.8852 and 0.42597958\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.99644 and 0.03172756\n",
            "Val acc and loss are 0.8852 and 0.42642733\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.99648 and 0.031560916\n",
            "Val acc and loss are 0.8851 and 0.4268593\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.99656 and 0.031395108\n",
            "Val acc and loss are 0.8851 and 0.42730674\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.99658 and 0.03123008\n",
            "Val acc and loss are 0.885 and 0.42777383\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.9967 and 0.031065874\n",
            "Val acc and loss are 0.8848 and 0.42823213\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.99672 and 0.03090248\n",
            "Val acc and loss are 0.8848 and 0.4286714\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.99672 and 0.030739874\n",
            "Val acc and loss are 0.8848 and 0.42911133\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.99674 and 0.03057808\n",
            "Val acc and loss are 0.8848 and 0.4295639\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.99676 and 0.030417085\n",
            "Val acc and loss are 0.8848 and 0.43001935\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.99676 and 0.030256875\n",
            "Val acc and loss are 0.8847 and 0.43046728\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.9968 and 0.030097485\n",
            "Val acc and loss are 0.8847 and 0.43091416\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.9968 and 0.02993886\n",
            "Val acc and loss are 0.8844 and 0.4313659\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.99684 and 0.02978105\n",
            "Val acc and loss are 0.8844 and 0.431816\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.99684 and 0.029624006\n",
            "Val acc and loss are 0.8843 and 0.43226215\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.99692 and 0.029467776\n",
            "Val acc and loss are 0.8842 and 0.43271104\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.99694 and 0.0293123\n",
            "Val acc and loss are 0.8842 and 0.43316466\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.997 and 0.029157637\n",
            "Val acc and loss are 0.8842 and 0.43361533\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.99704 and 0.029003734\n",
            "Val acc and loss are 0.8843 and 0.43405902\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.99704 and 0.02885062\n",
            "Val acc and loss are 0.8842 and 0.43450394\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.9971 and 0.02869828\n",
            "Val acc and loss are 0.8843 and 0.43495527\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.99714 and 0.028546713\n",
            "Val acc and loss are 0.8844 and 0.43540666\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.99714 and 0.028395928\n",
            "Val acc and loss are 0.8844 and 0.4358529\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.99714 and 0.028245904\n",
            "Val acc and loss are 0.8844 and 0.43629804\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.99716 and 0.028096654\n",
            "Val acc and loss are 0.8844 and 0.4367477\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.99722 and 0.027948167\n",
            "Val acc and loss are 0.8843 and 0.43719864\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.99722 and 0.02780045\n",
            "Val acc and loss are 0.8842 and 0.43764502\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.99726 and 0.02765349\n",
            "Val acc and loss are 0.8842 and 0.43808955\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.99732 and 0.027507296\n",
            "Val acc and loss are 0.8842 and 0.43853775\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.99738 and 0.027361855\n",
            "Val acc and loss are 0.8841 and 0.438989\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.99738 and 0.027217176\n",
            "Val acc and loss are 0.8841 and 0.4394373\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.99744 and 0.02707325\n",
            "Val acc and loss are 0.8841 and 0.43988085\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.9975 and 0.026930075\n",
            "Val acc and loss are 0.884 and 0.44032538\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.99758 and 0.026787652\n",
            "Val acc and loss are 0.8839 and 0.44077402\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.9976 and 0.02664598\n",
            "Val acc and loss are 0.8839 and 0.44122335\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.99766 and 0.026505053\n",
            "Val acc and loss are 0.8839 and 0.44166884\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.99768 and 0.026364872\n",
            "Val acc and loss are 0.8839 and 0.442112\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.99772 and 0.026225429\n",
            "Val acc and loss are 0.884 and 0.44255805\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.99776 and 0.026086729\n",
            "Val acc and loss are 0.884 and 0.443007\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.99778 and 0.025948765\n",
            "Val acc and loss are 0.884 and 0.44345444\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.99782 and 0.025811538\n",
            "Val acc and loss are 0.884 and 0.4438985\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.99782 and 0.025675047\n",
            "Val acc and loss are 0.8841 and 0.44434243\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.99784 and 0.025539286\n",
            "Val acc and loss are 0.884 and 0.44478896\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.99786 and 0.025404252\n",
            "Val acc and loss are 0.8838 and 0.4452358\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.9979 and 0.025269946\n",
            "Val acc and loss are 0.8838 and 0.4456804\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.99794 and 0.025136365\n",
            "Val acc and loss are 0.8839 and 0.44612402\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.99794 and 0.025003506\n",
            "Val acc and loss are 0.8837 and 0.4465687\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.99798 and 0.024871368\n",
            "Val acc and loss are 0.8837 and 0.44701406\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.99798 and 0.024739943\n",
            "Val acc and loss are 0.8837 and 0.44745854\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.99798 and 0.024609243\n",
            "Val acc and loss are 0.8836 and 0.44790238\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.998 and 0.024479248\n",
            "Val acc and loss are 0.8836 and 0.44834664\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.998 and 0.024349965\n",
            "Val acc and loss are 0.8837 and 0.44879064\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.99804 and 0.024221394\n",
            "Val acc and loss are 0.8837 and 0.44923374\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.99804 and 0.024093531\n",
            "Val acc and loss are 0.8837 and 0.44967657\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.99808 and 0.023966363\n",
            "Val acc and loss are 0.8837 and 0.45012012\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.9981 and 0.023839902\n",
            "Val acc and loss are 0.8835 and 0.45056337\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.9981 and 0.023714138\n",
            "Val acc and loss are 0.8836 and 0.45100552\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.99814 and 0.023589075\n",
            "Val acc and loss are 0.8837 and 0.4514473\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.99818 and 0.0234647\n",
            "Val acc and loss are 0.8838 and 0.45188963\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.9982 and 0.02334102\n",
            "Val acc and loss are 0.8839 and 0.45233184\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.9982 and 0.023218025\n",
            "Val acc and loss are 0.8839 and 0.4527728\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.99824 and 0.023095723\n",
            "Val acc and loss are 0.8839 and 0.45321348\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.99828 and 0.0229741\n",
            "Val acc and loss are 0.8839 and 0.4536549\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.99828 and 0.022853158\n",
            "Val acc and loss are 0.8838 and 0.454096\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.9983 and 0.022732895\n",
            "Val acc and loss are 0.8838 and 0.45453602\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.9983 and 0.022613311\n",
            "Val acc and loss are 0.8837 and 0.45497525\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.9983 and 0.022494402\n",
            "Val acc and loss are 0.8837 and 0.45541513\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.99836 and 0.022376157\n",
            "Val acc and loss are 0.8837 and 0.45585522\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.9984 and 0.022258587\n",
            "Val acc and loss are 0.8836 and 0.45629433\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.99842 and 0.02214168\n",
            "Val acc and loss are 0.8835 and 0.45673233\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.99848 and 0.022025434\n",
            "Val acc and loss are 0.8836 and 0.4571704\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.99854 and 0.021909855\n",
            "Val acc and loss are 0.8836 and 0.45760888\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.9986 and 0.02179493\n",
            "Val acc and loss are 0.8836 and 0.45804688\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.9986 and 0.021680659\n",
            "Val acc and loss are 0.8836 and 0.45848387\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.99866 and 0.021567041\n",
            "Val acc and loss are 0.8835 and 0.45892057\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.99866 and 0.021454072\n",
            "Val acc and loss are 0.8835 and 0.45935732\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.99866 and 0.021341756\n",
            "Val acc and loss are 0.8835 and 0.45979384\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.99874 and 0.021230077\n",
            "Val acc and loss are 0.8834 and 0.46022964\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.9988 and 0.021119043\n",
            "Val acc and loss are 0.8832 and 0.4606648\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.9988 and 0.021008648\n",
            "Val acc and loss are 0.8832 and 0.4610999\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.99882 and 0.02089889\n",
            "Val acc and loss are 0.8832 and 0.46153486\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.99882 and 0.020789763\n",
            "Val acc and loss are 0.8831 and 0.46196923\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.99884 and 0.02068127\n",
            "Val acc and loss are 0.883 and 0.46240297\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.99886 and 0.020573398\n",
            "Val acc and loss are 0.883 and 0.46283638\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.99888 and 0.02046616\n",
            "Val acc and loss are 0.883 and 0.4632696\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.9989 and 0.020359537\n",
            "Val acc and loss are 0.883 and 0.46370244\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.9989 and 0.02025354\n",
            "Val acc and loss are 0.8831 and 0.46413466\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.99892 and 0.020148152\n",
            "Val acc and loss are 0.8831 and 0.4645664\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.99894 and 0.020043384\n",
            "Val acc and loss are 0.8831 and 0.46499786\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.99894 and 0.019939223\n",
            "Val acc and loss are 0.8831 and 0.46542892\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.99896 and 0.019835677\n",
            "Val acc and loss are 0.8832 and 0.46585953\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.99896 and 0.019732729\n",
            "Val acc and loss are 0.8831 and 0.46628964\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.99896 and 0.019630387\n",
            "Val acc and loss are 0.8829 and 0.46671933\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.999 and 0.019528644\n",
            "Val acc and loss are 0.8829 and 0.46714857\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.99906 and 0.0194275\n",
            "Val acc and loss are 0.8829 and 0.46757734\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.99906 and 0.019326951\n",
            "Val acc and loss are 0.883 and 0.4680058\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.99906 and 0.01922699\n",
            "Val acc and loss are 0.8832 and 0.4684338\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.99906 and 0.019127617\n",
            "Val acc and loss are 0.8833 and 0.46886122\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.99906 and 0.019028833\n",
            "Val acc and loss are 0.8834 and 0.46928817\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.99908 and 0.01893063\n",
            "Val acc and loss are 0.8834 and 0.46971464\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.9991 and 0.01883301\n",
            "Val acc and loss are 0.8834 and 0.4701408\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.99912 and 0.018735966\n",
            "Val acc and loss are 0.8834 and 0.47056645\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.99912 and 0.018639494\n",
            "Val acc and loss are 0.8834 and 0.4709915\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.99914 and 0.018543595\n",
            "Val acc and loss are 0.8834 and 0.47141612\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.9992 and 0.018448267\n",
            "Val acc and loss are 0.8833 and 0.4718403\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9992 and 0.018353503\n",
            "Val acc and loss are 0.8833 and 0.47226405\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.99922 and 0.0182593\n",
            "Val acc and loss are 0.8833 and 0.47268724\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.99922 and 0.018165661\n",
            "Val acc and loss are 0.8833 and 0.47310987\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.99922 and 0.018072579\n",
            "Val acc and loss are 0.8833 and 0.47353214\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.99926 and 0.017980052\n",
            "Val acc and loss are 0.8833 and 0.4739539\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.99926 and 0.017888073\n",
            "Val acc and loss are 0.8833 and 0.47437516\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.99926 and 0.017796645\n",
            "Val acc and loss are 0.8833 and 0.4747958\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.99926 and 0.01770576\n",
            "Val acc and loss are 0.8833 and 0.475216\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.99926 and 0.017615423\n",
            "Val acc and loss are 0.8833 and 0.47563583\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.99926 and 0.017525626\n",
            "Val acc and loss are 0.8833 and 0.4760551\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.99928 and 0.017436367\n",
            "Val acc and loss are 0.8833 and 0.47647378\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.99928 and 0.017347638\n",
            "Val acc and loss are 0.8834 and 0.47689193\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.99932 and 0.017259443\n",
            "Val acc and loss are 0.8834 and 0.47730967\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.99934 and 0.01717178\n",
            "Val acc and loss are 0.8835 and 0.4777269\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.99938 and 0.017084641\n",
            "Val acc and loss are 0.8835 and 0.47814354\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.99938 and 0.016998027\n",
            "Val acc and loss are 0.8834 and 0.47855967\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.9994 and 0.016911931\n",
            "Val acc and loss are 0.8835 and 0.4789754\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.99942 and 0.016826356\n",
            "Val acc and loss are 0.8834 and 0.47939044\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.99944 and 0.016741294\n",
            "Val acc and loss are 0.8834 and 0.47980508\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.99944 and 0.016656745\n",
            "Val acc and loss are 0.8833 and 0.48021913\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.99946 and 0.016572705\n",
            "Val acc and loss are 0.8833 and 0.48063272\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.99946 and 0.01648917\n",
            "Val acc and loss are 0.8833 and 0.48104575\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.99946 and 0.016406143\n",
            "Val acc and loss are 0.8834 and 0.48145825\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.99948 and 0.016323613\n",
            "Val acc and loss are 0.8834 and 0.4818702\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.9995 and 0.016241584\n",
            "Val acc and loss are 0.8833 and 0.48228168\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.99952 and 0.016160049\n",
            "Val acc and loss are 0.8834 and 0.48269257\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.99952 and 0.016079007\n",
            "Val acc and loss are 0.8833 and 0.48310304\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.99952 and 0.015998453\n",
            "Val acc and loss are 0.8832 and 0.48351285\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.99952 and 0.015918389\n",
            "Val acc and loss are 0.8833 and 0.48392215\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.99952 and 0.01583881\n",
            "Val acc and loss are 0.8832 and 0.48433095\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.99952 and 0.01575971\n",
            "Val acc and loss are 0.8832 and 0.48473915\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.99952 and 0.015681088\n",
            "Val acc and loss are 0.8832 and 0.48514688\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.99952 and 0.015602947\n",
            "Val acc and loss are 0.8832 and 0.48555404\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.99954 and 0.015525273\n",
            "Val acc and loss are 0.8832 and 0.48596063\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.99956 and 0.015448076\n",
            "Val acc and loss are 0.8832 and 0.48636675\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.99956 and 0.015371343\n",
            "Val acc and loss are 0.883 and 0.48677227\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.99956 and 0.015295075\n",
            "Val acc and loss are 0.8829 and 0.48717728\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.99956 and 0.015219272\n",
            "Val acc and loss are 0.8829 and 0.48758173\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.99956 and 0.015143927\n",
            "Val acc and loss are 0.883 and 0.4879857\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.99956 and 0.0150690405\n",
            "Val acc and loss are 0.883 and 0.48838902\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.99958 and 0.014994605\n",
            "Val acc and loss are 0.883 and 0.4887918\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.99958 and 0.014920628\n",
            "Val acc and loss are 0.883 and 0.48919404\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.99958 and 0.014847095\n",
            "Val acc and loss are 0.883 and 0.48959574\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.99958 and 0.014774009\n",
            "Val acc and loss are 0.8829 and 0.48999688\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.99958 and 0.014701367\n",
            "Val acc and loss are 0.8829 and 0.49039745\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.99958 and 0.0146291675\n",
            "Val acc and loss are 0.8831 and 0.49079755\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.99958 and 0.014557406\n",
            "Val acc and loss are 0.8832 and 0.49119702\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.99958 and 0.014486079\n",
            "Val acc and loss are 0.8832 and 0.49159598\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.99958 and 0.014415186\n",
            "Val acc and loss are 0.8832 and 0.4919943\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.9996 and 0.014344724\n",
            "Val acc and loss are 0.8831 and 0.4923921\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.9996 and 0.014274691\n",
            "Val acc and loss are 0.8831 and 0.49278936\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.9996 and 0.01420508\n",
            "Val acc and loss are 0.8831 and 0.49318603\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.9996 and 0.014135896\n",
            "Val acc and loss are 0.883 and 0.4935822\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.9996 and 0.014067127\n",
            "Val acc and loss are 0.883 and 0.4939777\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.9996 and 0.01399878\n",
            "Val acc and loss are 0.883 and 0.4943727\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.99962 and 0.013930849\n",
            "Val acc and loss are 0.8831 and 0.4947671\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.99962 and 0.013863328\n",
            "Val acc and loss are 0.8831 and 0.49516094\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.99962 and 0.013796216\n",
            "Val acc and loss are 0.8831 and 0.49555424\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.99962 and 0.013729515\n",
            "Val acc and loss are 0.8831 and 0.4959469\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.99964 and 0.013663217\n",
            "Val acc and loss are 0.8831 and 0.49633905\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.99964 and 0.013597322\n",
            "Val acc and loss are 0.8831 and 0.49673066\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.99964 and 0.013531826\n",
            "Val acc and loss are 0.8832 and 0.49712163\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.99964 and 0.013466727\n",
            "Val acc and loss are 0.8832 and 0.497512\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.99966 and 0.013402024\n",
            "Val acc and loss are 0.8831 and 0.49790186\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.99966 and 0.013337715\n",
            "Val acc and loss are 0.8829 and 0.4982911\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.99966 and 0.013273794\n",
            "Val acc and loss are 0.8828 and 0.49867973\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.99966 and 0.013210258\n",
            "Val acc and loss are 0.8828 and 0.49906787\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.99966 and 0.013147112\n",
            "Val acc and loss are 0.8828 and 0.49945536\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.99966 and 0.013084347\n",
            "Val acc and loss are 0.8828 and 0.49984235\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.99968 and 0.013021964\n",
            "Val acc and loss are 0.8828 and 0.5002286\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.9997 and 0.012959956\n",
            "Val acc and loss are 0.8828 and 0.5006144\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.9997 and 0.012898325\n",
            "Val acc and loss are 0.8828 and 0.50099957\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.9997 and 0.012837065\n",
            "Val acc and loss are 0.883 and 0.5013841\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.99972 and 0.012776179\n",
            "Val acc and loss are 0.883 and 0.5017681\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.99972 and 0.01271566\n",
            "Val acc and loss are 0.8829 and 0.5021515\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.99972 and 0.012655505\n",
            "Val acc and loss are 0.8829 and 0.5025343\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.99972 and 0.012595715\n",
            "Val acc and loss are 0.8828 and 0.5029165\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.99972 and 0.012536288\n",
            "Val acc and loss are 0.8828 and 0.50329816\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.99972 and 0.012477219\n",
            "Val acc and loss are 0.8828 and 0.5036792\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.99972 and 0.012418507\n",
            "Val acc and loss are 0.8828 and 0.5040596\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.99974 and 0.012360149\n",
            "Val acc and loss are 0.8828 and 0.5044395\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.99974 and 0.0123021435\n",
            "Val acc and loss are 0.8828 and 0.50481874\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.99974 and 0.012244487\n",
            "Val acc and loss are 0.8828 and 0.50519747\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.99976 and 0.012187177\n",
            "Val acc and loss are 0.8828 and 0.5055754\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.99976 and 0.012130215\n",
            "Val acc and loss are 0.8827 and 0.5059529\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.99978 and 0.012073596\n",
            "Val acc and loss are 0.8827 and 0.5063298\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.99978 and 0.012017314\n",
            "Val acc and loss are 0.8827 and 0.50670606\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.99978 and 0.011961374\n",
            "Val acc and loss are 0.8827 and 0.50708175\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.99978 and 0.011905773\n",
            "Val acc and loss are 0.8827 and 0.5074568\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.99978 and 0.0118505005\n",
            "Val acc and loss are 0.8827 and 0.5078313\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.99978 and 0.011795564\n",
            "Val acc and loss are 0.8827 and 0.5082052\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.99978 and 0.0117409555\n",
            "Val acc and loss are 0.8826 and 0.5085785\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.99978 and 0.011686676\n",
            "Val acc and loss are 0.8826 and 0.5089511\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.99978 and 0.011632721\n",
            "Val acc and loss are 0.8826 and 0.5093232\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.99978 and 0.01157909\n",
            "Val acc and loss are 0.8826 and 0.50969476\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.99978 and 0.01152578\n",
            "Val acc and loss are 0.8826 and 0.51006556\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.99978 and 0.011472788\n",
            "Val acc and loss are 0.8826 and 0.5104359\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.99978 and 0.011420117\n",
            "Val acc and loss are 0.8827 and 0.51080555\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.99978 and 0.011367759\n",
            "Val acc and loss are 0.8827 and 0.5111747\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.99978 and 0.011315713\n",
            "Val acc and loss are 0.8827 and 0.5115431\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.9998 and 0.01126398\n",
            "Val acc and loss are 0.8826 and 0.51191103\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.99982 and 0.011212554\n",
            "Val acc and loss are 0.8827 and 0.51227826\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.99982 and 0.011161435\n",
            "Val acc and loss are 0.8827 and 0.51264495\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.99982 and 0.011110621\n",
            "Val acc and loss are 0.8828 and 0.5130111\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.99982 and 0.011060111\n",
            "Val acc and loss are 0.8828 and 0.51337653\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.99982 and 0.011009903\n",
            "Val acc and loss are 0.8828 and 0.51374143\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.99982 and 0.0109599875\n",
            "Val acc and loss are 0.8828 and 0.51410574\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.99982 and 0.010910376\n",
            "Val acc and loss are 0.8828 and 0.51446944\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.99982 and 0.010861056\n",
            "Val acc and loss are 0.8828 and 0.5148325\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.99982 and 0.010812029\n",
            "Val acc and loss are 0.8829 and 0.5151951\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.99982 and 0.010763293\n",
            "Val acc and loss are 0.8829 and 0.51555693\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.99982 and 0.010714846\n",
            "Val acc and loss are 0.8829 and 0.51591825\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.99982 and 0.010666688\n",
            "Val acc and loss are 0.8829 and 0.516279\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.99982 and 0.010618813\n",
            "Val acc and loss are 0.8829 and 0.516639\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.99982 and 0.010571223\n",
            "Val acc and loss are 0.883 and 0.5169985\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.99982 and 0.010523913\n",
            "Val acc and loss are 0.8831 and 0.51735747\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.99984 and 0.010476884\n",
            "Val acc and loss are 0.8831 and 0.5177158\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.99984 and 0.010430133\n",
            "Val acc and loss are 0.883 and 0.51807356\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.99984 and 0.01038366\n",
            "Val acc and loss are 0.883 and 0.5184306\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.99986 and 0.010337456\n",
            "Val acc and loss are 0.883 and 0.51878715\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.99986 and 0.010291527\n",
            "Val acc and loss are 0.883 and 0.5191431\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.99986 and 0.010245869\n",
            "Val acc and loss are 0.883 and 0.5194984\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.99986 and 0.01020048\n",
            "Val acc and loss are 0.883 and 0.5198531\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.99986 and 0.010155358\n",
            "Val acc and loss are 0.883 and 0.52020735\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.99986 and 0.010110499\n",
            "Val acc and loss are 0.8831 and 0.5205609\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.99986 and 0.010065905\n",
            "Val acc and loss are 0.883 and 0.52091384\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.99986 and 0.010021576\n",
            "Val acc and loss are 0.8829 and 0.5212662\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.99986 and 0.009977501\n",
            "Val acc and loss are 0.8829 and 0.52161795\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.99986 and 0.009933689\n",
            "Val acc and loss are 0.8829 and 0.5219692\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.99986 and 0.00989013\n",
            "Val acc and loss are 0.8829 and 0.5223198\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.99986 and 0.009846827\n",
            "Val acc and loss are 0.8829 and 0.52266985\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.99986 and 0.009803777\n",
            "Val acc and loss are 0.8829 and 0.52301925\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.99988 and 0.00976098\n",
            "Val acc and loss are 0.8829 and 0.52336806\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.99988 and 0.009718433\n",
            "Val acc and loss are 0.8828 and 0.52371633\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.99988 and 0.009676132\n",
            "Val acc and loss are 0.8828 and 0.52406394\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.99988 and 0.009634079\n",
            "Val acc and loss are 0.8827 and 0.52441096\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.99988 and 0.009592271\n",
            "Val acc and loss are 0.8827 and 0.52475744\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.99988 and 0.0095507065\n",
            "Val acc and loss are 0.8827 and 0.5251034\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.9999 and 0.009509384\n",
            "Val acc and loss are 0.8827 and 0.5254486\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.9999 and 0.009468301\n",
            "Val acc and loss are 0.8827 and 0.5257934\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.9999 and 0.009427457\n",
            "Val acc and loss are 0.8826 and 0.5261375\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.9999 and 0.00938685\n",
            "Val acc and loss are 0.8826 and 0.52648103\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.9999 and 0.009346479\n",
            "Val acc and loss are 0.8826 and 0.52682394\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.9999 and 0.0093063405\n",
            "Val acc and loss are 0.8826 and 0.52716637\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.9999 and 0.009266435\n",
            "Val acc and loss are 0.8826 and 0.5275082\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.9999 and 0.009226762\n",
            "Val acc and loss are 0.8826 and 0.5278493\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.9999 and 0.0091873165\n",
            "Val acc and loss are 0.8826 and 0.52819\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.9999 and 0.009148098\n",
            "Val acc and loss are 0.8826 and 0.52853\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.9999 and 0.009109107\n",
            "Val acc and loss are 0.8826 and 0.5288695\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.9999 and 0.009070342\n",
            "Val acc and loss are 0.8826 and 0.52920836\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.9999 and 0.0090318\n",
            "Val acc and loss are 0.8826 and 0.5295467\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.9999 and 0.008993479\n",
            "Val acc and loss are 0.8825 and 0.5298844\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.9999 and 0.00895538\n",
            "Val acc and loss are 0.8825 and 0.5302215\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.9999 and 0.008917497\n",
            "Val acc and loss are 0.8825 and 0.5305581\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.9999 and 0.008879834\n",
            "Val acc and loss are 0.8825 and 0.53089404\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.9999 and 0.008842389\n",
            "Val acc and loss are 0.8825 and 0.5312295\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.9999 and 0.008805155\n",
            "Val acc and loss are 0.8825 and 0.53156435\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.9999 and 0.008768136\n",
            "Val acc and loss are 0.8825 and 0.53189856\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.99992 and 0.008731331\n",
            "Val acc and loss are 0.8824 and 0.5322323\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.99992 and 0.008694735\n",
            "Val acc and loss are 0.8824 and 0.5325654\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.99992 and 0.008658347\n",
            "Val acc and loss are 0.8824 and 0.5328979\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.99992 and 0.008622168\n",
            "Val acc and loss are 0.8824 and 0.5332299\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.99992 and 0.008586195\n",
            "Val acc and loss are 0.8824 and 0.53356135\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.99992 and 0.008550429\n",
            "Val acc and loss are 0.8824 and 0.5338921\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.99992 and 0.008514864\n",
            "Val acc and loss are 0.8823 and 0.5342224\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.99992 and 0.008479505\n",
            "Val acc and loss are 0.8822 and 0.53455204\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.99992 and 0.008444345\n",
            "Val acc and loss are 0.8822 and 0.53488123\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.99992 and 0.008409386\n",
            "Val acc and loss are 0.8822 and 0.53520966\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.99992 and 0.008374625\n",
            "Val acc and loss are 0.8821 and 0.5355377\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.99992 and 0.008340062\n",
            "Val acc and loss are 0.8821 and 0.5358652\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.99992 and 0.008305692\n",
            "Val acc and loss are 0.8821 and 0.536192\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.99992 and 0.00827152\n",
            "Val acc and loss are 0.8821 and 0.5365183\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.99992 and 0.008237541\n",
            "Val acc and loss are 0.8821 and 0.536844\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.99992 and 0.008203752\n",
            "Val acc and loss are 0.8821 and 0.5371692\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.99992 and 0.008170158\n",
            "Val acc and loss are 0.8821 and 0.5374938\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.99992 and 0.00813675\n",
            "Val acc and loss are 0.8821 and 0.53781796\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.99992 and 0.008103535\n",
            "Val acc and loss are 0.8821 and 0.5381414\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.99992 and 0.008070504\n",
            "Val acc and loss are 0.8822 and 0.5384643\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.99992 and 0.00803766\n",
            "Val acc and loss are 0.8823 and 0.53878665\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.99992 and 0.008004999\n",
            "Val acc and loss are 0.8822 and 0.5391086\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.99992 and 0.0079725245\n",
            "Val acc and loss are 0.8821 and 0.53942984\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.99992 and 0.00794023\n",
            "Val acc and loss are 0.8821 and 0.5397505\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.99992 and 0.007908118\n",
            "Val acc and loss are 0.8821 and 0.5400707\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.99992 and 0.007876187\n",
            "Val acc and loss are 0.882 and 0.5403903\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.99992 and 0.007844432\n",
            "Val acc and loss are 0.882 and 0.5407094\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.99992 and 0.007812858\n",
            "Val acc and loss are 0.882 and 0.5410279\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.99992 and 0.0077814613\n",
            "Val acc and loss are 0.882 and 0.5413459\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.99992 and 0.0077502388\n",
            "Val acc and loss are 0.882 and 0.5416633\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.99992 and 0.0077191885\n",
            "Val acc and loss are 0.882 and 0.54198015\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.99992 and 0.007688314\n",
            "Val acc and loss are 0.882 and 0.5422966\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.99992 and 0.0076576113\n",
            "Val acc and loss are 0.882 and 0.5426123\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.99992 and 0.007627079\n",
            "Val acc and loss are 0.882 and 0.54292756\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.99992 and 0.0075967168\n",
            "Val acc and loss are 0.882 and 0.5432422\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.99992 and 0.007566524\n",
            "Val acc and loss are 0.882 and 0.54355645\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.99992 and 0.007536498\n",
            "Val acc and loss are 0.882 and 0.5438701\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.99992 and 0.0075066406\n",
            "Val acc and loss are 0.882 and 0.5441832\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.99992 and 0.0074769445\n",
            "Val acc and loss are 0.882 and 0.54449576\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.99992 and 0.007447418\n",
            "Val acc and loss are 0.882 and 0.5448078\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.99992 and 0.0074180514\n",
            "Val acc and loss are 0.882 and 0.5451192\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.99992 and 0.007388848\n",
            "Val acc and loss are 0.882 and 0.54543024\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.99992 and 0.0073598046\n",
            "Val acc and loss are 0.882 and 0.5457407\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.99992 and 0.0073309233\n",
            "Val acc and loss are 0.8821 and 0.54605067\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.99992 and 0.007302203\n",
            "Val acc and loss are 0.8821 and 0.54636\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.99992 and 0.0072736363\n",
            "Val acc and loss are 0.8821 and 0.5466688\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.99992 and 0.0072452305\n",
            "Val acc and loss are 0.882 and 0.54697716\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.99992 and 0.0072169765\n",
            "Val acc and loss are 0.8819 and 0.54728496\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.99992 and 0.0071888827\n",
            "Val acc and loss are 0.8819 and 0.5475922\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.99992 and 0.0071609407\n",
            "Val acc and loss are 0.8819 and 0.547899\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.99992 and 0.0071331514\n",
            "Val acc and loss are 0.8818 and 0.54820526\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.99992 and 0.0071055144\n",
            "Val acc and loss are 0.8818 and 0.5485109\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.99992 and 0.007078028\n",
            "Val acc and loss are 0.8818 and 0.5488162\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.99992 and 0.007050692\n",
            "Val acc and loss are 0.8818 and 0.5491209\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.99992 and 0.007023506\n",
            "Val acc and loss are 0.8819 and 0.549425\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.99992 and 0.0069964677\n",
            "Val acc and loss are 0.8819 and 0.5497287\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.99992 and 0.0069695762\n",
            "Val acc and loss are 0.8819 and 0.5500317\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.99992 and 0.006942832\n",
            "Val acc and loss are 0.8819 and 0.55033445\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.99992 and 0.0069162324\n",
            "Val acc and loss are 0.8818 and 0.5506366\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.99992 and 0.006889777\n",
            "Val acc and loss are 0.8816 and 0.5509382\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.99992 and 0.006863466\n",
            "Val acc and loss are 0.8816 and 0.55123925\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.99992 and 0.0068372954\n",
            "Val acc and loss are 0.8816 and 0.55153984\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.99992 and 0.0068112677\n",
            "Val acc and loss are 0.8816 and 0.55184\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.99992 and 0.0067853807\n",
            "Val acc and loss are 0.8816 and 0.5521396\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.99992 and 0.006759634\n",
            "Val acc and loss are 0.8815 and 0.5524387\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.99992 and 0.006734025\n",
            "Val acc and loss are 0.8815 and 0.55273724\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.99992 and 0.006708554\n",
            "Val acc and loss are 0.8815 and 0.55303526\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.99992 and 0.00668322\n",
            "Val acc and loss are 0.8815 and 0.5533329\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.99992 and 0.0066580237\n",
            "Val acc and loss are 0.8815 and 0.55363\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.99992 and 0.0066329604\n",
            "Val acc and loss are 0.8815 and 0.5539266\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.99994 and 0.0066080326\n",
            "Val acc and loss are 0.8815 and 0.55422264\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.99994 and 0.006583238\n",
            "Val acc and loss are 0.8816 and 0.5545183\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.99994 and 0.0065585766\n",
            "Val acc and loss are 0.8816 and 0.55481344\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.99994 and 0.0065340465\n",
            "Val acc and loss are 0.8816 and 0.555108\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.99994 and 0.0065096472\n",
            "Val acc and loss are 0.8816 and 0.55540216\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.99994 and 0.006485379\n",
            "Val acc and loss are 0.8817 and 0.5556958\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.99994 and 0.006461239\n",
            "Val acc and loss are 0.8817 and 0.55598897\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.99994 and 0.0064372267\n",
            "Val acc and loss are 0.8816 and 0.5562816\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.99994 and 0.0064133424\n",
            "Val acc and loss are 0.8816 and 0.5565738\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.99994 and 0.006389585\n",
            "Val acc and loss are 0.8816 and 0.5568655\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.99994 and 0.0063659535\n",
            "Val acc and loss are 0.8816 and 0.55715674\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.99994 and 0.0063424474\n",
            "Val acc and loss are 0.8816 and 0.55744743\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.99996 and 0.006319065\n",
            "Val acc and loss are 0.8816 and 0.55773777\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.99996 and 0.0062958067\n",
            "Val acc and loss are 0.8815 and 0.5580275\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.99996 and 0.006272671\n",
            "Val acc and loss are 0.8815 and 0.55831677\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.99996 and 0.0062496564\n",
            "Val acc and loss are 0.8815 and 0.5586057\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.99996 and 0.006226763\n",
            "Val acc and loss are 0.8814 and 0.55889404\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.99996 and 0.006203991\n",
            "Val acc and loss are 0.8814 and 0.5591819\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.99996 and 0.0061813374\n",
            "Val acc and loss are 0.8814 and 0.5594693\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.99996 and 0.0061588036\n",
            "Val acc and loss are 0.8814 and 0.5597563\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.99996 and 0.006136386\n",
            "Val acc and loss are 0.8814 and 0.5600427\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.99998 and 0.0061140857\n",
            "Val acc and loss are 0.8814 and 0.5603288\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.99998 and 0.0060919044\n",
            "Val acc and loss are 0.8814 and 0.5606142\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.99998 and 0.006069836\n",
            "Val acc and loss are 0.8815 and 0.5608993\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.99998 and 0.0060478845\n",
            "Val acc and loss are 0.8816 and 0.56118387\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.99998 and 0.0060260454\n",
            "Val acc and loss are 0.8816 and 0.5614681\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.99998 and 0.0060043223\n",
            "Val acc and loss are 0.8816 and 0.5617517\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.99998 and 0.0059827096\n",
            "Val acc and loss are 0.8816 and 0.56203496\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.99998 and 0.0059612095\n",
            "Val acc and loss are 0.8817 and 0.56231767\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.99998 and 0.0059398212\n",
            "Val acc and loss are 0.8817 and 0.5626\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.99998 and 0.0059185424\n",
            "Val acc and loss are 0.8817 and 0.5628818\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.99998 and 0.005897375\n",
            "Val acc and loss are 0.8817 and 0.56316316\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.99998 and 0.0058763158\n",
            "Val acc and loss are 0.8817 and 0.56344414\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 1.0 and 0.005855365\n",
            "Val acc and loss are 0.8818 and 0.56372464\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 1.0 and 0.0058345227\n",
            "Val acc and loss are 0.8818 and 0.5640046\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 1.0 and 0.005813786\n",
            "Val acc and loss are 0.8818 and 0.5642842\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 1.0 and 0.0057931556\n",
            "Val acc and loss are 0.8817 and 0.56456333\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 1.0 and 0.0057726316\n",
            "Val acc and loss are 0.8817 and 0.564842\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 1.0 and 0.0057522138\n",
            "Val acc and loss are 0.8816 and 0.56512016\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 1.0 and 0.0057318984\n",
            "Val acc and loss are 0.8816 and 0.5653979\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 1.0 and 0.005711688\n",
            "Val acc and loss are 0.8816 and 0.5656752\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 1.0 and 0.005691579\n",
            "Val acc and loss are 0.8816 and 0.5659521\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 1.0 and 0.005671575\n",
            "Val acc and loss are 0.8816 and 0.56622857\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 1.0 and 0.0056516705\n",
            "Val acc and loss are 0.8816 and 0.56650454\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 1.0 and 0.005631868\n",
            "Val acc and loss are 0.8817 and 0.56678015\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 1.0 and 0.0056121657\n",
            "Val acc and loss are 0.8817 and 0.5670552\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 1.0 and 0.0055925637\n",
            "Val acc and loss are 0.8817 and 0.5673299\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 1.0 and 0.0055730604\n",
            "Val acc and loss are 0.8817 and 0.5676041\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 1.0 and 0.0055536544\n",
            "Val acc and loss are 0.8817 and 0.5678779\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 1.0 and 0.005534346\n",
            "Val acc and loss are 0.8817 and 0.5681513\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 1.0 and 0.0055151368\n",
            "Val acc and loss are 0.8817 and 0.56842417\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 1.0 and 0.005496024\n",
            "Val acc and loss are 0.8817 and 0.56869656\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 1.0 and 0.005477007\n",
            "Val acc and loss are 0.8816 and 0.56896865\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 1.0 and 0.005458085\n",
            "Val acc and loss are 0.8817 and 0.5692402\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 1.0 and 0.005439257\n",
            "Val acc and loss are 0.8817 and 0.56951153\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 1.0 and 0.005420523\n",
            "Val acc and loss are 0.8816 and 0.56978226\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 1.0 and 0.0054018847\n",
            "Val acc and loss are 0.8816 and 0.57005256\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 1.0 and 0.0053833374\n",
            "Val acc and loss are 0.8816 and 0.57032245\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 1.0 and 0.005364884\n",
            "Val acc and loss are 0.8817 and 0.57059187\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 1.0 and 0.005346521\n",
            "Val acc and loss are 0.8817 and 0.570861\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 1.0 and 0.0053282506\n",
            "Val acc and loss are 0.8817 and 0.57112956\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 1.0 and 0.00531007\n",
            "Val acc and loss are 0.8817 and 0.5713978\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 1.0 and 0.0052919793\n",
            "Val acc and loss are 0.8817 and 0.5716655\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 1.0 and 0.0052739778\n",
            "Val acc and loss are 0.8817 and 0.57193285\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 1.0 and 0.0052560656\n",
            "Val acc and loss are 0.8817 and 0.5721998\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 1.0 and 0.005238242\n",
            "Val acc and loss are 0.8817 and 0.5724663\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 1.0 and 0.005220507\n",
            "Val acc and loss are 0.8817 and 0.57273245\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 1.0 and 0.005202858\n",
            "Val acc and loss are 0.8817 and 0.57299805\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 1.0 and 0.0051852968\n",
            "Val acc and loss are 0.8817 and 0.5732634\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 1.0 and 0.0051678205\n",
            "Val acc and loss are 0.8817 and 0.5735282\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 1.0 and 0.005150432\n",
            "Val acc and loss are 0.8817 and 0.5737926\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 1.0 and 0.0051331273\n",
            "Val acc and loss are 0.8817 and 0.57405657\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 1.0 and 0.0051159062\n",
            "Val acc and loss are 0.8817 and 0.57432026\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 1.0 and 0.0050987704\n",
            "Val acc and loss are 0.8817 and 0.5745835\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 1.0 and 0.0050817174\n",
            "Val acc and loss are 0.8818 and 0.57484627\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 1.0 and 0.0050647473\n",
            "Val acc and loss are 0.8818 and 0.5751086\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 1.0 and 0.0050478606\n",
            "Val acc and loss are 0.8818 and 0.5753706\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 1.0 and 0.0050310553\n",
            "Val acc and loss are 0.8818 and 0.57563215\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 1.0 and 0.0050143306\n",
            "Val acc and loss are 0.8818 and 0.57589334\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 1.0 and 0.0049976897\n",
            "Val acc and loss are 0.8818 and 0.576154\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 1.0 and 0.0049811276\n",
            "Val acc and loss are 0.8818 and 0.57641447\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 1.0 and 0.0049646446\n",
            "Val acc and loss are 0.8818 and 0.57667434\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 1.0 and 0.0049482426\n",
            "Val acc and loss are 0.8818 and 0.5769339\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 1.0 and 0.004931918\n",
            "Val acc and loss are 0.8816 and 0.5771931\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 1.0 and 0.004915673\n",
            "Val acc and loss are 0.8816 and 0.57745177\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 1.0 and 0.004899505\n",
            "Val acc and loss are 0.8816 and 0.57771003\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 1.0 and 0.004883416\n",
            "Val acc and loss are 0.8816 and 0.577968\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 1.0 and 0.0048674024\n",
            "Val acc and loss are 0.8815 and 0.5782256\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 1.0 and 0.0048514656\n",
            "Val acc and loss are 0.8814 and 0.5784828\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 1.0 and 0.0048356066\n",
            "Val acc and loss are 0.8814 and 0.5787395\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 1.0 and 0.0048198216\n",
            "Val acc and loss are 0.8814 and 0.5789959\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 1.0 and 0.004804112\n",
            "Val acc and loss are 0.8814 and 0.5792518\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 1.0 and 0.004788478\n",
            "Val acc and loss are 0.8813 and 0.5795074\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 1.0 and 0.0047729174\n",
            "Val acc and loss are 0.8813 and 0.57976264\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 1.0 and 0.004757431\n",
            "Val acc and loss are 0.8812 and 0.58001745\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 1.0 and 0.004742017\n",
            "Val acc and loss are 0.8812 and 0.5802719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeL40sP2Diir",
        "outputId": "3590a152-2595-46af-f224-bfc90a68f76c"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.8897 at epoch 365 with a corresponding training accuracy of 0.97694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOPtJ4aQbzeN"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hPkQo3efZX",
        "outputId": "31baf230-4857-4880-d681-9816f4f813ec"
      },
      "source": [
        "# Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 1.0 achieved at epoch: 937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73vS-fuwefcI",
        "outputId": "2f40656d-2e76-4313-f7e7-527ce787f453"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training set\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f98ea76d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzravxTkefhR",
        "outputId": "9efdeaa0-c490-40ac-b1b8-cd9d376fd195"
      },
      "source": [
        "# Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8897 achieved at epoch: 364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "W-g1A22-efku",
        "outputId": "19efc53a-fb62-4ad0-c7a7-807f2a968cfd"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation set\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f99161c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LEstoXhefow",
        "outputId": "d181f545-06bf-4689-e094-7863bfe6847b"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.62919533\n",
            "0.8744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auq3JEqveffm",
        "outputId": "a99490f4-02f0-4f50-e89b-2f6956cd9a09"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f9927dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHiCAYAAAB1IlqBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3wU1frH8c/JJgFCCSQkoQcQEAQuSBFFBQQVQRAsoD8QkSJWULHQVIoKqOi1Xb0iKKCgUq5UFRGlSFOaiIKAgtQQWgg1yW7O749dYqgJKTth832/XvuCnZmdec7sZHeeec6ZNdZaREREREREsiPI6QBEREREROTSp8RCRERERESyTYmFiIiIiIhkmxILERERERHJNiUWIiIiIiKSbUosREREREQk25RYiEiWGWMKGWNmGWMOG2OmZGM9nY0x3+ZkbE4wxnxtjOmaC+u93Rizwxhz1BhzZU6vX0REJCcosRDJB4wxnYwxK30npnt8J8DX5cCq7wJigEhrbYesrsRaO9Fae3MOxHMaY0wzY4w1xnx5xvQ6vukLMrmeIcaYTzNazlrbylo7PovhXsgo4DFrbRFr7Zp0cVXwvaenHtYYcyzd8+svdkPGmG3GmBtzNPpzb+ec+9TXhio5sP5xxpiXsrseERHJvGCnAxCR3GWM6Qv0Bx4C5gLJwC1AO+DHbK4+FthkrXVncz25aR9wjTEm0lp7wDetK7AppzZgjDGAsdam5tQ6zxAL/HbmRGvtdqBIujgsUMdauyWX4hARETkvVSxEApgxJhwYBjxqrf2ftfaYtTbFWjvLWvuMb5kCxpg3jTG7fY83jTEFfPOaGWN2GmOeMsbE+6od3XzzhgIvAHf7ro73OPMqtDGmou8KdLDv+f3GmL+MMUeMMVuNMZ3TTf8x3esaG2N+9nWx+tkY0zjdvAXGmBeNMUt86/nWGFPyArshGZgO3ON7vQu4G5h4xr56y9fdKNEYs+rU1X5jzC3AwHTt/CVdHC8bY5YAx4HKvmk9ffPfN8ZMS7f+V4wx831JyJnvU5Ax5jljzN++/TzBGBPue2+OAi7gF2PMnxdo55nrLGCMGWWM2W6M2WuM+a8xppBvXkljzGxjTIIx5qAxZrEvhk+ACsAsX1ufPcd6S/heu88Yc8j3/3Lp5p/zPc4KX0z9jTF/GmMOGGMmG2Mi0s2fYoyJ8x0ni4wxNX3TewGdgWd97Zjlm77NGPOMMWad8VZ2xhpjYoy3gnfEGPOdMaZERuv3zRvn26fzfK9daIyJzWpbRUQCgRILkcB2DVAQ+PICywwCrgbqAnWAq4Dn0s0vBYQDZYEewH+MMSWstYOB4cAXvi46Yy8UiDGmMPA20MpaWxRoDKw9x3IRwBzfspHAG8AcY0xkusU6Ad2AaCAUePpC2wYmAPf5/t8SWA/sPmOZn/HugwhgEjDFGFPQWvvNGe2sk+41XYBeQFHg7zPW9xRQ23eifT3efdfVWmvPEd/9vscNQGW8VYh3rbVJ1tpTFYk61trLMmhneiOBar42VcH7/r2QLradQBTermwDAWut7QJsB9r62vrqOdYbBHyMt4pSATgBvAuZf48vQm+gPdAUKAMcAv6Tbv7XQFW8x8FqfMmitXa07/+v+trRNt1r7gRuwrtv2vrWMdC3L4KAPhmtP53OwItASV87z5wvIpKvKLEQCWyRwP4Muip1BoZZa+OttfuAoXhPmE9J8c1PsdZ+BRwFLs9iPKlALWNMIWvtHmvtWd17gFuBzdbaT6y1bmvtZ8BGvCeBp3xsrd1krT0BTMZ78nxe1tqlQIQx5nK8CcaEcyzzqbX2gG+brwMFyLid46y1v/lek3LG+o7j3Y9vAJ8Cva21O8+zns7AG9bav6y1R4EBwD2nKj0Xy1cV6QU8aa09aK09gjc5use3SApQGoj1va+Lz5PwnMW3j6ZZa4/71vsy3hP/UzLzHp/S0Vc1SXucMf8hYJC1dqe1NgkYAtx1ar9Yaz+y1h5JN6+O8VbpLuQda+1ea+0uYDGwwlq7xlp7Em8CnjY4PhPrn2OtXeSbPwhvl7vyGWxfRCRgKbEQCWwHgJIZnKCW4fSr7X/7pqWt44zE5Djp+vVnlrX2GN4uSA8Be4wxc4wx1TMRz6mYyqZ7HpeFeD4BHsNbFTirgmOMedoYs8HX7SUBb5XmQl2sAHZcaKa1dgXwF2DwJkDnc673IBhvNSErooAwYFW6E/ZvfNMBXgO2AN/6ui31z+yKjTFhxpgPfN22EoFFQHFjjOsi3uNTJltri6d/nDE/FvgyXRs2AB4gxhjjMsaM9HWTSgS2+V6T0Xu2N93/T5zjeRFfOzOz/rT335cQHuT0vx0RkXxFiYVIYFsGJOHtTnI+u/GewJ1SgbO7CWXWMbwntKeUSj/TWjvXWnsT3qvlG4EPMxHPqZh2ZTGmUz4BHgG+8lUT0vi6Kj0LdARK+E5wD+NNCADOdzX/glf5jTGP4q187Pat/3zO9R64Of2k92Lsx3uSXDPdSXv4qW5VvqvwT1lrKwO3AX2NMS18r82ocvEU3kpOI2ttMaCJb7rxrTsz73Fm7cDbrSp98lHQV23ohPcGBDfiTQIrpo8jE+3ISEbrB0irThhjiuDtRpfVvx0RkUueEguRAGatPYy3X/1/jDHtfVebQ4wxrYwxp/rPfwY8Z4yJMt5B0C/g7bqTFWuBJsZ7G9RwvF16APANkm3n64efhLdL1bnuovQVUM14b5EbbIy5G7gCmJ3FmACw1m7F22Vn0DlmF8V7Ir8PCDbGvAAUSzd/L1DRGJPpz0xjTDXgJeBevF2injXGnK/L1mfAk8aYSr4T1FNjOrJ0ty3f3ak+BP5tjIn2xVPWGNPS9/82xpgqvi5Th/FWAU69F3vxjvM4n6J4k5YE33iYwenanNn3OLP+C7x8alC07xhtly6OJLxVuTC8+yy9jNqRkYzWD9DaGHOdMSYU71iL5dbaC1axREQCmRILkQDnGy/QF++A7H14rwI/hvdOSeA9+V0JrAN+xTtINUv3/7fWzgO+8K1rFacnA0G+OHbj7TLSFHj4HOs4ALTBe2X8AN4r/W2stfuzEtMZ6/7RWnuuK8pz8XYV2oS3G9JJTu/mdOrH/w4YY1ZntB1f17NPgVestb9YazfjHSD8ifHdcesMH+GtqCwCtvq23ztzrTqvfni7Oy33deX5jn/GjFT1PT+Kt6r1nrX2B9+8EXgTzQRjzLkGxb8JFMJbFVmOd7+dkqn3+CK8BczE22XriG97jXzzJuB9r3YBv/vmpTcWuMLXjulcvIzWD95B/oPxtrU+3iRSRCTfMpkcryciIiI+xphxwE5r7XMZLSsikl+oYiEiIiIiItmmxEJERERERLJNXaFERERERCTbVLEQEREREZFsU2IhIiIiIiLZdqFf480RMxoHq6+VT7vv4jJeKL9IzdLt+QOTzc5t/gOMK9TpCCSPsYnZ/V3EwGGKlc14ofxCn5v/yPzP6+QPYSVNxgs5b0j1kBw/Px6yMcXxtutoFBERERGRbMv1ioWIiIiIiPzD8dJCLlHFQkREREREsk0VCxERERERPzIBWrJQxUJERERERLJNFQsRERERET8K1Cv7SixERERERPxIXaFERERERETOQxULERERERE/CtCChSoWIiIiIiKSfapYiIiIiIj4UaCOsVBiISIiIiLiR4HaZShQ2yUiIiIiIn6kioWIiIiIiB8FalcoVSxERERERCTbVLEQEREREfGjAC1Y5N3EovLdjxPbtjtgSfxzPWte7kFqclLa/MvueYLYtt1J9bhJTtjPmuE9ORG3PVvbDClaggYvfkZY6ViO7/mblc/fQ8qRBMrd/H9UufcZjDG4jx/ll9ceJXHLumy20P8GDBnOgkVLiIwoweypnzodjl8NGDqSBYuXeds+eVza9E8+n8bEydNxuYJoet3VPPv4w84F6SdJScl0fuBxklOS8Xg8tGzRlD4PdmPHrj30HTiMhMOJ1KxRjVeHDSQ0JMTpcP1mT9xenn3+RQ4cOIQx0PHOdnTt1NHpsBwRCJ8VA0e+x4Jlq4ksEc6sca+fNX/WvMV8OGkG1loKhxViSN+eVK9SMVvbTE5Ood/wd/lt018UL1aUNwY/QbnS0Sz5eR2vj55ISoqbkJBgnn24C1fXq5WtbTll0ZLlvPzam6SmptKhfVt6de/idEh+M2DICBYsXur9u5gyAYCEw4k82X8wu3bHUbZMKd58ZRjhxYo6HKn/5efjIqvUFcqPCpYsQ+UOj7GweyN+uLcuJshF2RvvPm2Zw5vWsrB7IxbcV4/dP0yj5iMjM73+yCubcuWgsWdNr9qlH/tXfc/8u2uwf9X3VO3SD4Bju7ex5NHm/NDlSv74+GXq9vtv9hrokDvatmbMf95wOgxH3NG2FWPeee20act/Xs38hUuY+flY5kwZT48u9zgUnX+FhoYw/r9vMPOzsUyfNIbFS39i7a+/M+qdD7i/UwfmTZ9IsaJFmTrjK6dD9SuXy0X/vr356n8T+WLCaCZ98T+2/LnV6bAcEQifFbe3asaHrw087/yypaP55O0hzBr3Oo/cdycvjBqd6XXv3BNPl8eHnDV96pzvKVa0MN9OeoeuHW7l9Q8mAlAivCjvj+jHrHGvM3LAozz78jsX3Z68wOPxMGzk64x593XmTJvI7G++y1d/I3e0bcWYd0edNm30x59yzVX1+XbGZ1xzVX1Gf3xpJuLZkd+PCzldhomFMaa6MaafMeZt36OfMaZGrgfmCsZVoBDG5cJVMIyT+/ecNn//6gV4kk4AcOi3FRSMLpc2r0qnp2gydhnNJqzm8h6DM73N0te3ZftX3qsQ27+aQOnrb/Ouf/0yUo4k+La1nILRZbPVNqc0rF+X8PBiTofhiIb16hAefvpVpM+mzqDX/Z0IDQ0FIDKihBOh+Z0xhsJhhQBwu9243R6MgeU/r6Fli6YA3N6mJfMX/OhkmH4XHVWSmjUuB6BI4cJUrhTL3n37HI7KGYHwWdGwzhWEFy1y3vn1al2eNr9OzarE7TuQNm/mt4vo8OAA2vd4hhdGjcbjSc3UNucvWUn7ls0AaNn0apatXo+1liuqVSKmZAQAVSuVJykpmeTklCy2zDnr1m8gtnw5ypcrS2hICLe2bMH8BYudDstvzvV3MX/hj7RvcwsA7dvcwnf5aH+ckt+Pi6wyufDICy6YWBhj+gGf4433J9/DAJ8ZY/rnVlAn9+9my2dvcPOXW2k5cycpRw+z76d5512+QptuxC//BoCoq26icPkqLOpxDQu61qd49XpE1r0+U9stEBFD0oE4AJIOxFEgIuYc2+pO/LJvstAqyWu2bd/JyjXr6HDfQ9z7QB/W/bbB6ZD8xuPx0K5TTxrfdDuNG9WnfLmyFCtahOBgFwCloqPYG7/f4Sids3P3Hjb8sZk6tWo6HYr4wdQ539Ok0ZUA/LltJ199v5RJ/3mR6WNfwxUUxKx5mTtJit9/kNLRkQAEB7soWjiMhMNHTltm7sIVXFGtMqGhl143w73x+ygVE532PCYmOt8m36ccOHCI6KiSAESVjOTAgUMOR+R/Oi4kvYzGWPQAalprT7u0Yox5A/gNyHz/o4sQUrQ4pa6/jXl3VSHlSAINX/6Cci07sXPupLOWLdeyE8WrN2DJozcAEH3VTURfdRPNxq0EwBVWhMLlqnBg7WKafLiUoJBQXGFFCC0WkbbMb+8PZN+Kb89at7X2tOcl6zUjtm03Fj/UNKebLA7weDwcTkxk8vj3+fW3jTzRfwjzZ36OCdSOj+m4XC5mTBpD4pGjPPr08/y1LXvjkwLJsePH6fP0IAY+3YciRQo7HY7ksuWr1zNtzg9MfHcYAMtWr+e3TVvp8OAAAE4mJRNRwnuV+rFBr7EzLp6UFDd74vfTvsczAHS5szV3tr4hw21t3rqD1z+YyNhRg3KpNeIkY0zA9puXnBcUoMdKRolFKlAG+PuM6aV9887JGNML6AXwcGVDy5iLG8oR1aAFx3dvJTnBe8V0z4Iviah9zVmJRVSDFlTrOoAfH21OakryqY2zacIr/D3jw7PWu+iBxoB3jEWF1vex5uUep81POriXApGlvNWKyFIkH4pPm1fsstrUHfABy/q2ISXx4EW1R/KmmOgobrqhCcYY/lWrBkEmiEMJh4koUdzp0PymWNEiNGpQl7XrfiPxyFHcbg/BwS7i4vcRE13S6fD8LiXFTZ+nB9G21c3c3KKZ0+FILvvjz795/rUPGP3qAEr4ukpaa2l/S1Oe6tXprOXffdmbSOzcE8+Ake/xyVtDTpsfXTKCPfEHKBUdidvt4cix4xT3rTcu/gCPPTeKVwY+SoWypXK3YbkkJjqKuL3/fC/u3RtPTFSUgxE5LzKyBPH79hMdVZL4ffuJyCddatPTcSHpZXTG/wQw3xjztTFmtO/xDTAfePx8L7LWjrbWNrDWNrjYpALgxN4dlKjZCFcBbz/wkg2ac2TbxtOWCa9Wlzr93mPFs7eTfOifklv8im+JbdMNVyHvlcaCJcsQWiJzB/ieH2dTofV9AFRofR97Fs8CoFBMeRqOmMKqofdzbMfmi26P5E03NruOFSvXALD17x2kuFMoUTzc4ahy38FDCSQeOQrAyZNJLF2xissqxdKowZXMnb8QgC9nz6V502udDNPvrLUMGjqCypVi6ZZPBvLnZ7v37qf386N4ZdBjVCpfJm36NfVr8+2C5Rw4dBiAhMSj7IrLXLeO5tfWZ/rcBQDMXbicq6+siTGGxCPHeLD/SJ56sBP1alfP8bb4S+2a1dm2fSc7du0mOSWFOXPn07zZdU6H5ajmTa5l+mxv9+jps7+hRdP8tz90XGRNoI6xMGd29zlrAWOCgKuAUyOWdwE/W2s9mdnAjMbBF97AeVzeYzBlb+yA9bg5vGkta0f0olrXgSRsXEncj7Np/NZcil5WiyTfoO7je3fwU7/bAajcsbfvVrXgPnGMVUPv4/iuv9LWfb6KRUixCBq+9DmFYspzIm47Pz93DylHDlG3/weUbnYHJ+K8hRvrcbOwx9UX3aZ238VlZVfkmL79B/PTqjUcSkggMiKC3g/1oMPtbZ0JJtXt1831HTiUn1au5VDCYSIjI+j9YDfa3XozA4e+wsZNWwgJDubZJx7hmqvq+TUuAGzmBobmlI2b/6T/4JF4UlOxqancclMzHnugKzt27ubJgS9yODGRGpdXZdSLA9MGtvuNy8/bS2flml/o3P0RqlW9jCBff4a+jz1I0+sbOxaTU/LSZ4VN3JWl1/Ud+iY/r/2dQ4ePEBkRTu9uHXG7vZ8797S7mede/S/fLlxBmVLeypzL5WLaaG/v3q++X8roiV+SmmoJDnbxwhM9qFuzWtq6z1exSEpK5tmX32XDlq2EFy3CG4OfoHyZGN6fMI3RE6cTW+6fSsXYUc8RWeLiLmSYYs7fOGTh4qUMH/U2nlQPd7Zrw8M9uzoTiJ8/NwH6Dhji+7s47Pu76M6Nza7niX4vsCcunjKlY3jzlWEU9/eND4zzN/jMM8cFQFjJvHKOfUFv1QnJ0vnxhTz+S4rjbc8wsciurCYWgcjpxCJP8XNikac58AWZZzmYWEjelNXEIhDlhcQiz9Dn5j/yQGKRpyixcFSe/YE8EREREZFA5HgGkEuU5oqIiIiISLapYiEiIiIi4kdBJjBHCiixEBERERHxI3WFEhEREREROQ9VLERERERE/EgVCxERERERkfNQxUJERERExI9MgJYslFiIiIiIiPhRgOYV6golIiIiIiLZp4qFiIiIiIgfBQVoyUIVCxERERERyTZVLERERERE/ChACxZKLERERERE/ClQ7wqlrlAiIiIiIpJtqliIiIiIiPhRgBYsVLEQEREREZHsy/WKRbvv4nJ7E5eMIfVKOx1CnjFk9R6nQ8g7Ut1OR5CHWKcDyEMC9XrWxTHFyjodguRFRtdF5dKm282KiIiIiIich8ZYiIiIiIj4UYAWLJRYiIiIiIj4k243KyIiIiIich6qWIiIiIiI+FGAFixUsRARERERkexTxUJERERExI8CdYyFEgsRERERET8K1C5DgdouERERERHxI1UsRERERET8KFC7QqliISIiIiIi2aaKhYiIiIiIHwVowUKJhYiIiIiIPwUFaGahrlAiIiIiIpJtqliIiIiIiPhRgBYsVLEQEREREZHsC+iKxZ64vTz7/IscOHAIY6Djne3o2qmj02FdtEZdelO/Q3cwhtVTPmL5hLdPm9+4e1/+1bYTAEEuFyUvq8FrjUtz4vChLG/TFRLK7a98TJma9TiecJCpfTuRsOtvKjduwY1PDccVEoonJZl5r/Zj64oF2WmeIwYMGc6CRUuIjCjB7KmfOh2OXw0YOpIFi5d52z55XNr0Tz6fxsTJ03G5gmh63dU8+/jDzgXpII/Hw52dexITHcUHb7/qdDiOWbRkOS+/9iapqal0aN+WXt27OB2SI5KSkujc41GSk1PweNy0vPEG+jzc0+mwHKPjwitQzi9yio6LixeoYywCOrFwuVz079ubmjUu5+ixY9zZqQfXNmpIlcsqOR1apkVXrUn9Dt35sGNjPCnJ3PvhHDYtmMPB7X+mLbP0ozdY+tEbAFS74Vau6fp4ppOK4mVjaT9iLOPuu/G06fXu6s7JxATeblmDWq07cuNTw5natzPHDx3gs4fbcyR+D9FVa3LvmDm80bRijrXXX+5o25p7776Tfs+/6HQofndH21bc2/EO+g0enjZt+c+rmb9wCTM/H0toaCgHDmY9Kb3UTZg0hcsqxXL02HGnQ3GMx+Nh2MjX+fj9N4mJieauzj1p3vS6S+qzM6eEhoYyfvTbFA4LIyXFTafuD9Pk2qup+69aTofmdzou/hEI5xc5RceFpBfQXaGio0pSs8blABQpXJjKlWLZu2+fw1FdnJKVq7Nz3c+knDxBqsfDtp8XUeOm9uddvvatd/PrnC/Snv+rbScemLyUh75cSZuh72GCMveWX96iLWunfwLA73OnUfma5gDEbVjLkfg9AMRv/o2QAoVwhYRmtXmOaVi/LuHhxZwOwxEN69UhPLzoadM+mzqDXvd3IjTU+15GRpRwIjTHxe2NZ8GPy7jr9rZOh+Kodes3EFu+HOXLlSU0JIRbW7Zg/oLFToflCGMMhcPCAHC73bjdbkyg/rJVBnRc/CMQzi9yio6LrAnKhUdekFfiyHU7d+9hwx+bqVOrptOhXJT4zb8R2+BaChWPIKRgIao2bUWx0uXPuWxIwUJUua4lG779H+BNSmq27sDYTk347+0NsB5PWpepjBSLLkPinh0ApHo8nDxymLDikactc0XLO9jz+xo8KcnZaKHkBdu272TlmnV0uO8h7n2gD+t+2+B0SI4Y/trbPPP4wwQFao06k/bG76NUTHTa85iY6Hx70gTeK7Lt7u5K4xZtaHx1Q+rUvrS+R3KKjotzu1TPL3KKjousMSbnH3lBlrtCGWO6WWs/zslgcsux48fp8/QgBj7dhyJFCjsdzkXZ/9dGfvxwFF3Gfk3K8WPEbfgF6/Gcc9lqN7Rh+5qlad2gKl/TnDI169FrynIAggsW5NjBeADufmcKJcpVwhUSQnjpCjz05UoAln/yDmv/Nz7DuKKqXMGNTw3nkx6tc6KZ4jCPx8PhxEQmj3+fX3/byBP9hzB/5uf56srsD4uWEBFRnFpXVGfFytVOhyN5iMvlYsYX40k8coRH+w5g05a/qFalstNhSR5wKZ9fiOSG7IyxGAqcM7EwxvQCegF88M7r9Op+XzY2kz0pKW76PD2Itq1u5uYWzRyLIzvWTPuYNdO8u7rFky+SGLfrnMvVat2R9em6QWEMa6d/wvw3njtr2S96dwDOP8YiMX43xUqXJ3HvLoJcLgoWDed4wgEAisWU5Z53p/Blv+4c2vFXTjRRHBYTHcVNNzTBGMO/atUgyARxKOEwESWKOx2a36xe+yvfL1zCoh+Xk5SczNFjx3h60DBGvfyC06H5XUx0FHF749Oe790bT0xUlIMR5Q3FihalUYN6LF66PF8mFjouThcI5xc5QcdF1gRql6ELtssYs+48j1+BmPO9zlo72lrbwFrbwMmkwlrLoKEjqFwplm5d7nEsjuwqHOH9Aw0vXZ4aN7Xn19mfnbVMgSLFqNiwCRvnz0ybtnXZ91xx8x1pry8UXoLwMhUytc0/vp9N3fbeuzpc0fJOti7/AYCCRcPp9MFMvnt9EDvWLM1WuyTvuLHZdaxYuQaArX/vIMWdQoni4Q5H5V9P9XmIRXO/5PuvpvLGyCFc3bB+vkwqAGrXrM627TvZsWs3ySkpzJk7n+bNrnM6LEccPHiIxCNHADh5MomlK36mcsVYh6Nyho6LfwTK+UVO0HEh6WVUsYgBWgJn3iLGAHn+rHLV2nXMmPMN1apeRru7uwLQ97EHaXp9Y4cjuzgd355MWPEIPG43c4b14eSRwzS4uxcAK78YDUCNm9rz55J5pJz45042+/7cwPdvDabL2K8xQUF43Cl8NawPh3dvz3Cba6Z+xO2vjqPP3A2cOHyIqX07A3BV50eIqHAZTR95jqaPeCshn/RoxbGDl1Z/yr79B/PTqjUcSkigScv29H6oBx3yyYDdvgOH8tPKtRxKOEyTVnfR+8Fu3NmuNQOHvkKbjvcTEhzMyCED81U3KDldcHAwL/R7kp6P9MWT6uHOdm2oeln+u0IPEL//AP1feAlPaio2NZVbbmrODU2udTosR+i4+EegnF/kBB0XWROoX7HGWnv+mcaMBT621v54jnmTrLUZjwQ+vv/8G8hnhtQr7XQIecaQ1XucDiHvSHU7HUHeEeRyOoI8JEC/dUREclNYyUviw3P2da4cPz9u86PH8bZfsGJhre1xgXmZu72QiIiIiIgEvID+gTwRERERkbwmXw7eFhERERERyQxVLERERERE/ChQB2+rYiEiIiIi4kdBufDIDGPMk8aY34wx640xnxljChpjKhljVhhjthhjvjDGhPqWLeB7vsU3v2Jm2iUiIiIiIgHMGFMW6AM0sNbWAlzAPcArwL+ttVXw/sTEqZs39QAO+ab/27fcBSmxEBERERHxI2Ny/pFJwUAhY0wwEAbsAZoDU33zxwPtfbxIhIUAACAASURBVP9v53uOb34Lk8GPXCmxEBEREREJcNbaXcAoYDvehOIwsApIsNae+lGtnUBZ3//LAjt8r3X7lo+80DaUWIiIiIiI+FFujLEwxvQyxqxM9+iVfpvGmBJ4qxCVgDJAYeCWnGyX7golIiIiInKJs9aOBkZfYJEbga3W2n0Axpj/AdcCxY0xwb6qRDlgl2/5XUB5YKev61Q4cOBCMahiISIiIiLiR0Em5x+ZsB242hgT5hsr0QL4HfgBuMu3TFdghu//M33P8c3/3lprL7QBVSxERERERPzIid+xsNauMMZMBVYDbmAN3grHHOBzY8xLvmljfS8ZC3xijNkCHMR7B6kLUmIhIiIiIpIPWGsHA4PPmPwXcNU5lj0JdLiY9SuxEBERERHxo0AdixCo7RIRERERET9SxUJERERExI+cGGPhD0osRERERET8KFC7DOV+YpHqzniZfGLI6j1Oh5BnLGwd43QIeUbT2TudDiHPsMf2Ox1CnmEKRzkdQp5gj1/wlun5igm74A/eiog4ThULERERERE/yuTvTlxyArUSIyIiIiIifqSKhYiIiIiIHwVowUKJhYiIiIiIP6krlIiIiIiIyHmoYiEiIiIi4kcBWrBQxUJERERERLJPFQsRERERET/SGAsREREREZHzUMVCRERERMSPgox1OoRcocRCRERERMSPArQnlLpCiYiIiIhI9qliISIiIiLiRxq8LSIiIiIich6qWIiIiIiI+FGAFiyUWIiIiIiI+JO6QomIiIiIiJxHwFUsBgwdyYLFy4iMKMHsyePSpn/y+TQmTp6OyxVE0+uu5tnHH3YuSAckJSXRucejJCen4PG4aXnjDfR5uKfTYV2UQuWrccXgz9KeFyxdmW0fD2bX1LfTpgUXKc7l/cZSsExlUpNP8serPTm+9bdsbdeEhFJ9wHiKXl6PlMMH+H3Y/5EU9zcl6t9IpV7DMSGh2JRk/vpvPxLW/JCtbfnDnrh4nn1hOAcOHsIYQ8fb29C1010kHE7kyQFD2bU7jrJlSvHmyCGEFyvqdLi5btwXM5k6ax7GGKpWjmXEwN6s/nUDr703ntTUVMIKFWLEoD7ElivtdKh+M2DIcBYsWuL9HJ36qdPhOGLc5zOYOutb73FxWSwjBj7OoBHvsH7jFkKCXdS+oipDn32UkOCA+xq9oEVLlvPya2+SmppKh/Zt6dW9i9MhOWJP3F6eff5FDhw4hDHQ8c52dO3U0emwHKPj4uIF6pX9gGvXHW1bMead106btvzn1cxfuISZn49lzpTx9Ohyj0PROSc0NJTxo99m5uTxTP98PIuXrmDtuvVOh3VRTuzYxKqe9b2PXg1JTTrO/sXTT1umwr0DOLplLat6XMnGEfdT5bF/Z3r9BUrFUufN+WdNL926O+6jh/ip8+XsnPoWlXuNBCDl8H7WD2zHqu512TiyG9UHjs9eA/3E5XLR/8lH+GrqeL4Y9x6Tpkxny1/bGD1uEtc0rMe30ydyTcN6jB43yelQc93efQf4ZOpspo4dxaxP3iY11cOc+YsZMuoDXnvhSaaPe5M2N13P++MnOx2qX93RtjVj/vOG02E4xntczGLqR28w69N3SU1NZc53i2l7c1O+/uw9Zn7yDieTkpk661unQ/Urj8fDsJGvM+bd15kzbSKzv/mOLX9udTosR7hcLvr37c1X/5vIFxNGM+mL/+XbfaHjQtLLMLEwxlQ3xrQwxhQ5Y/otuRdW1jWsV4fw8NOvsn42dQa97u9EaGgoAJERJZwIzVHGGAqHhQHgdrtxu90Yc+l28CtRrwUndv1J0t7tp00Pi72ChNXeqsGJ7X9QsFRFQkpEAxB9U2eufH8Z9cesomrf9yEoc3l15LXt2PvNBAD2LZxKifrNATi6ZS3JB/YAcHzrbwQVKIQJCc2R9uWm6KhIataoBkCRwmFUrhTL3vj9zF+4hPZtvH/W7dvcwncLfnQyTL/xeDycTErG7fZwIimZ6JIRGANHj50A4Mix40SXjHA4Sv9qWL8u4eHFnA7DUR5P6j/HxckkoktG0LRxA4wxGGP4V41qxMUfcDpMv1q3fgOx5ctRvlxZQkNCuLVlC+YvWOx0WI6IjipJzRqXA1CkcGHv5+i+fQ5H5QwdF1ljTM4/8oILnlkZY/oAM4DewHpjTLt0s4fnZmA5adv2naxcs44O9z3EvQ/0Yd1vG5wOyREej4d2d3elcYs2NL66IXVq13Q6pCyLan438d9/ftb0Y3/+QskmtwNQtHpDCpaKpUBUOcIqVCf6ho6sfex6VvWsj031EHNj50xtq0BUGU7u2+F94vHgPnqY4PDI05Yp2fROjm5ejU1Jzl7D/Gzn7j1s2LiZOrVqcODAQaKjvO2KKhnBgQMHHY4u98VERdL9nvY0v/MBrm/fjaKFw7juqit5qf+j9HrmRZre3oOZcxfQ6947nQ5V/CgmKpLu/9ee5nf04Pp2XSlauDDXNboybX6K283MuT9wfaN6Dkbpf3vj91EqJjrteUxMdL49mU5v5+49bPhjM3VqXbrfqdmh4yJrgkzOP/KCjC7ZPgDUt9a2B5oBzxtjHvfNyyNNyJjH4+FwYiKTx7/Ps48/zBP9h2CtdTosv3O5XMz4YjwL537JuvW/s2nLX06HlCUmOISS17Zl34KpZ83bPukVgosUp/6YVZS94zGObF6DTfVQvH5zilSrR70PVlB/zCpK1GtOwTKVAKj54jTqj1lF7ZGzKXp5A+qPWUX9MauIueX+TMUTVvEKKvcawabXL61xO8eOH6fPM4MZ+PRjFClS+LR5p67KBrrDiUeZ/+NPfDf5AxZN/4gTJ08yc+4Cxn8xi9GvPc/CL8dyR+sWjHznI6dDFT86nHiU+YtX8N2UD1k0Y5zvuPhn/NSwUf+lQZ2aNKibP08k5R/Hjh+nz9ODGPh0n7M+R0Xyo4xGnQVZa48CWGu3GWOaAVONMbFcILEwxvQCegF88Narjg/iiYmO4qYbmnjL17VqEGSCOJRwmIgSxR2NyynFihalUYN6LF66nGpVKjsdzkWLaNSKI5vWkHIo/qx5nuNH+OOVHmnPG33+Jyd3/0V47evYO3cCWz8cdNZrfnveezW6QKlYqvf/iF+eaHHa/KR9uykYVZ7kfbvA5SK4SDjuw94uEKFRZan54jQ2jrifk7svnUQtJcVNn2cG07bVjdzcvAkAkZERxO87QHRUJPH7DhCRD7oMLlv5C+VKRxNRIhyAm5pcw+pfN7Jxy1bq1PR2F2vV/DoeeHqok2GKny1buZZyZWL+OS6aXsOaXzdyW8sbePejzziYcJh3hg9wOEr/i4mOIm7vP5+7e/fGExMV5WBEzkpJcdPn6UG0bXUzN7do5nQ4jtFxkTWBeukuo4rFXmNM3VNPfElGG6AkUPt8L7LWjrbWNrDWNnA6qQC4sdl1rFi5BoCtf+8gxZ1CieLhDkflXwcPHiLxyBEATp5MYumKn6lcMdbhqLImusU9xM8/uxsUgKtIOCY4BIBSt/Yk4ZfFeI4fIWH195Rseichxb0fdsFFS1AgpkKmtndg6UxibrkPgKimd3HIN4bDVSSc2iNmsXX0QBLXL81us/zGWsugF1+lcqUKdLv3n7uYNG/SmOmzvwFg+uxvaNH0WqdC9JvSMVH88tsmTpxMwlrLslXruKxiOY4cO87W7bsAWLpyLZVjyzkcqfhT6Zgofln/xz/HxcpfqBxbnikzv+XHFWt4fejTBGVyjFYgqV2zOtu272THrt0kp6QwZ+58mje7zumwHGGtZdDQEVSuFEu3fHhDmPR0XEh65kJdgowx5QC3tTbuHPOutdYuyXALR+P82ueo78Ch/LRyLYcSDhMZGUHvB7vR7tabGTj0FTZu2kJIcDDPPvEI11zlQN/YIOduS7hx0xb6v/ASntRUbGoqt9zUnMce7O5YPAtbx2TpdUEFw7j6i22s6FQFz7FEAErf9iAAe2Z+QLErrubyAR+DtRzb9jubXu2J+2gCAFE3dKRC535ggrDuFDa/1Zsjv69IW/f5KhYmtAA1Bk6gSNW6pCQeZMOwTpzcs5UKXQZSoVN/TuzanLbsuqdvISXh4vqWNp29M0v7IqtWrllH5559qFalMkG+Tpl9H32Af9WqwRP9h7Inbi9lSsfw5sghFPfzAF57IsGv2wN4e+xnfD3/R4JdLmpUq8RL/R5j4bJVvD12EkEmiGJFCzN8QG/Kly3l17hMYeeu+PXtP5ifVq3hUEICkRER9H6oBx1ub+tILPa4MwOk3x4zia/nL/YdF5V5qX9vrryxA2VioikcVgjwVjIe7e6/k0oTFpnxQrls4eKlDB/1Np5UD3e2a8PDPbs6HZIjVq75hc7dH6Fa1csI8nUb7fvYgzS9vrHDkTkjTx0XYSUviWLA5tauHD8/rvqVx/G2XzCxyBF+TizyNAcTi7wmq4lFIPJ3YpGXOZFY5FVOJhZ5iVOJRV6UFxILkTxPiYWjdKYrIiIiIuJHgXp/FCUWIiIiIiL+FKCZRf4bfSYiIiIiIjlOFQsRERERET8K0IKFKhYiIiIiIpJ9qliIiIiIiPiRCdCShRILERERERE/CtTEQl2hREREREQk21SxEBERERHxpwC9tB+gzRIREREREX9SxUJERERExI8CdYyFEgsRERERET8K0LxCXaFERERERCT7VLEQEREREfGjQO0KpYqFiIiIiIhkmyoWIiIiIiL+FJgFC1UsREREREQk+3K/YmFTc30TlwztizRN5+x2OoQ8Y0yTsk6HkGf0XLDd6RAkjzGhRZwOQfIk63QAeUiAXvoOcIE6xkJdoURERERE/ChA8wp1hRIRERERkexTxUJERERExI8CtSuUKhYiIiIiIpJtqliIiIiIiPhTgFYslFiIiIiIiPhRgOYV6golIiIiIiLZp4qFiIiIiIgfafC2iIiIiIjIeahiISIiIiLiRwFasFBiISIiIiLiVwGaWagrlIiIiIiIZJsqFiIiIiIifhSgBQtVLEREREREJPtUsRARERER8SPdblZEREREROQ8Aq5ikZSUTOcHHic5JRmPx0PLFk3p82A3duzaQ9+Bw0g4nEjNGtV4ddhAQkNCnA43Vw0YMoIFi5cSGVGC2VMmAPD1vB9494OP+HPr30z5ZDS1r6jucJT+MWDIyHT7YjwAr/z7PX5YvJSQ4GAqlC/LiCH9KVa0qMORZl6tTo9zefvuWGs5tGU9i4b2wJOclDa/cKnyNB36MaFFwglyufjpnUHsXPJ1trZZpExFmo+YRIHwCA5sWM2C57uS6k6hVucnvLF43Jw4tJ/FQ3tyNG57dpuY6/bExfPs4BEcOHgIY6Dj7W3o+n938cSAoWz9ewcAR44cpWjRIsyYNMbhaP1nwJDhLFi0xPv3MvVTp8Pxu6SkZDr3epzklBQ87lPfI/fz1HMvs37DH4QEB1O7ZnWGDexLSHDAfY1e0KIly3n5tTdJTU2lQ/u29OrexemQHJN45AjPDX2FTX/+hTGG4YMHcGWdWk6H5QgdFxdPFYtLRGhoCOP/+wYzPxvL9EljWLz0J9b++juj3vmA+zt1YN70iRQrWpSpM75yOtRcd0fbVox5d9Rp06pdVol3Rr1Mw3p1HIrKGXe0vYUx77522rRrr27A7MnjmDV5HBUrlOODjy6dE6iwqDLUvOcxpndpxP/urotxuajc8u7Tlrmyx0C2zpvC9M4N+X5AZ67t/06m11+17X3U6/XCWdOv6jOC9RPfZEr76iQlHuLy9t0BOPDHWm8s99Rj2/xpXPX4yOw10E9cwS76P/kwX00Zxxcfv8ekKTPY8tc23hwxmBmTxjBj0hhubt6Em2643ulQ/eqOtq0Z8583nA7DMaGhIYx//w1mThrD9EkfsniZ93vktlYt+GbqeGZ9PpakpCSmTJ/jdKh+5fF4GDbydca8+zpzpk1k9jffseXPrU6H5ZiXX32L6xs34psvJzHji3FcVjnW6ZAcoeMia4zJ+UdeEHCJhTGGwmGFAHC73bjdHoyB5T+voWWLpgDc3qYl8xf86GSYftGwfl3Cw4udNu2yyhWpXLGCQxE551z74rprriLYd7Wxbu2axMXvcyK0LDOuYIILFMK4XAQXDOP4vj2nzbfWElLY2+bQIuFp801QEFc9/grtJizjjs9XU/2OBzK9zTINb2Dr/GkAbJ79CbHN2gGwZ+UCPCdPABD/6woKR5fLdvv8IbpkJDWrVwOgSOEwKleswN74/WnzrbV8/d0C2rRs4VSIjjjX30t+cvb3iBtjDE2vvRpjDMYY/lWz+mnHSn6wbv0GYsuXo3y5soSGhHBryxbMX7DY6bAcceTIUX5e/Qt33d4GgNCQkEuq4p2TdFxIehnWcI0xVwHWWvuzMeYK4BZgo7U2z17y93g83NHlQbbv2EWnDu0pX64sxYoWITjYBUCp6Kh894UgFzZtxle0urm502Fk2vF9u/n10ze4Z85W3Ekn2LV8HruWzzttmdWjh9HqP19T8+5HCS5UmK8ebglAtXbdST56mBn3XUNQSChtP1rEzuXzOLp72wW3WaB4JElHErAeDwDH4ncSFlXmrOWqtevGjqXf5ExD/Wjn7jg2/LGFOrVqpE1buWYdkRElqFjh0kiUJOd4v0ceYvtO7/dI+uMixe1mxlfzGPTUYw5G6H974/dRKiY67XlMTDTr1v/mYETO2bl7DxElijNg8HA2btpCzRqXM+jZxwkrVMjp0PxOx0UW5ZUSQw67YGJhjBkMtAKCjTHzgEbAD0B/Y8yV1tqX/RDjRXO5XMyYNIbEI0d59Onn+Wtb3u/rLc55f8wEXMEubmt9k9OhZFpo0eLENr2NL9pWIeloAi1e+YIqrTqx5etJactc1vIeNs+awK+f/pvo2lfT7MVxTOtYh3JX30RE1dpUanGHd11FwgmvUJWUY4m0fv9bAAqERxAUHEpss9sAWPDC/Rzfv+fsQM5QpVUnoq5owOwHbsiFVueeY8dP0OfZFxj41KMUKVI4bfrsud/nu2qFeHm/Rz70fo888wKbtmylWpVKAAwd+SYNrvwXDa78l8NRilPcbg+/b9zE8/2eoE7tmrz06puM/uhTnng08xVgkUCUUcXiLqAuUACIA8pZaxONMaOAFcA5EwtjTC+gF8AHb71Cr2735lzEF6FY0SI0alCXtet+I/HIUdxuD8HBLuLi9xETXdKRmCRv+d/Mr1mweBnj/vvvS2ogVdlGLTiyaysnE7yVt23ff0l0nWtOSywub9eNb3rfCkD8r8txhRakYPGSYAxLX3uCXcu+PWu9X3ZqAHjHWBQtXZHVo4edNr9A0eIYlwvr8VA4uhzH9+1Om1fmqhbU7TGA2Q80JzUlOcfbnFtS3G76PPsCbW+5kZubN0mb7nZ7mPfDYv73yQcORidOK1a0CI3q12Xxsp+oVqUS7344noMJh3l3YF+nQ/O7mOgo4vbGpz3fuzeemKgoByNyTqmYKEpFR1Gndk0AbrnxBkZ/fOmM08tJOi6y5hI65bgoGY2xcFtrPdba48Cf1tpEAGvtCSD1fC+y1o621jaw1jbwd1Jx8FACiUeOAnDyZBJLV6ziskqxNGpwJXPnLwTgy9lzad70Wr/GJXnPoiUrGDN+Eu+/OYJChQo6Hc5FORq3g+jajXAV9Jbdy1zVnIStG89apsxV3u5dxStWx1WgICcP7WPnsm+pcdeDGN/4kmIVqhJcMCxT2929cgGVWtwJQNU2Xfh74UwAIi+vy3WD3uPbJ2/n5KFLZ6yKtZZBw16lcqVYut3b8bR5S39aReWK5SkVoy/I/Oas75GfVlG5YgWmTJ/Dj8t+5o2XniMoKOCGKGaods3qbNu+kx27dpOcksKcufNp3uw6p8NyRFTJSEqVik7rEbHsp5VcVrmis0E5RMdF1pwar5WTj7zAWGvPP9OYFcAN1trjxpgga22qb3o48IO1tl6GWziy+/wbyAUbN/9J/8Ej8aSmYlNTueWmZjz2QFd27NzNkwNf5HBiIjUur8qoFwcSGhrqz9AgyL+3Jew7YAg/rVrDoYTDREZE0Puh7hQvVowXX32Tg4cSKFa0CDWqVWHse07c/cWvhwV9Bww9Y190Y/RHE0lOSaZ4eDgAdWpfwbBBT/s1LoAxTcpm6XX1HhxM5Zs7kOp2c+CPtSx+sRd1ewxk/+8r2b5oNsUr1eD65z4gOKwwWMtPbw/wjsMwhgaPvEiFJrcChpMJ+5n31B2kHE1MW/f5KhZFy1bihuGTKBBeggN/rGXBc/eRmpJMq/fmElGlVlp3qaNxO5jX9/aLblPPBf7ttrhy7a907tmHalUqExTk/VDu+0hPml53Nf2HjKROrSv4v7tu82tMaVx+/nxKp2//wb6/lwTf30sPOtze1plg3EkZL5PDNm7+k/5DXvnne+TGZjz2wH1ccfWNlCkVQ+EwbyJ+0w3X89gD9/kvsOAC/tvWeSxcvJTho97Gk+rhznZteLhnV4ci8e93yLls+GMzg4aOJMXtpnzZMowYOoDwYk7c9MD5E8q8c1wAYSWd3yGZcLBniRw/iCPGHHK87RklFgWstWd9qhtjSgKlrbW/ZrgFPycWeZqfE4u8TYfFKVlNLAKRvxOLPM3BxCJPcSCxyLPyQGKRd+g75B+On0vmLZdIYnHogZxPLEp86HxiccEz3XMlFb7p+wHdVklERERERIAA/OVtEREREZE8LY+MichpSixERERERPworwy2zmn577YWIiIiIiKS41SxEBERERHxowAtWKhiISIiIiIi2aeKhYiIiIiIH2mMhYiIiIiIZJ/JhUdmNmtMcWPMVGPMRmPMBmPMNcaYCGPMPGPMZt+/JXzLGmPM28aYLcaYdcaYDH8YW4mFiIiIiEj+8BbwjbW2OlAH2AD0B+Zba6sC833PAVoBVX2PXsD7Ga1cXaFERERERPzIBPn/2r4xJhxoAtwPYK1NBpKNMe2AZr7FxgMLgH5AO2CCtdYCy33VjtLW2j3n24YqFiIiIiIilzhjTC9jzMp0j15nLFIJ2Ad8bIxZY4wZY4wpDMSkSxbigBjf/8sCO9K9fqdv2nmpYiEiIiIi4k+5MHjbWjsaGH2BRYKBekBva+0KY8xb/NPt6dQ6rDHGZjUGVSxERERERALfTmCntXaF7/lUvInGXmNMaQDfv/G++buA8uleX8437byUWIiIiIiI+JMxOf/IgLU2DthhjLncN6kF8DswE+jqm9YVmOH7/0zgPt/doa4GDl9ofAWoK5SIiIiIiF8Z49i1/d7ARGNMKPAX0A1voWGyMaYH8DfQ0bfsV0BrYAtw3LfsBSmxEBERERHJB6y1a4EG55jV4hzLWuDRi1m/EgsREREREX8K0F/ezv3EwhWa65uQS489Fp/xQvlEzx/+djqEPOP7NuUzXiifaP71XqdDyBOs+4TTIeQZJriA0yGIiFyQKhYiIiIiIv6kioWIiIiIiGSXCdDEQrebFRERERGRbFPFQkRERETEn5y73WyuCsxWiYiIiIiIX6liISIiIiLiRyYoMMdYKLEQEREREfEnDd4WERERERE5N1UsRERERET8SYO3RUREREREzk0VCxERERERP9IP5ImIiIiIiJyHKhYiIiIiIv4UoBULJRYiIiIiIv4UoImFukKJiIiIiEi2qWIhIiIiIuJHRrebFRERERERObeAr1gMGDKcBYuWEBlRgtlTP3U6HEfl930xYfIspsyah7XQ4bab6Nqxbdq8jz6bwav/Gcey2eMpUbyYg1HmvqSkZDr3epzklBQ8bg8tWzSlz4P3M/DF11i/4Q+shUoVyjFicD8KhxVyOtwsKd/hcUrf2gOs5djW9WwY2Z3U5KQsry+2cz9Kt+6OTfWw+e0nOPjztxSIKscVg8YRWiIGay27Z33Izmnv5GArnLVoyXJefu1NUlNT6dC+Lb26d3E6JL/56++d9H1+ZNrzHbvi6PPAvcRERfLu2En8uW0Hk8f+m9o1qjoYpTPy83Fxpuat76Jw4TCCgoJwuVz8b9JYp0NyjI6LLNAYi0vTHW1bM+Y/bzgdRp6Qn/fFpr/+ZsqseUz+8DWmj/s3C5as5O+dewDYs3c/S35eS5mYKIej9I/Q0BDGv/8GMyeNYfqkD1m87CfW/vo7A598hJmTxjDrszGULhXNxMlfOh1qloSWLEO5O3uzstdV/NStDgS5iG5+T6Zee83nf541LSy2BtHN72bF/bX55ZnWXP7kuxAUhPW42fyfZ1jRtTarHm5MudsfISy2Rk43xxEej4dhI19nzLuvM2faRGZ/8x1b/tzqdFh+Uzm2HNMnvMv0Ce8y7eO3KFSwADc2bUzVy2J5e8QgGtSt5XSIjsjvx8W5jB/9NjO+GJevkwodF1ljgkyOP/KCi04sjDETciOQ3NKwfl3CwwP7CnRm5ed98de2nfzrimoUKliA4GAXDa+sybyFywEY8c5HPPPwfZA3/iZznTEmrRLhdrtxu90YYyhSpDAA1lpOJiVd0ldTjCuYoAKFMC4XrgJhJO/fTdFq9bjyre9pMPon6rz2NaERpTK1rqjrbiP++y+wKcmcjNvG8V1/UqzGVSQfjOPo5jUAeE4c5djfGykQVTY3m+U369ZvILZ8OcqXK0toSAi3tmzB/AWLnQ7LEctW/kL5sqUpWzqayypWoHJsOadDcoyOCzkXHReS3gW7QhljZp45CbjBGFMcwFp7W24FJpKTqlauwL9HT+TQ4UQKFijAwmWrqFW9CvMXryCmZATVq1ZyOkS/8ng83NHlIbbv3EWnDu2pU8t7pX3A0FdYuPQnLqsUS/8nHnY4yqxJ3r+b7Z+/TuPJ20hNPsHBn+dxaM0P1Hv7B9YNvJ2Uw/uJvqEjlR94iY2v9MxwfQVKluXw7yvSnift20mBkqcnEAVL/T979x0eVbG4cfw7KUBCCS2E3kGagpQrKiLSUZoUQRSVYi6ioqI/aRawYMfeKCJWe2W+SwAAIABJREFUpAkolqso0qQKoogFEelJCIQACSG7md8fyU3MJRgkyc6yeT/Ps4+7Z8/ueWecDWd2Zs7WoGS9ZiT+Zb9zWUxsHBWjKmQ+joqqwJYftzpM5M4nXyznqk6Xu47hF9Qu/ocxDBs5GmNgQN9eDOjby3UiJ9QuzlKALt7ObY1FVeAnYDpgSe9YtASeKeBcIvmqTs1q3Hx9H4bdNYnwsGI0rFeLkydTef2t+cx49kHX8XwuODiYRe9NI/HoMW79vwf4dfsf1K9bi8ceHIPX6+Xhp17kk/98Td+e3VxH/cdCSpQmsk1Pvh1YB8+xBJpMmkONQfdSvFYTmj3zOQAmOJiT8QcAqHH9OCq06wdA0fKVaTV9IwBHflzNr8/dnuvxgsOK0+Shufz24mi8SUcLqFTiwsnUVL5auZbRI290HUX80PszXyGqQiTxhw4zZMSd1K5Zg1YtmrmOJeJUbh2LlsAdwATg/6y1m40xydbab/7uRcaYaCAa4PUXnyF66A35ElYkL/p170i/7h0BmPL6O5QvE8HSFWvpddNdAMTExdNn6N3MmfYkkeXKuIzqM6VKluCiFs1Y8e066tdNH7UJDg7mqs5XMP3tD87JjkWZlh1J3r+T1CMHAYhb8SGVut3I8Z1b2TiyzSn7//nOY/z5zmNA+hqL9cNbZHs+5eBeilXImv5SNLIqKQf3AulTrpo8NI+YL98jbsW5uSYlJ1EVIjkQE5v5OCYmlqjIwrEG6a9WfLuBRufVoXzZwvH3IDdqF9lFVUgve7myZejUvi1btv5UKDsWahdn6Ryebvx3/nYcxlqbZq19FhgCTDDGvMQZXEnKWjvVWtvSWttSnQrxF/GHEwDYdyCOL75ZQ+9u7Vn98Sy+mjeVr+ZNJSqyHAveeCbgOxWHDieQePQYACdOpLB63UZq1ajGn7vTT5attXy1fDW1a1RzGfOspcTsolSjiwgqmr6OpEzz9sStWERoRCSlGrcG0jsExWs2OqP3O7jqIyq0H4AJLUKxijUJr1qXxG3rAGgwZjpJf25j95znCqYwjpzfuAE7d+1h9959nExNZcnnS2nf7tROWaBbomlQ2ahdZElKTubY8aTM+6u+XU+9OrUdp3JD7eLsGGPy/eYPzuhys9baPUB/Y8xVQGLBRspfo8c+yLqNmzickEDbLr25fcQw+l/dI/cXBqDCXhejJjxJQuJRQoJDeGB0NKVKFncdyYnYg/GMnfgE3rQ0bFoaXTu2o12b1gy6+Q6OH0/CWst59eowaeydrqOelcRt64j7Zj6tpm3Aej0c276ZvYtfJ+H75dQf9RzBxSMwwSHsmfcCx3f+lOv7Hd/5E7Ffz6X1rB9J83r45bnbIS2NiPMvpVKXwRz7fUvm9Kkd0+4jfu2nBV3EAhcSEsIDY+5i+MjReNO89O3VvdCdNCUln2DVuk1MGnNb5rYvlq3mkSmvcSjhCCPunkiD+rWZ8dzDDlP6ltpFlvj4Q9w6ejyQvmate7dOtL20teNUbqhdyF8Za23BHiHpYAEfQM5F9nhs7jsVEqZohOsIfuOrHtVdR/Ab7T+NcR3BL9gTCa4j+A1TrLTrCH5EpxZZ/OObar8RXv6cqJCTDzXN90Zc5IHvnZc9MJeki4iIiIiITwX8L2+LiIiIiPiVAL3cbGCWSkREREREfEojFiIiIiIiPuQvV3HKb+pYiIiIiIj4UlBgdiw0FUpERERERPJMIxYiIiIiIj5ktHhbREREREQkZxqxEBERERHxJS3eFhERERGRPAvQjoWmQomIiIiISJ5pxEJERERExIcC9XcsNGIhIiIiIiJ5phELERERERFfCtDLzapjISIiIiLiS5oKJSIiIiIikjONWIiIiIiI+FCgLt5Wx0KcMMUruI4gfqj9pzGuI/iNJ1pVch3BL4xZv991BP/hTXWdwH8Eh7pO4D/ULsSPqGMhIiIiIuJLQYG5GiEwSyUiIiIiIj6lEQsREREREV/SGgsREREREcmzAP0di8AslYiIiIiI+JRGLEREREREfClAp0JpxEJERERERPJMIxYiIiIiIr4UoGss1LEQEREREfElTYUSERERERHJmUYsRERERER8KUCnQgVmqURERERExKc0YiEiIiIi4ksBusZCHQsREREREV/SVCgREREREZGcacRCRERERMSXNBXq3LP/QAz33v8w8fGHMQau6duLGwdd4zqWM8tXreHRp54jLS2N/r17ED10sOtIzoybOJlly1dRrmwZPp73jus4zqgesguEz0iL62+nab+hGGP4ft4bbHj7hRz3q9ikJYPfXcHi/7uOX/6zIE/HLBZRhl5Pv0epKjVI3PsnC+++lpTEBBpddS0XDfs/jDGcPH6Uzx++jbhftuTpWC4EQrs4W/sPxHLvg5OJP3QYYwzXXN2dG6/tx3OvzmDpN6sICjKUK1OGxyaOJSqyvOu4PlPYzy9O1y4A3p69gHfnfkhwcDCXX9qae+8Y4Tit+FJAT4UKDg5m7Ojb+WTBu3zw1lTe+2AB23//w3UsJ7xeLw89/gzTX3qGJfPf5ePPviy0dQHQp8eVTH95iusYzqkesgTCZ6R83cY07TeUtwZewht9WlDn8ispXb3OKfuZoCDajZ7MH6u/+EfvX61VW658dMYp21sPv5eda79i2pWN2Ln2K1oPvxeAI3t38t5N7Xnj6gtZ/dqjdJ346tkVzKFAaBd5ERwSzNi7RvLJ3Fl8MPMV3pu7kO07djJ88EA+mv0Gi96bQbvLLublabNcR/Wpwn5+cbp2sWbDJpYuX8ni92ewZM6bDBs8wHVU/2VM/t/8QEB3LCpElqdxw/MAKFG8OLVr1SAmLs5xKje2/LiNGtWqUq1qFYqEhnJVlw4sXbbCdSxnWrVoRkREKdcxnFM9ZAmEz0i52g3Yv2U9nhPJWK+X3RuWU79j71P2a3HdbfzyxYckHcr+9/BfQ0ZzwwffMmTBd7S59YEzPm7dK3rw48K3Afhx4dvUa98TgL2bvyUlMSH9/pa1lIyqcrZFcyYQ2kVeVChfjsYN6gNQong4tWvWICb2ICVKFM/cJzn5BMZPTmp8pbCfX5yuXbw/bxHRNw6iSJEiAJQrW8ZlTHHgH3UsjDFtjDGjjTGdCypQQdmzbz/bfvmNpk0au47iRExsHBWjKmQ+joqqUKj+CIrkJhA+Iwe3b6Vqi0spFlGWkGJh1L6sG6UqVsu2T4kKlanXoRebZr+WbXvNSzpSpkY93hpwMTP7tiCqUXOqtmhzRsctXi6K4wcPAHD84AGKl4s6ZZ+mfYawY8XnZ1kydwKhXeSXrH9HGwLw7MvTufyq/nz06RfcMWKo43TuFPbzi7+2i527drNh8w/0v/EWro++gy1bf3Ydz3+ZoPy/+YG/XWNhjFlnrf1Xxv2bgVuBD4EHjTHNrbWP+yBjnh1PSmLUPRMYf8+obN+yiIgEkvgdP7N2xtMMmPYpqcnHif35e2yaN9s+HcY+wzdTxoO12bbXuqQTtS7pyE3zNwBQJLw4ZWvUY8/GlQx+fxXBRYpSJLw4xSLKZu7zzZRx/LEqh+lU//Pe1f91ORf0GcI7g9vlX2HFp44nJTHq3gcZf/dtmf+O3nXrcO66dTivz3yXd+Z8yKh/D3Gc0vcK+/nF/7YLr8fLkSOJzHnzFX7Y+jN3jpvI0kXvF7oRrTMSoHWS2+Lt0L/cjwY6WWvjjDFPA2uAHDsWxpjojP15/cVniB56Q35kPSupqR5G3TOBHt0607lDO2c5XIuqEMmBmNjMxzExsURFRjpMJOJfAuUzsmXBTLYsmAlA2zse5mjM3mzPV2zcgp5Ppy/UDytTntqXdSXN4wFj+Hbak3w/d9op7/n2tZcC6Wsszu99I59MGJbt+ePxMRQvXzF9tKJ8RY4fyqrHyPrn03XS68wd0YMTRw7la1l9IVDaRV6kejyMuvdBenTtSOf2bU95vke3jkSPGlPoOhaF/fwip3YRFRVJp/ZtMcZwQZOGBJkgDiccoWyZ0o7Tiq/kNm4SZIwpY4wpBxhrbRyAtfY44Dndi6y1U621La21LV12Kqy1TJj0GLVr1WDI4IHOcviD8xs3YOeuPezeu4+Tqaks+Xwp7dud2TQHkcIgUD4j4WXTT3pLVqpG/Y69+WnJ+9mef71LfV7rXI/XOtfjl/8s4ItHbue3rxbzx6r/cEGfmwgNT//WtUSFypnvlZvtX39Mk97pV0pq0nsw27/+KDPD1c/PYcm4IRz+87f8KqJPBUq7OFvWWiY89CS1a1VnyPVZVz3auWtP5v2ly1ZRu2Z1F/GcKeznF6drFx0vb8PaDZsA+OPP3aR6UilTOsJVTP9WGKdCARHARsAA1hhTyVq73xhTImObX9u4eQuLlnxG/Xp16DXgRgBG3/ZvLr/sEsfJfC8kJIQHxtzF8JGj8aZ56durO/Xq1HYdy5nRYx9k3cZNHE5IoG2X3tw+Yhj9r+7hOpbPqR6yBMpnpPdzcwgrXZY0j4cvHhlFytEjNLsmGoDNc6ae9nU7V39JudoNGfzuSgBOJh3j47E3nrLAOydrpj9Jrynvc0GfISTu28Wiu68F4NIR9xEWUY5O978IQJrHw1sDWue1iD4VKO3ibG38/gcWffIf6tetTa9B6SNVo0fezLxFn/DHn7swQUFUqRTFpHGjHSf1rcJ+fnG6dtG315WMf+gJul9zE6GhoTw+cZymQRUyxv7PXNgzepEx4UCUtTb3a6slHfznBxARKeSeaFXJdQS/MGb9ftcR/Ic31XUC/xEcmvs+hYXaRXYlK50TPZm0GX3y/fw4aNgC52U/qx/Is9YmAYXngs0iIiIiIvnFT6Yu5bfALJWIiIiIiPjUWY1YiIiIiIjIWQrQtScasRARERERkTzTiIWIiIiIiC8F6BoLdSxERERERHxJU6FERERERERyphELERERERFfCtCpUIFZKhERERER8Sl1LEREREREfMmY/L+d8aFNsDFmkzHm44zHtYwxa40x240xHxhjimRsL5rxeHvG8zVze291LERERERECo87gG1/efwE8Ky1ti5wGBiWsX0YcDhj+7MZ+/0tdSxERERERHzJBOX/7UwOa0xV4CpgesZjA7QH5mXsMgvonXG/V8ZjMp7vkLH/aWnxtoiIiIiIL7m73OxzwL1AyYzH5YAEa60n4/EeoErG/SrAbgBrrccYcyRj/4One3ONWIiIiIiInOOMMdHGmA1/uUX/z/PdgVhr7caCyqARCxERERERXyqAy81aa6cCU/9ml0uBnsaYK4FiQCngeaC0MSYkY9SiKrA3Y/+9QDVgjzEmBIgA4v8ug0YsREREREQCnLV2nLW2qrW2JjAQ+Mpaex3wNdAvY7cbgUUZ9xdnPCbj+a+stfbvjqERCxERERERX3K3xiInY4DZxphHgE3AjIztM4C3jTHbgUOkd0b+lsml45Fndt/Ggj3AOcSUruE6gt+wSadd91PomCIlc9+pkLCeE64j+A1TLMJ1BL+wd2h11xH8RpU3drmO4D+s13UC/2GCXSfwL+Hl/eqM/XTSZg/L9/PjoIEznJddU6FERERERCTPNBVKRERERMSX/GsqVL7RiIWIiIiIiOSZRixERERERHypAC436w/UsRARERER8aUgTYUSERERERHJkUYsRERERER8SYu3RUREREREcqYRCxERERERXwrQxduBWSoREREREfEpjViIiIiIiPhSgK6xUMdCRERERMSXNBVKREREREQkZxqxEBERERHxJY1YiIiIiIiI5EwjFiIiIiIivhSgIxbqWIiIiIiI+JKuCuU74594nWVrNlGudCk+mvnkKc8vXbmB52fOJcgEERwcxPjbBtPi/AZ5OmZC4jFGP/QCew/EUaViJM8+OIqIkiX46IuVTJv9EdZC8fBiTLxzKA3q1sjTsVzYfyCGe+9/mPj4wxgD1/TtxY2DrnEdy6fenL2IeR99gTGGenVq8Nj4URQtWgSAR56dyoIlS/nuyw8cpyx4KSknuS76Dk6mpuL1eOnS4XJG/fsm7r7vUX7c9guhISGc37gBD40fTWiIX/6JyFeJR49x32PP89vvf2KM4dEJd7Jy7UbmLvqcsmUiALhrxI1cfkkrx0l9Z9zEySxbvopyZcvw8bx3XMc5ayY8gjL/foWQqo0AS8JrIzj527rM50t0v5OwNgPT9w0OJqRKA/bfXB17/PDZHzSkCGVunU6RWheSduwQh54fjDduF0XPb0+pax/GhIRiPakceXc8J7d+k8cS+t7yVWt49KnnSEtLo3/vHkQPHew6ks+Mm/g4y1asTv9czJ0FwHOvTGfpspUEBQVRrmxpHps0nqjI8o6T+lag/L2Q/GGstQV6ALtv4z8+wPrvtxEeVoyxj72aY8fiePIJwosVxRjDL7/v4s5Jz/PpW8+c0Xuv3fwTH362nMfHjsi2/anX3iOiVAmiB/Vk6nuLSTx6nHv+fS3f/fgrdWpUJqJkCZav3cxLb85nzqsP/9MiAWBKu+uQxMYdJO5gPI0bnsex48fpO2gYL095jLp1ajnJY5MO+vR4MXHxDLplLEvefYliRYty5/1P0rZ1C/pc1YEftv3G23M/5svla5x0LEyRkj49nrWWpOQTFA8PI9XjYdDwUUy4+zaOJCbS9pKLALj7vkdoeeEFDOrXy7fZPCd8ejyAMQ89Q8tmjenfsysnU1M5cSKFWR8sJDwsjGHX9fV5nv8yxSKcHXv9xs2Eh4cx5v6HnZ8o7B1a/axfW+aWqaT8vJqkr9+E4FBM0XBs0pEc9y3W/EpKXHkbBx+58ozeOziyOmVumcrBh7pm2168UzSh1ZuQMGMUYRf3o9i/enL4+RsIrdkU75FY0g7vJ6RqI8qPX8yBkXX/UXmqvLHrH+2f37xeL116D2Tmq88RFVWBftcNZ8pjE938O2K9Pj9k5ufigcmZHYtjx45TokRxAN56fx7bd+zkoQn3+DaYCfbt8f6HP/29ACC8/DkxFJD28T35fgIe1P1p52X/2wlexpiLjDGlMu6HGWMmGWM+MsY8YYwpsH/1WjVtSESpEqd9vnhYMUzGEFLSiROZ9wFmzP6IfiPuo+ewMbwwc94ZH3Pp6o307nIZAL27XMaXqzYA0LxJfSJKpmdp2qguBw4e+sfl8QcVIsvTuOF5AJQoXpzatWoQExfnOJVveb1eTqScxOPxknwihQrly+L1ennq5Te5Z+SNruP5jDGG4uFhAHg8HjweD8YYLr+0NcYYjDFc0LgBMbG+7fy5cPTYcTZs/pF+PboAUCQ0lFIlT/+3p7Bo1aIZERGlXMfIExNWiiIN26R3KgC8qaftVACEXdqfpNVzsx63GUjkI8uJfHwNpYe/eMbzoYu1vIqk5eknV8lrP6Ro43YApO78nrTD+wHw7PkJU6QYhBT55wVzaMuP26hRrSrVqlahSGgoV3XpwNJlK1zH8pmcPhf/7VQAJCdnPx8pLALh74Xkn9z+Ur4BJGXcfx6IAJ7I2DazAHPl6osV6+l2w92MGPcUj94bDcDK9VvYuecAc199mIXTHmPrr3+w/vttZ/R+8YeOUKFcGQAiy5Ym/tCp/wDN+2QZbf/VNP8K4cieffvZ9stvNG3S2HUUn4mKLMfQa6+mfZ/hXNbrJkoWD6fNRRfy7vxPaN/mX1QoX9Z1RJ/yer30GnQzl3TuwyUXtaRpk4aZz6V6PCz65Asuuzjwp/7s2XeAsqUjGPfIs1x9w23cN/k5kpLTR03enfcRPa8fyfhHnuVI4lHHSeWfCqlQk7TEg5S+5XUiH/uW0tGvYIqG57ivKRJGsaadSF67MP21lc8j7OJ+xD3YnrixrbFp3swpU7kJLlsZT/ze9AdpXmxyIkEly2Xbp9hFvTn5x2bwnDz7AjoQExtHxagKmY+joioUui+ocvLsS9O4vFtfPvr0C+64ZZjrOHKuMEH5f/MDuaUIstZ6Mu63tNbeaa1daa2dBNQu4Gx/q9Nlrfj0rWd46eHRvPBG+rdMqzb8wKoNP3D1zePpEz2BP3bt4889BwC45pb76T18HPc/NY2vV2+k9/Bx9B4+jhXrvj/lvdO/tc2+bc2mrcz/ZBl3R19b4GUrSMeTkhh1zwTG3zMq2zctge5I4jGWrljLl3OnsnzRTJJPpLDw06/47OtVXN+vu+t4PhccHMyi96bxzZI5bNn6M79u/yPzuUmPP0fLCy+g5YUXOEzoGx6vl59+3c61fa7kw7deIiysGNPemsO1fa7ii3kzWPjWS0SWL8sTL0x3HVX+qeAQQms14/gX04kbdzE25TgleuU8RaVYiytJ+WVN5tqKoudfQZFaFxL56EoiH19D0SbtCIlKn+5TdvRsIh9fQ7kxHxJauzmRj68h8vE1hF9+ZmsNQqo2JGLQIyRMvz1/yinO3XXbzXzz6Xx6dOvEO7MXuI4j5wpj8v/mB3JbmfmjMWaItXYm8L0xpqW1doMxpj6QeroXGWOigWiA154YT/T1ffIv8f9o1bQhu/fHcvhIItZaogf1YmDPDqfs9991EadbY1GubASx8YepUK4MsfGHMxdtAvzy+y7uf3oaUx8fQ5kI386Hz0+pqR5G3TOBHt0607lDO9dxfOrbDd9TtXJU5v/XTpe35sUZ75OScpLOA9LbQvKJFDpf82/+M+d1l1F9qlTJElzUohkrvl1H/bq1eGnaLA4lHOGl8aNdR/OJihXKExVZnqaN0y/+0OWKNkx7ey7ly5bJ3Kd/r67ccs9ERwnlbHnj9+I9tJfU7euB9GlJJXvm3LEIu7g/yavnZNuWtPwdEmc/eMq+h6akj1ycbo2F99A+QspV4eShvRAUjAkrRdrReACCylah3N2zOfzycLwxf5zy3v4uqkIkB2JiMx/HxMQSFRnpMJF/6dGtE9Gj7mXULUNdRxFxJrcRi+HA5caY34FGwLfGmB3AtIzncmStnWqtbWmtbVkQnYo/9x7gv4vOt/76BydTPZQuVZI2rS5gwafLOJ4xlSEm7hDxh08/p/av2l/SnIWfp88VXfj5Cjpc0gKAfTEHuf2BZ3li3EhqVauU72XxFWstEyY9Ru1aNRgy+MyG9ANJpajyfP/jLySfSMFay7cbtnDTgF6s/GgWX82fxlfzpxFWrGih6FQcOpxA4tFjAJw4kcLqdRupXbM6cxcuYeW365nyyH0EBfnHkGpBiyxXlkpRkez4cw8A327YTJ2a1Yn9y1qqL5etpl7tc+9KcIVd2pEYvPF7CKlUD4CiTa4gde+pU2NNWCmKNmrDiQ0fZ25L+XEZYRddTVCp9JNmU7wMweWrndFxT2z8hPC21wMQdtHVpGRc+cmER1B+zHyOvPcAJ39dk6eyuXJ+4wbs3LWH3Xv3cTI1lSWfL6V9uzauYzm1c9fuzPtLv1lJ7Zpnf7EBKWQCdCrU345YWGuPADdlLOCulbH/HmttTEGGGv3wi6zfvI3DR45yef/buP2mvni86VeAGNizI/9Zvo5Fn68gJCSEokVDefaB2zHG0KbVBez4cy8Db03/lik8rChPjb+VcmVyX2d+87U9uWvSC8z/5GsqR5Xn2QfvAOCVtxaQkHiUh55LX1ISHBzE/NcfLaCSF5yNm7ewaMln1K9Xh14D0hcqj77t31x+2SWOk/lG08bn0fmKS+gz5C5CgoNpWL82A3p1cR3LidiD8Yyd+ATetDRsWhpdO7bjissuplHrjlSuGMWAobcB0OmKy7jt5hscpy14940ewf9NfJLUVA/VqlRk8oS7ePTZ19j26w6MMVSpFMWkMYVr2srosQ+ybuMmDick0LZLb24fMYz+V/dwHesfOzLzbsrcNhMTEoondieHX/s34R3TvxNL+jJ9elvYv3pyYstSbEpS5us8e38mcc4kyo3/CGMM1ush4Y078R7cneNx/ur4129S9tYZRD33A2nHDnPohfTPUIkuIwiOqkPJvuMo2XccAPGTe5CWeO6sUQgJCeGBMXcxfORovGle+vbqTr06TmdF+9TocZMyPhdHaNu1L7ePGMLylWv448/dGX8rKjJpwt2uY/pcoPy9kPzhl5ebDVQuLzfrb3x9uVl/5uvLzfozF5eb9VcuLzfrT/JyudlA4/pys37FweVm/Zbjy836nXPlcrOfTcj/y812fdR52f1j3ERERERERM5pgf+zuiIiIiIi/sRP1kTkN3UsRERERER8yU8uD5vfArO7JCIiIiIiPqURCxERERERXwrQqVCBWSoREREREfEpjViIiIiIiPhSgI5YqGMhIiIiIuJLQYHZsQjMUomIiIiIiE9pxEJERERExJd0uVkREREREZGcacRCRERERMSXtHhbRERERETyLEA7FoFZKhERERER8SmNWIiIiIiI+JIWb4uIiIiIiORMIxYiIiIiIr4UoGssCrxjYUrXKOhDyDnIhJd3HUH8kAkp6jqC/0jzuE7gF6q8sct1BL8xuWUl1xH8xvgN+11H8B+eFNcJ5GwEaMciMEslIiIiIiI+palQIiIiIiK+pBELERERERGRnGnEQkRERETEl3S5WRERERERkZxpxEJERERExJcCdI2FOhYiIiIiIr4UoB2LwCyViIiIiIj4lEYsRERERER8SYu3RUREREREcqYRCxERERERXwrQNRbqWIiIiIiI+FKAdiwCs1QiIiIiIuJTGrEQEREREfEljViIiIiIiIjkTCMWIiIiIiK+FKTLzZ6Tlq9aQ5feA+nU8xqmvvG26zhOqS6yqC6yqC6yFOa6GDfpcS7u2Ivu19yUbfvbs+fTtc9grup/I08+/6qbcI6d6+2i1fW3c/OiTdy8eDOtBo865fnqrdoyeu1Bhi3YwLAFG2hzy4Q8HzM4tAi9n3mXEZ9t48bZq4ioXAOAmhd3YMjctQxfuIkhc9dS46J2eT6WC+MmTubi9lfRvd/1rqM4kZJykn433kLPQcO56pohvPD6mwDcfd+jdOl7A90HDGXcQ0+S6vF5VYLMAAAgAElEQVS4DerPTFD+3/yAf6QoIF6vl4cef4bpLz3Dkvnv8vFnX7L99z9cx3JCdZFFdZFFdZGlsNdFnx7dmP7iU9m2rVn/HUu/WcXi2TNYMncWwwYPdJTOnXO9XUTWbUyz/kOZOeASpl/dgrrtrqRM9Tqn7Ld740pm9GnJjD4tWfnqo2f8/hGVa3Ddm1+esr1p36GcSEzgta4NWT/rea64ezIAyQnxzB3Zm+m9L+TjcUPp+fibZ102l/r0uJLpL09xHcOZIkVCmfXqFBa/N52F701jxbfr2PzDT/Ts1oHP5s3io9kzSElJYe7CJa6jio8FdMdiy4/bqFGtKtWqVqFIaChXdenA0mUrXMdyQnWRRXWRRXWRpbDXRavmTYmIKJlt2/vzFhF90yCKFCkCQLmyZVxEc+pcbxfl6jRg75b1eE4kY71edq1fznkde5/x6xv3GMRNs1czbMEGuk18BRN0ZqcN9dv34IeF6aM72/4zn5qt2wMQs20zx+L2AxC3fSshxcIIDi3yD0vlXqsWzYiIKOU6hjPGGIqHhwHg8XjweDwYY7j80tYYYzDGcEHjBsTEHnSc1I8VxhELY8woY0w1X4XJbzGxcVSMqpD5OCqqAjFxcQ4TuaO6yKK6yKK6yKK6ONXOXXvYsGkL/W8YwfU3j2LL1m2uI/ncud4u4n7bSrUWlxIWUZaQYmHUaduNUpVO/We9SrPWDFuwkQGvf0T5uo0AKFe7AY269uet69syo09L0rxeGncfdEbHLRlVmcQDuwGwXi8pR48QVrpctn0adO7DgZ824U09mcdSigter5deg27mks59uOSiljRt0jDzuVSPh0WffMFlF7dymFBcyG3x9sPAWGPM78D7wFxr7bnzF1VERM6a1+vlSGIic2a9yg9bf+bOsRNZung2xgTmosNAFL/jZ9ZMf5qB0z8lNfk4sT9/T5rXm22fAz9t4uWOdUhNOk6dtl3p9+I8XuvWiJqt21OxcXOGzFkDQEjRYiQdigWg7wtzKV21FsGhoZSqVJ1hCzYAsP7tF9ny4axcc5Wv24grRk/m/ZuvzOcSi68EBwez6L1pJB49xq3/9wC/bv+D+nVrATDp8edoeeEFtLzwAscp/ZifjDDkt9w6FjuAFkBHYAAwyRizkfROxgJr7dGcXmSMiQaiAV5/8Rmih96Qf4n/gagKkRyIic18HBMTS1RkpJMsrqkusqgusqgusqguThVVIZJOV7RNn9bQpCFBJojDCUcoW6a062g+Ewjt4vsFM/l+wUwALr/zYY4e2Jvt+ZPHs/4p/335Z3S5/0XCSpfDGMMPi95m2bP3nfKe80f1B9LXWHSfPIN3b+qY7fmjMfsoVbEaR2P2YoKDKVoyguSEeABKRlWh7wtz+WjcUBJ278jXsorvlSpZgotaNGPFt+uoX7cWL02bxaGEI7w0frTraOJAbt0la61Ns9b+x1o7DKgMvAJ0Jb3TcboXTbXWtrTWtnTVqQA4v3EDdu7aw+69+ziZmsqSz5fSvl0bZ3lcUl1kUV1kUV1kUV2cqmO7NqzdsAmAP/7cTaonlTKlIxyn8q1AaBfhZdM7QqUqVaNBx95sXfJ+tueLl4/KvF/p/FaYoCCSE+LZueYrGnTuk/n6YhFlKFW5+hkd87evP+b83oMBaNi5L3+u/RqAoiUjuObVxSybMoE9m1bnuWzixqHDCSQePQbAiRMprF63kdo1qzN34RJWfrueKY/cR9AZrscptIzJ/5sfyG3EIltKa20qsBhYbIwJL7BU+SQkJIQHxtzF8JGj8aZ56durO/Xq1HYdywnVRRbVRRbVRZbCXhejx09i3YbNHE44Qttu/bj930Po2+tKxk96gu7X3ERoSAiPTxxf6KZBBUK76Pv8HMJKl8Wb6uHzR0aRcvQIFw6IBmDTB1Np0LkvzQdGk+bx4klJZuHd6ZdQPfj7Nr55/kGunf4pxgTh9aTy+cOjSNy3K9djbp7/Bj2feJMRn23jRMJhFt5zHQAtB42kTPU6tBl5H21Gpo+EvD+8G0mHzq1Z1qPHPsi6jZs4nJBA2y69uX3EMPpf3cN1LJ+JPRjP2IlP4E1Lw6al0bVjO6647GIate5I5YpRDBh6GwCdrriM22529wWzfwvMv6XGWnv6J42pb639NU9HSDp4+gOIiEjO0nT9dwCC9Duu/zW5ZSXXEfzG+A37XUfwH54U1wn8S6kq58QZe9qWd/P9/Djoguucl/1v/2LnuVMhIiIiIiLZBeji7cAslYiIiIiI+JTGmEVEREREfClA16upYyEiIiIi4lOBOWkoMEslIiIiIiI+pRELERERERFfCtCpUBqxEBERERGRPNOIhYiIiIiILwXoiIU6FiIiIiIiPhWYk4YCs1QiIiIiIuJTGrEQEREREfGlAJ0KpRELERERERHJM3UsRERERER8yZj8v+V6SFPNGPO1MeYnY8xWY8wdGdvLGmO+MMb8lvHfMhnbjTHmBWPMdmPMFmNM89yOoY6FiIiIiEjg8wB3W2sbAa2BW40xjYCxwFJrbT1gacZjgG5AvYxbNPBqbgdQx0JERERExKeCCuD296y1+62132XcPwpsA6oAvYBZGbvNAnpn3O8FvGXTrQFKG2Mq5VYqERERERHxlQKYCmWMiTbGbPjLLfr0hzc1gQuBtUCUtXZ/xlMHgKiM+1WA3X952Z6Mbaelq0KJiIiIiJzjrLVTgam57WeMKQHMB+601iaav6zPsNZaY4w92ww+6FicdbYAFJiXFjs7aheSE31GMgXpex8ArNd1Ar8xfsM+1xH8xiMt/nY2RqFy38b9ue8k/se4mTRkjAklvVPxrrV2QcbmGGNMJWvt/oypTrEZ2/cC1f7y8qoZ205LU6FERERERAKcSR+amAFss9ZO+ctTi4EbM+7fCCz6y/YbMq4O1Ro48pcpUznSV2IiIiIiIj7lZIT+UmAw8IMxZnPGtvHA48AcY8ww4E/gmoznPgGuBLYDScCQ3A6gjoWIiIiIiC85+OVta+1KTt+j6ZDD/ha49Z8cQ1OhREREREQkzzRiISIiIiLiS44Wbxe0wCyViIiIiIj4lEYsRERERER8yDhYY+EL6liIiIiIiPhUYE4aCsxSiYiIiIiIT2nEQkRERETElwJ0KpRGLEREREREJM80YiEiIiIi4ksasRAREREREcmZRixERERERHwqML/bV8dCRERERMSXNBVKREREREQkZxqxEBERERHxpQAdsQj4jkXi0aPcN+kJfv19B8YYJj84jgubNnEdy4nlq9bw6FPPkZaWRv/ePYgeOth1JGfULrLzer30vW44URUief2FJ13HcWL/gRjuvf9h4uMPYwxc07cXNw66xnUsJ1JSUrhu2K2cPJmK1+uhS8crGHXLcNexfGbcxMdZtmI15cqW4eO5swB47pXpLF22kqCgIMqVLc1jk8YTFVnecVLf2rFzF3eNeSDz8e69+xh1y3Buuu7c+Zy0Gnw7F/YfijGGTXPfYN1bL5yyT41Wbek0bgrBoSEkHY7n7Rs65OmYwaFF6PnETCo1ak5ywiEWjB7EkX1/UuuSDrQfPZng0CJ4U0+y9Kkx7Fy7LE/HcmHcxMksW74q/fMy7x3XccQxY60t2CMkxRXwAf7emPsfoeWFTenfpwcnU1M5ceIEpUqWdJTGXe/U6/XSpfdAZr76HFFRFeh33XCmPDaRunVqOUrktFn4Wbtwb+bbs/nxp585djzJccfC3WckNu4gcQfjadzwPI4dP07fQcN4ecpjDj8j7lhrSUpOpnh4OKmpHgYNvYUJ/3cHzS5w0Pm2Xp8fcv3GzYSHhzHmgcmZHYtjx45TokRxAN56fx7bd+zkoQn3+DaY8Z/Zy16vl7ZdrmbOW1OpUrmiz4//SIvK//g1kfUac/Uz7/DGNZfgTT3JoGlL+GTirRze9XvmPkVLRnDTe8t5P7o7ift3E142kqRDcWf0/hGVa9DzsRm8fWPHbNtbXDuCCvXP59NJt9Loyms4r2MvPhx9HVENm3H8YAzH4vYTWa8x105bwgvtav7jct23cf8/fk1+yvy83P+wf3QswsufE0MB9s+V+X4iZGq0cV72v/0rZYwpYoy5wRjTMePxIGPMS8aYW40xob6JePaOHj3G+u++p9/V3QEoEhpaaE8et/y4jRrVqlKtahWKhIZyVZcOLF22wnUsJ9QusjsQE8uyld/S7+oerqM4VSGyPI0bngdAieLFqV2rBjFxZ3ZCEWiMMRQPDwfA4/Hg8XgwATpsn5NWLZoREVEq27b/dioAkpNPFKr6yMm36zZSrWoVJ52Ks1W+dgP2bVmP50Qy1uvlz/XLadCpd7Z9mnS/ll++XEji/t0A2ToVTXoMYsgHqxm+YANXTnwFE3RmHb367XuwZdHbAGz7fD61WrcHIGbbZo7FpXcK4n7bSmjRMIJDi+S5nL6W0+dFzoAx+X/zA7l9KmYCVwF3GGPeBvoDa4FWwPQCzpZne/btp2yZ0ox7cDK9Bw5hwqTHSUpOdh3LiZjYOCpGVch8HBVVodCeNKldZDf5qRf4vztuISjIP/4o+YM9+/az7ZffaNqksesozni9XnoNuJFLOnTnktataHp+4a2L/3r2pWlc3q0vH336BXfcMsx1HKeWfP4l3bt2zH1HPxL721aqtbiUsNJlCSkWRt223ShVsVq2fcrWrEexUmUYPOtLhs1by/m9rgegXO0GNOrWn1nXtWV6n5akpXlp0mPQGR23ZFTlzI6K9XpJOXqEsNLlsu3ToHMfDmzbhDf1ZD6UVMSd3DoW51trBwBXA52Bftbat4EhwIUFHS6vPB4vP/38K9f2783C2TMJCyvG1Df8YJhOnFK7yPL18lWULVuaJo0auI7iN44nJTHqngmMv2dUtm+pC5vg4GAWfTCLbz7/kC0//sSv23e4juTcXbfdzDefzqdHt068M3uB6zjOnExN5atvVtG10xWuo/wj8Tt+5tvpTzNo+qcMmraEmJ+/Jy0t+1S7oOAQKjZuzuwRPXlv+JVcdst4ytasR63W7anUuDlD56xh+IIN1Gp9BWWqpk+T7PfiXIYv2MDA1xdTqXELhi/YwPAFG2h69Y1nlKt83UZ0uHsynzw4Mt/LLH7MBOX/zQ/ktng7yBhTBCgOhAMRwCGgKHDaqVDGmGggGuD1F58meugN+ZP2H6oYFUnFCpGZ37R17XgFU2cWzhPIqAqRHIiJzXwcExNLVGSkw0TuqF1k+W7zD3z1zSqWr1xDysmTHDt+nHsmPMTTjz6Q+4sDUGqqh1H3TKBHt8507tDOdRy/UKpkSS5q2ZwVq9dQv25t13H8Qo9unYgedS+jbhnqOooTy1euoXGD+pQvV9Z1lH9s8/yZbJ4/E4Ar7nyYxJi92Z4/emAPyQnxpCYnkZqcxK4NK4k67wIwhi0L3+brZ+875T3n3d4fOP0ai6Mx+yhVqRpHY/ZigoMpWjKC5IR4AEpGVaH/i3NZNHYoh3er8y7nvty6NzOAn4HNwARgrjFmGrAemH26F1lrp1prW1prW7rqVABEli9HxYoV2LFzFwDfrttAndo1neVx6fzGDdi5aw+79+7jZGoqSz5fSvt2bVzHckLtIsvdo0aw/PMP+eqTeUx5fCKtW7UotJ0Kay0TJj1G7Vo1GDJ4oOs4Th06dJjEo0cBOHEihdVr11O7Zg3HqdzauWt35v2l36ykds3qDtO4teSzL7nqHJsG9V/hZdO/UCtVqRrnderNjx+/n+35X776iGrNL8UEBxNSLIzKF7Ti4I6f2bnmKxp26ZP5+mIRZYiofGZt4NevP+aCXulXYWzYpS8713wNpC8UH/jaYr6aMoE9m1bnVxHlnGEK4Obe345YWGufNcZ8kHF/nzHmLaAjMM1au84XAfPq/jF3cc/4SaR6PFSrUpnHJo1zHcmJkJAQHhhzF8NHjsab5qVvr+7Uq1N4v31Uu5D/tXHzFhYt+Yz69erQa0D6FIbRt/2byy+7xHEy34s9GM/YBx7Bm5aGTUuja6f2XNH2UtexfGb0uEms27iJwwlHaNu1L7ePGMLylWv448/dGGOoUqkikybc7TqmE0nJyaxeu56H7vs/11HOSr/n5xBWuixpHg+fPTyKlKNHaD4gGoDvPphK/I6f+X3l50Qv/A5r09g8byZxv20FYNnzDzJo+qeYoCDSPKl89vAojuzblesxN897g15PvMnIz7aRfOQwH959HQCtrhtJmep1uOyW+7jslvSRkPeGdzvjq1D5i9FjH8z4vCTQtktvbh8xjP6F/GIgZ8RPFlvnt4C/3Kx/CcxGdHbULCQn+ozI/3BwuVm/5SdzqP3B2VxuNlC5vtys3zlXLje7Z23+X2626kXOyx7wP5AnIiIiIuJXAvSLgsAslYiIiIiI+JRGLEREREREfClA11hoxEJERERERPJMIxYiIiIiIj4VmCMW6liIiIiIiPiSFm+LiIiIiIjkTCMWIiIiIiI+FZhToTRiISIiIiIieaYRCxERERERXwrQy82qYyEiIiIi4lOB2bHQVCgREREREckzjViIiIiIiPhSgE6F0oiFiIiIiIjkmToWIiIiIiKSZ5oKJSIiIiLiS5oKJSIiIiIikjNjrS3YIyQdLOADiIgEIJvmOoF/MPr+K5MnxXUC/xFS1HUCv/Foy0quI/iVCT+lnhNDATZ2a76fH5sKjZ2XXX+xRUREREQkz7TGQkRERETEl7TGQkREREREJGcasRARERER8anAHLFQx0JERERExJc0FUpERERERCRnGrEQEREREfEpjViIiIiIiIjkSCMWIiIiIiK+FKBrLNSxEBERERHxqcDsWGgqlIiIiIiI5JlGLEREREREfClAp0JpxEJERERERPJMIxYiIiIiIj6lEQsREREREZEcqWMhIiIiIiJ5FvAdi+Wr1tCl90A69byGqW+87TqOU6qLLKqLdOMmTubi9lfRvd/1rqP4hcLcLsZNfIyLO/Sge/8bMrd9+sXXXNVvMA1atOWHn352mM6dwv4ZSUk5Sb8bb6HnoOFcdc0QXnj9TQDemfMhna6+nvNatedQwhG3IR0IhHbR6vrbuXnRJqIXb6bV4FE57lO9VVuGL9hA9OLNXD9raZ6PGRxahKufeZdbPtvGTbNXEVG5BgC1Lu7A0LlruXnhJobOXUuNi9rl+Vj+zhiT7zd/ENAdC6/Xy0OPP8P0l55hyfx3+fizL9n++x+uYzmhusiiusjSp8eVTH95iusYfqGwt4s+Pbox/aWns22rX6cWLz79KK2aN3WUyr3C/hkpUiSUWa9OYfF701n43jRWfLuOzT/8RPOmTZj58tNUqRTlOqIT53q7iKzbmGb9hzJzwCVMu7oF9dpdSZnqdbLtU7RkBF0feJE5t17N1J7NWHDXwDN+/4jKNbj+zS9P2d6s71BOJCbwateGrJv1PO3vngxAUkI8c0b2ZlrvC/lo3FB6Pf5mnson7gR0x2LLj9uoUa0q1apWoUhoKFd16cDSZStcx3JCdZFFdZGlVYtmRESUch3DLxT2dpFTW6hTuya1a1Z3lMg/FPbPiDGG4uFhAHg8HjweD8YYGp1Xj6qVKzpO58653i7K1WnAvi3r8ZxIxnq97Fq/nPM69s62T5OrruWXLxaSuH83AEmH4rKe6zGIIbNXM3zBBrpNfAUTdGank/Xa92DLwvTR4G3/mU/N1u0BiNm2mWNx+wGI276VkGJhBIcWyXM5/ZspgJt7ubYEY0xtY8w9xpjnjTFTjDEjjDHnxKcpJjaOilEVMh9HRVUgJi7ub14RuFQXWVQXkhO1C5Gceb1eeg26mUs69+GSi1rStElD15Ekj+J+20q1FpcSFlGWkGJh1GnbjVKVqmXbp2zNehQrVYbr3/ySoXPXcn7P9Glf5Wo3oFHX/sy6vi3T+7TEer006T7ojI5bMqoyiQfSOyrW6yXl6BHCSpfLtk+Dzn048NMmvKkn86Gk4mt/e7lZY8wooDuwHGgFbAKqAWuMMSOttcsKPKGIiIg4ExwczKL3ppF49Bi3/t8D/Lr9D+rXreU6luRB/I6f+Xb601w7/VNSk48T8/P3WK832z5BwSFUatycd4d2JqRoGDe9v4K936+lVuv2VGzcnKFz1gAQUrQYxw/FAtDvhbmUrlqLoNBQIipVZ/iCDQCse/tFtnw4K9dc5es2ov3oybx385X5XGI/5CdrIvJbbr9jcTPQzFrrNcZMAT6x1rYzxrwOLAIuzOlFxphoIBrg9RefIXroDTntVuCiKkRyICY283FMTCxRkZFOsrimusiiupCcqF2I/L1SJUtwUYtmrPh2nToWAeD7BTP5fsFMANrd+TBHD+zN9nxizB6Sj8STmpxEanISuzaspEKDC8AYtix6m2XP3nfKe84b1R9IX2PRY/IM3rmpY7bnj8bso1TFahyN2YsJDqZoyQiSE+IBKBlVhX4vzGXxuKEk7N5REEX2M4HZsTiTSXH/7XwUBUoAWGt3AaGne4G1dqq1tqW1tqWrTgXA+Y0bsHPXHnbv3cfJ1FSWfL6U9u3aOMvjkuoii+pCcqJ2IXKqQ4cTSDx6DIATJ1JYvW5joV93EyjCy6Z/cVKqUjXO69ibH5e8n+35X7/6iKrNL8UEBxNSLIzKF7Qi/vef2bnmKxp27pP5+mIRZShV+czaxG9ff8wFvQcD0LBzX3au/RpIXyg+4NXFfD1lAns2rc6vIooDuY1YTAfWG2PWApcBTwAYYyKBQwWcLc9CQkJ4YMxdDB85Gm+al769ulOvTm3XsZxQXWRRXWQZPfZB1m3cxOGEBNp26c3tI4bR/+oermM5UdjbxehxEzPawhHadu3D7SOGUrpUKR5+8jkOHU7g36PupWH9usx45dy9Es7ZKOyfkdiD8Yyd+ATetDRsWhpdO7bjissu5q3ZC5j+9mwOxh+i57XDufzSi3j0vntcx/WZQGgXfZ+fQ1jpsqSlevj8kVGkHD1C8wHRAHz3wVTid/zMjpWfc/PC77BpaWyeN5O47VsBWPb8gwya/imYINI8qXz28CgS9+3K9Zib579Bryfe5JbPtnEi4TAf3nMdAC0HjaRM9TpcNvI+LhuZPhLy3vBu2RaMB5wAnQplrLV/v4MxjYGGwI/W2n9+IfOkg39/ABEROZVNc53AP5iAvnjhP+NJcZ3Af4QUdZ3AbzzaspLrCH5lwk+p58YZ+5Fd+X9+HFHdedlzG7HAWrsV2OqDLCIiIiIihYDzPkCByLVjISIiIiIi+ShAp0JpjFlERERERPJMIxYiIiIiIj6lEQsREREREZEcacRCRERERMSXAnSNhToWIiIiIiI+FZgdC02FEhERERGRPNOIhYiIiIiILwXmgIVGLEREREREJO80YiEiIiIi4lOBOWShEQsREREREckzjViIiIiIiPiSLjcrIiIiIiJ5F5gdC02FEhEREREpBIwxXY0xvxhjthtjxub3+2vEQkRERETElxxMhTLGBAMvA52APcB6Y8xia+1P+XUMjViIiIiIiAS+fwHbrbU7rLUngdlAr/w8gDoWIiIiIiI+ZQrglqsqwO6/PN6TsS3fFPxUqPDyfrE6xRgTba2d6jqHP1BdZFFdZFFdZFFdpFM9ZFFdZFFdZPGHupjwU6rLw2fyh7o4pxTA+bExJhqI/sumqb7+f1KYRiyic9+l0FBdZFFdZFFdZFFdpFM9ZFFdZFFdZFFdZFFdOGatnWqtbfmX2/92KvYC1f7yuGrGtnxTmDoWIiIiIiKF1XqgnjGmljGmCDAQWJyfB9BVoUREREREApy11mOMuQ34HAgG3rDWbs3PYxSmjoXm/WVRXWRRXWRRXWRRXaRTPWRRXWRRXWRRXWRRXZwDrLWfAJ8U1Psba21BvbeIiIiIiBQSWmMhIiIiIiJ5FvAdi4L+6fJziTHmDWNMrDHmR9dZXDLGVDPGfG2M+ckYs9UYc4frTK4YY4oZY9YZY77PqItJrjO5ZowJNsZsMsZ87DqLS8aYncaYH4wxm40xG1zncckYU9oYM88Y87MxZpsx5mLXmVwwxpyX0R7+e0s0xtzpOpcrxpi7Mv5u/miMed8YU8x1JleMMXdk1MPWwtwmJMCnQmX8dPmv/OWny4Fr8/Ony88lxpi2wDHgLWttE9d5XDHGVAIqWWu/M8aUBDYCvQtjuzDGGKC4tfaYMSYUWAncYa1d4ziaM8aY0UBLoJS1trvrPK4YY3YCLa21B11ncc0YMwtYYa2dnnEllXBrbYLrXC5l/Pu6F7jIWvun6zy+ZoypQvrfy0bW2mRjzBzgE2vtm26T+Z4xpgnpv+D8L+Ak8Bkwwlq73WkwcSLQRywK/KfLzyXW2uXAIdc5XLPW7rfWfpdx/yiwjXz+5clzhU13LONhaMYtcL9tyIUxpipwFTDddRbxD8aYCKAtMAPAWnuysHcqMnQAfi+MnYq/CAHCjDEhQDiwz3EeVxoCa621SdZaD/AN0MdxJnEk0DsWBf7T5XJuM8bUBC4E1rpN4k7G1J/NQCzwhbW20NYF8BxwL5DmOogfsMB/jDEbM37NtbCqBcQBMzOmyE03xhR3HcoPDATedx3CFWvtXuBpYBewHzhirf2P21TO/AhcZowpZ4wJB64k+4+wSSES6B0LkdMyxpQA5gN3WmsTXedxxVrrtdY2I/0XOP+VMaxd6BhjugOx1tqNrrP4iTbW2uZAN+DWjKmUhVEI0Bx41Vp7IXAcKOzr9YoAPYG5rrO4YowpQ/oMiFpAZaC4MeZ6t6ncsP/f3t3z2BRFYRz/L9GgkXiLRETnA2jEJCIGodErFCqF+AA0ao1vMIXEDJF5aUVBrRkSxahImMJL4QMMeRRnF6I9mdnmnv+vuTe3eqp7z9p3rbWTDeAh8JKhDeod8LtrKHUz64XFtl9drt2pzROsAItJVnvn+R+09o7XwNXeWTqZA6632YJnwMWqetI3Uj/tRJYk34E1htbSKdoENsfIhXEAAAFdSURBVP/6J2+ZodCYsmvAepJvvYN0dAn4lORHki1gFTjXOVM3SRaSnElyHvjJMN+qCZr1wmLbry7X7tMGlheAjSSPeufpqaqOVNXB9n4fw6KDD31T9ZHkXpITSU4xfFe8SjLJE8iqOtAWG9Dafq4wtDtMTpKvwJeqOt0+mgcmt+jhHzeYcBtU8xk4W1X722/KPMO83iRV1dH2epJhvmKpbyL1MtM3b+/E1eW7SVU9BS4Ah6tqE3iQZKFvqi7mgJvA+zZbAHC/3UY5NceBx23Dyx7geZJJr1kVAMeAteF5ib3AUpIXfSN1dRdYbAdUH4FbnfN00wrNy8Dt3ll6SvKmqpaBdeAX8JZp3zy9UlWHgC3gjgsOpmum181KkiRJ2hmz3golSZIkaQdYWEiSJEkazcJCkiRJ0mgWFpIkSZJGs7CQJEmSNJqFhSRJkqTRLCwkSZIkjWZhIUmSJGm0P6rySWLVK0olAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6e5ysOElNZb"
      },
      "source": [
        "# **Test 1** *(Revised: learn_rate=0.01; reg_coeff=2e-06; drop_prob=0.04)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeshw_D2bzhJ"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 200, 100, num_labels]; learn_rate = 0.01; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMFvqhT8bzka",
        "outputId": "927ee332-6b3b-4f8d-850a-0c2e1505b50e"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.58938 and 1.4000121\n",
            "Val acc and loss are 0.5847 and 1.3804358\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.57786 and 1.0738899\n",
            "Val acc and loss are 0.5796 and 1.0760227\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.71138 and 0.83320487\n",
            "Val acc and loss are 0.7043 and 0.8421507\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.7087 and 0.7861169\n",
            "Val acc and loss are 0.7011 and 0.79436135\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.71266 and 0.77089685\n",
            "Val acc and loss are 0.7064 and 0.77811587\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.742 and 0.70405227\n",
            "Val acc and loss are 0.7399 and 0.70540595\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.75142 and 0.6763807\n",
            "Val acc and loss are 0.7511 and 0.6759165\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.76232 and 0.65385807\n",
            "Val acc and loss are 0.7607 and 0.65499324\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.77696 and 0.6301841\n",
            "Val acc and loss are 0.7733 and 0.63526994\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.7832 and 0.60698354\n",
            "Val acc and loss are 0.7755 and 0.61602646\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.78238 and 0.5870734\n",
            "Val acc and loss are 0.7749 and 0.59801143\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.78554 and 0.57521915\n",
            "Val acc and loss are 0.7786 and 0.5867308\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.79254 and 0.56464833\n",
            "Val acc and loss are 0.7833 and 0.5766063\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.79994 and 0.54754543\n",
            "Val acc and loss are 0.7912 and 0.56009275\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.80998 and 0.5276628\n",
            "Val acc and loss are 0.8007 and 0.5411022\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.81876 and 0.51417977\n",
            "Val acc and loss are 0.8088 and 0.5285668\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.82188 and 0.50810206\n",
            "Val acc and loss are 0.8151 and 0.5229973\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.82426 and 0.5021415\n",
            "Val acc and loss are 0.8178 and 0.5170195\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.8273 and 0.49388087\n",
            "Val acc and loss are 0.8199 and 0.5084086\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.82918 and 0.48643094\n",
            "Val acc and loss are 0.8218 and 0.501018\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.83024 and 0.4805486\n",
            "Val acc and loss are 0.8235 and 0.49580756\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.83212 and 0.47496328\n",
            "Val acc and loss are 0.8256 and 0.491295\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.83398 and 0.46879798\n",
            "Val acc and loss are 0.8268 and 0.4864272\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.83648 and 0.4624199\n",
            "Val acc and loss are 0.8282 and 0.4814664\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.83862 and 0.4561414\n",
            "Val acc and loss are 0.8285 and 0.47644487\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.84066 and 0.45000678\n",
            "Val acc and loss are 0.829 and 0.47128263\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.8428 and 0.44416505\n",
            "Val acc and loss are 0.8304 and 0.4660884\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.84462 and 0.43903378\n",
            "Val acc and loss are 0.8335 and 0.4615398\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.84718 and 0.4343425\n",
            "Val acc and loss are 0.8347 and 0.45745483\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.84874 and 0.4295886\n",
            "Val acc and loss are 0.8389 and 0.45355785\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.85034 and 0.42508462\n",
            "Val acc and loss are 0.8406 and 0.44998974\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.85218 and 0.4208894\n",
            "Val acc and loss are 0.841 and 0.44680244\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.85316 and 0.4166116\n",
            "Val acc and loss are 0.8427 and 0.4434815\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.85448 and 0.41271818\n",
            "Val acc and loss are 0.8434 and 0.44044238\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.85538 and 0.4089943\n",
            "Val acc and loss are 0.8451 and 0.43718946\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.8558 and 0.405872\n",
            "Val acc and loss are 0.844 and 0.43433046\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.8569 and 0.40268403\n",
            "Val acc and loss are 0.8455 and 0.43150297\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.8583 and 0.39918554\n",
            "Val acc and loss are 0.8473 and 0.42880458\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.85946 and 0.3962479\n",
            "Val acc and loss are 0.8481 and 0.42670542\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.86134 and 0.39325047\n",
            "Val acc and loss are 0.8489 and 0.4241635\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.86272 and 0.39045027\n",
            "Val acc and loss are 0.8507 and 0.4215842\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.8639 and 0.38766292\n",
            "Val acc and loss are 0.8508 and 0.41922164\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.86462 and 0.38498086\n",
            "Val acc and loss are 0.8522 and 0.41754013\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.86528 and 0.38284636\n",
            "Val acc and loss are 0.8534 and 0.41643995\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.86586 and 0.3803329\n",
            "Val acc and loss are 0.8532 and 0.41484526\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.86694 and 0.37764353\n",
            "Val acc and loss are 0.8546 and 0.41302618\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.86832 and 0.37493742\n",
            "Val acc and loss are 0.855 and 0.41138187\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.86936 and 0.3721182\n",
            "Val acc and loss are 0.8553 and 0.40962896\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.87054 and 0.36928976\n",
            "Val acc and loss are 0.8555 and 0.40754178\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.87146 and 0.36670852\n",
            "Val acc and loss are 0.8564 and 0.40571508\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.8724 and 0.3643784\n",
            "Val acc and loss are 0.8571 and 0.40411094\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.87308 and 0.36230376\n",
            "Val acc and loss are 0.8574 and 0.40280536\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.87392 and 0.35996285\n",
            "Val acc and loss are 0.8583 and 0.40114897\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.87472 and 0.3575066\n",
            "Val acc and loss are 0.8588 and 0.3992755\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.87518 and 0.35514376\n",
            "Val acc and loss are 0.8598 and 0.39765835\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.87598 and 0.3528599\n",
            "Val acc and loss are 0.8602 and 0.39619568\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.87656 and 0.3504602\n",
            "Val acc and loss are 0.8615 and 0.39435107\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.87758 and 0.34831977\n",
            "Val acc and loss are 0.862 and 0.39287668\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.878 and 0.34640428\n",
            "Val acc and loss are 0.8624 and 0.39166656\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.87876 and 0.3443585\n",
            "Val acc and loss are 0.8627 and 0.39033276\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.87908 and 0.34235275\n",
            "Val acc and loss are 0.8627 and 0.38908786\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.8801 and 0.3403411\n",
            "Val acc and loss are 0.8638 and 0.38807756\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.88114 and 0.33850697\n",
            "Val acc and loss are 0.8633 and 0.38724586\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.88158 and 0.33658797\n",
            "Val acc and loss are 0.8635 and 0.3860907\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.88258 and 0.3346915\n",
            "Val acc and loss are 0.8649 and 0.3848431\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.8832 and 0.33302742\n",
            "Val acc and loss are 0.8664 and 0.38408822\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.88398 and 0.3314645\n",
            "Val acc and loss are 0.8676 and 0.3834279\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.88428 and 0.32948276\n",
            "Val acc and loss are 0.8672 and 0.38197377\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.88516 and 0.32749763\n",
            "Val acc and loss are 0.8667 and 0.38080138\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.8859 and 0.32572788\n",
            "Val acc and loss are 0.8664 and 0.3800996\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.88632 and 0.3238766\n",
            "Val acc and loss are 0.8666 and 0.3789317\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.88702 and 0.32235572\n",
            "Val acc and loss are 0.8676 and 0.37800074\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.88712 and 0.3209762\n",
            "Val acc and loss are 0.8676 and 0.37754202\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.88768 and 0.31982186\n",
            "Val acc and loss are 0.8681 and 0.3774123\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.8882 and 0.31815687\n",
            "Val acc and loss are 0.8694 and 0.37643734\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.8888 and 0.31662035\n",
            "Val acc and loss are 0.869 and 0.3759113\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.88986 and 0.31490907\n",
            "Val acc and loss are 0.8692 and 0.37509\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.89058 and 0.3133053\n",
            "Val acc and loss are 0.8699 and 0.37422186\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.89092 and 0.31173506\n",
            "Val acc and loss are 0.8716 and 0.3731969\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.89134 and 0.3103044\n",
            "Val acc and loss are 0.8712 and 0.37263218\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.89176 and 0.30868348\n",
            "Val acc and loss are 0.872 and 0.3721892\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.89242 and 0.30703202\n",
            "Val acc and loss are 0.8718 and 0.371609\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.89306 and 0.30562866\n",
            "Val acc and loss are 0.8726 and 0.37082607\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.8939 and 0.30468154\n",
            "Val acc and loss are 0.8721 and 0.37109983\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.89394 and 0.3036742\n",
            "Val acc and loss are 0.8722 and 0.3712139\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.89478 and 0.30170274\n",
            "Val acc and loss are 0.8721 and 0.36980453\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.89482 and 0.29989594\n",
            "Val acc and loss are 0.8718 and 0.36895755\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.89542 and 0.29840523\n",
            "Val acc and loss are 0.8727 and 0.36792427\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.8963 and 0.2969683\n",
            "Val acc and loss are 0.8726 and 0.3667263\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.89656 and 0.2959457\n",
            "Val acc and loss are 0.8743 and 0.36651525\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.89662 and 0.29517934\n",
            "Val acc and loss are 0.8736 and 0.36658034\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.8974 and 0.29381666\n",
            "Val acc and loss are 0.8734 and 0.36561158\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.89722 and 0.29267144\n",
            "Val acc and loss are 0.8742 and 0.36565238\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.89788 and 0.29105663\n",
            "Val acc and loss are 0.8739 and 0.36491588\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.89878 and 0.28947082\n",
            "Val acc and loss are 0.8747 and 0.36397442\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.89934 and 0.28800765\n",
            "Val acc and loss are 0.8749 and 0.3636493\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.89924 and 0.28774202\n",
            "Val acc and loss are 0.8753 and 0.3643663\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.89972 and 0.2859813\n",
            "Val acc and loss are 0.8757 and 0.36302128\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.90008 and 0.2847145\n",
            "Val acc and loss are 0.8768 and 0.3624896\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.9001 and 0.28423885\n",
            "Val acc and loss are 0.8757 and 0.36320484\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.9014 and 0.2822359\n",
            "Val acc and loss are 0.8755 and 0.36176246\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.90146 and 0.28104636\n",
            "Val acc and loss are 0.8762 and 0.36172208\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.9015 and 0.28021744\n",
            "Val acc and loss are 0.8767 and 0.36156613\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.9022 and 0.27892274\n",
            "Val acc and loss are 0.8774 and 0.36025384\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.90242 and 0.27777\n",
            "Val acc and loss are 0.8767 and 0.36042857\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.9032 and 0.27587044\n",
            "Val acc and loss are 0.8778 and 0.35910943\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.90356 and 0.27483755\n",
            "Val acc and loss are 0.8778 and 0.35881346\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.9043 and 0.27431107\n",
            "Val acc and loss are 0.8769 and 0.3591131\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.9046 and 0.2727353\n",
            "Val acc and loss are 0.8775 and 0.35830286\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.90528 and 0.27152744\n",
            "Val acc and loss are 0.8778 and 0.35824874\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.90512 and 0.27077872\n",
            "Val acc and loss are 0.8779 and 0.3583363\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.90576 and 0.27015167\n",
            "Val acc and loss are 0.8777 and 0.35786808\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.90584 and 0.26950267\n",
            "Val acc and loss are 0.8787 and 0.35824403\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.90628 and 0.26795992\n",
            "Val acc and loss are 0.8775 and 0.35737392\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.90648 and 0.26701885\n",
            "Val acc and loss are 0.8781 and 0.35705587\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.90684 and 0.26661956\n",
            "Val acc and loss are 0.8779 and 0.3578216\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.90748 and 0.26460952\n",
            "Val acc and loss are 0.8774 and 0.35627988\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.90784 and 0.26401794\n",
            "Val acc and loss are 0.8797 and 0.3560528\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.90872 and 0.2628161\n",
            "Val acc and loss are 0.8792 and 0.35546404\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.9089 and 0.26134977\n",
            "Val acc and loss are 0.8788 and 0.35400268\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.90946 and 0.2599453\n",
            "Val acc and loss are 0.8789 and 0.3540335\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.90936 and 0.2595785\n",
            "Val acc and loss are 0.8791 and 0.35508925\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.90938 and 0.25817215\n",
            "Val acc and loss are 0.879 and 0.35420763\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.9095 and 0.25748947\n",
            "Val acc and loss are 0.8794 and 0.35392693\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.91032 and 0.25720677\n",
            "Val acc and loss are 0.8797 and 0.3549445\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.91018 and 0.25627208\n",
            "Val acc and loss are 0.8808 and 0.35377857\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.91122 and 0.25604466\n",
            "Val acc and loss are 0.8798 and 0.35501915\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.91202 and 0.25391746\n",
            "Val acc and loss are 0.88 and 0.35359752\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.9119 and 0.25278825\n",
            "Val acc and loss are 0.8796 and 0.35413375\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.91158 and 0.2526553\n",
            "Val acc and loss are 0.8792 and 0.35566628\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.9121 and 0.25090387\n",
            "Val acc and loss are 0.8788 and 0.3541549\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.91262 and 0.24994418\n",
            "Val acc and loss are 0.8803 and 0.35374254\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.91308 and 0.24894707\n",
            "Val acc and loss are 0.8809 and 0.35311243\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.91346 and 0.24792781\n",
            "Val acc and loss are 0.8808 and 0.35298735\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.91338 and 0.24702328\n",
            "Val acc and loss are 0.8801 and 0.35290125\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.9134 and 0.24649578\n",
            "Val acc and loss are 0.8806 and 0.35302\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.91342 and 0.24749334\n",
            "Val acc and loss are 0.8805 and 0.35571083\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.91338 and 0.24632992\n",
            "Val acc and loss are 0.8795 and 0.35363096\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.91384 and 0.24568245\n",
            "Val acc and loss are 0.881 and 0.35432038\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.91456 and 0.24370274\n",
            "Val acc and loss are 0.8808 and 0.35225883\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.91554 and 0.24197116\n",
            "Val acc and loss are 0.8806 and 0.35186777\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.9154 and 0.24181907\n",
            "Val acc and loss are 0.8803 and 0.35268086\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.91598 and 0.23985736\n",
            "Val acc and loss are 0.8818 and 0.35061514\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.91632 and 0.23937412\n",
            "Val acc and loss are 0.8822 and 0.3509851\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.91666 and 0.23920467\n",
            "Val acc and loss are 0.8817 and 0.35166958\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.91626 and 0.23885043\n",
            "Val acc and loss are 0.8818 and 0.35068095\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.91662 and 0.23970027\n",
            "Val acc and loss are 0.881 and 0.3539015\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.91662 and 0.23803376\n",
            "Val acc and loss are 0.8805 and 0.35238776\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.91746 and 0.23596475\n",
            "Val acc and loss are 0.8816 and 0.35073516\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.91736 and 0.23580228\n",
            "Val acc and loss are 0.8814 and 0.3514987\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.91748 and 0.23465441\n",
            "Val acc and loss are 0.8823 and 0.35039562\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.91856 and 0.23358727\n",
            "Val acc and loss are 0.8826 and 0.3506023\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.9189 and 0.23208904\n",
            "Val acc and loss are 0.8824 and 0.34954965\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.9184 and 0.23251505\n",
            "Val acc and loss are 0.8819 and 0.35085726\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.91862 and 0.23240809\n",
            "Val acc and loss are 0.881 and 0.35196415\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.91908 and 0.23097482\n",
            "Val acc and loss are 0.8815 and 0.3493853\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.91964 and 0.22985232\n",
            "Val acc and loss are 0.8812 and 0.34939578\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.91982 and 0.22838603\n",
            "Val acc and loss are 0.8823 and 0.34872\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.92046 and 0.22724019\n",
            "Val acc and loss are 0.8833 and 0.34809127\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.92042 and 0.2274307\n",
            "Val acc and loss are 0.8821 and 0.34910008\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.92048 and 0.22659054\n",
            "Val acc and loss are 0.882 and 0.34849638\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.9212 and 0.2252338\n",
            "Val acc and loss are 0.8827 and 0.34785712\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.92184 and 0.22459967\n",
            "Val acc and loss are 0.8825 and 0.34725296\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.92086 and 0.22503644\n",
            "Val acc and loss are 0.8832 and 0.34796587\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.92138 and 0.2247308\n",
            "Val acc and loss are 0.8828 and 0.34949446\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.92214 and 0.22253144\n",
            "Val acc and loss are 0.8835 and 0.3473158\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.92224 and 0.22189672\n",
            "Val acc and loss are 0.8832 and 0.34779355\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.92202 and 0.2227509\n",
            "Val acc and loss are 0.8816 and 0.34998325\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.9226 and 0.22109383\n",
            "Val acc and loss are 0.8827 and 0.34751606\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.92194 and 0.2219541\n",
            "Val acc and loss are 0.8807 and 0.3508397\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.923 and 0.21964963\n",
            "Val acc and loss are 0.883 and 0.34782213\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.92282 and 0.21918976\n",
            "Val acc and loss are 0.8838 and 0.34705248\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.92246 and 0.22091156\n",
            "Val acc and loss are 0.8812 and 0.35149512\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.92356 and 0.21792668\n",
            "Val acc and loss are 0.8821 and 0.34917802\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.92442 and 0.21541569\n",
            "Val acc and loss are 0.8827 and 0.347331\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.9241 and 0.21596703\n",
            "Val acc and loss are 0.8823 and 0.3486869\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.92358 and 0.21718195\n",
            "Val acc and loss are 0.8832 and 0.34970593\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.92516 and 0.21399508\n",
            "Val acc and loss are 0.8829 and 0.34628433\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.92478 and 0.214074\n",
            "Val acc and loss are 0.8833 and 0.34667182\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.92526 and 0.21417414\n",
            "Val acc and loss are 0.8827 and 0.34919438\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.92524 and 0.21277252\n",
            "Val acc and loss are 0.8832 and 0.3485046\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.9253 and 0.2124014\n",
            "Val acc and loss are 0.8834 and 0.34808287\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.92582 and 0.21116681\n",
            "Val acc and loss are 0.8829 and 0.3478979\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.9258 and 0.21248816\n",
            "Val acc and loss are 0.8829 and 0.34984478\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.92738 and 0.2090905\n",
            "Val acc and loss are 0.8854 and 0.34495646\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.92724 and 0.20828982\n",
            "Val acc and loss are 0.8837 and 0.345869\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.92688 and 0.20926467\n",
            "Val acc and loss are 0.883 and 0.34842652\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.92776 and 0.20712617\n",
            "Val acc and loss are 0.8847 and 0.34599352\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.92782 and 0.2073916\n",
            "Val acc and loss are 0.8833 and 0.3465709\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.9279 and 0.20696871\n",
            "Val acc and loss are 0.8839 and 0.34772182\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.9285 and 0.20462126\n",
            "Val acc and loss are 0.8846 and 0.3448445\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.92812 and 0.20441703\n",
            "Val acc and loss are 0.8847 and 0.3460714\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.92826 and 0.20506871\n",
            "Val acc and loss are 0.8846 and 0.34900138\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.92898 and 0.20350371\n",
            "Val acc and loss are 0.8848 and 0.34830672\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.92986 and 0.20275925\n",
            "Val acc and loss are 0.8843 and 0.34705773\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.9288 and 0.20339574\n",
            "Val acc and loss are 0.884 and 0.34975937\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.93026 and 0.20103407\n",
            "Val acc and loss are 0.8843 and 0.34731424\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.92996 and 0.20034365\n",
            "Val acc and loss are 0.8853 and 0.34539667\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.9301 and 0.20125598\n",
            "Val acc and loss are 0.8845 and 0.3483856\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.92978 and 0.2005625\n",
            "Val acc and loss are 0.8836 and 0.34732008\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.93016 and 0.19952871\n",
            "Val acc and loss are 0.8855 and 0.34574595\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.93032 and 0.19940867\n",
            "Val acc and loss are 0.884 and 0.3483026\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.93056 and 0.19806711\n",
            "Val acc and loss are 0.8845 and 0.34779185\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.93148 and 0.19777364\n",
            "Val acc and loss are 0.8853 and 0.34725738\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.93072 and 0.19767186\n",
            "Val acc and loss are 0.8849 and 0.34770527\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.93148 and 0.1966977\n",
            "Val acc and loss are 0.8843 and 0.34810257\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.9319 and 0.19623117\n",
            "Val acc and loss are 0.8855 and 0.34678712\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.93182 and 0.19569921\n",
            "Val acc and loss are 0.8839 and 0.34775466\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.932 and 0.1946616\n",
            "Val acc and loss are 0.8847 and 0.3471089\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.93254 and 0.19434641\n",
            "Val acc and loss are 0.8853 and 0.34570575\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.93244 and 0.19337077\n",
            "Val acc and loss are 0.8851 and 0.34543556\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.93314 and 0.19299905\n",
            "Val acc and loss are 0.8841 and 0.346532\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.93376 and 0.19152996\n",
            "Val acc and loss are 0.8845 and 0.34467945\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.93324 and 0.19077435\n",
            "Val acc and loss are 0.8836 and 0.34664494\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.93328 and 0.19152927\n",
            "Val acc and loss are 0.8848 and 0.34910384\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.93434 and 0.19031528\n",
            "Val acc and loss are 0.8861 and 0.34662697\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.93334 and 0.19023797\n",
            "Val acc and loss are 0.8849 and 0.34841886\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.93418 and 0.18920502\n",
            "Val acc and loss are 0.8854 and 0.34780562\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.9345 and 0.18923023\n",
            "Val acc and loss are 0.8851 and 0.34864405\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.93378 and 0.18866651\n",
            "Val acc and loss are 0.886 and 0.348697\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.93496 and 0.18786174\n",
            "Val acc and loss are 0.8853 and 0.34958106\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.9354 and 0.1868135\n",
            "Val acc and loss are 0.8843 and 0.3480205\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.93484 and 0.18700625\n",
            "Val acc and loss are 0.8844 and 0.34970865\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.93458 and 0.18737485\n",
            "Val acc and loss are 0.8852 and 0.35081372\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.93486 and 0.18678196\n",
            "Val acc and loss are 0.8846 and 0.34813774\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.93518 and 0.18744119\n",
            "Val acc and loss are 0.883 and 0.35225892\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.93468 and 0.18581134\n",
            "Val acc and loss are 0.8844 and 0.35003075\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.93624 and 0.18444738\n",
            "Val acc and loss are 0.885 and 0.34900898\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.93606 and 0.183647\n",
            "Val acc and loss are 0.8847 and 0.3487092\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.9358 and 0.18379772\n",
            "Val acc and loss are 0.8854 and 0.3493501\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.93698 and 0.18234187\n",
            "Val acc and loss are 0.8847 and 0.34785062\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.9366 and 0.18135008\n",
            "Val acc and loss are 0.8845 and 0.34723622\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.93568 and 0.18280137\n",
            "Val acc and loss are 0.8842 and 0.35082886\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.93718 and 0.18046454\n",
            "Val acc and loss are 0.8849 and 0.34712622\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.93772 and 0.18051937\n",
            "Val acc and loss are 0.8856 and 0.34768203\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.93658 and 0.18139747\n",
            "Val acc and loss are 0.8861 and 0.35122365\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.93756 and 0.17942482\n",
            "Val acc and loss are 0.8852 and 0.34825307\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.93854 and 0.17888883\n",
            "Val acc and loss are 0.8839 and 0.3487073\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.93832 and 0.1782734\n",
            "Val acc and loss are 0.8847 and 0.349046\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.93836 and 0.17790258\n",
            "Val acc and loss are 0.8847 and 0.34881958\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.93776 and 0.17973214\n",
            "Val acc and loss are 0.8836 and 0.35213605\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.93774 and 0.17792559\n",
            "Val acc and loss are 0.8861 and 0.3491061\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.93808 and 0.17744862\n",
            "Val acc and loss are 0.884 and 0.35026386\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.93922 and 0.17571527\n",
            "Val acc and loss are 0.8842 and 0.34808558\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.93868 and 0.17608854\n",
            "Val acc and loss are 0.8856 and 0.34864998\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.93922 and 0.17582335\n",
            "Val acc and loss are 0.8847 and 0.34998995\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.9397 and 0.1744741\n",
            "Val acc and loss are 0.8857 and 0.34815454\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.93988 and 0.17423968\n",
            "Val acc and loss are 0.8853 and 0.3489551\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.94004 and 0.17370787\n",
            "Val acc and loss are 0.8851 and 0.3480925\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.9403 and 0.17357877\n",
            "Val acc and loss are 0.8849 and 0.3493934\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.94074 and 0.17261973\n",
            "Val acc and loss are 0.8855 and 0.34919074\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.94126 and 0.17245135\n",
            "Val acc and loss are 0.8861 and 0.34997815\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.94064 and 0.17279711\n",
            "Val acc and loss are 0.8862 and 0.3507748\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.941 and 0.17185645\n",
            "Val acc and loss are 0.886 and 0.34922215\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.94154 and 0.17036527\n",
            "Val acc and loss are 0.8855 and 0.34830773\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.94126 and 0.16974765\n",
            "Val acc and loss are 0.8861 and 0.3480077\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.9413 and 0.16948752\n",
            "Val acc and loss are 0.8865 and 0.3469743\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.94128 and 0.17104457\n",
            "Val acc and loss are 0.8854 and 0.35066676\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.94194 and 0.16918738\n",
            "Val acc and loss are 0.8865 and 0.3480875\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.9422 and 0.1683345\n",
            "Val acc and loss are 0.8866 and 0.34786344\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.94204 and 0.16889504\n",
            "Val acc and loss are 0.8856 and 0.3506712\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.942 and 0.16749261\n",
            "Val acc and loss are 0.8849 and 0.34983683\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.94328 and 0.16672392\n",
            "Val acc and loss are 0.8854 and 0.3494688\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.9427 and 0.16776352\n",
            "Val acc and loss are 0.8861 and 0.3507969\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.94226 and 0.1676937\n",
            "Val acc and loss are 0.8872 and 0.34834898\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.942 and 0.16867243\n",
            "Val acc and loss are 0.886 and 0.35088006\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.94198 and 0.16706137\n",
            "Val acc and loss are 0.8853 and 0.34943536\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.94358 and 0.16538376\n",
            "Val acc and loss are 0.8851 and 0.35008618\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.94358 and 0.16446286\n",
            "Val acc and loss are 0.8868 and 0.34870967\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.94308 and 0.16470477\n",
            "Val acc and loss are 0.8869 and 0.34918237\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.94426 and 0.16343868\n",
            "Val acc and loss are 0.8867 and 0.3487408\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.94402 and 0.16302243\n",
            "Val acc and loss are 0.8857 and 0.3495288\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.94444 and 0.16270861\n",
            "Val acc and loss are 0.8865 and 0.34986302\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.94502 and 0.16167365\n",
            "Val acc and loss are 0.8863 and 0.34785306\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.9444 and 0.16193257\n",
            "Val acc and loss are 0.8862 and 0.34899935\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.9447 and 0.16162856\n",
            "Val acc and loss are 0.8868 and 0.35032955\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.94608 and 0.15992452\n",
            "Val acc and loss are 0.8857 and 0.3488989\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.945 and 0.16055155\n",
            "Val acc and loss are 0.886 and 0.35106027\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.94506 and 0.16087262\n",
            "Val acc and loss are 0.8857 and 0.3526377\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.94578 and 0.1590784\n",
            "Val acc and loss are 0.8863 and 0.35013896\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.94528 and 0.15969916\n",
            "Val acc and loss are 0.8868 and 0.3522188\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.9457 and 0.15927742\n",
            "Val acc and loss are 0.8875 and 0.35200512\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.94644 and 0.15824977\n",
            "Val acc and loss are 0.8857 and 0.35020864\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.94624 and 0.15787017\n",
            "Val acc and loss are 0.8853 and 0.35103223\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.94588 and 0.15781477\n",
            "Val acc and loss are 0.8845 and 0.35234088\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.94644 and 0.15701686\n",
            "Val acc and loss are 0.8858 and 0.35060924\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.94648 and 0.15738969\n",
            "Val acc and loss are 0.8846 and 0.35231948\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.94656 and 0.15661068\n",
            "Val acc and loss are 0.8847 and 0.35263014\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.94718 and 0.15541486\n",
            "Val acc and loss are 0.8851 and 0.3509574\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.9473 and 0.15553752\n",
            "Val acc and loss are 0.8854 and 0.35122415\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.947 and 0.15498297\n",
            "Val acc and loss are 0.8851 and 0.35090044\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.94678 and 0.15559828\n",
            "Val acc and loss are 0.8854 and 0.35315135\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.94708 and 0.15561521\n",
            "Val acc and loss are 0.8863 and 0.3526475\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.94786 and 0.15411064\n",
            "Val acc and loss are 0.8851 and 0.35223243\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.9476 and 0.15455754\n",
            "Val acc and loss are 0.8857 and 0.35327375\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.94684 and 0.15503232\n",
            "Val acc and loss are 0.8858 and 0.35318953\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.94746 and 0.15449324\n",
            "Val acc and loss are 0.8857 and 0.35566968\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.94774 and 0.15275118\n",
            "Val acc and loss are 0.8866 and 0.35240543\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.94798 and 0.15186666\n",
            "Val acc and loss are 0.8854 and 0.3513431\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.94764 and 0.15365033\n",
            "Val acc and loss are 0.8845 and 0.3553173\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.94742 and 0.15298752\n",
            "Val acc and loss are 0.8855 and 0.35348332\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.94848 and 0.1506742\n",
            "Val acc and loss are 0.8847 and 0.35274693\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.94874 and 0.15034063\n",
            "Val acc and loss are 0.8847 and 0.3529533\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.94842 and 0.15020421\n",
            "Val acc and loss are 0.8856 and 0.35276228\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.94896 and 0.15017906\n",
            "Val acc and loss are 0.8849 and 0.3544525\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.94942 and 0.14882022\n",
            "Val acc and loss are 0.8857 and 0.35317525\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.94916 and 0.14958179\n",
            "Val acc and loss are 0.8865 and 0.3539057\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.94938 and 0.14999765\n",
            "Val acc and loss are 0.8846 and 0.35504466\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.94898 and 0.1494353\n",
            "Val acc and loss are 0.8865 and 0.35440582\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.95002 and 0.14765364\n",
            "Val acc and loss are 0.8861 and 0.353883\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.94926 and 0.148492\n",
            "Val acc and loss are 0.8849 and 0.3562556\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.94846 and 0.14887531\n",
            "Val acc and loss are 0.8859 and 0.35587636\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.9496 and 0.14805125\n",
            "Val acc and loss are 0.8858 and 0.35578436\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9503 and 0.14622971\n",
            "Val acc and loss are 0.8863 and 0.35331652\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.94992 and 0.14663896\n",
            "Val acc and loss are 0.8863 and 0.35466993\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.95074 and 0.14593695\n",
            "Val acc and loss are 0.8853 and 0.35481593\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.9503 and 0.14610454\n",
            "Val acc and loss are 0.8869 and 0.3544804\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.95072 and 0.14583683\n",
            "Val acc and loss are 0.8852 and 0.35677925\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.95124 and 0.14520462\n",
            "Val acc and loss are 0.8856 and 0.35613358\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.95024 and 0.14617051\n",
            "Val acc and loss are 0.8877 and 0.35563168\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.95078 and 0.14523675\n",
            "Val acc and loss are 0.8858 and 0.3575998\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.95118 and 0.14397758\n",
            "Val acc and loss are 0.8858 and 0.35655355\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.95152 and 0.14269604\n",
            "Val acc and loss are 0.8865 and 0.35446137\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.95152 and 0.1433962\n",
            "Val acc and loss are 0.8848 and 0.3555038\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.95076 and 0.1438784\n",
            "Val acc and loss are 0.8861 and 0.3562566\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.95218 and 0.14208728\n",
            "Val acc and loss are 0.8861 and 0.35559103\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.9522 and 0.14156197\n",
            "Val acc and loss are 0.8863 and 0.35572344\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.95192 and 0.14141558\n",
            "Val acc and loss are 0.8862 and 0.35566425\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.95256 and 0.14087698\n",
            "Val acc and loss are 0.8859 and 0.35469058\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.95232 and 0.14169426\n",
            "Val acc and loss are 0.8868 and 0.35612184\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.95204 and 0.14096136\n",
            "Val acc and loss are 0.8866 and 0.35472867\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.95364 and 0.13966715\n",
            "Val acc and loss are 0.8864 and 0.35441443\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.95366 and 0.13909954\n",
            "Val acc and loss are 0.8859 and 0.35533553\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.95284 and 0.1395805\n",
            "Val acc and loss are 0.8854 and 0.3557372\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.95328 and 0.1386975\n",
            "Val acc and loss are 0.8861 and 0.35594717\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.95314 and 0.13862976\n",
            "Val acc and loss are 0.8865 and 0.3555986\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.95358 and 0.13828109\n",
            "Val acc and loss are 0.8859 and 0.35673398\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.95416 and 0.13783967\n",
            "Val acc and loss are 0.8854 and 0.35598025\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.95346 and 0.13828468\n",
            "Val acc and loss are 0.8861 and 0.3573065\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.95298 and 0.13998505\n",
            "Val acc and loss are 0.8858 and 0.36175707\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.9541 and 0.13698827\n",
            "Val acc and loss are 0.8852 and 0.35760596\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.9543 and 0.13681993\n",
            "Val acc and loss are 0.8849 and 0.3573917\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.95388 and 0.1379686\n",
            "Val acc and loss are 0.8854 and 0.3601561\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.95428 and 0.13635936\n",
            "Val acc and loss are 0.8864 and 0.3579984\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.95474 and 0.13573776\n",
            "Val acc and loss are 0.8859 and 0.35724983\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.95452 and 0.13580191\n",
            "Val acc and loss are 0.8853 and 0.35866988\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.95368 and 0.13677974\n",
            "Val acc and loss are 0.8878 and 0.35993153\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.95502 and 0.13537414\n",
            "Val acc and loss are 0.8854 and 0.35642663\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.95454 and 0.13485062\n",
            "Val acc and loss are 0.8856 and 0.3574645\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.95408 and 0.13645665\n",
            "Val acc and loss are 0.8869 and 0.3618453\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.95564 and 0.1345133\n",
            "Val acc and loss are 0.8862 and 0.35713327\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.95504 and 0.13392574\n",
            "Val acc and loss are 0.886 and 0.35699916\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.95528 and 0.13354626\n",
            "Val acc and loss are 0.8862 and 0.3582518\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.9555 and 0.13318734\n",
            "Val acc and loss are 0.8862 and 0.35688704\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.95606 and 0.13279943\n",
            "Val acc and loss are 0.888 and 0.35738534\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.9553 and 0.13392316\n",
            "Val acc and loss are 0.8869 and 0.36121786\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.95712 and 0.13081802\n",
            "Val acc and loss are 0.8857 and 0.35571548\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.95622 and 0.1311013\n",
            "Val acc and loss are 0.8856 and 0.3564871\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.9554 and 0.13268055\n",
            "Val acc and loss are 0.8873 and 0.35887134\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.95668 and 0.13053744\n",
            "Val acc and loss are 0.8868 and 0.35733423\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.95664 and 0.13145041\n",
            "Val acc and loss are 0.8853 and 0.36031398\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.95588 and 0.13218\n",
            "Val acc and loss are 0.8875 and 0.36076397\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.95732 and 0.13044974\n",
            "Val acc and loss are 0.8859 and 0.3599995\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.95752 and 0.12897588\n",
            "Val acc and loss are 0.8865 and 0.3602723\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.9562 and 0.13011184\n",
            "Val acc and loss are 0.8864 and 0.3624416\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.95696 and 0.13068558\n",
            "Val acc and loss are 0.8856 and 0.36618394\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.95682 and 0.13014176\n",
            "Val acc and loss are 0.8853 and 0.36263227\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.95658 and 0.1299876\n",
            "Val acc and loss are 0.8865 and 0.36421797\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.9567 and 0.1304299\n",
            "Val acc and loss are 0.8836 and 0.3658965\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.9576 and 0.12889902\n",
            "Val acc and loss are 0.8855 and 0.36165708\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.9574 and 0.1289512\n",
            "Val acc and loss are 0.8847 and 0.3620059\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.95754 and 0.12775795\n",
            "Val acc and loss are 0.8863 and 0.3604637\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.95784 and 0.12742205\n",
            "Val acc and loss are 0.8862 and 0.36148173\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.957 and 0.12799369\n",
            "Val acc and loss are 0.8856 and 0.36460304\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.95832 and 0.12592165\n",
            "Val acc and loss are 0.8851 and 0.36093175\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.95852 and 0.12622856\n",
            "Val acc and loss are 0.8866 and 0.3623369\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.95802 and 0.12662935\n",
            "Val acc and loss are 0.8854 and 0.36429527\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.9592 and 0.12477147\n",
            "Val acc and loss are 0.8867 and 0.36157995\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.95966 and 0.124250315\n",
            "Val acc and loss are 0.8855 and 0.36169127\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.95802 and 0.12617283\n",
            "Val acc and loss are 0.8857 and 0.36449486\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.95828 and 0.1254696\n",
            "Val acc and loss are 0.8873 and 0.3618442\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.95942 and 0.124911875\n",
            "Val acc and loss are 0.8871 and 0.36025333\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.95842 and 0.1248834\n",
            "Val acc and loss are 0.8874 and 0.36301672\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.95918 and 0.12438661\n",
            "Val acc and loss are 0.8872 and 0.36176437\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.95986 and 0.123540446\n",
            "Val acc and loss are 0.8868 and 0.36071166\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.95894 and 0.12406295\n",
            "Val acc and loss are 0.8856 and 0.36345905\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.95902 and 0.12393659\n",
            "Val acc and loss are 0.8866 and 0.36380795\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.96002 and 0.12274422\n",
            "Val acc and loss are 0.8867 and 0.3635207\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.9597 and 0.12263943\n",
            "Val acc and loss are 0.8868 and 0.36384052\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.95964 and 0.121789075\n",
            "Val acc and loss are 0.887 and 0.36248612\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.95946 and 0.12244881\n",
            "Val acc and loss are 0.8858 and 0.3638205\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.95928 and 0.12306625\n",
            "Val acc and loss are 0.887 and 0.36502674\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.95994 and 0.1222992\n",
            "Val acc and loss are 0.8866 and 0.36299074\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.9604 and 0.121183395\n",
            "Val acc and loss are 0.8872 and 0.36341825\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.96042 and 0.12102061\n",
            "Val acc and loss are 0.8878 and 0.36323595\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.96112 and 0.120378025\n",
            "Val acc and loss are 0.8857 and 0.36256248\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.96004 and 0.12116219\n",
            "Val acc and loss are 0.8863 and 0.36579025\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.96056 and 0.11989474\n",
            "Val acc and loss are 0.8855 and 0.36570528\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.96154 and 0.11918488\n",
            "Val acc and loss are 0.8863 and 0.36403504\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.96068 and 0.11960962\n",
            "Val acc and loss are 0.886 and 0.36476645\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.96106 and 0.11990957\n",
            "Val acc and loss are 0.886 and 0.3647606\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.96112 and 0.119383395\n",
            "Val acc and loss are 0.8855 and 0.36368594\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.96006 and 0.120198034\n",
            "Val acc and loss are 0.8849 and 0.36842942\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.96124 and 0.119003914\n",
            "Val acc and loss are 0.885 and 0.36749804\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.96146 and 0.118281804\n",
            "Val acc and loss are 0.8851 and 0.3665096\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.9607 and 0.11979785\n",
            "Val acc and loss are 0.8862 and 0.36944258\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.96112 and 0.11870439\n",
            "Val acc and loss are 0.8865 and 0.36737868\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.96172 and 0.117295876\n",
            "Val acc and loss are 0.8857 and 0.36666152\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.96118 and 0.11808905\n",
            "Val acc and loss are 0.8859 and 0.36965212\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.96168 and 0.117705606\n",
            "Val acc and loss are 0.8852 and 0.36730564\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.96186 and 0.117212616\n",
            "Val acc and loss are 0.8862 and 0.36750245\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.96196 and 0.11631311\n",
            "Val acc and loss are 0.8859 and 0.36734745\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.9619 and 0.11677267\n",
            "Val acc and loss are 0.8868 and 0.36717218\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.96242 and 0.115983635\n",
            "Val acc and loss are 0.8853 and 0.36638138\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.96224 and 0.11536076\n",
            "Val acc and loss are 0.8875 and 0.36520943\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.96242 and 0.11497326\n",
            "Val acc and loss are 0.8876 and 0.36474726\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.96264 and 0.11536936\n",
            "Val acc and loss are 0.8862 and 0.36589974\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.96212 and 0.11537357\n",
            "Val acc and loss are 0.887 and 0.365152\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.96302 and 0.11449431\n",
            "Val acc and loss are 0.886 and 0.3661272\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.96222 and 0.11520015\n",
            "Val acc and loss are 0.8844 and 0.36922434\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.96244 and 0.114369124\n",
            "Val acc and loss are 0.8861 and 0.36910045\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.96284 and 0.114558235\n",
            "Val acc and loss are 0.8857 and 0.37046695\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.96324 and 0.11291922\n",
            "Val acc and loss are 0.8861 and 0.3678282\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.96284 and 0.113792345\n",
            "Val acc and loss are 0.8856 and 0.37028754\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.96318 and 0.11388022\n",
            "Val acc and loss are 0.8858 and 0.37021938\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.96254 and 0.11375741\n",
            "Val acc and loss are 0.8867 and 0.36682826\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.96386 and 0.111626536\n",
            "Val acc and loss are 0.887 and 0.3649835\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.96314 and 0.11342088\n",
            "Val acc and loss are 0.8855 and 0.36933324\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.96334 and 0.112748116\n",
            "Val acc and loss are 0.8857 and 0.36660865\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.96362 and 0.1120619\n",
            "Val acc and loss are 0.8863 and 0.36920023\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.96358 and 0.11233209\n",
            "Val acc and loss are 0.8851 and 0.3707907\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.96366 and 0.111629\n",
            "Val acc and loss are 0.8862 and 0.3682855\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.96384 and 0.111277044\n",
            "Val acc and loss are 0.8869 and 0.36884233\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.96416 and 0.11079194\n",
            "Val acc and loss are 0.8878 and 0.36752814\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.96382 and 0.11045568\n",
            "Val acc and loss are 0.8868 and 0.36726788\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.96402 and 0.11057574\n",
            "Val acc and loss are 0.8874 and 0.3697731\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.9636 and 0.11047691\n",
            "Val acc and loss are 0.8875 and 0.3699207\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.96504 and 0.10937694\n",
            "Val acc and loss are 0.8859 and 0.36922476\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.96448 and 0.109664075\n",
            "Val acc and loss are 0.8871 and 0.371324\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.96476 and 0.10980785\n",
            "Val acc and loss are 0.8862 and 0.37115127\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.9644 and 0.109222054\n",
            "Val acc and loss are 0.8859 and 0.37180358\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.96484 and 0.10864744\n",
            "Val acc and loss are 0.8855 and 0.37067083\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.96434 and 0.10887161\n",
            "Val acc and loss are 0.8873 and 0.36937368\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.96444 and 0.1082036\n",
            "Val acc and loss are 0.8856 and 0.3670826\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.96492 and 0.10792082\n",
            "Val acc and loss are 0.8878 and 0.3684659\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.965 and 0.10819961\n",
            "Val acc and loss are 0.8868 and 0.36866567\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.96454 and 0.108244\n",
            "Val acc and loss are 0.887 and 0.369552\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.96482 and 0.108925745\n",
            "Val acc and loss are 0.8884 and 0.37303385\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.96518 and 0.10806285\n",
            "Val acc and loss are 0.8871 and 0.37149644\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.96572 and 0.10734125\n",
            "Val acc and loss are 0.8869 and 0.3717178\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.96564 and 0.10683542\n",
            "Val acc and loss are 0.8858 and 0.37231946\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.9659 and 0.106339626\n",
            "Val acc and loss are 0.8851 and 0.37106186\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.96596 and 0.10592429\n",
            "Val acc and loss are 0.8877 and 0.37076312\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.96544 and 0.10713638\n",
            "Val acc and loss are 0.8875 and 0.37302828\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.96556 and 0.1060633\n",
            "Val acc and loss are 0.8874 and 0.36958432\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.96564 and 0.10562833\n",
            "Val acc and loss are 0.8871 and 0.37112832\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.96532 and 0.106413126\n",
            "Val acc and loss are 0.8871 and 0.37592745\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.96576 and 0.10569359\n",
            "Val acc and loss are 0.8858 and 0.37187943\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.96584 and 0.10547579\n",
            "Val acc and loss are 0.8875 and 0.37459522\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.96508 and 0.106102444\n",
            "Val acc and loss are 0.886 and 0.3762482\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.96586 and 0.10517059\n",
            "Val acc and loss are 0.885 and 0.3715101\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.96584 and 0.10521861\n",
            "Val acc and loss are 0.8872 and 0.3737647\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.96534 and 0.10568672\n",
            "Val acc and loss are 0.8885 and 0.3738464\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.96634 and 0.104589\n",
            "Val acc and loss are 0.8856 and 0.37137353\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.96642 and 0.10407593\n",
            "Val acc and loss are 0.8869 and 0.37295485\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.96606 and 0.104169905\n",
            "Val acc and loss are 0.8864 and 0.37427333\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.96678 and 0.10366217\n",
            "Val acc and loss are 0.8855 and 0.3735354\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.96618 and 0.103748046\n",
            "Val acc and loss are 0.8868 and 0.37398672\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.966 and 0.10388084\n",
            "Val acc and loss are 0.8865 and 0.37587664\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.96686 and 0.103123166\n",
            "Val acc and loss are 0.885 and 0.37275785\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.96694 and 0.10280113\n",
            "Val acc and loss are 0.8871 and 0.37351748\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.96666 and 0.10312556\n",
            "Val acc and loss are 0.887 and 0.3767693\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.96686 and 0.10228508\n",
            "Val acc and loss are 0.8865 and 0.3721874\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.96714 and 0.10147311\n",
            "Val acc and loss are 0.8871 and 0.3719877\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.9675 and 0.10136801\n",
            "Val acc and loss are 0.8873 and 0.37345907\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.96732 and 0.10152481\n",
            "Val acc and loss are 0.886 and 0.37350366\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.96726 and 0.1017023\n",
            "Val acc and loss are 0.8872 and 0.37492225\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.96706 and 0.101767145\n",
            "Val acc and loss are 0.8877 and 0.37727466\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.96756 and 0.101173714\n",
            "Val acc and loss are 0.8863 and 0.37573844\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.96702 and 0.10163721\n",
            "Val acc and loss are 0.8857 and 0.3761715\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.96818 and 0.10035004\n",
            "Val acc and loss are 0.8868 and 0.37510335\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.96826 and 0.10003609\n",
            "Val acc and loss are 0.8877 and 0.37259302\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.96858 and 0.10010005\n",
            "Val acc and loss are 0.8877 and 0.37385675\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.96698 and 0.10252838\n",
            "Val acc and loss are 0.8858 and 0.38084552\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.96762 and 0.10021432\n",
            "Val acc and loss are 0.8867 and 0.37824458\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.96818 and 0.10007588\n",
            "Val acc and loss are 0.8859 and 0.3777579\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.9673 and 0.101166375\n",
            "Val acc and loss are 0.8854 and 0.37971902\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.96708 and 0.10048647\n",
            "Val acc and loss are 0.8873 and 0.3769895\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.96816 and 0.09951705\n",
            "Val acc and loss are 0.8867 and 0.37600896\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.96736 and 0.10056262\n",
            "Val acc and loss are 0.8852 and 0.378\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.96796 and 0.100149885\n",
            "Val acc and loss are 0.886 and 0.3782641\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.9675 and 0.099322796\n",
            "Val acc and loss are 0.8852 and 0.3793959\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.96778 and 0.10005001\n",
            "Val acc and loss are 0.8847 and 0.37928593\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.96802 and 0.09905902\n",
            "Val acc and loss are 0.8865 and 0.37534267\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.96792 and 0.099997215\n",
            "Val acc and loss are 0.8861 and 0.3770417\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.9673 and 0.09992149\n",
            "Val acc and loss are 0.8851 and 0.3766154\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.9681 and 0.09921453\n",
            "Val acc and loss are 0.8866 and 0.37710363\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.96812 and 0.09911956\n",
            "Val acc and loss are 0.8872 and 0.37702954\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.9678 and 0.099599674\n",
            "Val acc and loss are 0.8855 and 0.37854648\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.96884 and 0.09719049\n",
            "Val acc and loss are 0.8863 and 0.37517247\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.9682 and 0.098771706\n",
            "Val acc and loss are 0.8874 and 0.37612194\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.96828 and 0.098942064\n",
            "Val acc and loss are 0.8867 and 0.37599102\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.96892 and 0.097300544\n",
            "Val acc and loss are 0.8868 and 0.376869\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.96842 and 0.098131835\n",
            "Val acc and loss are 0.8874 and 0.3777553\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.96842 and 0.097396076\n",
            "Val acc and loss are 0.8854 and 0.37626514\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.96932 and 0.096012615\n",
            "Val acc and loss are 0.8853 and 0.37667486\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.96846 and 0.09828757\n",
            "Val acc and loss are 0.8862 and 0.38009647\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.96882 and 0.097244374\n",
            "Val acc and loss are 0.8867 and 0.37606764\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.9698 and 0.09530385\n",
            "Val acc and loss are 0.8868 and 0.37563747\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.96918 and 0.09670256\n",
            "Val acc and loss are 0.8863 and 0.3795559\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.96906 and 0.09568277\n",
            "Val acc and loss are 0.8872 and 0.3771001\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.96966 and 0.09475976\n",
            "Val acc and loss are 0.8858 and 0.37883273\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.96924 and 0.09564334\n",
            "Val acc and loss are 0.8866 and 0.38144127\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.96918 and 0.09550012\n",
            "Val acc and loss are 0.8852 and 0.3784229\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.97022 and 0.09432299\n",
            "Val acc and loss are 0.8845 and 0.3762562\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.96954 and 0.09451672\n",
            "Val acc and loss are 0.8858 and 0.37777576\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.96972 and 0.09410871\n",
            "Val acc and loss are 0.8858 and 0.37782308\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.97002 and 0.093543045\n",
            "Val acc and loss are 0.8849 and 0.37891093\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.96982 and 0.094091445\n",
            "Val acc and loss are 0.8862 and 0.38047832\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.96952 and 0.093709625\n",
            "Val acc and loss are 0.8862 and 0.3805074\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.97044 and 0.093037196\n",
            "Val acc and loss are 0.8865 and 0.37783834\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.9696 and 0.094068535\n",
            "Val acc and loss are 0.8867 and 0.3777891\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.96992 and 0.09298814\n",
            "Val acc and loss are 0.8865 and 0.37695336\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.9703 and 0.09225794\n",
            "Val acc and loss are 0.8855 and 0.3768706\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.97026 and 0.092307664\n",
            "Val acc and loss are 0.8858 and 0.37998858\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.97038 and 0.092255056\n",
            "Val acc and loss are 0.8858 and 0.3787739\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.97072 and 0.09208567\n",
            "Val acc and loss are 0.8861 and 0.3761271\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.97084 and 0.09162654\n",
            "Val acc and loss are 0.8867 and 0.37754706\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.9701 and 0.092301205\n",
            "Val acc and loss are 0.8864 and 0.37967128\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.97104 and 0.09115036\n",
            "Val acc and loss are 0.8856 and 0.37691367\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.97082 and 0.09120603\n",
            "Val acc and loss are 0.8856 and 0.37800473\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.97098 and 0.09107243\n",
            "Val acc and loss are 0.8854 and 0.38053736\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.97172 and 0.09061222\n",
            "Val acc and loss are 0.8846 and 0.3810573\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.97186 and 0.08989087\n",
            "Val acc and loss are 0.8849 and 0.37839884\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.97144 and 0.09014647\n",
            "Val acc and loss are 0.8861 and 0.3794164\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.97112 and 0.09079171\n",
            "Val acc and loss are 0.8857 and 0.38163275\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.97156 and 0.09013036\n",
            "Val acc and loss are 0.8866 and 0.38010374\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.9713 and 0.089438714\n",
            "Val acc and loss are 0.8857 and 0.380764\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.97122 and 0.08973729\n",
            "Val acc and loss are 0.8837 and 0.38311166\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.9713 and 0.089322686\n",
            "Val acc and loss are 0.8867 and 0.38052917\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.97136 and 0.08916721\n",
            "Val acc and loss are 0.8868 and 0.38077942\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.97134 and 0.08965781\n",
            "Val acc and loss are 0.8853 and 0.3811157\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.97184 and 0.08896715\n",
            "Val acc and loss are 0.8861 and 0.3792796\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.97232 and 0.088204026\n",
            "Val acc and loss are 0.8861 and 0.38127398\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.97242 and 0.08791273\n",
            "Val acc and loss are 0.8842 and 0.38113067\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.97166 and 0.08854078\n",
            "Val acc and loss are 0.8867 and 0.38146827\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.9714 and 0.08870269\n",
            "Val acc and loss are 0.8872 and 0.38515195\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.97142 and 0.08866429\n",
            "Val acc and loss are 0.8848 and 0.38499364\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.9719 and 0.08766134\n",
            "Val acc and loss are 0.8851 and 0.38113463\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.97232 and 0.087388635\n",
            "Val acc and loss are 0.8865 and 0.38347095\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.9723 and 0.08710635\n",
            "Val acc and loss are 0.8862 and 0.3823738\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.97248 and 0.08704547\n",
            "Val acc and loss are 0.886 and 0.38077733\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.97232 and 0.087897986\n",
            "Val acc and loss are 0.8852 and 0.3845822\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.97236 and 0.08679469\n",
            "Val acc and loss are 0.8853 and 0.38196754\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.9721 and 0.087598234\n",
            "Val acc and loss are 0.887 and 0.38037926\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.97288 and 0.08653453\n",
            "Val acc and loss are 0.8869 and 0.38163418\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.9722 and 0.087268144\n",
            "Val acc and loss are 0.8855 and 0.3827732\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.97262 and 0.08548908\n",
            "Val acc and loss are 0.8862 and 0.37985292\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.9727 and 0.08606953\n",
            "Val acc and loss are 0.8856 and 0.38200063\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.97214 and 0.0869635\n",
            "Val acc and loss are 0.8853 and 0.3858414\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.9725 and 0.0863706\n",
            "Val acc and loss are 0.8858 and 0.38357553\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.97306 and 0.08564895\n",
            "Val acc and loss are 0.8836 and 0.38444984\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.97324 and 0.08541135\n",
            "Val acc and loss are 0.8849 and 0.38523033\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.97198 and 0.087234415\n",
            "Val acc and loss are 0.8859 and 0.3845561\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.97334 and 0.08494256\n",
            "Val acc and loss are 0.8854 and 0.3818122\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.97332 and 0.08519015\n",
            "Val acc and loss are 0.886 and 0.38379386\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.97328 and 0.08477229\n",
            "Val acc and loss are 0.8861 and 0.38153327\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.97332 and 0.08449295\n",
            "Val acc and loss are 0.8869 and 0.3811218\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.97278 and 0.08501989\n",
            "Val acc and loss are 0.8863 and 0.387142\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.97344 and 0.084210314\n",
            "Val acc and loss are 0.8857 and 0.38442293\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.97296 and 0.08492752\n",
            "Val acc and loss are 0.8859 and 0.38695556\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.97262 and 0.08572539\n",
            "Val acc and loss are 0.8842 and 0.3937107\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.9732 and 0.083711624\n",
            "Val acc and loss are 0.8845 and 0.38756254\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.97336 and 0.08376166\n",
            "Val acc and loss are 0.8849 and 0.38670585\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.97344 and 0.08368988\n",
            "Val acc and loss are 0.8847 and 0.3872363\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.97376 and 0.0834157\n",
            "Val acc and loss are 0.8856 and 0.38538173\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.97378 and 0.08344599\n",
            "Val acc and loss are 0.8873 and 0.3850374\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.97372 and 0.08343051\n",
            "Val acc and loss are 0.8861 and 0.38632417\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.97398 and 0.082778685\n",
            "Val acc and loss are 0.885 and 0.3853462\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.97416 and 0.082817405\n",
            "Val acc and loss are 0.8864 and 0.38568622\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.97442 and 0.08136929\n",
            "Val acc and loss are 0.8857 and 0.38083172\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.97438 and 0.08165119\n",
            "Val acc and loss are 0.886 and 0.38200277\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.97398 and 0.08237508\n",
            "Val acc and loss are 0.8874 and 0.3858147\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.97462 and 0.08120628\n",
            "Val acc and loss are 0.8867 and 0.38603008\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.97446 and 0.081644416\n",
            "Val acc and loss are 0.8849 and 0.3872332\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.97404 and 0.081785046\n",
            "Val acc and loss are 0.8861 and 0.38700345\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.97422 and 0.08157414\n",
            "Val acc and loss are 0.8871 and 0.38550922\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.9745 and 0.08121739\n",
            "Val acc and loss are 0.8865 and 0.38422498\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.97474 and 0.08098138\n",
            "Val acc and loss are 0.8855 and 0.38508356\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.97494 and 0.08035147\n",
            "Val acc and loss are 0.8848 and 0.38604307\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.97498 and 0.0806219\n",
            "Val acc and loss are 0.8854 and 0.38722506\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.97424 and 0.08128136\n",
            "Val acc and loss are 0.8851 and 0.38877726\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.97496 and 0.0800456\n",
            "Val acc and loss are 0.8852 and 0.3864642\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.97506 and 0.07995879\n",
            "Val acc and loss are 0.885 and 0.38562763\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.97532 and 0.07943968\n",
            "Val acc and loss are 0.8859 and 0.38487026\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.9751 and 0.07992665\n",
            "Val acc and loss are 0.8864 and 0.38702878\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.97582 and 0.07892801\n",
            "Val acc and loss are 0.8855 and 0.38397643\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.97544 and 0.07881446\n",
            "Val acc and loss are 0.8852 and 0.3846503\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.97464 and 0.07975909\n",
            "Val acc and loss are 0.8848 and 0.38815486\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.97492 and 0.07919253\n",
            "Val acc and loss are 0.8853 and 0.3877138\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.97514 and 0.07975447\n",
            "Val acc and loss are 0.8857 and 0.38761383\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.97508 and 0.07896175\n",
            "Val acc and loss are 0.8853 and 0.39061436\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.9755 and 0.07892767\n",
            "Val acc and loss are 0.8838 and 0.390294\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.97576 and 0.0778182\n",
            "Val acc and loss are 0.8834 and 0.38569418\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.97546 and 0.07810332\n",
            "Val acc and loss are 0.8839 and 0.3888898\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.97426 and 0.07950713\n",
            "Val acc and loss are 0.8862 and 0.39027885\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.97522 and 0.07855542\n",
            "Val acc and loss are 0.8844 and 0.38773048\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.97488 and 0.07990553\n",
            "Val acc and loss are 0.8842 and 0.39420697\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.97542 and 0.0783589\n",
            "Val acc and loss are 0.8856 and 0.39195856\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.97546 and 0.07838194\n",
            "Val acc and loss are 0.8846 and 0.38952368\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.97552 and 0.07834557\n",
            "Val acc and loss are 0.8867 and 0.39245182\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.9751 and 0.078717165\n",
            "Val acc and loss are 0.8858 and 0.39280152\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.97584 and 0.07707564\n",
            "Val acc and loss are 0.8849 and 0.38703203\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.9754 and 0.078009896\n",
            "Val acc and loss are 0.8847 and 0.3899591\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.97518 and 0.07865901\n",
            "Val acc and loss are 0.8862 and 0.39276117\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.97592 and 0.07749035\n",
            "Val acc and loss are 0.884 and 0.38724113\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.97558 and 0.07767824\n",
            "Val acc and loss are 0.8848 and 0.3894446\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.97516 and 0.078332044\n",
            "Val acc and loss are 0.885 and 0.39474913\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.97638 and 0.07629087\n",
            "Val acc and loss are 0.8834 and 0.388406\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.97624 and 0.07663852\n",
            "Val acc and loss are 0.8852 and 0.388534\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.9752 and 0.07842957\n",
            "Val acc and loss are 0.8846 and 0.395352\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.9763 and 0.075708054\n",
            "Val acc and loss are 0.8856 and 0.389092\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.97634 and 0.0758138\n",
            "Val acc and loss are 0.8846 and 0.3885704\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.97536 and 0.07767999\n",
            "Val acc and loss are 0.8857 and 0.39339232\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.97606 and 0.076260574\n",
            "Val acc and loss are 0.8846 and 0.39067087\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.97698 and 0.075188436\n",
            "Val acc and loss are 0.8848 and 0.3870329\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.97606 and 0.076560654\n",
            "Val acc and loss are 0.8853 and 0.39142624\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.97648 and 0.07571308\n",
            "Val acc and loss are 0.8847 and 0.39258838\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.97648 and 0.07575523\n",
            "Val acc and loss are 0.8863 and 0.3881588\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.97582 and 0.077069655\n",
            "Val acc and loss are 0.8846 and 0.39291415\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.9762 and 0.075885184\n",
            "Val acc and loss are 0.8835 and 0.39542496\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.9766 and 0.074836954\n",
            "Val acc and loss are 0.8843 and 0.38819152\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.97598 and 0.07649257\n",
            "Val acc and loss are 0.8852 and 0.39124054\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.9767 and 0.07516843\n",
            "Val acc and loss are 0.8847 and 0.3951201\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.97606 and 0.07541768\n",
            "Val acc and loss are 0.8855 and 0.38958228\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.97638 and 0.07586444\n",
            "Val acc and loss are 0.886 and 0.39157158\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.97658 and 0.07545877\n",
            "Val acc and loss are 0.8847 and 0.39556825\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.97626 and 0.07535262\n",
            "Val acc and loss are 0.8852 and 0.39235044\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.97606 and 0.07531454\n",
            "Val acc and loss are 0.886 and 0.3917401\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.97696 and 0.074058056\n",
            "Val acc and loss are 0.8864 and 0.39250547\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.97696 and 0.07455253\n",
            "Val acc and loss are 0.8838 and 0.39307284\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.97714 and 0.07363425\n",
            "Val acc and loss are 0.8861 and 0.39121214\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.97666 and 0.07438279\n",
            "Val acc and loss are 0.8864 and 0.39336574\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.97706 and 0.07380768\n",
            "Val acc and loss are 0.8833 and 0.395303\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.97786 and 0.07287699\n",
            "Val acc and loss are 0.8845 and 0.39098772\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.97718 and 0.07300736\n",
            "Val acc and loss are 0.885 and 0.39266536\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.97712 and 0.07300251\n",
            "Val acc and loss are 0.8849 and 0.3945347\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.97782 and 0.07198603\n",
            "Val acc and loss are 0.8849 and 0.39193717\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.9771 and 0.07272193\n",
            "Val acc and loss are 0.8853 and 0.39137354\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.97752 and 0.07185819\n",
            "Val acc and loss are 0.8859 and 0.39205474\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.97734 and 0.0725225\n",
            "Val acc and loss are 0.8855 and 0.3937932\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.97754 and 0.07255053\n",
            "Val acc and loss are 0.8862 and 0.39180195\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.97818 and 0.071312346\n",
            "Val acc and loss are 0.8867 and 0.3912332\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.9776 and 0.07204167\n",
            "Val acc and loss are 0.8856 and 0.39437318\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.977 and 0.07306314\n",
            "Val acc and loss are 0.8855 and 0.39611748\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.97786 and 0.07154081\n",
            "Val acc and loss are 0.885 and 0.3927416\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.97792 and 0.07129478\n",
            "Val acc and loss are 0.8844 and 0.39355698\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.97814 and 0.071506575\n",
            "Val acc and loss are 0.8858 and 0.39204824\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.97818 and 0.07113207\n",
            "Val acc and loss are 0.8853 and 0.3907628\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.97796 and 0.07115748\n",
            "Val acc and loss are 0.8854 and 0.39365\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.9783 and 0.07104933\n",
            "Val acc and loss are 0.8862 and 0.39513448\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.97794 and 0.07143665\n",
            "Val acc and loss are 0.8847 and 0.39442572\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.97816 and 0.070498295\n",
            "Val acc and loss are 0.8851 and 0.39480382\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.97832 and 0.07066702\n",
            "Val acc and loss are 0.8847 and 0.395998\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.97824 and 0.07095526\n",
            "Val acc and loss are 0.8844 and 0.3964276\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.97844 and 0.07003623\n",
            "Val acc and loss are 0.8851 and 0.39363104\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.978 and 0.07004495\n",
            "Val acc and loss are 0.8847 and 0.3952469\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.9779 and 0.0705623\n",
            "Val acc and loss are 0.885 and 0.39668173\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.97878 and 0.0695901\n",
            "Val acc and loss are 0.8859 and 0.39470875\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.97814 and 0.070125215\n",
            "Val acc and loss are 0.8845 and 0.39543608\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.97806 and 0.07052539\n",
            "Val acc and loss are 0.8858 and 0.39948872\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.97846 and 0.069479264\n",
            "Val acc and loss are 0.8863 and 0.3971954\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.97844 and 0.06944864\n",
            "Val acc and loss are 0.884 and 0.3971318\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.97798 and 0.07018695\n",
            "Val acc and loss are 0.8855 and 0.3986701\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.97802 and 0.0695677\n",
            "Val acc and loss are 0.8854 and 0.39686513\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.97836 and 0.06918133\n",
            "Val acc and loss are 0.8843 and 0.3959701\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9784 and 0.06959987\n",
            "Val acc and loss are 0.8839 and 0.39805025\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.9786 and 0.068654075\n",
            "Val acc and loss are 0.885 and 0.39794576\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.9784 and 0.069509774\n",
            "Val acc and loss are 0.8842 and 0.397433\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.97842 and 0.069335714\n",
            "Val acc and loss are 0.8853 and 0.3980985\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.97914 and 0.06790249\n",
            "Val acc and loss are 0.8833 and 0.39687523\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.97806 and 0.06932636\n",
            "Val acc and loss are 0.8848 and 0.39730823\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.97832 and 0.06922942\n",
            "Val acc and loss are 0.8854 and 0.39829257\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.97886 and 0.06873518\n",
            "Val acc and loss are 0.8854 and 0.39705166\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.9789 and 0.068762794\n",
            "Val acc and loss are 0.8838 and 0.3991044\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.9792 and 0.067920975\n",
            "Val acc and loss are 0.8841 and 0.39839122\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.97916 and 0.068024665\n",
            "Val acc and loss are 0.8848 and 0.39752978\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.97826 and 0.06926281\n",
            "Val acc and loss are 0.8847 and 0.3989566\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.97878 and 0.067600146\n",
            "Val acc and loss are 0.885 and 0.39839432\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.97906 and 0.06771692\n",
            "Val acc and loss are 0.885 and 0.39781606\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.9785 and 0.0684046\n",
            "Val acc and loss are 0.8843 and 0.40155536\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.97928 and 0.066964015\n",
            "Val acc and loss are 0.8856 and 0.40024424\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.97902 and 0.06757869\n",
            "Val acc and loss are 0.8858 and 0.3981854\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.97908 and 0.06708281\n",
            "Val acc and loss are 0.8839 and 0.3992267\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.97904 and 0.06692568\n",
            "Val acc and loss are 0.8849 and 0.401502\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.97938 and 0.06655576\n",
            "Val acc and loss are 0.8847 and 0.40122947\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.97946 and 0.066808455\n",
            "Val acc and loss are 0.8858 and 0.4009569\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.9791 and 0.06727538\n",
            "Val acc and loss are 0.8845 and 0.40276515\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.97962 and 0.0661287\n",
            "Val acc and loss are 0.8851 and 0.39973915\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.97968 and 0.06618407\n",
            "Val acc and loss are 0.8838 and 0.40003756\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.97898 and 0.06711299\n",
            "Val acc and loss are 0.8849 and 0.40498367\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.97982 and 0.06581392\n",
            "Val acc and loss are 0.8855 and 0.40326187\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.98024 and 0.06525855\n",
            "Val acc and loss are 0.8837 and 0.40100348\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.98024 and 0.065196544\n",
            "Val acc and loss are 0.8861 and 0.40044445\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.98014 and 0.06479387\n",
            "Val acc and loss are 0.8852 and 0.40058136\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.97978 and 0.06493299\n",
            "Val acc and loss are 0.884 and 0.40104732\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.9798 and 0.065647595\n",
            "Val acc and loss are 0.8842 and 0.40445957\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.98062 and 0.06459865\n",
            "Val acc and loss are 0.8841 and 0.40528002\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.97984 and 0.06450402\n",
            "Val acc and loss are 0.8855 and 0.4021316\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.97968 and 0.065455325\n",
            "Val acc and loss are 0.8845 and 0.40327218\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.9798 and 0.06547428\n",
            "Val acc and loss are 0.8846 and 0.4043167\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.97992 and 0.06492197\n",
            "Val acc and loss are 0.8842 and 0.40365964\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.97948 and 0.06529853\n",
            "Val acc and loss are 0.8841 and 0.40422478\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.98018 and 0.06467807\n",
            "Val acc and loss are 0.8858 and 0.40284145\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.981 and 0.06355154\n",
            "Val acc and loss are 0.8847 and 0.40044832\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.98056 and 0.06459398\n",
            "Val acc and loss are 0.8849 and 0.40111297\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.98082 and 0.0640833\n",
            "Val acc and loss are 0.8865 and 0.40104088\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.98052 and 0.06423737\n",
            "Val acc and loss are 0.8858 and 0.40071678\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.98078 and 0.06335417\n",
            "Val acc and loss are 0.8837 and 0.39985418\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.98068 and 0.06356821\n",
            "Val acc and loss are 0.8844 and 0.40259787\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.9803 and 0.06327872\n",
            "Val acc and loss are 0.8853 and 0.4029038\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.98034 and 0.06300625\n",
            "Val acc and loss are 0.884 and 0.40199867\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.98076 and 0.062759265\n",
            "Val acc and loss are 0.8852 and 0.40428147\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.9807 and 0.06291974\n",
            "Val acc and loss are 0.8836 and 0.4075184\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.98022 and 0.06396629\n",
            "Val acc and loss are 0.8841 and 0.40804845\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.98012 and 0.0640438\n",
            "Val acc and loss are 0.8841 and 0.40686297\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.9805 and 0.06336055\n",
            "Val acc and loss are 0.8837 and 0.4084883\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.98086 and 0.062429003\n",
            "Val acc and loss are 0.8836 and 0.40494886\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.98104 and 0.062429912\n",
            "Val acc and loss are 0.8843 and 0.4045474\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.98058 and 0.06363552\n",
            "Val acc and loss are 0.8852 and 0.4081739\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.98102 and 0.06293249\n",
            "Val acc and loss are 0.8857 and 0.4052831\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.98088 and 0.06294542\n",
            "Val acc and loss are 0.8853 and 0.40349633\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.9809 and 0.063410185\n",
            "Val acc and loss are 0.8856 and 0.4046855\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.9811 and 0.06269083\n",
            "Val acc and loss are 0.8855 and 0.4033168\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.98118 and 0.062315814\n",
            "Val acc and loss are 0.8847 and 0.40224537\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.98068 and 0.06275793\n",
            "Val acc and loss are 0.8861 and 0.40521842\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.98122 and 0.062419515\n",
            "Val acc and loss are 0.8859 and 0.40402257\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.98156 and 0.062282052\n",
            "Val acc and loss are 0.8837 and 0.40116978\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.98088 and 0.06277653\n",
            "Val acc and loss are 0.8844 and 0.4044586\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.98088 and 0.061983462\n",
            "Val acc and loss are 0.8845 and 0.40418544\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.98104 and 0.061921142\n",
            "Val acc and loss are 0.8853 and 0.4050436\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.98138 and 0.061683998\n",
            "Val acc and loss are 0.8838 and 0.40693524\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.98118 and 0.06202948\n",
            "Val acc and loss are 0.8848 and 0.408529\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.98184 and 0.060577627\n",
            "Val acc and loss are 0.885 and 0.40461954\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.98218 and 0.060017772\n",
            "Val acc and loss are 0.8845 and 0.4039101\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.98174 and 0.060779154\n",
            "Val acc and loss are 0.885 and 0.40694088\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.9813 and 0.06063522\n",
            "Val acc and loss are 0.8846 and 0.40713474\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.98194 and 0.059849136\n",
            "Val acc and loss are 0.8836 and 0.40721506\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.98154 and 0.060411103\n",
            "Val acc and loss are 0.8844 and 0.4103258\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.98154 and 0.060562894\n",
            "Val acc and loss are 0.8845 and 0.4088751\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.98154 and 0.059951458\n",
            "Val acc and loss are 0.8847 and 0.40679288\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.98202 and 0.059600607\n",
            "Val acc and loss are 0.8862 and 0.40749502\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.98212 and 0.059558164\n",
            "Val acc and loss are 0.8854 and 0.40743622\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.98216 and 0.059421077\n",
            "Val acc and loss are 0.8845 and 0.40667778\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.98198 and 0.06010025\n",
            "Val acc and loss are 0.8849 and 0.4089101\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.98152 and 0.06017936\n",
            "Val acc and loss are 0.8861 and 0.40819117\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.98222 and 0.05918499\n",
            "Val acc and loss are 0.8839 and 0.4078265\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.98186 and 0.060243253\n",
            "Val acc and loss are 0.8837 and 0.4106978\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.98182 and 0.060282506\n",
            "Val acc and loss are 0.8848 and 0.41104442\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.98256 and 0.059171416\n",
            "Val acc and loss are 0.8828 and 0.4087208\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.9822 and 0.05910832\n",
            "Val acc and loss are 0.8837 and 0.40784165\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.98088 and 0.061002158\n",
            "Val acc and loss are 0.8843 and 0.40980083\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.9824 and 0.05906053\n",
            "Val acc and loss are 0.8832 and 0.40670204\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.98244 and 0.05886508\n",
            "Val acc and loss are 0.884 and 0.4092986\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.98108 and 0.060662694\n",
            "Val acc and loss are 0.8855 and 0.4090564\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.98242 and 0.059027065\n",
            "Val acc and loss are 0.8851 and 0.40870914\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.98248 and 0.05857261\n",
            "Val acc and loss are 0.884 and 0.41080478\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.98206 and 0.05891516\n",
            "Val acc and loss are 0.8848 and 0.4081127\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.9817 and 0.05832656\n",
            "Val acc and loss are 0.8846 and 0.4064488\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.98234 and 0.058459703\n",
            "Val acc and loss are 0.8846 and 0.40994838\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.98178 and 0.05951644\n",
            "Val acc and loss are 0.8852 and 0.41042855\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.98196 and 0.059198134\n",
            "Val acc and loss are 0.8844 and 0.40908104\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.9817 and 0.0595355\n",
            "Val acc and loss are 0.8853 and 0.4121146\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.98248 and 0.05841044\n",
            "Val acc and loss are 0.8848 and 0.411279\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.9831 and 0.05741405\n",
            "Val acc and loss are 0.8836 and 0.4090575\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.98176 and 0.05880803\n",
            "Val acc and loss are 0.8846 and 0.4114211\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.98292 and 0.057463408\n",
            "Val acc and loss are 0.8838 and 0.40937227\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.9822 and 0.058410976\n",
            "Val acc and loss are 0.884 and 0.41217613\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.98218 and 0.058736652\n",
            "Val acc and loss are 0.8845 and 0.41092452\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.9823 and 0.05826208\n",
            "Val acc and loss are 0.8851 and 0.41012266\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.9827 and 0.05784241\n",
            "Val acc and loss are 0.8836 and 0.4104834\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.98258 and 0.05746242\n",
            "Val acc and loss are 0.8852 and 0.40825018\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.98258 and 0.057919256\n",
            "Val acc and loss are 0.8863 and 0.4078521\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.98266 and 0.058039706\n",
            "Val acc and loss are 0.8854 and 0.40918648\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.9834 and 0.057315756\n",
            "Val acc and loss are 0.8856 and 0.40872407\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.98322 and 0.057556562\n",
            "Val acc and loss are 0.8856 and 0.40888375\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.98322 and 0.056814726\n",
            "Val acc and loss are 0.8856 and 0.40818653\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.98306 and 0.05726891\n",
            "Val acc and loss are 0.8847 and 0.41090924\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.98278 and 0.056700964\n",
            "Val acc and loss are 0.8844 and 0.41019213\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.98292 and 0.056638177\n",
            "Val acc and loss are 0.8846 and 0.41032293\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.98256 and 0.056890633\n",
            "Val acc and loss are 0.8838 and 0.41376758\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.98336 and 0.05630368\n",
            "Val acc and loss are 0.8855 and 0.4142758\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.98332 and 0.055973664\n",
            "Val acc and loss are 0.8844 and 0.41167846\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.98316 and 0.055773377\n",
            "Val acc and loss are 0.8847 and 0.41142267\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.98288 and 0.056275353\n",
            "Val acc and loss are 0.8848 and 0.4109218\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.9832 and 0.056110594\n",
            "Val acc and loss are 0.8856 and 0.4102879\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.98344 and 0.055927437\n",
            "Val acc and loss are 0.8852 and 0.4100145\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.98292 and 0.05596132\n",
            "Val acc and loss are 0.8861 and 0.4106042\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.98326 and 0.055226803\n",
            "Val acc and loss are 0.886 and 0.40773347\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.9839 and 0.05486291\n",
            "Val acc and loss are 0.8855 and 0.40941608\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.98382 and 0.054884084\n",
            "Val acc and loss are 0.8844 and 0.41099814\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.98342 and 0.054984286\n",
            "Val acc and loss are 0.8846 and 0.40953058\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.98336 and 0.054694977\n",
            "Val acc and loss are 0.8835 and 0.409773\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.98418 and 0.053929538\n",
            "Val acc and loss are 0.8845 and 0.4102706\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.98396 and 0.054608908\n",
            "Val acc and loss are 0.8836 and 0.41115147\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.98382 and 0.054730516\n",
            "Val acc and loss are 0.8837 and 0.41110614\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.98352 and 0.054905996\n",
            "Val acc and loss are 0.8851 and 0.41346446\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.98366 and 0.05522689\n",
            "Val acc and loss are 0.8858 and 0.41337025\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.98382 and 0.054467846\n",
            "Val acc and loss are 0.8832 and 0.41317227\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.98398 and 0.054409023\n",
            "Val acc and loss are 0.8828 and 0.414257\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.98366 and 0.055055868\n",
            "Val acc and loss are 0.8847 and 0.41542837\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.984 and 0.053949326\n",
            "Val acc and loss are 0.884 and 0.41193038\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.98382 and 0.053605806\n",
            "Val acc and loss are 0.8858 and 0.41304755\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.98374 and 0.054151777\n",
            "Val acc and loss are 0.8842 and 0.41528648\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.98402 and 0.05323033\n",
            "Val acc and loss are 0.8833 and 0.41382816\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.98374 and 0.053622957\n",
            "Val acc and loss are 0.8837 and 0.41627538\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.98408 and 0.053962193\n",
            "Val acc and loss are 0.8857 and 0.4157585\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.984 and 0.053336274\n",
            "Val acc and loss are 0.8853 and 0.41398755\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.98468 and 0.052232206\n",
            "Val acc and loss are 0.8842 and 0.41160205\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.98396 and 0.053883556\n",
            "Val acc and loss are 0.8845 and 0.41529408\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.98408 and 0.052784022\n",
            "Val acc and loss are 0.8843 and 0.41310626\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.98376 and 0.05416149\n",
            "Val acc and loss are 0.8843 and 0.41426417\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.98318 and 0.05528214\n",
            "Val acc and loss are 0.8864 and 0.41628265\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.98428 and 0.052914023\n",
            "Val acc and loss are 0.8838 and 0.41228017\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.98422 and 0.052959286\n",
            "Val acc and loss are 0.8844 and 0.4124505\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.98374 and 0.05398756\n",
            "Val acc and loss are 0.8862 and 0.41358975\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.98402 and 0.05294499\n",
            "Val acc and loss are 0.885 and 0.4124026\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.984 and 0.05333512\n",
            "Val acc and loss are 0.8845 and 0.41421232\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.98354 and 0.054232676\n",
            "Val acc and loss are 0.8852 and 0.41539893\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.98426 and 0.052758664\n",
            "Val acc and loss are 0.8835 and 0.41172352\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.98432 and 0.052475385\n",
            "Val acc and loss are 0.8837 and 0.41233438\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.98454 and 0.053205714\n",
            "Val acc and loss are 0.8845 and 0.4153311\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.98474 and 0.052321207\n",
            "Val acc and loss are 0.8845 and 0.4141266\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.98424 and 0.05244593\n",
            "Val acc and loss are 0.8843 and 0.4145403\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.98454 and 0.051862676\n",
            "Val acc and loss are 0.8851 and 0.4134826\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.98542 and 0.05089625\n",
            "Val acc and loss are 0.8858 and 0.41108653\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.98478 and 0.051615633\n",
            "Val acc and loss are 0.8849 and 0.41229242\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.98438 and 0.05240552\n",
            "Val acc and loss are 0.886 and 0.41744068\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.98508 and 0.05102792\n",
            "Val acc and loss are 0.8845 and 0.41623476\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.98488 and 0.051509384\n",
            "Val acc and loss are 0.8846 and 0.41584623\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.98408 and 0.053265385\n",
            "Val acc and loss are 0.885 and 0.41981602\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.98462 and 0.051726356\n",
            "Val acc and loss are 0.8851 and 0.41674125\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.98422 and 0.05207448\n",
            "Val acc and loss are 0.884 and 0.41367617\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.9842 and 0.052844252\n",
            "Val acc and loss are 0.8847 and 0.41620728\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.98494 and 0.051789347\n",
            "Val acc and loss are 0.8844 and 0.41744918\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.98434 and 0.052713037\n",
            "Val acc and loss are 0.884 and 0.41546232\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.98408 and 0.05282166\n",
            "Val acc and loss are 0.8843 and 0.41455856\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.98464 and 0.05182142\n",
            "Val acc and loss are 0.8847 and 0.41431373\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.98402 and 0.052872863\n",
            "Val acc and loss are 0.8828 and 0.41652516\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.98474 and 0.051871974\n",
            "Val acc and loss are 0.8838 and 0.41535625\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.98488 and 0.051183026\n",
            "Val acc and loss are 0.8829 and 0.41426587\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.98392 and 0.052620683\n",
            "Val acc and loss are 0.883 and 0.41831598\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.98346 and 0.053694345\n",
            "Val acc and loss are 0.8855 and 0.4190293\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.98448 and 0.05216419\n",
            "Val acc and loss are 0.8852 and 0.41494146\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.98418 and 0.05266825\n",
            "Val acc and loss are 0.885 and 0.41719288\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.98428 and 0.051983602\n",
            "Val acc and loss are 0.8846 and 0.41737682\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.9848 and 0.05180341\n",
            "Val acc and loss are 0.8855 and 0.4155636\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.9842 and 0.052492846\n",
            "Val acc and loss are 0.8854 and 0.41479293\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.98464 and 0.051170744\n",
            "Val acc and loss are 0.887 and 0.41494223\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.98454 and 0.051690254\n",
            "Val acc and loss are 0.8864 and 0.41566065\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.98472 and 0.051107023\n",
            "Val acc and loss are 0.8843 and 0.41516376\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.98506 and 0.050602186\n",
            "Val acc and loss are 0.8845 and 0.41651064\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.98488 and 0.05130363\n",
            "Val acc and loss are 0.8849 and 0.4168935\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.98478 and 0.050936993\n",
            "Val acc and loss are 0.8849 and 0.41573018\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.98438 and 0.051813554\n",
            "Val acc and loss are 0.8852 and 0.41659784\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.98546 and 0.05048568\n",
            "Val acc and loss are 0.8862 and 0.41509765\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.98508 and 0.04992198\n",
            "Val acc and loss are 0.8856 and 0.41752765\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.98476 and 0.05107321\n",
            "Val acc and loss are 0.8848 and 0.42083633\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.98526 and 0.04978779\n",
            "Val acc and loss are 0.8845 and 0.4176102\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.9854 and 0.049503576\n",
            "Val acc and loss are 0.8844 and 0.41848916\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.98518 and 0.050199293\n",
            "Val acc and loss are 0.8846 and 0.42244327\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.98566 and 0.0488831\n",
            "Val acc and loss are 0.8866 and 0.41835898\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.9857 and 0.04924959\n",
            "Val acc and loss are 0.8853 and 0.41585034\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.98546 and 0.04931887\n",
            "Val acc and loss are 0.8858 and 0.4177583\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.98476 and 0.050070934\n",
            "Val acc and loss are 0.8839 and 0.4187521\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.98558 and 0.04897925\n",
            "Val acc and loss are 0.8837 and 0.4144729\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.98574 and 0.0489861\n",
            "Val acc and loss are 0.8846 and 0.4165534\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.98538 and 0.049328037\n",
            "Val acc and loss are 0.8846 and 0.4197542\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.98526 and 0.049090732\n",
            "Val acc and loss are 0.885 and 0.4149961\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.98572 and 0.048724473\n",
            "Val acc and loss are 0.8842 and 0.41493052\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.98542 and 0.049407586\n",
            "Val acc and loss are 0.8843 and 0.41896972\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.98528 and 0.048845146\n",
            "Val acc and loss are 0.8848 and 0.4181172\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.9854 and 0.04897492\n",
            "Val acc and loss are 0.8841 and 0.41871646\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.9854 and 0.049781945\n",
            "Val acc and loss are 0.8846 and 0.42173275\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.98622 and 0.048059512\n",
            "Val acc and loss are 0.8851 and 0.4183331\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.98514 and 0.048974257\n",
            "Val acc and loss are 0.8843 and 0.41821748\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.98494 and 0.049744815\n",
            "Val acc and loss are 0.8853 and 0.42166343\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.98582 and 0.04844289\n",
            "Val acc and loss are 0.8837 and 0.42509776\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.98616 and 0.04780607\n",
            "Val acc and loss are 0.8852 and 0.42006308\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.98556 and 0.048346613\n",
            "Val acc and loss are 0.8846 and 0.42142135\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.98558 and 0.04812804\n",
            "Val acc and loss are 0.8843 and 0.42421928\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.98654 and 0.04707154\n",
            "Val acc and loss are 0.8859 and 0.42159367\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.98616 and 0.04761959\n",
            "Val acc and loss are 0.8847 and 0.42095596\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.98578 and 0.047958594\n",
            "Val acc and loss are 0.8843 and 0.42473584\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.98584 and 0.047838163\n",
            "Val acc and loss are 0.8845 and 0.4250191\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.98618 and 0.047144867\n",
            "Val acc and loss are 0.8836 and 0.42210078\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.98568 and 0.04742538\n",
            "Val acc and loss are 0.8833 and 0.4238937\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.98618 and 0.0469053\n",
            "Val acc and loss are 0.8847 and 0.42323634\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.98596 and 0.047403775\n",
            "Val acc and loss are 0.8855 and 0.42245415\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.98598 and 0.04713184\n",
            "Val acc and loss are 0.8854 and 0.42038172\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.98646 and 0.046663497\n",
            "Val acc and loss are 0.8859 and 0.42002836\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.9865 and 0.046357714\n",
            "Val acc and loss are 0.8845 and 0.42072108\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.98616 and 0.046463672\n",
            "Val acc and loss are 0.8854 and 0.42154977\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.9866 and 0.0461634\n",
            "Val acc and loss are 0.8857 and 0.421497\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.98646 and 0.045982763\n",
            "Val acc and loss are 0.8847 and 0.42228448\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.98682 and 0.046344034\n",
            "Val acc and loss are 0.8842 and 0.4240133\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.98672 and 0.046595108\n",
            "Val acc and loss are 0.8848 and 0.424092\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.98636 and 0.04665088\n",
            "Val acc and loss are 0.8845 and 0.42261177\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.98616 and 0.04671713\n",
            "Val acc and loss are 0.8845 and 0.42215025\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.98648 and 0.04613789\n",
            "Val acc and loss are 0.8841 and 0.42259112\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.9868 and 0.04607073\n",
            "Val acc and loss are 0.8841 and 0.4244831\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.98648 and 0.046166256\n",
            "Val acc and loss are 0.8837 and 0.42501864\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.98632 and 0.046730068\n",
            "Val acc and loss are 0.8845 and 0.42627007\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.98618 and 0.04652748\n",
            "Val acc and loss are 0.8847 and 0.42422277\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.9861 and 0.046361618\n",
            "Val acc and loss are 0.886 and 0.42465997\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.9859 and 0.047580287\n",
            "Val acc and loss are 0.8852 and 0.42769286\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.98592 and 0.046651747\n",
            "Val acc and loss are 0.8838 and 0.4255993\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.98664 and 0.045903124\n",
            "Val acc and loss are 0.8849 and 0.4238198\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.98666 and 0.046907745\n",
            "Val acc and loss are 0.885 and 0.4268493\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.98664 and 0.04534247\n",
            "Val acc and loss are 0.8842 and 0.42459044\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.98678 and 0.04574788\n",
            "Val acc and loss are 0.8844 and 0.42390665\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.98632 and 0.046075575\n",
            "Val acc and loss are 0.8849 and 0.42583546\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.9871 and 0.04549149\n",
            "Val acc and loss are 0.884 and 0.42565787\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.98662 and 0.045608927\n",
            "Val acc and loss are 0.884 and 0.42539957\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.98594 and 0.04659183\n",
            "Val acc and loss are 0.8854 and 0.4257157\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.98714 and 0.04479418\n",
            "Val acc and loss are 0.8841 and 0.4250813\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.98708 and 0.045054443\n",
            "Val acc and loss are 0.8849 and 0.4282755\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.9869 and 0.04482342\n",
            "Val acc and loss are 0.8852 and 0.426652\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.98684 and 0.044900015\n",
            "Val acc and loss are 0.8855 and 0.4275492\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.98678 and 0.04482397\n",
            "Val acc and loss are 0.885 and 0.4276957\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.98706 and 0.044586845\n",
            "Val acc and loss are 0.8852 and 0.42754555\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.98698 and 0.04458423\n",
            "Val acc and loss are 0.8848 and 0.42601776\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.98714 and 0.044249795\n",
            "Val acc and loss are 0.8848 and 0.4257756\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.98666 and 0.04491773\n",
            "Val acc and loss are 0.8858 and 0.4285367\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.9863 and 0.0462825\n",
            "Val acc and loss are 0.8863 and 0.43224823\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.98682 and 0.044805143\n",
            "Val acc and loss are 0.8853 and 0.42776886\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.9867 and 0.04521417\n",
            "Val acc and loss are 0.8836 and 0.42728266\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.98644 and 0.045189425\n",
            "Val acc and loss are 0.8834 and 0.43036962\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.98706 and 0.045014303\n",
            "Val acc and loss are 0.8837 and 0.42773008\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.98704 and 0.044631533\n",
            "Val acc and loss are 0.8849 and 0.4258868\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.98704 and 0.044597216\n",
            "Val acc and loss are 0.8845 and 0.42763174\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.98656 and 0.04529521\n",
            "Val acc and loss are 0.8837 and 0.42705983\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.98718 and 0.04433159\n",
            "Val acc and loss are 0.8852 and 0.42551056\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.98722 and 0.044315998\n",
            "Val acc and loss are 0.8841 and 0.42803758\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.98716 and 0.04476418\n",
            "Val acc and loss are 0.8849 and 0.4294279\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.98698 and 0.044271816\n",
            "Val acc and loss are 0.8846 and 0.4257538\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.98742 and 0.044196043\n",
            "Val acc and loss are 0.8861 and 0.42491168\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.98714 and 0.044280957\n",
            "Val acc and loss are 0.885 and 0.42797372\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.9871 and 0.04402651\n",
            "Val acc and loss are 0.8854 and 0.42533892\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.98678 and 0.044863153\n",
            "Val acc and loss are 0.8849 and 0.423205\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.98744 and 0.04315838\n",
            "Val acc and loss are 0.8846 and 0.4228427\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.98714 and 0.0441266\n",
            "Val acc and loss are 0.8848 and 0.42626378\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.98734 and 0.0444023\n",
            "Val acc and loss are 0.8844 and 0.42622304\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.98734 and 0.043698944\n",
            "Val acc and loss are 0.8849 and 0.4251729\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.98668 and 0.044826563\n",
            "Val acc and loss are 0.8847 and 0.42928126\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.98716 and 0.0439013\n",
            "Val acc and loss are 0.8862 and 0.42742598\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.98756 and 0.043302022\n",
            "Val acc and loss are 0.8859 and 0.4255541\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.98772 and 0.043514878\n",
            "Val acc and loss are 0.8845 and 0.42734835\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.98726 and 0.043306578\n",
            "Val acc and loss are 0.8846 and 0.42761323\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.98716 and 0.043296997\n",
            "Val acc and loss are 0.8859 and 0.42738223\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.98734 and 0.042924326\n",
            "Val acc and loss are 0.8846 and 0.42862445\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.98752 and 0.04278083\n",
            "Val acc and loss are 0.8852 and 0.42819375\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.98772 and 0.042538233\n",
            "Val acc and loss are 0.8847 and 0.4264908\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.98714 and 0.0432132\n",
            "Val acc and loss are 0.8842 and 0.42753536\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.9876 and 0.042933892\n",
            "Val acc and loss are 0.8846 and 0.43093893\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.9875 and 0.043166406\n",
            "Val acc and loss are 0.8844 and 0.42969215\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.98764 and 0.042432558\n",
            "Val acc and loss are 0.8847 and 0.42894208\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.98784 and 0.04272114\n",
            "Val acc and loss are 0.8849 and 0.43021065\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.98798 and 0.042319484\n",
            "Val acc and loss are 0.8839 and 0.4292035\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.9881 and 0.041715443\n",
            "Val acc and loss are 0.8834 and 0.4271977\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.98766 and 0.04199789\n",
            "Val acc and loss are 0.882 and 0.42823422\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.98792 and 0.041525826\n",
            "Val acc and loss are 0.8842 and 0.4269368\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.98786 and 0.041577216\n",
            "Val acc and loss are 0.8839 and 0.42701817\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.98818 and 0.04164629\n",
            "Val acc and loss are 0.8848 and 0.42907342\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.98758 and 0.04244315\n",
            "Val acc and loss are 0.8841 and 0.4312914\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.98792 and 0.041755106\n",
            "Val acc and loss are 0.8841 and 0.4287375\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.98784 and 0.04173068\n",
            "Val acc and loss are 0.8842 and 0.42939082\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.98822 and 0.041282617\n",
            "Val acc and loss are 0.8849 and 0.43017256\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.9877 and 0.041372977\n",
            "Val acc and loss are 0.8859 and 0.42948046\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.98788 and 0.041584898\n",
            "Val acc and loss are 0.8841 and 0.4318789\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.98826 and 0.041563176\n",
            "Val acc and loss are 0.8841 and 0.43141463\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.98814 and 0.041215047\n",
            "Val acc and loss are 0.884 and 0.4317399\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.98756 and 0.041770555\n",
            "Val acc and loss are 0.8837 and 0.43048686\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.98808 and 0.041082516\n",
            "Val acc and loss are 0.8842 and 0.42689562\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.98806 and 0.042299982\n",
            "Val acc and loss are 0.8848 and 0.433696\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.98806 and 0.04102562\n",
            "Val acc and loss are 0.8841 and 0.42944482\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.98794 and 0.041733474\n",
            "Val acc and loss are 0.8842 and 0.4301627\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.98844 and 0.04085213\n",
            "Val acc and loss are 0.8832 and 0.43293583\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.9882 and 0.040788066\n",
            "Val acc and loss are 0.8839 and 0.43231118\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.98824 and 0.041612506\n",
            "Val acc and loss are 0.8847 and 0.43096486\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.98812 and 0.04065032\n",
            "Val acc and loss are 0.8849 and 0.42974767\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.9883 and 0.04090614\n",
            "Val acc and loss are 0.8849 and 0.42919594\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.98806 and 0.04159542\n",
            "Val acc and loss are 0.8858 and 0.4291629\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.98834 and 0.041248515\n",
            "Val acc and loss are 0.885 and 0.42721367\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.9878 and 0.042226095\n",
            "Val acc and loss are 0.8838 and 0.4304922\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.9882 and 0.04091207\n",
            "Val acc and loss are 0.8843 and 0.4292768\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.98866 and 0.040109683\n",
            "Val acc and loss are 0.8846 and 0.4266918\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.9885 and 0.040878028\n",
            "Val acc and loss are 0.8848 and 0.43120885\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.98826 and 0.041161273\n",
            "Val acc and loss are 0.8851 and 0.43279126\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.98856 and 0.04033673\n",
            "Val acc and loss are 0.8848 and 0.42899874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiOj2bIlHkax",
        "outputId": "be082004-087a-4202-9576-d0e1fcecd5b4"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.8885 at epoch 463 with a corresponding training accuracy of 0.96534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STrn5MkZmclB"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ZIaiHPDwmcpM",
        "outputId": "23055625-83ed-4453-c11c-e066b6416188"
      },
      "source": [
        "# Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.98866 achieved at epoch: 996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1aeXzjwmcue",
        "outputId": "aee8fa93-0697-4df9-b83f-dfbe78531ddb"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4640,    3,   27,   53,    6,    1,  239,    0,    8,    0],\n",
              "       [   2, 4985,    3,   17,    1,    0,    3,    0,    1,    0],\n",
              "       [  19,    1, 4594,   30,  217,    0,  129,    0,    2,    0],\n",
              "       [  60,    9,   14, 4783,   56,    0,   50,    0,    7,    0],\n",
              "       [   2,    4,  213,   63, 4539,    0,  123,    0,    6,    0],\n",
              "       [   0,    2,    0,    1,    0, 4975,    1,   12,    5,    8],\n",
              "       [ 209,    2,  163,   42,  122,    1, 4479,    0,   11,    1],\n",
              "       [   0,    0,    0,    0,    0,   45,    0, 4963,    3,   34],\n",
              "       [   5,    1,    3,    8,   13,    4,   10,    3, 4983,    2],\n",
              "       [   0,    0,    0,    0,    0,   21,    0,   48,    3, 4907]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "g48kjrjimc1n",
        "outputId": "e5e27674-a2d3-4d08-9cc6-25cc8d32ee9d"
      },
      "source": [
        "# Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8885 achieved at epoch: 462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yUPo5xfmjOH",
        "outputId": "4fe5b882-fd06-4cf7-9392-44de22499be8"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[829,   1,  18,  25,   3,   1, 138,   0,   7,   1],\n",
              "       [  1, 963,   1,  16,   1,   0,   5,   0,   1,   0],\n",
              "       [ 14,   0, 827,   6,  99,   0,  59,   0,   3,   0],\n",
              "       [ 23,   3,   8, 927,  34,   0,  22,   0,   4,   0],\n",
              "       [  4,   3,  83,  31, 864,   1,  59,   0,   5,   0],\n",
              "       [  1,   0,   0,   0,   0, 955,   0,  24,   6,  10],\n",
              "       [103,   3,  88,  16,  71,   0, 675,   0,  14,   0],\n",
              "       [  0,   0,   1,   0,   0,  18,   0, 907,   0,  29],\n",
              "       [  6,   0,   5,   2,   8,   4,   9,   3, 931,   0],\n",
              "       [  0,   0,   0,   0,   0,  14,   0,  37,   0, 970]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWUmUdP0mjQ8",
        "outputId": "3677f014-1d18-4d05-9f3a-17fa3271bf96"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4621793\n",
            "0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQc-prlmjTg",
        "outputId": "7ca295b9-0867-4692-8784-7afee2cb3ff5"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[829,   2,  10,  23,   4,   1, 126,   0,   5,   0],\n",
              "       [  2, 963,   3,  24,   3,   0,   5,   0,   0,   0],\n",
              "       [ 16,   1, 807,  11,  94,   1,  69,   0,   1,   0],\n",
              "       [ 22,   7,  12, 887,  27,   0,  39,   0,   5,   1],\n",
              "       [  0,   1,  86,  28, 822,   0,  57,   0,   6,   0],\n",
              "       [  1,   0,   0,   1,   0, 947,   0,  29,   2,  20],\n",
              "       [111,   2,  93,  28,  68,   0, 684,   0,  14,   0],\n",
              "       [  0,   0,   0,   0,   0,  29,   0, 940,   1,  30],\n",
              "       [  5,   1,   9,   4,   4,   4,  11,   5, 957,   0],\n",
              "       [  0,   0,   0,   0,   0,  14,   1,  33,   0, 952]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBc2HzA3mw3C"
      },
      "source": [
        "# **Test 2** *(Revised from Test 1: nodes=[dim, 128, 128, 10])*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZoRQtlCmv-x"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 128, 128, num_labels]; learn_rate = 0.01; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFM2Q74ZnJGf",
        "outputId": "5ecd7ee6-dcf1-48d6-f53e-e568f8140f7f"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.56218 and 1.2171384\n",
            "Val acc and loss are 0.5607 and 1.2097318\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.61244 and 1.087631\n",
            "Val acc and loss are 0.6126 and 1.082109\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.68916 and 0.86478984\n",
            "Val acc and loss are 0.6896 and 0.8628072\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.72296 and 0.73185396\n",
            "Val acc and loss are 0.7167 and 0.7310382\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.74102 and 0.6927511\n",
            "Val acc and loss are 0.7381 and 0.6904955\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.75788 and 0.6658801\n",
            "Val acc and loss are 0.7557 and 0.66548187\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.76684 and 0.6491036\n",
            "Val acc and loss are 0.7603 and 0.65222937\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.77284 and 0.63793\n",
            "Val acc and loss are 0.7658 and 0.6456857\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.7802 and 0.62217355\n",
            "Val acc and loss are 0.7731 and 0.63326794\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.78684 and 0.6047065\n",
            "Val acc and loss are 0.78 and 0.61704093\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.79332 and 0.58692044\n",
            "Val acc and loss are 0.7868 and 0.5999427\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.79946 and 0.56846076\n",
            "Val acc and loss are 0.7919 and 0.5824786\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.80552 and 0.5511466\n",
            "Val acc and loss are 0.7972 and 0.5660381\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.8113 and 0.5362218\n",
            "Val acc and loss are 0.8027 and 0.5516761\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.81578 and 0.5234149\n",
            "Val acc and loss are 0.8088 and 0.5389608\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.81918 and 0.51185507\n",
            "Val acc and loss are 0.8117 and 0.52736807\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.8223 and 0.5015319\n",
            "Val acc and loss are 0.8154 and 0.51722825\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.82578 and 0.49258745\n",
            "Val acc and loss are 0.8162 and 0.50890183\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.82848 and 0.48507616\n",
            "Val acc and loss are 0.8182 and 0.50218403\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.82906 and 0.4790257\n",
            "Val acc and loss are 0.8193 and 0.4965242\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.83076 and 0.4734366\n",
            "Val acc and loss are 0.8227 and 0.4912712\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.83232 and 0.46756408\n",
            "Val acc and loss are 0.8259 and 0.4858098\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.83506 and 0.4610802\n",
            "Val acc and loss are 0.8256 and 0.4800197\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.83752 and 0.4548243\n",
            "Val acc and loss are 0.8279 and 0.47487682\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.83954 and 0.4489175\n",
            "Val acc and loss are 0.8295 and 0.4702237\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.8415 and 0.44286054\n",
            "Val acc and loss are 0.8317 and 0.46539772\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.8432 and 0.43744856\n",
            "Val acc and loss are 0.8344 and 0.4611088\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.84466 and 0.4330493\n",
            "Val acc and loss are 0.8358 and 0.45784414\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.84628 and 0.42893773\n",
            "Val acc and loss are 0.837 and 0.45492548\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.8478 and 0.42490706\n",
            "Val acc and loss are 0.8373 and 0.45206308\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.84972 and 0.42144945\n",
            "Val acc and loss are 0.8386 and 0.4497623\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.85058 and 0.41834077\n",
            "Val acc and loss are 0.8381 and 0.44759107\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.85112 and 0.41496333\n",
            "Val acc and loss are 0.8395 and 0.4449336\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.85236 and 0.41148773\n",
            "Val acc and loss are 0.8423 and 0.4419896\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.8536 and 0.40776384\n",
            "Val acc and loss are 0.845 and 0.4387018\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.85626 and 0.40362364\n",
            "Val acc and loss are 0.8452 and 0.43505195\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.85834 and 0.39976358\n",
            "Val acc and loss are 0.8475 and 0.43176308\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.85974 and 0.3963569\n",
            "Val acc and loss are 0.8484 and 0.42888048\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.86058 and 0.39336073\n",
            "Val acc and loss are 0.8497 and 0.4264413\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.86144 and 0.39095843\n",
            "Val acc and loss are 0.8497 and 0.42457265\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.86226 and 0.3888078\n",
            "Val acc and loss are 0.8502 and 0.42305392\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.86304 and 0.38600296\n",
            "Val acc and loss are 0.8517 and 0.4210463\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.86456 and 0.38236278\n",
            "Val acc and loss are 0.8528 and 0.41870433\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.8654 and 0.3789536\n",
            "Val acc and loss are 0.8532 and 0.41661543\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.86652 and 0.37599614\n",
            "Val acc and loss are 0.8543 and 0.41473976\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.86836 and 0.37365648\n",
            "Val acc and loss are 0.8552 and 0.41307372\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.86892 and 0.37152797\n",
            "Val acc and loss are 0.8557 and 0.4114783\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.86972 and 0.36913532\n",
            "Val acc and loss are 0.8558 and 0.40954804\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.87012 and 0.36670572\n",
            "Val acc and loss are 0.8562 and 0.40770346\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.87092 and 0.3643025\n",
            "Val acc and loss are 0.8569 and 0.40606833\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.87214 and 0.36165547\n",
            "Val acc and loss are 0.8565 and 0.40418002\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.8731 and 0.35896274\n",
            "Val acc and loss are 0.8572 and 0.40224424\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.87426 and 0.356104\n",
            "Val acc and loss are 0.8589 and 0.40014252\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.87518 and 0.3536372\n",
            "Val acc and loss are 0.8596 and 0.39840233\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.87586 and 0.35167274\n",
            "Val acc and loss are 0.8599 and 0.3970205\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.87634 and 0.34998852\n",
            "Val acc and loss are 0.8607 and 0.39577338\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.87708 and 0.34823743\n",
            "Val acc and loss are 0.8616 and 0.39439097\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.87784 and 0.34623492\n",
            "Val acc and loss are 0.8614 and 0.39288825\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.8785 and 0.34399238\n",
            "Val acc and loss are 0.8622 and 0.3912231\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.87906 and 0.34182444\n",
            "Val acc and loss are 0.8624 and 0.38995248\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.88008 and 0.34006348\n",
            "Val acc and loss are 0.862 and 0.38912198\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.8808 and 0.33835337\n",
            "Val acc and loss are 0.8631 and 0.38835454\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.88104 and 0.33682773\n",
            "Val acc and loss are 0.8625 and 0.38759103\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.88152 and 0.3350665\n",
            "Val acc and loss are 0.8628 and 0.3864876\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.88252 and 0.33344617\n",
            "Val acc and loss are 0.8648 and 0.3853352\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.88318 and 0.33148152\n",
            "Val acc and loss are 0.8647 and 0.383801\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.88354 and 0.32958445\n",
            "Val acc and loss are 0.8654 and 0.38248754\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.88448 and 0.32737973\n",
            "Val acc and loss are 0.8657 and 0.38121316\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.8851 and 0.32548317\n",
            "Val acc and loss are 0.8663 and 0.3803485\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.88572 and 0.3242533\n",
            "Val acc and loss are 0.8667 and 0.38007143\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.88608 and 0.32267007\n",
            "Val acc and loss are 0.8673 and 0.37938717\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.88688 and 0.3213515\n",
            "Val acc and loss are 0.867 and 0.37882355\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.88734 and 0.31976527\n",
            "Val acc and loss are 0.8676 and 0.37764546\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.88764 and 0.3182103\n",
            "Val acc and loss are 0.8687 and 0.37626767\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.8886 and 0.31674188\n",
            "Val acc and loss are 0.8688 and 0.37536034\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.88886 and 0.31519476\n",
            "Val acc and loss are 0.8688 and 0.3745434\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.88926 and 0.31370428\n",
            "Val acc and loss are 0.8698 and 0.37395847\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.88994 and 0.31214195\n",
            "Val acc and loss are 0.8696 and 0.37298018\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.8908 and 0.31042817\n",
            "Val acc and loss are 0.87 and 0.3715687\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.89168 and 0.30914402\n",
            "Val acc and loss are 0.8709 and 0.37056017\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.89158 and 0.3081673\n",
            "Val acc and loss are 0.8714 and 0.37038273\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.89208 and 0.30705652\n",
            "Val acc and loss are 0.8722 and 0.3701468\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.89256 and 0.30557564\n",
            "Val acc and loss are 0.8722 and 0.36943835\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.89276 and 0.30420354\n",
            "Val acc and loss are 0.8728 and 0.36841127\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.89342 and 0.30320087\n",
            "Val acc and loss are 0.874 and 0.36797568\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.89386 and 0.30201775\n",
            "Val acc and loss are 0.8737 and 0.36734688\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.89438 and 0.30043393\n",
            "Val acc and loss are 0.8737 and 0.36630395\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.89466 and 0.2993242\n",
            "Val acc and loss are 0.8738 and 0.36614877\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.89508 and 0.29788396\n",
            "Val acc and loss are 0.8742 and 0.36573043\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.89542 and 0.29673302\n",
            "Val acc and loss are 0.8744 and 0.36590806\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.8961 and 0.29558054\n",
            "Val acc and loss are 0.8741 and 0.3658163\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.89614 and 0.29437461\n",
            "Val acc and loss are 0.8745 and 0.3649086\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.89692 and 0.2935563\n",
            "Val acc and loss are 0.8757 and 0.36456302\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.89696 and 0.2929408\n",
            "Val acc and loss are 0.875 and 0.36436048\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.8971 and 0.2913705\n",
            "Val acc and loss are 0.8753 and 0.36329862\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.89824 and 0.29013956\n",
            "Val acc and loss are 0.8751 and 0.36249816\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.89846 and 0.2889851\n",
            "Val acc and loss are 0.8746 and 0.3624647\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.8988 and 0.28788522\n",
            "Val acc and loss are 0.8742 and 0.36262026\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.89914 and 0.28630808\n",
            "Val acc and loss are 0.875 and 0.36153427\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.89906 and 0.28528914\n",
            "Val acc and loss are 0.8753 and 0.36144027\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.89986 and 0.2845167\n",
            "Val acc and loss are 0.8757 and 0.36169642\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.89998 and 0.28309834\n",
            "Val acc and loss are 0.8757 and 0.36059478\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.9004 and 0.28220356\n",
            "Val acc and loss are 0.8759 and 0.36011592\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.90072 and 0.28190926\n",
            "Val acc and loss are 0.8759 and 0.36043754\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.90092 and 0.28027776\n",
            "Val acc and loss are 0.8769 and 0.35917696\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.902 and 0.27886602\n",
            "Val acc and loss are 0.8763 and 0.35817823\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.9024 and 0.27824542\n",
            "Val acc and loss are 0.8755 and 0.35909662\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.9027 and 0.27771336\n",
            "Val acc and loss are 0.8763 and 0.35978335\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.90286 and 0.27595174\n",
            "Val acc and loss are 0.8765 and 0.35830066\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.90264 and 0.27479804\n",
            "Val acc and loss are 0.8763 and 0.35762852\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.90292 and 0.2744867\n",
            "Val acc and loss are 0.8766 and 0.35801458\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.90366 and 0.27304664\n",
            "Val acc and loss are 0.8765 and 0.35753053\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.90404 and 0.27163398\n",
            "Val acc and loss are 0.8771 and 0.35700732\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.90514 and 0.27045542\n",
            "Val acc and loss are 0.8769 and 0.35714597\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.90488 and 0.2703718\n",
            "Val acc and loss are 0.8748 and 0.35801885\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.90528 and 0.26883072\n",
            "Val acc and loss are 0.8757 and 0.3567097\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.90536 and 0.2680062\n",
            "Val acc and loss are 0.8758 and 0.3568431\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.90574 and 0.2673498\n",
            "Val acc and loss are 0.8756 and 0.35748988\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.90612 and 0.26646543\n",
            "Val acc and loss are 0.876 and 0.35681257\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.90626 and 0.26552206\n",
            "Val acc and loss are 0.8765 and 0.35631135\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.9067 and 0.26425463\n",
            "Val acc and loss are 0.8766 and 0.35584426\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.90742 and 0.2629473\n",
            "Val acc and loss are 0.8763 and 0.35513082\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.90778 and 0.26196772\n",
            "Val acc and loss are 0.8761 and 0.35497636\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.90824 and 0.26123446\n",
            "Val acc and loss are 0.8761 and 0.354977\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.90864 and 0.260113\n",
            "Val acc and loss are 0.8757 and 0.3543436\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.90898 and 0.2597336\n",
            "Val acc and loss are 0.8768 and 0.35454538\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.90936 and 0.25905418\n",
            "Val acc and loss are 0.8773 and 0.35396543\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.90938 and 0.25812134\n",
            "Val acc and loss are 0.8784 and 0.35308546\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.90938 and 0.257928\n",
            "Val acc and loss are 0.8775 and 0.35389078\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.90994 and 0.2573822\n",
            "Val acc and loss are 0.8765 and 0.35492373\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.91024 and 0.25522125\n",
            "Val acc and loss are 0.8772 and 0.35294747\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.91032 and 0.25521612\n",
            "Val acc and loss are 0.878 and 0.35405248\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.91132 and 0.2548025\n",
            "Val acc and loss are 0.8782 and 0.35496238\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.91118 and 0.25310355\n",
            "Val acc and loss are 0.8793 and 0.35371265\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.91148 and 0.25265706\n",
            "Val acc and loss are 0.8781 and 0.35382745\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.91118 and 0.25389522\n",
            "Val acc and loss are 0.8778 and 0.35603312\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.91166 and 0.25104806\n",
            "Val acc and loss are 0.8797 and 0.35318756\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.91224 and 0.24957609\n",
            "Val acc and loss are 0.8793 and 0.35266584\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.91278 and 0.24964094\n",
            "Val acc and loss are 0.8787 and 0.35428333\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.91252 and 0.24875529\n",
            "Val acc and loss are 0.8782 and 0.35377726\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.91282 and 0.24784613\n",
            "Val acc and loss are 0.8796 and 0.35245982\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.91276 and 0.24786648\n",
            "Val acc and loss are 0.8793 and 0.35401285\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.91342 and 0.24654263\n",
            "Val acc and loss are 0.8793 and 0.35397148\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.91454 and 0.2444155\n",
            "Val acc and loss are 0.8787 and 0.35168147\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.91436 and 0.24463156\n",
            "Val acc and loss are 0.8786 and 0.35224387\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.914 and 0.24472636\n",
            "Val acc and loss are 0.8787 and 0.35362333\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.91418 and 0.24322298\n",
            "Val acc and loss are 0.8778 and 0.3523523\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.91508 and 0.24265426\n",
            "Val acc and loss are 0.879 and 0.35309863\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.91442 and 0.24369238\n",
            "Val acc and loss are 0.8791 and 0.35504434\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.9154 and 0.24097386\n",
            "Val acc and loss are 0.8794 and 0.35240468\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.916 and 0.23996773\n",
            "Val acc and loss are 0.8805 and 0.35196245\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.91586 and 0.2405498\n",
            "Val acc and loss are 0.88 and 0.35378397\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.91666 and 0.23823084\n",
            "Val acc and loss are 0.8804 and 0.35104945\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.91618 and 0.23824945\n",
            "Val acc and loss are 0.8797 and 0.35064864\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.9167 and 0.23848096\n",
            "Val acc and loss are 0.8793 and 0.3533299\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.91766 and 0.23670183\n",
            "Val acc and loss are 0.8796 and 0.35258082\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.91788 and 0.23580508\n",
            "Val acc and loss are 0.8807 and 0.35140756\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.91754 and 0.23596598\n",
            "Val acc and loss are 0.881 and 0.35277182\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.91728 and 0.23522171\n",
            "Val acc and loss are 0.8812 and 0.35153422\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.91778 and 0.23361537\n",
            "Val acc and loss are 0.8801 and 0.35025445\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.91848 and 0.2332054\n",
            "Val acc and loss are 0.8813 and 0.35213152\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.91852 and 0.23298174\n",
            "Val acc and loss are 0.8802 and 0.35311446\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.91924 and 0.23160593\n",
            "Val acc and loss are 0.8798 and 0.35250306\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.91916 and 0.2305643\n",
            "Val acc and loss are 0.8796 and 0.35170743\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.9186 and 0.23141135\n",
            "Val acc and loss are 0.8805 and 0.35352647\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.91944 and 0.23085967\n",
            "Val acc and loss are 0.8818 and 0.35296422\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.92022 and 0.22862133\n",
            "Val acc and loss are 0.8805 and 0.35083053\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.91958 and 0.22906722\n",
            "Val acc and loss are 0.8805 and 0.35265312\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.9193 and 0.23040968\n",
            "Val acc and loss are 0.8813 and 0.3540992\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.91998 and 0.22746703\n",
            "Val acc and loss are 0.8811 and 0.35073158\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.9213 and 0.22584535\n",
            "Val acc and loss are 0.8812 and 0.35102978\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.9208 and 0.22811225\n",
            "Val acc and loss are 0.8808 and 0.35397866\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.92034 and 0.2267311\n",
            "Val acc and loss are 0.8804 and 0.3521291\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.92148 and 0.22463308\n",
            "Val acc and loss are 0.8812 and 0.3509546\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.9215 and 0.22504254\n",
            "Val acc and loss are 0.8812 and 0.35193193\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.92054 and 0.224563\n",
            "Val acc and loss are 0.8828 and 0.35080904\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.92188 and 0.22277941\n",
            "Val acc and loss are 0.8824 and 0.3509912\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.92212 and 0.22227597\n",
            "Val acc and loss are 0.8808 and 0.35074288\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.92044 and 0.22426014\n",
            "Val acc and loss are 0.8801 and 0.3528594\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.92168 and 0.22270212\n",
            "Val acc and loss are 0.8813 and 0.3529687\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.9228 and 0.22092849\n",
            "Val acc and loss are 0.8813 and 0.35221615\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.92246 and 0.22077826\n",
            "Val acc and loss are 0.8825 and 0.3518568\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.92306 and 0.22036621\n",
            "Val acc and loss are 0.8825 and 0.35228896\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.92354 and 0.21900313\n",
            "Val acc and loss are 0.8819 and 0.35198262\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.92236 and 0.21939291\n",
            "Val acc and loss are 0.8813 and 0.3526608\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.92278 and 0.22082207\n",
            "Val acc and loss are 0.8819 and 0.3542522\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.92338 and 0.21731022\n",
            "Val acc and loss are 0.8808 and 0.35149613\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.92354 and 0.2169997\n",
            "Val acc and loss are 0.8809 and 0.3516207\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.92318 and 0.21849768\n",
            "Val acc and loss are 0.8805 and 0.35426039\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.9242 and 0.21551953\n",
            "Val acc and loss are 0.8808 and 0.35225922\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.92464 and 0.21485816\n",
            "Val acc and loss are 0.8805 and 0.35266343\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.9252 and 0.21455278\n",
            "Val acc and loss are 0.8809 and 0.35260195\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.92546 and 0.21291946\n",
            "Val acc and loss are 0.882 and 0.3514775\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.92554 and 0.21265648\n",
            "Val acc and loss are 0.8821 and 0.35247153\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.9258 and 0.21270023\n",
            "Val acc and loss are 0.8815 and 0.35351434\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.92588 and 0.2114063\n",
            "Val acc and loss are 0.8819 and 0.3520858\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.92566 and 0.21156426\n",
            "Val acc and loss are 0.8822 and 0.3529585\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.9264 and 0.2108539\n",
            "Val acc and loss are 0.8801 and 0.3532129\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.92668 and 0.21011357\n",
            "Val acc and loss are 0.8818 and 0.35354576\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.92628 and 0.20997483\n",
            "Val acc and loss are 0.8823 and 0.35374603\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.92646 and 0.20916672\n",
            "Val acc and loss are 0.8814 and 0.35296834\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.92684 and 0.20799725\n",
            "Val acc and loss are 0.8819 and 0.35144854\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.92686 and 0.20754\n",
            "Val acc and loss are 0.8816 and 0.3525718\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.92672 and 0.20807426\n",
            "Val acc and loss are 0.8814 and 0.35453883\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.9276 and 0.20661949\n",
            "Val acc and loss are 0.8816 and 0.35328892\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.92748 and 0.20602544\n",
            "Val acc and loss are 0.8821 and 0.3538784\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.92778 and 0.20571029\n",
            "Val acc and loss are 0.8827 and 0.35316327\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.92782 and 0.20585616\n",
            "Val acc and loss are 0.8825 and 0.35300875\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.92808 and 0.20497707\n",
            "Val acc and loss are 0.8819 and 0.35231417\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.9287 and 0.20363273\n",
            "Val acc and loss are 0.8824 and 0.35143206\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.92918 and 0.20334494\n",
            "Val acc and loss are 0.8816 and 0.35195103\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.9281 and 0.20409586\n",
            "Val acc and loss are 0.8814 and 0.35328373\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.92914 and 0.20237604\n",
            "Val acc and loss are 0.8824 and 0.3523351\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.92904 and 0.20294973\n",
            "Val acc and loss are 0.8815 and 0.3533896\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.92874 and 0.20239894\n",
            "Val acc and loss are 0.8808 and 0.35288927\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.92992 and 0.20102778\n",
            "Val acc and loss are 0.8828 and 0.35120538\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.92936 and 0.20081288\n",
            "Val acc and loss are 0.882 and 0.35339803\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.92958 and 0.20009544\n",
            "Val acc and loss are 0.8818 and 0.35334367\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.93078 and 0.19874838\n",
            "Val acc and loss are 0.882 and 0.35293174\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.93052 and 0.19842823\n",
            "Val acc and loss are 0.8818 and 0.35421813\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.93038 and 0.19888349\n",
            "Val acc and loss are 0.8822 and 0.3553495\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.93094 and 0.19853343\n",
            "Val acc and loss are 0.8827 and 0.35482368\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.93112 and 0.19740085\n",
            "Val acc and loss are 0.8813 and 0.35403404\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.93098 and 0.19691375\n",
            "Val acc and loss are 0.881 and 0.35470885\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.93166 and 0.19612102\n",
            "Val acc and loss are 0.8817 and 0.3530961\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.93158 and 0.19676594\n",
            "Val acc and loss are 0.8808 and 0.3548\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.93188 and 0.19624768\n",
            "Val acc and loss are 0.8819 and 0.35429257\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.932 and 0.19483165\n",
            "Val acc and loss are 0.8815 and 0.35253742\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.93216 and 0.19417785\n",
            "Val acc and loss are 0.8815 and 0.3528271\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.93216 and 0.19523504\n",
            "Val acc and loss are 0.8814 and 0.3554785\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.93216 and 0.19484559\n",
            "Val acc and loss are 0.8828 and 0.35548654\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.93268 and 0.19398192\n",
            "Val acc and loss are 0.882 and 0.35635716\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.93264 and 0.19316582\n",
            "Val acc and loss are 0.8823 and 0.3547948\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.93132 and 0.19418949\n",
            "Val acc and loss are 0.8815 and 0.35484037\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.93242 and 0.19425605\n",
            "Val acc and loss are 0.8827 and 0.35534042\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.93352 and 0.19138695\n",
            "Val acc and loss are 0.882 and 0.3523265\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.93308 and 0.19134246\n",
            "Val acc and loss are 0.882 and 0.35421532\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.9327 and 0.19190773\n",
            "Val acc and loss are 0.8819 and 0.3563975\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.93278 and 0.19151555\n",
            "Val acc and loss are 0.8811 and 0.3558195\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.93344 and 0.19132076\n",
            "Val acc and loss are 0.8807 and 0.35739028\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.93426 and 0.18957217\n",
            "Val acc and loss are 0.8812 and 0.35551533\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.93392 and 0.18951015\n",
            "Val acc and loss are 0.8817 and 0.35521832\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.934 and 0.19056773\n",
            "Val acc and loss are 0.8817 and 0.35792455\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.93492 and 0.18796639\n",
            "Val acc and loss are 0.8816 and 0.35496747\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.93504 and 0.18772025\n",
            "Val acc and loss are 0.8817 and 0.3551031\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.93468 and 0.18766381\n",
            "Val acc and loss are 0.8811 and 0.3569222\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.93518 and 0.18651229\n",
            "Val acc and loss are 0.8821 and 0.35592383\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.93546 and 0.18638055\n",
            "Val acc and loss are 0.8824 and 0.35552445\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.93488 and 0.18681897\n",
            "Val acc and loss are 0.8822 and 0.3565455\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.9346 and 0.18591988\n",
            "Val acc and loss are 0.8813 and 0.3554648\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.93528 and 0.18533239\n",
            "Val acc and loss are 0.8838 and 0.35514706\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.93564 and 0.18498242\n",
            "Val acc and loss are 0.8828 and 0.3557633\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.93576 and 0.18500066\n",
            "Val acc and loss are 0.8821 and 0.3569242\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.93666 and 0.1836148\n",
            "Val acc and loss are 0.8816 and 0.35521784\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.93626 and 0.18294482\n",
            "Val acc and loss are 0.8808 and 0.355072\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.93556 and 0.1839682\n",
            "Val acc and loss are 0.8824 and 0.35574603\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.93674 and 0.18218052\n",
            "Val acc and loss are 0.8828 and 0.35318977\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.93782 and 0.18129453\n",
            "Val acc and loss are 0.8821 and 0.35413414\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.93694 and 0.18244739\n",
            "Val acc and loss are 0.8815 and 0.35683066\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.9372 and 0.1812497\n",
            "Val acc and loss are 0.8822 and 0.35576132\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.93742 and 0.18128835\n",
            "Val acc and loss are 0.8826 and 0.35611224\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.93656 and 0.18150973\n",
            "Val acc and loss are 0.8824 and 0.35812226\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.93792 and 0.17912108\n",
            "Val acc and loss are 0.8827 and 0.35593373\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.93826 and 0.179929\n",
            "Val acc and loss are 0.8832 and 0.3570673\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.93758 and 0.17991687\n",
            "Val acc and loss are 0.8817 and 0.35679954\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.93828 and 0.17852782\n",
            "Val acc and loss are 0.8825 and 0.35572687\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.93796 and 0.17807291\n",
            "Val acc and loss are 0.882 and 0.356109\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.93856 and 0.17774536\n",
            "Val acc and loss are 0.8817 and 0.35611773\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.93906 and 0.17684394\n",
            "Val acc and loss are 0.8821 and 0.35631806\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.93848 and 0.17819645\n",
            "Val acc and loss are 0.8818 and 0.35808995\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.93926 and 0.1768475\n",
            "Val acc and loss are 0.8833 and 0.35601303\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.939 and 0.17650312\n",
            "Val acc and loss are 0.8826 and 0.35577378\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.93854 and 0.17745313\n",
            "Val acc and loss are 0.8826 and 0.35851994\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.93966 and 0.17589027\n",
            "Val acc and loss are 0.8825 and 0.35600442\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.93916 and 0.17577511\n",
            "Val acc and loss are 0.8825 and 0.35714003\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.93872 and 0.17632414\n",
            "Val acc and loss are 0.8819 and 0.35809118\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.93938 and 0.17488502\n",
            "Val acc and loss are 0.8826 and 0.35512725\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.9396 and 0.17429008\n",
            "Val acc and loss are 0.8821 and 0.35602692\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.93942 and 0.17422926\n",
            "Val acc and loss are 0.8809 and 0.3569778\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.94026 and 0.1735709\n",
            "Val acc and loss are 0.8819 and 0.35556367\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.94036 and 0.17267926\n",
            "Val acc and loss are 0.8822 and 0.35568872\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.93922 and 0.17544274\n",
            "Val acc and loss are 0.8816 and 0.3589524\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.9403 and 0.17317423\n",
            "Val acc and loss are 0.8839 and 0.35699376\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.94054 and 0.1723284\n",
            "Val acc and loss are 0.8832 and 0.35715494\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.93994 and 0.1740425\n",
            "Val acc and loss are 0.8822 and 0.36031544\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.94094 and 0.17214562\n",
            "Val acc and loss are 0.884 and 0.35666335\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.94178 and 0.17003208\n",
            "Val acc and loss are 0.8833 and 0.3551974\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.94068 and 0.17230648\n",
            "Val acc and loss are 0.882 and 0.35956022\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.94054 and 0.17164972\n",
            "Val acc and loss are 0.8837 and 0.35677788\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.94186 and 0.1694558\n",
            "Val acc and loss are 0.8844 and 0.35529023\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.9404 and 0.17284334\n",
            "Val acc and loss are 0.882 and 0.36090213\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.94096 and 0.1704151\n",
            "Val acc and loss are 0.8824 and 0.35813478\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.94196 and 0.17001578\n",
            "Val acc and loss are 0.8831 and 0.35767686\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.94146 and 0.1703409\n",
            "Val acc and loss are 0.8825 and 0.35958076\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.94112 and 0.16986336\n",
            "Val acc and loss are 0.8827 and 0.35786763\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.94218 and 0.16944273\n",
            "Val acc and loss are 0.883 and 0.35560414\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.94188 and 0.16854742\n",
            "Val acc and loss are 0.8832 and 0.35767728\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.94206 and 0.16837527\n",
            "Val acc and loss are 0.8825 and 0.3578173\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.94304 and 0.16698582\n",
            "Val acc and loss are 0.8836 and 0.35560235\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.94166 and 0.1686314\n",
            "Val acc and loss are 0.8832 and 0.35699698\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.94244 and 0.16749795\n",
            "Val acc and loss are 0.8832 and 0.3573273\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.94396 and 0.16578713\n",
            "Val acc and loss are 0.8832 and 0.35634252\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.9426 and 0.16656215\n",
            "Val acc and loss are 0.8826 and 0.35865328\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.9422 and 0.16646923\n",
            "Val acc and loss are 0.8843 and 0.35919383\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.9438 and 0.16520652\n",
            "Val acc and loss are 0.8831 and 0.35759345\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.94342 and 0.16503264\n",
            "Val acc and loss are 0.8832 and 0.35888007\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.94388 and 0.16450547\n",
            "Val acc and loss are 0.8834 and 0.35908732\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.94388 and 0.16376337\n",
            "Val acc and loss are 0.8834 and 0.3568285\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.9434 and 0.16529846\n",
            "Val acc and loss are 0.884 and 0.35891712\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.9441 and 0.16314143\n",
            "Val acc and loss are 0.8851 and 0.35550576\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.94516 and 0.16219069\n",
            "Val acc and loss are 0.8838 and 0.35583958\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.94484 and 0.16193983\n",
            "Val acc and loss are 0.8847 and 0.35736012\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.9443 and 0.16143289\n",
            "Val acc and loss are 0.8842 and 0.35721466\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.94514 and 0.16099924\n",
            "Val acc and loss are 0.8839 and 0.3570668\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.94462 and 0.16159773\n",
            "Val acc and loss are 0.8842 and 0.3583981\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.94468 and 0.16139618\n",
            "Val acc and loss are 0.884 and 0.3587996\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.94504 and 0.16089642\n",
            "Val acc and loss are 0.8846 and 0.3593567\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.94526 and 0.16034764\n",
            "Val acc and loss are 0.8835 and 0.35970414\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.94546 and 0.16048555\n",
            "Val acc and loss are 0.8843 and 0.35967314\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.9455 and 0.15969522\n",
            "Val acc and loss are 0.8847 and 0.3588647\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.94566 and 0.15828077\n",
            "Val acc and loss are 0.8837 and 0.358007\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.94622 and 0.15829024\n",
            "Val acc and loss are 0.8841 and 0.35778838\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.94608 and 0.15836616\n",
            "Val acc and loss are 0.8839 and 0.3575083\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.94632 and 0.15924688\n",
            "Val acc and loss are 0.885 and 0.3584833\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.94632 and 0.15796565\n",
            "Val acc and loss are 0.8833 and 0.3578703\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.9463 and 0.15712717\n",
            "Val acc and loss are 0.8832 and 0.35855967\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.94556 and 0.15901011\n",
            "Val acc and loss are 0.8835 and 0.36105287\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.94616 and 0.15732168\n",
            "Val acc and loss are 0.8843 and 0.35829097\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.94666 and 0.15672067\n",
            "Val acc and loss are 0.8839 and 0.35853502\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.94612 and 0.15766904\n",
            "Val acc and loss are 0.884 and 0.36101568\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.94684 and 0.1552784\n",
            "Val acc and loss are 0.8847 and 0.35741362\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.94734 and 0.15574701\n",
            "Val acc and loss are 0.8844 and 0.35781157\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.94576 and 0.15807118\n",
            "Val acc and loss are 0.8837 and 0.3609606\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.94722 and 0.1548691\n",
            "Val acc and loss are 0.8841 and 0.35749698\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.94726 and 0.15478909\n",
            "Val acc and loss are 0.884 and 0.3591957\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.9462 and 0.1571522\n",
            "Val acc and loss are 0.8823 and 0.36264998\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.94756 and 0.15493637\n",
            "Val acc and loss are 0.8832 and 0.36078858\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.94838 and 0.15321155\n",
            "Val acc and loss are 0.8836 and 0.35823452\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.94798 and 0.15483896\n",
            "Val acc and loss are 0.8849 and 0.3606562\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.94918 and 0.15244895\n",
            "Val acc and loss are 0.8837 and 0.35869423\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.94846 and 0.15267932\n",
            "Val acc and loss are 0.8827 and 0.35985976\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.94772 and 0.15341276\n",
            "Val acc and loss are 0.8844 and 0.36106083\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.94744 and 0.15326408\n",
            "Val acc and loss are 0.8837 and 0.36211553\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.94788 and 0.1526074\n",
            "Val acc and loss are 0.8838 and 0.361941\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.94832 and 0.15213722\n",
            "Val acc and loss are 0.8843 and 0.3606009\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.9477 and 0.15227285\n",
            "Val acc and loss are 0.8852 and 0.360174\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.94864 and 0.1510041\n",
            "Val acc and loss are 0.8851 and 0.3600724\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.94888 and 0.15074322\n",
            "Val acc and loss are 0.8838 and 0.3595232\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.9482 and 0.15164606\n",
            "Val acc and loss are 0.8835 and 0.36120072\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.94906 and 0.15035735\n",
            "Val acc and loss are 0.8848 and 0.3591893\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.94906 and 0.14990479\n",
            "Val acc and loss are 0.8843 and 0.3594316\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.94928 and 0.15004343\n",
            "Val acc and loss are 0.8834 and 0.35955122\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.94908 and 0.14955613\n",
            "Val acc and loss are 0.8846 and 0.3593662\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.9491 and 0.14984755\n",
            "Val acc and loss are 0.8838 and 0.35959408\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.94922 and 0.14911805\n",
            "Val acc and loss are 0.8844 and 0.36013937\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.9499 and 0.1482794\n",
            "Val acc and loss are 0.8847 and 0.36023334\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.94974 and 0.14862555\n",
            "Val acc and loss are 0.8858 and 0.3596625\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.94972 and 0.1478379\n",
            "Val acc and loss are 0.8849 and 0.3589333\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.95028 and 0.14781186\n",
            "Val acc and loss are 0.8844 and 0.3605105\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.94938 and 0.1481686\n",
            "Val acc and loss are 0.8846 and 0.36114296\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.95026 and 0.14744775\n",
            "Val acc and loss are 0.8856 and 0.3609922\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.95026 and 0.14756258\n",
            "Val acc and loss are 0.8849 and 0.36165595\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.94968 and 0.14738524\n",
            "Val acc and loss are 0.8843 and 0.36075863\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.95014 and 0.14729145\n",
            "Val acc and loss are 0.8825 and 0.36165366\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.9512 and 0.1463853\n",
            "Val acc and loss are 0.8836 and 0.36058438\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.95048 and 0.14617437\n",
            "Val acc and loss are 0.8836 and 0.36124846\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.95086 and 0.14731267\n",
            "Val acc and loss are 0.884 and 0.3636102\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.95072 and 0.14565389\n",
            "Val acc and loss are 0.8847 and 0.36064595\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.95078 and 0.1445733\n",
            "Val acc and loss are 0.886 and 0.36047584\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.95064 and 0.14627285\n",
            "Val acc and loss are 0.8841 and 0.36454225\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.95122 and 0.14418322\n",
            "Val acc and loss are 0.8841 and 0.36114955\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.9514 and 0.14391708\n",
            "Val acc and loss are 0.8853 and 0.36195943\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.9504 and 0.14620192\n",
            "Val acc and loss are 0.8831 and 0.36680165\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.95156 and 0.14380752\n",
            "Val acc and loss are 0.8853 and 0.36139044\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.95218 and 0.14328858\n",
            "Val acc and loss are 0.8857 and 0.36003187\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.95066 and 0.14571586\n",
            "Val acc and loss are 0.8854 and 0.3644203\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.9512 and 0.14318016\n",
            "Val acc and loss are 0.8854 and 0.36078915\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.95178 and 0.1432383\n",
            "Val acc and loss are 0.884 and 0.3622397\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.9507 and 0.14412926\n",
            "Val acc and loss are 0.8841 and 0.3641231\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.95184 and 0.14194825\n",
            "Val acc and loss are 0.8838 and 0.36136413\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.95196 and 0.14273888\n",
            "Val acc and loss are 0.885 and 0.3622018\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.95082 and 0.14395554\n",
            "Val acc and loss are 0.8837 and 0.36356732\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.95296 and 0.14170536\n",
            "Val acc and loss are 0.8865 and 0.3620328\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.9523 and 0.1419412\n",
            "Val acc and loss are 0.8843 and 0.36374503\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.95144 and 0.14213271\n",
            "Val acc and loss are 0.8858 and 0.36324394\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.95248 and 0.1416403\n",
            "Val acc and loss are 0.8852 and 0.363613\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.95216 and 0.14122656\n",
            "Val acc and loss are 0.8845 and 0.36182466\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.95234 and 0.14020298\n",
            "Val acc and loss are 0.8849 and 0.36147842\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.95262 and 0.14057925\n",
            "Val acc and loss are 0.8842 and 0.36235437\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.95186 and 0.1416144\n",
            "Val acc and loss are 0.884 and 0.36384994\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.95274 and 0.13986516\n",
            "Val acc and loss are 0.8842 and 0.3628337\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.95358 and 0.1393179\n",
            "Val acc and loss are 0.8844 and 0.36148268\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.95278 and 0.13994664\n",
            "Val acc and loss are 0.8847 and 0.3620191\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.9534 and 0.13949986\n",
            "Val acc and loss are 0.8846 and 0.36243975\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.95436 and 0.13786362\n",
            "Val acc and loss are 0.8856 and 0.36069724\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.95312 and 0.13877836\n",
            "Val acc and loss are 0.8852 and 0.3628341\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.9527 and 0.14077006\n",
            "Val acc and loss are 0.8837 and 0.3676548\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.95386 and 0.13746719\n",
            "Val acc and loss are 0.8846 and 0.36253077\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.95412 and 0.13718995\n",
            "Val acc and loss are 0.8859 and 0.362556\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.9531 and 0.1396592\n",
            "Val acc and loss are 0.8849 and 0.36636138\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.95408 and 0.13674007\n",
            "Val acc and loss are 0.886 and 0.3611552\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.9548 and 0.13561366\n",
            "Val acc and loss are 0.8848 and 0.3624999\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.95342 and 0.1382951\n",
            "Val acc and loss are 0.8844 and 0.36806569\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.95462 and 0.13603246\n",
            "Val acc and loss are 0.8862 and 0.3637788\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.95448 and 0.13627201\n",
            "Val acc and loss are 0.8854 and 0.3636237\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.95386 and 0.13785511\n",
            "Val acc and loss are 0.8835 and 0.36744592\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.95502 and 0.13495506\n",
            "Val acc and loss are 0.8851 and 0.36433044\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.9555 and 0.13458093\n",
            "Val acc and loss are 0.8848 and 0.36440682\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.9536 and 0.1368434\n",
            "Val acc and loss are 0.8852 and 0.36910525\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.95526 and 0.13453093\n",
            "Val acc and loss are 0.8861 and 0.3652023\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.9552 and 0.13456592\n",
            "Val acc and loss are 0.8857 and 0.36393777\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.95356 and 0.13694698\n",
            "Val acc and loss are 0.8849 and 0.36782578\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.95548 and 0.13327534\n",
            "Val acc and loss are 0.886 and 0.36395878\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.956 and 0.13333416\n",
            "Val acc and loss are 0.8863 and 0.36297065\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.95448 and 0.13405755\n",
            "Val acc and loss are 0.8859 and 0.3642757\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.95486 and 0.13461833\n",
            "Val acc and loss are 0.8855 and 0.3661678\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.95644 and 0.13248983\n",
            "Val acc and loss are 0.8865 and 0.36353534\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.95572 and 0.13276328\n",
            "Val acc and loss are 0.8865 and 0.36535558\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.9555 and 0.13265389\n",
            "Val acc and loss are 0.8863 and 0.36601242\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.95588 and 0.13128403\n",
            "Val acc and loss are 0.8859 and 0.36499515\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.95644 and 0.13134041\n",
            "Val acc and loss are 0.8866 and 0.36600813\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.95674 and 0.13154969\n",
            "Val acc and loss are 0.8862 and 0.36706996\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.95568 and 0.13145182\n",
            "Val acc and loss are 0.8854 and 0.36704662\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.95602 and 0.13135235\n",
            "Val acc and loss are 0.8853 and 0.36835206\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.95668 and 0.1304266\n",
            "Val acc and loss are 0.8861 and 0.36629134\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.95626 and 0.13029885\n",
            "Val acc and loss are 0.8863 and 0.36571652\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.9556 and 0.13220838\n",
            "Val acc and loss are 0.8841 and 0.36986792\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.957 and 0.13009836\n",
            "Val acc and loss are 0.8856 and 0.36632085\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.95664 and 0.1303811\n",
            "Val acc and loss are 0.8858 and 0.3666871\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.95658 and 0.13157591\n",
            "Val acc and loss are 0.8841 and 0.3696104\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.9561 and 0.13015676\n",
            "Val acc and loss are 0.8855 and 0.36658442\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.95722 and 0.12938143\n",
            "Val acc and loss are 0.8858 and 0.36565074\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.95688 and 0.13014343\n",
            "Val acc and loss are 0.8861 and 0.36832345\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.95676 and 0.12923676\n",
            "Val acc and loss are 0.8848 and 0.36798102\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.95734 and 0.12820974\n",
            "Val acc and loss are 0.8834 and 0.36589053\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.9559 and 0.130137\n",
            "Val acc and loss are 0.885 and 0.36866605\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.95682 and 0.12841348\n",
            "Val acc and loss are 0.8857 and 0.3679317\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.95698 and 0.12818648\n",
            "Val acc and loss are 0.8849 and 0.36680073\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.95726 and 0.12760755\n",
            "Val acc and loss are 0.8869 and 0.36728698\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.95672 and 0.12930073\n",
            "Val acc and loss are 0.8866 and 0.36983928\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.95738 and 0.12713341\n",
            "Val acc and loss are 0.8845 and 0.36648256\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.95812 and 0.12680101\n",
            "Val acc and loss are 0.885 and 0.366091\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.95762 and 0.12746559\n",
            "Val acc and loss are 0.886 and 0.368292\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.95776 and 0.12686297\n",
            "Val acc and loss are 0.8859 and 0.36813805\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.95794 and 0.1260189\n",
            "Val acc and loss are 0.8861 and 0.36807027\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.95882 and 0.12588038\n",
            "Val acc and loss are 0.8861 and 0.3685289\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.95792 and 0.12599924\n",
            "Val acc and loss are 0.8871 and 0.3683905\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.95816 and 0.12577744\n",
            "Val acc and loss are 0.8865 and 0.36594668\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.95838 and 0.12622999\n",
            "Val acc and loss are 0.8874 and 0.36786506\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.95756 and 0.12621748\n",
            "Val acc and loss are 0.8859 and 0.36910352\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.95858 and 0.12486994\n",
            "Val acc and loss are 0.8859 and 0.3672609\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.95874 and 0.12517722\n",
            "Val acc and loss are 0.8853 and 0.36796016\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.95838 and 0.12501083\n",
            "Val acc and loss are 0.8855 and 0.37017503\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.9589 and 0.124000505\n",
            "Val acc and loss are 0.8853 and 0.36860165\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.95886 and 0.12449628\n",
            "Val acc and loss are 0.886 and 0.37019464\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.95874 and 0.123875536\n",
            "Val acc and loss are 0.8853 and 0.37008652\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.95872 and 0.123762794\n",
            "Val acc and loss are 0.8849 and 0.36835536\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.95886 and 0.12390669\n",
            "Val acc and loss are 0.8856 and 0.36974317\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.95848 and 0.12367834\n",
            "Val acc and loss are 0.8843 and 0.37060973\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.95936 and 0.122600876\n",
            "Val acc and loss are 0.8852 and 0.37014642\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.95912 and 0.1228951\n",
            "Val acc and loss are 0.8854 and 0.37037042\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.95778 and 0.12377814\n",
            "Val acc and loss are 0.8848 and 0.3716045\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.95946 and 0.12224012\n",
            "Val acc and loss are 0.8844 and 0.37079027\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.95928 and 0.12157826\n",
            "Val acc and loss are 0.8855 and 0.36958712\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.95922 and 0.12213535\n",
            "Val acc and loss are 0.8852 and 0.36921218\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.95972 and 0.12170033\n",
            "Val acc and loss are 0.8852 and 0.36894974\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.9601 and 0.121569\n",
            "Val acc and loss are 0.8845 and 0.3697434\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.95914 and 0.12335611\n",
            "Val acc and loss are 0.8846 and 0.37367502\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.95922 and 0.122310214\n",
            "Val acc and loss are 0.8857 and 0.37084055\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.95958 and 0.1215623\n",
            "Val acc and loss are 0.8863 and 0.36899742\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.95904 and 0.12281637\n",
            "Val acc and loss are 0.8853 and 0.37213424\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.95982 and 0.12075507\n",
            "Val acc and loss are 0.8857 and 0.3696215\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.95942 and 0.12173535\n",
            "Val acc and loss are 0.8846 and 0.36985356\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.95926 and 0.12289809\n",
            "Val acc and loss are 0.8853 and 0.37367162\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.95994 and 0.12032078\n",
            "Val acc and loss are 0.8857 and 0.3697802\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.96038 and 0.12139886\n",
            "Val acc and loss are 0.885 and 0.36941615\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.9583 and 0.12367678\n",
            "Val acc and loss are 0.8851 and 0.37370142\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.96012 and 0.12020298\n",
            "Val acc and loss are 0.8856 and 0.37065205\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.95988 and 0.120551445\n",
            "Val acc and loss are 0.8859 and 0.36975816\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.9592 and 0.12190037\n",
            "Val acc and loss are 0.8853 and 0.3729236\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.95966 and 0.11986105\n",
            "Val acc and loss are 0.8848 and 0.37168285\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.96032 and 0.11995617\n",
            "Val acc and loss are 0.8857 and 0.36993212\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.9593 and 0.121237\n",
            "Val acc and loss are 0.8851 and 0.37290773\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.96064 and 0.118624724\n",
            "Val acc and loss are 0.8864 and 0.36954555\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.96078 and 0.118740134\n",
            "Val acc and loss are 0.8862 and 0.36994302\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.96044 and 0.11975933\n",
            "Val acc and loss are 0.8856 and 0.37325612\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.96048 and 0.117944874\n",
            "Val acc and loss are 0.8861 and 0.37125546\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.96128 and 0.11763031\n",
            "Val acc and loss are 0.886 and 0.36996585\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.96044 and 0.119856544\n",
            "Val acc and loss are 0.8844 and 0.37255022\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.96046 and 0.11842741\n",
            "Val acc and loss are 0.8865 and 0.3720489\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.9614 and 0.1172993\n",
            "Val acc and loss are 0.8858 and 0.3709449\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.96102 and 0.11843158\n",
            "Val acc and loss are 0.8851 and 0.374606\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.96064 and 0.11793591\n",
            "Val acc and loss are 0.8851 and 0.37416533\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.96118 and 0.11686545\n",
            "Val acc and loss are 0.8848 and 0.3704951\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.9611 and 0.11705093\n",
            "Val acc and loss are 0.8849 and 0.371069\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.96086 and 0.117093936\n",
            "Val acc and loss are 0.885 and 0.37312263\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.96168 and 0.116557285\n",
            "Val acc and loss are 0.8842 and 0.37226468\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.96146 and 0.11664891\n",
            "Val acc and loss are 0.8856 and 0.37186864\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.96122 and 0.11749545\n",
            "Val acc and loss are 0.8857 and 0.3733293\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.962 and 0.11614832\n",
            "Val acc and loss are 0.8869 and 0.3722857\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.96148 and 0.116080984\n",
            "Val acc and loss are 0.8839 and 0.37257612\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.96124 and 0.116279475\n",
            "Val acc and loss are 0.884 and 0.3752895\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.96212 and 0.11522564\n",
            "Val acc and loss are 0.8856 and 0.3744003\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.9617 and 0.11539065\n",
            "Val acc and loss are 0.8859 and 0.37329027\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.96126 and 0.11592549\n",
            "Val acc and loss are 0.8851 and 0.37567317\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.96248 and 0.11384883\n",
            "Val acc and loss are 0.8865 and 0.373212\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.96226 and 0.11415932\n",
            "Val acc and loss are 0.8852 and 0.37287915\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.96184 and 0.11462506\n",
            "Val acc and loss are 0.8853 and 0.37523624\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.96196 and 0.11401314\n",
            "Val acc and loss are 0.8857 and 0.37401623\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.96272 and 0.11347539\n",
            "Val acc and loss are 0.885 and 0.3731732\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.96218 and 0.11403078\n",
            "Val acc and loss are 0.8836 and 0.37553594\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.96288 and 0.11342311\n",
            "Val acc and loss are 0.8855 and 0.3754831\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.96266 and 0.112810135\n",
            "Val acc and loss are 0.8853 and 0.3744174\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.96288 and 0.112993166\n",
            "Val acc and loss are 0.8851 and 0.3745114\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.96268 and 0.11249896\n",
            "Val acc and loss are 0.8863 and 0.37605298\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.96272 and 0.11210391\n",
            "Val acc and loss are 0.8852 and 0.37641194\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.96258 and 0.11297757\n",
            "Val acc and loss are 0.8848 and 0.37812635\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.96306 and 0.1125183\n",
            "Val acc and loss are 0.8855 and 0.37933287\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.96332 and 0.11176768\n",
            "Val acc and loss are 0.8856 and 0.37721726\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.9635 and 0.11133856\n",
            "Val acc and loss are 0.8846 and 0.375083\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.96282 and 0.11197135\n",
            "Val acc and loss are 0.8862 and 0.37580112\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.96318 and 0.11164496\n",
            "Val acc and loss are 0.8854 and 0.37654427\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.96294 and 0.11132601\n",
            "Val acc and loss are 0.8837 and 0.3759781\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.96316 and 0.11113403\n",
            "Val acc and loss are 0.8859 and 0.37612402\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.96328 and 0.11155792\n",
            "Val acc and loss are 0.8857 and 0.37710664\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.96348 and 0.11057231\n",
            "Val acc and loss are 0.8844 and 0.37663043\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.96356 and 0.110940784\n",
            "Val acc and loss are 0.8854 and 0.37674698\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.96324 and 0.111491956\n",
            "Val acc and loss are 0.8841 and 0.37897643\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.96324 and 0.110687815\n",
            "Val acc and loss are 0.884 and 0.37766045\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.96306 and 0.110836156\n",
            "Val acc and loss are 0.8846 and 0.37655646\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.96286 and 0.111126564\n",
            "Val acc and loss are 0.8856 and 0.37755904\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.96398 and 0.11048601\n",
            "Val acc and loss are 0.8848 and 0.37785426\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.9638 and 0.11078916\n",
            "Val acc and loss are 0.8846 and 0.37717196\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.96398 and 0.11012053\n",
            "Val acc and loss are 0.8844 and 0.3798037\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.96434 and 0.10958636\n",
            "Val acc and loss are 0.8854 and 0.37887812\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.96392 and 0.11008646\n",
            "Val acc and loss are 0.8852 and 0.3776363\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.96362 and 0.11029926\n",
            "Val acc and loss are 0.8849 and 0.37983778\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.96442 and 0.10883847\n",
            "Val acc and loss are 0.8858 and 0.37654957\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.96484 and 0.108228125\n",
            "Val acc and loss are 0.8855 and 0.37651318\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.96446 and 0.10848104\n",
            "Val acc and loss are 0.8857 and 0.37876594\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.965 and 0.107834354\n",
            "Val acc and loss are 0.8858 and 0.37763488\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.9646 and 0.1080859\n",
            "Val acc and loss are 0.8848 and 0.37764692\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.9645 and 0.10887164\n",
            "Val acc and loss are 0.8842 and 0.38009986\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.96456 and 0.10766016\n",
            "Val acc and loss are 0.8851 and 0.37988782\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.9642 and 0.108562686\n",
            "Val acc and loss are 0.8842 and 0.3798839\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.96394 and 0.10983279\n",
            "Val acc and loss are 0.8841 and 0.38248459\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.96406 and 0.10851419\n",
            "Val acc and loss are 0.8842 and 0.38193914\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.96532 and 0.10685201\n",
            "Val acc and loss are 0.8846 and 0.37938055\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.96482 and 0.10749392\n",
            "Val acc and loss are 0.8841 and 0.38018832\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.9641 and 0.108548395\n",
            "Val acc and loss are 0.8844 and 0.38156796\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.9653 and 0.106914796\n",
            "Val acc and loss are 0.8847 and 0.37762418\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.96506 and 0.10661859\n",
            "Val acc and loss are 0.8855 and 0.37959796\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.96458 and 0.10791871\n",
            "Val acc and loss are 0.8844 and 0.38133854\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.96542 and 0.106892936\n",
            "Val acc and loss are 0.8847 and 0.37897357\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.96498 and 0.1065457\n",
            "Val acc and loss are 0.884 and 0.37749174\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.96484 and 0.10725191\n",
            "Val acc and loss are 0.8853 and 0.37830856\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.96556 and 0.10649683\n",
            "Val acc and loss are 0.8856 and 0.37806183\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.96564 and 0.10634371\n",
            "Val acc and loss are 0.8854 and 0.37758496\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.96428 and 0.10752791\n",
            "Val acc and loss are 0.8856 and 0.37910014\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.9653 and 0.106235676\n",
            "Val acc and loss are 0.886 and 0.37889808\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.96592 and 0.105941296\n",
            "Val acc and loss are 0.8855 and 0.37683693\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.96376 and 0.1081948\n",
            "Val acc and loss are 0.8864 and 0.37796474\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.96582 and 0.105625294\n",
            "Val acc and loss are 0.8852 and 0.37896875\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.9657 and 0.106373265\n",
            "Val acc and loss are 0.8836 and 0.37938285\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.96492 and 0.106594235\n",
            "Val acc and loss are 0.8855 and 0.37998432\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.96504 and 0.10670599\n",
            "Val acc and loss are 0.8858 and 0.38069326\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.96574 and 0.10528137\n",
            "Val acc and loss are 0.8852 and 0.38044268\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.96584 and 0.10478024\n",
            "Val acc and loss are 0.8844 and 0.38015795\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.96544 and 0.105379984\n",
            "Val acc and loss are 0.8841 and 0.38179156\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.96586 and 0.10498755\n",
            "Val acc and loss are 0.885 and 0.3800308\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.96536 and 0.104950294\n",
            "Val acc and loss are 0.884 and 0.38010767\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.96602 and 0.1041421\n",
            "Val acc and loss are 0.8851 and 0.37976408\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.9655 and 0.105861895\n",
            "Val acc and loss are 0.8861 and 0.3822848\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.96658 and 0.10387939\n",
            "Val acc and loss are 0.8851 and 0.38191432\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.96612 and 0.10421068\n",
            "Val acc and loss are 0.8843 and 0.38078505\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.9663 and 0.10349326\n",
            "Val acc and loss are 0.8855 and 0.3792167\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.96626 and 0.10300801\n",
            "Val acc and loss are 0.8849 and 0.37949845\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.9665 and 0.10284795\n",
            "Val acc and loss are 0.8844 and 0.3799303\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.96672 and 0.10264092\n",
            "Val acc and loss are 0.8858 and 0.38046393\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.9662 and 0.103438064\n",
            "Val acc and loss are 0.8863 and 0.38148186\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.9672 and 0.10229938\n",
            "Val acc and loss are 0.8859 and 0.37909168\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.96644 and 0.102785096\n",
            "Val acc and loss are 0.8852 and 0.38161215\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.96658 and 0.10274881\n",
            "Val acc and loss are 0.8847 and 0.3839968\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.96708 and 0.10186417\n",
            "Val acc and loss are 0.8871 and 0.38005635\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.96698 and 0.101704024\n",
            "Val acc and loss are 0.8848 and 0.38049403\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.9665 and 0.10263257\n",
            "Val acc and loss are 0.8852 and 0.38447833\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.9675 and 0.10095768\n",
            "Val acc and loss are 0.8853 and 0.38020036\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.96746 and 0.10078351\n",
            "Val acc and loss are 0.8852 and 0.38026395\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.9665 and 0.10255488\n",
            "Val acc and loss are 0.8853 and 0.38629264\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.96724 and 0.10092656\n",
            "Val acc and loss are 0.8848 and 0.38174513\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.96732 and 0.10161448\n",
            "Val acc and loss are 0.8846 and 0.38028678\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.96738 and 0.10197871\n",
            "Val acc and loss are 0.8862 and 0.38422725\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.96752 and 0.10096801\n",
            "Val acc and loss are 0.8862 and 0.3829095\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.96808 and 0.100098655\n",
            "Val acc and loss are 0.8852 and 0.38018072\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.96684 and 0.100884356\n",
            "Val acc and loss are 0.8837 and 0.38299254\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.96712 and 0.10147105\n",
            "Val acc and loss are 0.8859 and 0.3851065\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.96844 and 0.09996926\n",
            "Val acc and loss are 0.8858 and 0.38060617\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.96738 and 0.10097086\n",
            "Val acc and loss are 0.8839 and 0.38264927\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.96728 and 0.101260915\n",
            "Val acc and loss are 0.8858 and 0.38477123\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.9678 and 0.09922521\n",
            "Val acc and loss are 0.8857 and 0.38191405\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.96758 and 0.09964938\n",
            "Val acc and loss are 0.8848 and 0.3821961\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.96738 and 0.09952574\n",
            "Val acc and loss are 0.8849 and 0.383627\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.96816 and 0.09851126\n",
            "Val acc and loss are 0.8855 and 0.38410687\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.9682 and 0.09839094\n",
            "Val acc and loss are 0.8842 and 0.3836415\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.96848 and 0.09861477\n",
            "Val acc and loss are 0.885 and 0.3848265\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.96796 and 0.09870516\n",
            "Val acc and loss are 0.8845 and 0.38589337\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.96846 and 0.09830624\n",
            "Val acc and loss are 0.884 and 0.3847455\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.96854 and 0.09850586\n",
            "Val acc and loss are 0.8836 and 0.38563612\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.96804 and 0.09872854\n",
            "Val acc and loss are 0.8846 and 0.3860961\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.9694 and 0.09723805\n",
            "Val acc and loss are 0.8837 and 0.38286278\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.96874 and 0.09745372\n",
            "Val acc and loss are 0.8841 and 0.38322312\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.96826 and 0.09863489\n",
            "Val acc and loss are 0.8848 and 0.38596123\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.96864 and 0.097649984\n",
            "Val acc and loss are 0.8837 and 0.38555542\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.96896 and 0.097287074\n",
            "Val acc and loss are 0.8833 and 0.3843926\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.96762 and 0.09872666\n",
            "Val acc and loss are 0.8851 and 0.3867172\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.96828 and 0.098163575\n",
            "Val acc and loss are 0.8851 and 0.38629076\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.96914 and 0.09702996\n",
            "Val acc and loss are 0.8834 and 0.38354638\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.9693 and 0.096978694\n",
            "Val acc and loss are 0.8849 and 0.3841486\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.96818 and 0.09747129\n",
            "Val acc and loss are 0.8859 and 0.38660187\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.9689 and 0.09697585\n",
            "Val acc and loss are 0.8847 and 0.38520676\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.96908 and 0.09633346\n",
            "Val acc and loss are 0.8849 and 0.38658726\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.96888 and 0.09622223\n",
            "Val acc and loss are 0.8841 and 0.38772485\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.96924 and 0.096664645\n",
            "Val acc and loss are 0.8862 and 0.3867286\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.96914 and 0.096529834\n",
            "Val acc and loss are 0.8861 and 0.38601476\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.96868 and 0.09649007\n",
            "Val acc and loss are 0.8853 and 0.387073\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.96866 and 0.09690994\n",
            "Val acc and loss are 0.8848 and 0.38810018\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.96916 and 0.0959163\n",
            "Val acc and loss are 0.885 and 0.38552517\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.96934 and 0.09597861\n",
            "Val acc and loss are 0.8838 and 0.384705\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.96968 and 0.09568795\n",
            "Val acc and loss are 0.8849 and 0.38451806\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.9696 and 0.095303416\n",
            "Val acc and loss are 0.8841 and 0.3858263\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.97016 and 0.094908446\n",
            "Val acc and loss are 0.8835 and 0.3851768\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.96978 and 0.095054656\n",
            "Val acc and loss are 0.8848 and 0.38548476\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.9696 and 0.09529525\n",
            "Val acc and loss are 0.8856 and 0.38616958\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.97018 and 0.094771445\n",
            "Val acc and loss are 0.8839 and 0.38474813\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.96988 and 0.09440873\n",
            "Val acc and loss are 0.8838 and 0.38715568\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.96934 and 0.09504576\n",
            "Val acc and loss are 0.8841 and 0.3885187\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.96982 and 0.0945269\n",
            "Val acc and loss are 0.8853 and 0.38565934\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.96914 and 0.09490881\n",
            "Val acc and loss are 0.8846 and 0.38685787\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.96904 and 0.09515996\n",
            "Val acc and loss are 0.8851 and 0.38795587\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.9705 and 0.093948476\n",
            "Val acc and loss are 0.8855 and 0.38428867\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.9698 and 0.094171464\n",
            "Val acc and loss are 0.8844 and 0.38577127\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.96914 and 0.09581662\n",
            "Val acc and loss are 0.8843 and 0.39048535\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.96984 and 0.094256185\n",
            "Val acc and loss are 0.8848 and 0.38774574\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.97002 and 0.09378756\n",
            "Val acc and loss are 0.8844 and 0.38794702\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.96984 and 0.09398213\n",
            "Val acc and loss are 0.8853 and 0.38997105\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.96988 and 0.09396295\n",
            "Val acc and loss are 0.8838 and 0.38740763\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.96996 and 0.09341875\n",
            "Val acc and loss are 0.883 and 0.3875327\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.97 and 0.09369848\n",
            "Val acc and loss are 0.883 and 0.39129952\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.96976 and 0.093760915\n",
            "Val acc and loss are 0.8834 and 0.39081427\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.9706 and 0.09237125\n",
            "Val acc and loss are 0.8825 and 0.3896161\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.97026 and 0.09271164\n",
            "Val acc and loss are 0.883 and 0.3889295\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.97034 and 0.09321537\n",
            "Val acc and loss are 0.8852 and 0.38702902\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.97076 and 0.09224184\n",
            "Val acc and loss are 0.884 and 0.38549247\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.97032 and 0.0921157\n",
            "Val acc and loss are 0.8845 and 0.38672444\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.97098 and 0.092100374\n",
            "Val acc and loss are 0.8835 and 0.38700145\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.97124 and 0.091497615\n",
            "Val acc and loss are 0.8841 and 0.38680008\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.97082 and 0.09173501\n",
            "Val acc and loss are 0.8841 and 0.38695937\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.97068 and 0.09213079\n",
            "Val acc and loss are 0.885 and 0.38836128\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.97162 and 0.09115967\n",
            "Val acc and loss are 0.884 and 0.38840294\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.97122 and 0.09146615\n",
            "Val acc and loss are 0.8839 and 0.3891391\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.97056 and 0.09210838\n",
            "Val acc and loss are 0.8842 and 0.39084038\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.97084 and 0.091252536\n",
            "Val acc and loss are 0.8842 and 0.39030084\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.9716 and 0.090019666\n",
            "Val acc and loss are 0.8831 and 0.38948727\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.97186 and 0.0898941\n",
            "Val acc and loss are 0.885 and 0.3893835\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.97074 and 0.09126013\n",
            "Val acc and loss are 0.8848 and 0.39323252\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.97142 and 0.08990427\n",
            "Val acc and loss are 0.8818 and 0.39100194\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.97116 and 0.09023299\n",
            "Val acc and loss are 0.8825 and 0.3922009\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.97004 and 0.09159529\n",
            "Val acc and loss are 0.8838 and 0.39446813\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.9716 and 0.08982411\n",
            "Val acc and loss are 0.8824 and 0.39055\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.9713 and 0.089348465\n",
            "Val acc and loss are 0.8841 and 0.38954914\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.97102 and 0.09052567\n",
            "Val acc and loss are 0.884 and 0.39144486\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.97182 and 0.08989582\n",
            "Val acc and loss are 0.8848 and 0.3893564\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.97124 and 0.08982702\n",
            "Val acc and loss are 0.8842 and 0.3905124\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.9706 and 0.09058553\n",
            "Val acc and loss are 0.8835 and 0.39102665\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.97214 and 0.08906363\n",
            "Val acc and loss are 0.8841 and 0.3880434\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.97212 and 0.08870426\n",
            "Val acc and loss are 0.8846 and 0.38969478\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9713 and 0.08986745\n",
            "Val acc and loss are 0.8843 and 0.3917582\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.97172 and 0.08910391\n",
            "Val acc and loss are 0.8853 and 0.39006636\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.97164 and 0.08934164\n",
            "Val acc and loss are 0.8842 and 0.39075404\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.97136 and 0.08915131\n",
            "Val acc and loss are 0.8849 and 0.39118588\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.9725 and 0.088110246\n",
            "Val acc and loss are 0.883 and 0.39060873\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.97178 and 0.08903329\n",
            "Val acc and loss are 0.8837 and 0.3932469\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.97196 and 0.0887015\n",
            "Val acc and loss are 0.8838 and 0.3929097\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.97204 and 0.08847881\n",
            "Val acc and loss are 0.885 and 0.39350188\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.97178 and 0.088765584\n",
            "Val acc and loss are 0.8847 and 0.39483684\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.97186 and 0.08816\n",
            "Val acc and loss are 0.8833 and 0.39476088\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.97258 and 0.08703491\n",
            "Val acc and loss are 0.8835 and 0.3937318\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.97244 and 0.08774854\n",
            "Val acc and loss are 0.8836 and 0.39329657\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.97194 and 0.088126354\n",
            "Val acc and loss are 0.8847 and 0.39264417\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.97256 and 0.087698884\n",
            "Val acc and loss are 0.8849 and 0.39226428\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.9721 and 0.08820368\n",
            "Val acc and loss are 0.8832 and 0.3952336\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.97224 and 0.088376835\n",
            "Val acc and loss are 0.8842 and 0.39544502\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.97234 and 0.0880892\n",
            "Val acc and loss are 0.883 and 0.39495796\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.97168 and 0.08899868\n",
            "Val acc and loss are 0.8826 and 0.39595824\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.97202 and 0.088108204\n",
            "Val acc and loss are 0.8837 and 0.39303145\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.97302 and 0.086743124\n",
            "Val acc and loss are 0.8843 and 0.39031303\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.97192 and 0.0875423\n",
            "Val acc and loss are 0.8831 and 0.39396846\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.97198 and 0.08760913\n",
            "Val acc and loss are 0.8842 and 0.39567432\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.97264 and 0.08700389\n",
            "Val acc and loss are 0.8846 and 0.39314917\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.97156 and 0.08850131\n",
            "Val acc and loss are 0.8836 and 0.3952929\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.97178 and 0.087559685\n",
            "Val acc and loss are 0.8852 and 0.39517182\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.97318 and 0.08628872\n",
            "Val acc and loss are 0.8843 and 0.39272764\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.9725 and 0.08710912\n",
            "Val acc and loss are 0.8829 and 0.39149946\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.972 and 0.08748977\n",
            "Val acc and loss are 0.8831 and 0.3931724\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.97268 and 0.086059704\n",
            "Val acc and loss are 0.8846 and 0.39185107\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.9728 and 0.08662942\n",
            "Val acc and loss are 0.8838 and 0.39305797\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.97184 and 0.087983996\n",
            "Val acc and loss are 0.8836 and 0.39509735\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.97328 and 0.085915685\n",
            "Val acc and loss are 0.8843 and 0.39417017\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.97304 and 0.08594939\n",
            "Val acc and loss are 0.8836 and 0.39466727\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.97276 and 0.08599958\n",
            "Val acc and loss are 0.8841 and 0.3941034\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.97266 and 0.0856734\n",
            "Val acc and loss are 0.8832 and 0.39584127\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.973 and 0.08522889\n",
            "Val acc and loss are 0.8834 and 0.396157\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.97298 and 0.08510834\n",
            "Val acc and loss are 0.883 and 0.39543185\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.97294 and 0.08547448\n",
            "Val acc and loss are 0.8838 and 0.39793867\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.97306 and 0.085267246\n",
            "Val acc and loss are 0.8831 and 0.3997356\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.97326 and 0.084660925\n",
            "Val acc and loss are 0.8842 and 0.39722377\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.9731 and 0.085200086\n",
            "Val acc and loss are 0.8841 and 0.39681715\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.97378 and 0.08411483\n",
            "Val acc and loss are 0.8854 and 0.39615685\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.97322 and 0.084686875\n",
            "Val acc and loss are 0.8834 and 0.39662793\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.97326 and 0.084471405\n",
            "Val acc and loss are 0.8845 and 0.39594638\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.97376 and 0.08394334\n",
            "Val acc and loss are 0.8855 and 0.39342356\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.97346 and 0.08473643\n",
            "Val acc and loss are 0.8844 and 0.39430508\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.97364 and 0.0840864\n",
            "Val acc and loss are 0.8828 and 0.39509365\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.97446 and 0.0835598\n",
            "Val acc and loss are 0.8835 and 0.39439392\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.97322 and 0.08489614\n",
            "Val acc and loss are 0.8845 and 0.39562485\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.97352 and 0.083930984\n",
            "Val acc and loss are 0.8846 and 0.39334062\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.97344 and 0.08404834\n",
            "Val acc and loss are 0.8825 and 0.39398268\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.97228 and 0.08589656\n",
            "Val acc and loss are 0.884 and 0.39642042\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.97298 and 0.084326915\n",
            "Val acc and loss are 0.8845 and 0.39496014\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.97328 and 0.08478456\n",
            "Val acc and loss are 0.8829 and 0.39588982\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.97298 and 0.0851827\n",
            "Val acc and loss are 0.883 and 0.39640012\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.97336 and 0.08470857\n",
            "Val acc and loss are 0.885 and 0.3951052\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.9727 and 0.084833734\n",
            "Val acc and loss are 0.8849 and 0.39633405\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.97268 and 0.08518236\n",
            "Val acc and loss are 0.883 and 0.39777938\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.97336 and 0.08348968\n",
            "Val acc and loss are 0.8843 and 0.39710167\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.97276 and 0.08531062\n",
            "Val acc and loss are 0.8843 and 0.39693174\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.97292 and 0.084541544\n",
            "Val acc and loss are 0.8855 and 0.39805064\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.97292 and 0.083727196\n",
            "Val acc and loss are 0.8846 and 0.3974675\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.9736 and 0.083678246\n",
            "Val acc and loss are 0.8838 and 0.39676797\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.97348 and 0.08389725\n",
            "Val acc and loss are 0.8856 and 0.39633265\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.97442 and 0.08201133\n",
            "Val acc and loss are 0.8848 and 0.39540863\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.97386 and 0.08320717\n",
            "Val acc and loss are 0.8837 and 0.39628595\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.97378 and 0.082865275\n",
            "Val acc and loss are 0.8848 and 0.39571396\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.9745 and 0.08206443\n",
            "Val acc and loss are 0.8846 and 0.3941223\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.97446 and 0.081252374\n",
            "Val acc and loss are 0.8832 and 0.39583033\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.97468 and 0.0816188\n",
            "Val acc and loss are 0.8846 and 0.3981697\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.9739 and 0.08256879\n",
            "Val acc and loss are 0.885 and 0.3960121\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.9742 and 0.08200308\n",
            "Val acc and loss are 0.8855 and 0.39513403\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.9747 and 0.08125395\n",
            "Val acc and loss are 0.8848 and 0.39642602\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.97474 and 0.08169253\n",
            "Val acc and loss are 0.8831 and 0.3969765\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.97468 and 0.08158365\n",
            "Val acc and loss are 0.8818 and 0.39779806\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.97472 and 0.08111708\n",
            "Val acc and loss are 0.8839 and 0.39787874\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.97414 and 0.08237891\n",
            "Val acc and loss are 0.8846 and 0.3987472\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.9743 and 0.08186491\n",
            "Val acc and loss are 0.8838 and 0.39797276\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.97452 and 0.081131496\n",
            "Val acc and loss are 0.8841 and 0.39668837\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.9747 and 0.08097392\n",
            "Val acc and loss are 0.8844 and 0.3986264\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.9744 and 0.08150903\n",
            "Val acc and loss are 0.8836 and 0.40083078\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.97464 and 0.0809648\n",
            "Val acc and loss are 0.8838 and 0.39816943\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.97432 and 0.08157795\n",
            "Val acc and loss are 0.8837 and 0.39960778\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.97362 and 0.08235936\n",
            "Val acc and loss are 0.8842 and 0.40227905\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.97466 and 0.080559276\n",
            "Val acc and loss are 0.884 and 0.39757323\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.97528 and 0.079800695\n",
            "Val acc and loss are 0.8851 and 0.39710358\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.97334 and 0.082989104\n",
            "Val acc and loss are 0.8838 and 0.4031617\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.97554 and 0.079862565\n",
            "Val acc and loss are 0.8848 and 0.39589676\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.97566 and 0.07992129\n",
            "Val acc and loss are 0.8836 and 0.39509585\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.97312 and 0.08340021\n",
            "Val acc and loss are 0.8838 and 0.40399304\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.97522 and 0.07944567\n",
            "Val acc and loss are 0.8839 and 0.3979314\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.97512 and 0.07937799\n",
            "Val acc and loss are 0.884 and 0.3961219\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.97404 and 0.08124788\n",
            "Val acc and loss are 0.8856 and 0.39962232\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.9749 and 0.07972462\n",
            "Val acc and loss are 0.884 and 0.39919814\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.97552 and 0.078832164\n",
            "Val acc and loss are 0.8834 and 0.39607543\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.97496 and 0.079423904\n",
            "Val acc and loss are 0.8835 and 0.39630046\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.97552 and 0.07950894\n",
            "Val acc and loss are 0.8848 and 0.39696655\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.97542 and 0.07978257\n",
            "Val acc and loss are 0.8849 and 0.3975426\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.97546 and 0.07912539\n",
            "Val acc and loss are 0.8848 and 0.3981982\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.97524 and 0.07908637\n",
            "Val acc and loss are 0.8833 and 0.39870092\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.97564 and 0.07889292\n",
            "Val acc and loss are 0.8825 and 0.40020463\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.97506 and 0.079338275\n",
            "Val acc and loss are 0.8849 and 0.4000967\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.97514 and 0.07884394\n",
            "Val acc and loss are 0.8844 and 0.39893663\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.9755 and 0.078239396\n",
            "Val acc and loss are 0.8846 and 0.39922088\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.9753 and 0.07892719\n",
            "Val acc and loss are 0.8841 and 0.4026618\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.97528 and 0.07901625\n",
            "Val acc and loss are 0.8843 and 0.40243062\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.97572 and 0.07878896\n",
            "Val acc and loss are 0.8824 and 0.40058994\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.9758 and 0.07825843\n",
            "Val acc and loss are 0.8837 and 0.40213558\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.97544 and 0.07868127\n",
            "Val acc and loss are 0.8853 and 0.40221915\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.97566 and 0.07808621\n",
            "Val acc and loss are 0.8846 and 0.4000291\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.9756 and 0.07788077\n",
            "Val acc and loss are 0.8835 and 0.40138534\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.9758 and 0.077903666\n",
            "Val acc and loss are 0.8846 and 0.40173852\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.9757 and 0.07818087\n",
            "Val acc and loss are 0.884 and 0.40185454\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.97582 and 0.07804269\n",
            "Val acc and loss are 0.8836 and 0.4033298\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.9762 and 0.077309616\n",
            "Val acc and loss are 0.8834 and 0.40178978\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.97602 and 0.07700047\n",
            "Val acc and loss are 0.8827 and 0.4012591\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.9759 and 0.07751566\n",
            "Val acc and loss are 0.8833 and 0.4030232\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.97594 and 0.076762855\n",
            "Val acc and loss are 0.8839 and 0.40177563\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.97644 and 0.07611139\n",
            "Val acc and loss are 0.8846 and 0.40104264\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.97626 and 0.07642916\n",
            "Val acc and loss are 0.8835 and 0.4026939\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.97608 and 0.07665801\n",
            "Val acc and loss are 0.8834 and 0.40366822\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.976 and 0.0761559\n",
            "Val acc and loss are 0.8835 and 0.40424913\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.97612 and 0.07617621\n",
            "Val acc and loss are 0.8833 and 0.40311542\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.97654 and 0.07587466\n",
            "Val acc and loss are 0.8842 and 0.4025134\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.97662 and 0.07568508\n",
            "Val acc and loss are 0.884 and 0.4023183\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.9763 and 0.076061115\n",
            "Val acc and loss are 0.8853 and 0.40357685\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.97642 and 0.075685084\n",
            "Val acc and loss are 0.8855 and 0.40360573\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.97632 and 0.07532599\n",
            "Val acc and loss are 0.8849 and 0.4032346\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.9764 and 0.07569472\n",
            "Val acc and loss are 0.8835 and 0.40315935\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.97624 and 0.07569275\n",
            "Val acc and loss are 0.8841 and 0.4022941\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.97656 and 0.075662315\n",
            "Val acc and loss are 0.8855 and 0.40152392\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.9765 and 0.07530251\n",
            "Val acc and loss are 0.8844 and 0.40255487\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.97652 and 0.07556974\n",
            "Val acc and loss are 0.8838 and 0.4031728\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.97642 and 0.075801454\n",
            "Val acc and loss are 0.8829 and 0.40236783\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.97596 and 0.07633065\n",
            "Val acc and loss are 0.8834 and 0.40439585\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.97652 and 0.075541146\n",
            "Val acc and loss are 0.8846 and 0.40212697\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.97662 and 0.07538307\n",
            "Val acc and loss are 0.8834 and 0.40136236\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.97644 and 0.07550731\n",
            "Val acc and loss are 0.883 and 0.4034609\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.97682 and 0.07441057\n",
            "Val acc and loss are 0.8839 and 0.40172464\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.9774 and 0.074093126\n",
            "Val acc and loss are 0.8826 and 0.40035248\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.9758 and 0.076722756\n",
            "Val acc and loss are 0.8829 and 0.40454343\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.97708 and 0.07487492\n",
            "Val acc and loss are 0.884 and 0.40173274\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.97702 and 0.07513995\n",
            "Val acc and loss are 0.883 and 0.40160707\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.97642 and 0.07496626\n",
            "Val acc and loss are 0.8842 and 0.40495\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.97702 and 0.07448319\n",
            "Val acc and loss are 0.8847 and 0.40467665\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.97676 and 0.074558005\n",
            "Val acc and loss are 0.8847 and 0.4042234\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.97618 and 0.07515171\n",
            "Val acc and loss are 0.8855 and 0.40562242\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.97692 and 0.07422019\n",
            "Val acc and loss are 0.8855 and 0.405606\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.9771 and 0.074163325\n",
            "Val acc and loss are 0.8833 and 0.40384045\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.97732 and 0.07386463\n",
            "Val acc and loss are 0.8835 and 0.40422183\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.97626 and 0.07554679\n",
            "Val acc and loss are 0.8843 and 0.40834156\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.97656 and 0.07459228\n",
            "Val acc and loss are 0.8849 and 0.4062109\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.97686 and 0.07478639\n",
            "Val acc and loss are 0.8836 and 0.40458313\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.97702 and 0.07431052\n",
            "Val acc and loss are 0.8822 and 0.4058811\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.9774 and 0.07417221\n",
            "Val acc and loss are 0.884 and 0.405133\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.9775 and 0.07382193\n",
            "Val acc and loss are 0.8838 and 0.4015787\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.97718 and 0.07471554\n",
            "Val acc and loss are 0.8847 and 0.40383515\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.97714 and 0.074939005\n",
            "Val acc and loss are 0.8833 and 0.4081941\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.97788 and 0.07362278\n",
            "Val acc and loss are 0.883 and 0.40523452\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.97684 and 0.074924335\n",
            "Val acc and loss are 0.8832 and 0.40404642\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.97606 and 0.075542346\n",
            "Val acc and loss are 0.8854 and 0.40448892\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.97714 and 0.07436674\n",
            "Val acc and loss are 0.8857 and 0.40231875\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.9775 and 0.07363916\n",
            "Val acc and loss are 0.8846 and 0.4023611\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.97692 and 0.07388498\n",
            "Val acc and loss are 0.8837 and 0.4045816\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.9775 and 0.07341711\n",
            "Val acc and loss are 0.8838 and 0.40529868\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.97756 and 0.073292956\n",
            "Val acc and loss are 0.8848 and 0.4032582\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.9771 and 0.07389376\n",
            "Val acc and loss are 0.8851 and 0.40496868\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.97762 and 0.07291995\n",
            "Val acc and loss are 0.8856 and 0.40743694\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.97786 and 0.072606005\n",
            "Val acc and loss are 0.8843 and 0.4069291\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.9772 and 0.07360692\n",
            "Val acc and loss are 0.8836 and 0.40753368\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.97726 and 0.07336976\n",
            "Val acc and loss are 0.8856 and 0.406819\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.9774 and 0.07273482\n",
            "Val acc and loss are 0.8837 and 0.4069534\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.9772 and 0.07328161\n",
            "Val acc and loss are 0.8847 and 0.4076213\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.97766 and 0.07254924\n",
            "Val acc and loss are 0.8829 and 0.40515795\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.9776 and 0.072525255\n",
            "Val acc and loss are 0.8855 and 0.40884572\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.97672 and 0.07354891\n",
            "Val acc and loss are 0.8853 and 0.4092305\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.9779 and 0.07202644\n",
            "Val acc and loss are 0.886 and 0.40498722\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.97816 and 0.07153421\n",
            "Val acc and loss are 0.8859 and 0.40809843\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.97772 and 0.07258812\n",
            "Val acc and loss are 0.8853 and 0.40943304\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.97794 and 0.07158716\n",
            "Val acc and loss are 0.8856 and 0.40513113\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.978 and 0.07127393\n",
            "Val acc and loss are 0.8839 and 0.40641642\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.9777 and 0.071930945\n",
            "Val acc and loss are 0.8852 and 0.41078016\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.97786 and 0.07130779\n",
            "Val acc and loss are 0.8852 and 0.40999156\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.97826 and 0.07124823\n",
            "Val acc and loss are 0.8844 and 0.4091911\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.9776 and 0.07206909\n",
            "Val acc and loss are 0.8839 and 0.41146034\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.97724 and 0.07256559\n",
            "Val acc and loss are 0.8844 and 0.41093615\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.97762 and 0.071900114\n",
            "Val acc and loss are 0.8844 and 0.40887773\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.97838 and 0.071333356\n",
            "Val acc and loss are 0.8827 and 0.4074215\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.9776 and 0.07238962\n",
            "Val acc and loss are 0.8842 and 0.40935862\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.97788 and 0.07228031\n",
            "Val acc and loss are 0.884 and 0.40992618\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.97782 and 0.07240089\n",
            "Val acc and loss are 0.8832 and 0.40986675\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.97876 and 0.07102862\n",
            "Val acc and loss are 0.8833 and 0.40655473\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.97758 and 0.071635924\n",
            "Val acc and loss are 0.8837 and 0.40716976\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.9781 and 0.07147444\n",
            "Val acc and loss are 0.884 and 0.40813354\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.9782 and 0.071474425\n",
            "Val acc and loss are 0.8836 and 0.4065419\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.9778 and 0.07155265\n",
            "Val acc and loss are 0.8846 and 0.40673718\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.97838 and 0.07059392\n",
            "Val acc and loss are 0.8855 and 0.40729427\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.97856 and 0.070507824\n",
            "Val acc and loss are 0.884 and 0.40832537\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.97818 and 0.070695266\n",
            "Val acc and loss are 0.884 and 0.40761647\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.97838 and 0.070341974\n",
            "Val acc and loss are 0.8843 and 0.4071652\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.97806 and 0.07087294\n",
            "Val acc and loss are 0.8845 and 0.4087418\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.9774 and 0.0715998\n",
            "Val acc and loss are 0.8839 and 0.4083562\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.9782 and 0.07077359\n",
            "Val acc and loss are 0.8828 and 0.4062752\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.97836 and 0.070444725\n",
            "Val acc and loss are 0.8839 and 0.4080544\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.97748 and 0.07192227\n",
            "Val acc and loss are 0.8843 and 0.41125992\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.9784 and 0.06995338\n",
            "Val acc and loss are 0.8836 and 0.4065115\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.97836 and 0.07083676\n",
            "Val acc and loss are 0.8831 and 0.40833297\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.9781 and 0.070894025\n",
            "Val acc and loss are 0.8837 and 0.41240263\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.97794 and 0.070830725\n",
            "Val acc and loss are 0.8846 and 0.41069278\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.9782 and 0.070336565\n",
            "Val acc and loss are 0.8835 and 0.40842417\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.9781 and 0.07117903\n",
            "Val acc and loss are 0.8847 and 0.40905264\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.97768 and 0.07171169\n",
            "Val acc and loss are 0.8851 and 0.4109449\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.97834 and 0.070139855\n",
            "Val acc and loss are 0.8841 and 0.41035888\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.97876 and 0.06999022\n",
            "Val acc and loss are 0.8825 and 0.4100224\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.97836 and 0.070629984\n",
            "Val acc and loss are 0.8845 and 0.40958068\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.97872 and 0.06976352\n",
            "Val acc and loss are 0.8847 and 0.4101782\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.97888 and 0.069469795\n",
            "Val acc and loss are 0.8853 and 0.40868878\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.97894 and 0.06953419\n",
            "Val acc and loss are 0.884 and 0.40853184\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.97928 and 0.06852676\n",
            "Val acc and loss are 0.885 and 0.40849316\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.97914 and 0.06908336\n",
            "Val acc and loss are 0.8846 and 0.4101108\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.9791 and 0.06904587\n",
            "Val acc and loss are 0.8847 and 0.40953842\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.97918 and 0.068570994\n",
            "Val acc and loss are 0.8858 and 0.4083183\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.97852 and 0.069721624\n",
            "Val acc and loss are 0.8842 and 0.41229898\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.978 and 0.070451885\n",
            "Val acc and loss are 0.8829 and 0.41350323\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.97862 and 0.068749286\n",
            "Val acc and loss are 0.8842 and 0.41021156\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.97832 and 0.06967852\n",
            "Val acc and loss are 0.8836 and 0.40978256\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.9772 and 0.07061196\n",
            "Val acc and loss are 0.8831 and 0.4113501\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.9785 and 0.06889701\n",
            "Val acc and loss are 0.8834 and 0.40863597\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.97892 and 0.06927015\n",
            "Val acc and loss are 0.8819 and 0.40941733\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.97894 and 0.068421654\n",
            "Val acc and loss are 0.8835 and 0.40840146\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.97782 and 0.070099294\n",
            "Val acc and loss are 0.8845 and 0.40921587\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.97914 and 0.06868645\n",
            "Val acc and loss are 0.8853 and 0.40726557\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.97892 and 0.068105295\n",
            "Val acc and loss are 0.8859 and 0.40776435\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.9783 and 0.06971122\n",
            "Val acc and loss are 0.8842 and 0.4112158\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.9798 and 0.06705135\n",
            "Val acc and loss are 0.8834 and 0.40861118\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.97966 and 0.06741465\n",
            "Val acc and loss are 0.8838 and 0.4077011\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.97928 and 0.06850804\n",
            "Val acc and loss are 0.8845 and 0.408869\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.9795 and 0.06755024\n",
            "Val acc and loss are 0.8842 and 0.41029432\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.97964 and 0.06728927\n",
            "Val acc and loss are 0.8834 and 0.4105603\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.97976 and 0.06726715\n",
            "Val acc and loss are 0.8833 and 0.41057813\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.97968 and 0.06707561\n",
            "Val acc and loss are 0.885 and 0.40970552\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.98006 and 0.066967614\n",
            "Val acc and loss are 0.8842 and 0.40848348\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.97952 and 0.067449845\n",
            "Val acc and loss are 0.8846 and 0.40874663\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.9797 and 0.0668435\n",
            "Val acc and loss are 0.886 and 0.40789706\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.98 and 0.066501\n",
            "Val acc and loss are 0.8851 and 0.40954798\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.9792 and 0.06824412\n",
            "Val acc and loss are 0.8834 and 0.41467023\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.97986 and 0.06672013\n",
            "Val acc and loss are 0.8853 and 0.41158026\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.97996 and 0.06664595\n",
            "Val acc and loss are 0.8852 and 0.410616\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.97966 and 0.06681831\n",
            "Val acc and loss are 0.884 and 0.4126843\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.98006 and 0.066507116\n",
            "Val acc and loss are 0.8835 and 0.4127352\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.97972 and 0.066460006\n",
            "Val acc and loss are 0.8843 and 0.411343\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.9797 and 0.06662587\n",
            "Val acc and loss are 0.8854 and 0.4129144\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.98026 and 0.065674454\n",
            "Val acc and loss are 0.8854 and 0.41325435\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.98052 and 0.065260634\n",
            "Val acc and loss are 0.8845 and 0.4138497\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.98 and 0.06545068\n",
            "Val acc and loss are 0.8845 and 0.41495648\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.98036 and 0.06532695\n",
            "Val acc and loss are 0.8853 and 0.4139057\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.98084 and 0.065016724\n",
            "Val acc and loss are 0.8854 and 0.41251388\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.98028 and 0.06538028\n",
            "Val acc and loss are 0.8845 and 0.41311055\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.98026 and 0.06495083\n",
            "Val acc and loss are 0.8853 and 0.41325668\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.9804 and 0.06503359\n",
            "Val acc and loss are 0.8848 and 0.41492182\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.98016 and 0.0655789\n",
            "Val acc and loss are 0.8841 and 0.4163191\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.9802 and 0.06585309\n",
            "Val acc and loss are 0.8844 and 0.4167294\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.97968 and 0.06655937\n",
            "Val acc and loss are 0.8848 and 0.41624835\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.98036 and 0.065141\n",
            "Val acc and loss are 0.8839 and 0.41354448\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.98028 and 0.06530358\n",
            "Val acc and loss are 0.8831 and 0.41430777\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.98058 and 0.06494176\n",
            "Val acc and loss are 0.886 and 0.41297364\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.98004 and 0.06609019\n",
            "Val acc and loss are 0.8866 and 0.41489598\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.98004 and 0.06596885\n",
            "Val acc and loss are 0.8846 and 0.41553462\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.98076 and 0.0643497\n",
            "Val acc and loss are 0.8829 and 0.41349605\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.9804 and 0.06449235\n",
            "Val acc and loss are 0.8846 and 0.41439277\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.98062 and 0.064725876\n",
            "Val acc and loss are 0.8849 and 0.41359326\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.98048 and 0.06508437\n",
            "Val acc and loss are 0.8839 and 0.41350567\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.98048 and 0.06484389\n",
            "Val acc and loss are 0.8837 and 0.4142478\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.9802 and 0.065292\n",
            "Val acc and loss are 0.8843 and 0.41691586\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.98042 and 0.064456664\n",
            "Val acc and loss are 0.8844 and 0.41428256\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.9805 and 0.06490826\n",
            "Val acc and loss are 0.8854 and 0.41416168\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.9798 and 0.065689236\n",
            "Val acc and loss are 0.8855 and 0.41518876\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.98042 and 0.06479536\n",
            "Val acc and loss are 0.8832 and 0.41167822\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.98096 and 0.06412336\n",
            "Val acc and loss are 0.8824 and 0.40982306\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.9807 and 0.064958744\n",
            "Val acc and loss are 0.8838 and 0.4132571\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.98 and 0.065877564\n",
            "Val acc and loss are 0.8838 and 0.41618097\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.98072 and 0.06449036\n",
            "Val acc and loss are 0.8847 and 0.4132541\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.98056 and 0.0647759\n",
            "Val acc and loss are 0.8842 and 0.41432425\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.9801 and 0.06567646\n",
            "Val acc and loss are 0.8841 and 0.41529098\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.98036 and 0.06429115\n",
            "Val acc and loss are 0.8832 and 0.41110244\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.98084 and 0.064625494\n",
            "Val acc and loss are 0.8842 and 0.41136762\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.98066 and 0.06449592\n",
            "Val acc and loss are 0.8853 and 0.41555068\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.98112 and 0.06425565\n",
            "Val acc and loss are 0.8857 and 0.41622886\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.9809 and 0.06437203\n",
            "Val acc and loss are 0.8833 and 0.41369325\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.98106 and 0.06382681\n",
            "Val acc and loss are 0.8836 and 0.41438466\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.98104 and 0.06341137\n",
            "Val acc and loss are 0.8849 and 0.41380543\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.9809 and 0.06363229\n",
            "Val acc and loss are 0.8854 and 0.4125125\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.98074 and 0.06351628\n",
            "Val acc and loss are 0.8854 and 0.41312334\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.98098 and 0.06336056\n",
            "Val acc and loss are 0.8837 and 0.41534027\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.98092 and 0.063277684\n",
            "Val acc and loss are 0.8837 and 0.41461572\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.98108 and 0.063072\n",
            "Val acc and loss are 0.8855 and 0.41346285\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.98126 and 0.06332152\n",
            "Val acc and loss are 0.8857 and 0.41499248\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.9807 and 0.06361463\n",
            "Val acc and loss are 0.8849 and 0.41486034\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.98092 and 0.063469715\n",
            "Val acc and loss are 0.8842 and 0.4142788\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.98122 and 0.06320597\n",
            "Val acc and loss are 0.8839 and 0.41526723\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.9809 and 0.063617654\n",
            "Val acc and loss are 0.8837 and 0.4172094\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.981 and 0.063136786\n",
            "Val acc and loss are 0.8839 and 0.41552228\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.98138 and 0.062862664\n",
            "Val acc and loss are 0.8852 and 0.41291884\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.98104 and 0.06301131\n",
            "Val acc and loss are 0.8834 and 0.41527003\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.98144 and 0.06226636\n",
            "Val acc and loss are 0.8843 and 0.41519234\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.98124 and 0.062000986\n",
            "Val acc and loss are 0.8835 and 0.4167045\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.98112 and 0.06240697\n",
            "Val acc and loss are 0.8854 and 0.41593248\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.98108 and 0.062464043\n",
            "Val acc and loss are 0.885 and 0.41378883\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.98126 and 0.062473178\n",
            "Val acc and loss are 0.884 and 0.41786295\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.98158 and 0.061988384\n",
            "Val acc and loss are 0.884 and 0.41715348\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.9814 and 0.062315136\n",
            "Val acc and loss are 0.8844 and 0.4161871\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.98098 and 0.06306254\n",
            "Val acc and loss are 0.8829 and 0.41863593\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.98164 and 0.062220763\n",
            "Val acc and loss are 0.8857 and 0.4192566\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.98142 and 0.062557474\n",
            "Val acc and loss are 0.8857 and 0.4169121\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.9818 and 0.06154501\n",
            "Val acc and loss are 0.8841 and 0.41473955\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.98166 and 0.06148289\n",
            "Val acc and loss are 0.8833 and 0.4151296\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.98112 and 0.06250482\n",
            "Val acc and loss are 0.883 and 0.41754356\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.98176 and 0.0617474\n",
            "Val acc and loss are 0.8832 and 0.41833907\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.98174 and 0.061696544\n",
            "Val acc and loss are 0.8838 and 0.4179205\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.98108 and 0.062352177\n",
            "Val acc and loss are 0.8846 and 0.4171502\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.98158 and 0.061535396\n",
            "Val acc and loss are 0.8847 and 0.4153219\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.98154 and 0.06117131\n",
            "Val acc and loss are 0.8843 and 0.41731635\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.9812 and 0.061876375\n",
            "Val acc and loss are 0.8835 and 0.42038018\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.98128 and 0.061632313\n",
            "Val acc and loss are 0.8852 and 0.41971934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMXD4EKIUrt0",
        "outputId": "842d28e6-9aae-4c54-a86f-e354da397146"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.8874 at epoch 449 with a corresponding training accuracy of 0.95838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS_8tkdMnJLt"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "qLIk8y1_nJPy",
        "outputId": "cb60e5fd-d322-49dd-e34a-cacf12c84dcc"
      },
      "source": [
        "# Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.9818 achieved at epoch: 990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MeChJ-lnJJp",
        "outputId": "d0d7c7e1-ce72-44fb-91bb-dbe1531b4910"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4651,    7,   31,   53,    5,    1,  218,    0,   11,    0],\n",
              "       [   6, 4974,    7,   18,    5,    0,    1,    0,    1,    0],\n",
              "       [  44,    4, 4519,   24,  230,    0,  165,    0,    6,    0],\n",
              "       [  64,   18,   18, 4745,   85,    0,   43,    0,    6,    0],\n",
              "       [   5,    1,  237,   78, 4468,    0,  157,    0,    4,    0],\n",
              "       [   1,    0,    0,    0,    0, 4966,    0,   15,    4,   18],\n",
              "       [ 323,    5,  180,   64,  149,    1, 4295,    0,   13,    0],\n",
              "       [   0,    0,    0,    0,    0,   39,    0, 4928,    4,   74],\n",
              "       [  14,    3,    9,    8,   10,   12,   13,    2, 4961,    0],\n",
              "       [   0,    0,    0,    0,    0,   17,    1,   28,    0, 4933]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "oqA6YaQNnOql",
        "outputId": "65b01bf1-2e3e-4e1d-c7a5-311d15acdad9"
      },
      "source": [
        "# Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8874 achieved at epoch: 448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR1_CSlbnOtQ",
        "outputId": "ed1e7d85-e469-4f60-c369-1b664f36133a"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[858,   0,  16,  32,   1,   2, 107,   0,   7,   0],\n",
              "       [  1, 965,   1,  15,   1,   0,   4,   0,   1,   0],\n",
              "       [ 14,   2, 826,   8,  95,   0,  57,   0,   6,   0],\n",
              "       [ 27,   6,   7, 916,  35,   0,  26,   0,   3,   1],\n",
              "       [  3,   2,  77,  32, 875,   0,  57,   0,   4,   0],\n",
              "       [  0,   0,   1,   0,   0, 942,   1,  33,   6,  13],\n",
              "       [127,   2,  73,  15,  76,   0, 665,   0,  12,   0],\n",
              "       [  0,   0,   0,   0,   0,  19,   0, 903,   1,  32],\n",
              "       [  4,   0,   6,   1,   8,   2,  12,   4, 930,   1],\n",
              "       [  0,   0,   0,   0,   0,  11,   0,  38,   0, 972]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WRwZb75nOzB",
        "outputId": "8210106f-8013-478f-a716-852698b04cf5"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44592676\n",
            "0.8781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfUV74pYnO2p",
        "outputId": "8ef9c8be-d688-4ef5-91aa-943094a6b2f6"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[848,   3,  13,  31,   4,   1,  91,   0,   9,   0],\n",
              "       [  3, 963,   0,  25,   5,   0,   4,   0,   0,   0],\n",
              "       [ 19,   1, 803,  14,  92,   1,  68,   0,   2,   0],\n",
              "       [ 29,  12,  12, 874,  35,   1,  32,   0,   5,   0],\n",
              "       [  1,   1,  90,  32, 817,   0,  52,   0,   7,   0],\n",
              "       [  1,   0,   0,   1,   0, 946,   0,  25,   2,  25],\n",
              "       [129,   1,  96,  23,  68,   0, 670,   0,  13,   0],\n",
              "       [  0,   0,   0,   0,   0,  24,   0, 943,   0,  33],\n",
              "       [  5,   1,   6,   6,   3,   4,  12,   5, 958,   0],\n",
              "       [  0,   0,   0,   0,   0,   7,   1,  33,   0, 959]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nant_Oj-ncAi"
      },
      "source": [
        "# **Test 3** *(Revised from Test 2: learn_rate = 0.001)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbCyVwK-nOwJ"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 128, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF9BndfhnpcN",
        "outputId": "ad953758-bbae-4bb1-fa3c-0ecfa870c89a"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.37326 and 1.7969756\n",
            "Val acc and loss are 0.3729 and 1.7885436\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.53474 and 1.4193331\n",
            "Val acc and loss are 0.5388 and 1.4115381\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.61896 and 1.1866249\n",
            "Val acc and loss are 0.6188 and 1.1797061\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.65538 and 1.04076\n",
            "Val acc and loss are 0.6559 and 1.0346099\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.67444 and 0.94623065\n",
            "Val acc and loss are 0.6758 and 0.94092286\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.68884 and 0.8804435\n",
            "Val acc and loss are 0.6892 and 0.87616426\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.70302 and 0.8308199\n",
            "Val acc and loss are 0.7003 and 0.8276121\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.71484 and 0.79072124\n",
            "Val acc and loss are 0.713 and 0.7885441\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.72684 and 0.7570468\n",
            "Val acc and loss are 0.7241 and 0.75584817\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.73734 and 0.72832483\n",
            "Val acc and loss are 0.7349 and 0.7280175\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.74662 and 0.7037235\n",
            "Val acc and loss are 0.7445 and 0.7042383\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.755 and 0.68248874\n",
            "Val acc and loss are 0.7529 and 0.6837362\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.7619 and 0.6638201\n",
            "Val acc and loss are 0.7608 and 0.6657375\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.7682 and 0.64693826\n",
            "Val acc and loss are 0.7673 and 0.6494814\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.77336 and 0.63168436\n",
            "Val acc and loss are 0.7729 and 0.6348449\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.77788 and 0.6180242\n",
            "Val acc and loss are 0.7781 and 0.6217853\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.78198 and 0.60585755\n",
            "Val acc and loss are 0.7825 and 0.6101777\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.7855 and 0.595068\n",
            "Val acc and loss are 0.7842 and 0.5999062\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.7887 and 0.5853062\n",
            "Val acc and loss are 0.7859 and 0.5906295\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.7912 and 0.5762934\n",
            "Val acc and loss are 0.7895 and 0.5820568\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.7941 and 0.56756413\n",
            "Val acc and loss are 0.7913 and 0.5737364\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.7964 and 0.55903786\n",
            "Val acc and loss are 0.7949 and 0.5655999\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.7996 and 0.5506774\n",
            "Val acc and loss are 0.7973 and 0.5576249\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.80284 and 0.5425962\n",
            "Val acc and loss are 0.7998 and 0.549948\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.80538 and 0.53483236\n",
            "Val acc and loss are 0.803 and 0.54263514\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.8081 and 0.52751327\n",
            "Val acc and loss are 0.8061 and 0.53581935\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.8108 and 0.5206949\n",
            "Val acc and loss are 0.8072 and 0.5295012\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.81378 and 0.5143356\n",
            "Val acc and loss are 0.8091 and 0.5236282\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.81638 and 0.5084565\n",
            "Val acc and loss are 0.811 and 0.51819295\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.81868 and 0.5029884\n",
            "Val acc and loss are 0.8134 and 0.51308197\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.82078 and 0.4979467\n",
            "Val acc and loss are 0.8158 and 0.5082993\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.82282 and 0.4932725\n",
            "Val acc and loss are 0.8171 and 0.5038224\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.82482 and 0.4889172\n",
            "Val acc and loss are 0.8188 and 0.4996167\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.82646 and 0.4847396\n",
            "Val acc and loss are 0.8206 and 0.49553952\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.82786 and 0.48065078\n",
            "Val acc and loss are 0.8217 and 0.4915493\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.82978 and 0.47671956\n",
            "Val acc and loss are 0.8231 and 0.4877269\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.8314 and 0.47309077\n",
            "Val acc and loss are 0.8248 and 0.48419306\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.83272 and 0.46966693\n",
            "Val acc and loss are 0.8252 and 0.48091796\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.8339 and 0.46647704\n",
            "Val acc and loss are 0.8262 and 0.47791255\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.8352 and 0.4634921\n",
            "Val acc and loss are 0.8269 and 0.47514814\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.83646 and 0.46057874\n",
            "Val acc and loss are 0.8275 and 0.47249752\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.83726 and 0.4577289\n",
            "Val acc and loss are 0.828 and 0.46992472\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.83796 and 0.4549259\n",
            "Val acc and loss are 0.8285 and 0.46743438\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.83866 and 0.45220032\n",
            "Val acc and loss are 0.8292 and 0.4650257\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.83978 and 0.44952285\n",
            "Val acc and loss are 0.8307 and 0.46268106\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.84074 and 0.44689602\n",
            "Val acc and loss are 0.8321 and 0.4603988\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.84158 and 0.44424382\n",
            "Val acc and loss are 0.8331 and 0.45813915\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.84278 and 0.44154343\n",
            "Val acc and loss are 0.8336 and 0.45587695\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.84378 and 0.4388873\n",
            "Val acc and loss are 0.8348 and 0.45369315\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.84462 and 0.43632406\n",
            "Val acc and loss are 0.8367 and 0.4515931\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.84562 and 0.43383172\n",
            "Val acc and loss are 0.8372 and 0.4495348\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.84646 and 0.43144992\n",
            "Val acc and loss are 0.8379 and 0.44753662\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.84718 and 0.42918095\n",
            "Val acc and loss are 0.8392 and 0.44560948\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.8479 and 0.4269837\n",
            "Val acc and loss are 0.8403 and 0.44375888\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.84894 and 0.4248364\n",
            "Val acc and loss are 0.8415 and 0.44193146\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.84972 and 0.42267835\n",
            "Val acc and loss are 0.8423 and 0.44010538\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.85044 and 0.42051038\n",
            "Val acc and loss are 0.8428 and 0.4382803\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.8514 and 0.4183758\n",
            "Val acc and loss are 0.8434 and 0.43648514\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.85214 and 0.4163304\n",
            "Val acc and loss are 0.8442 and 0.4348039\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.85284 and 0.4144089\n",
            "Val acc and loss are 0.845 and 0.433277\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.85368 and 0.41254386\n",
            "Val acc and loss are 0.8457 and 0.4318418\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.85422 and 0.41073188\n",
            "Val acc and loss are 0.8465 and 0.4304577\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.85502 and 0.40898705\n",
            "Val acc and loss are 0.8465 and 0.42911133\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.8555 and 0.40739882\n",
            "Val acc and loss are 0.8473 and 0.4278917\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.85608 and 0.40586\n",
            "Val acc and loss are 0.8472 and 0.4266919\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.85642 and 0.40440273\n",
            "Val acc and loss are 0.848 and 0.4255538\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.85728 and 0.4028256\n",
            "Val acc and loss are 0.848 and 0.4243223\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.85812 and 0.40122062\n",
            "Val acc and loss are 0.8483 and 0.42306954\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.85858 and 0.3996146\n",
            "Val acc and loss are 0.8492 and 0.42187062\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.85906 and 0.3980639\n",
            "Val acc and loss are 0.8499 and 0.42073673\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.85958 and 0.39660507\n",
            "Val acc and loss are 0.8503 and 0.41968906\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.86016 and 0.39525726\n",
            "Val acc and loss are 0.8505 and 0.41874307\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.8607 and 0.39388484\n",
            "Val acc and loss are 0.8508 and 0.41771886\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.86102 and 0.39254108\n",
            "Val acc and loss are 0.8511 and 0.41669488\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.86134 and 0.39121047\n",
            "Val acc and loss are 0.8512 and 0.41566294\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.86186 and 0.38988718\n",
            "Val acc and loss are 0.8512 and 0.41469276\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.86242 and 0.38854542\n",
            "Val acc and loss are 0.8514 and 0.41377613\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.86294 and 0.38729516\n",
            "Val acc and loss are 0.8525 and 0.41293925\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.86342 and 0.38608968\n",
            "Val acc and loss are 0.853 and 0.41211024\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.86346 and 0.38491306\n",
            "Val acc and loss are 0.8542 and 0.41126925\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.864 and 0.38376144\n",
            "Val acc and loss are 0.8542 and 0.41037774\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.8645 and 0.38261688\n",
            "Val acc and loss are 0.8544 and 0.40944803\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.8646 and 0.38147685\n",
            "Val acc and loss are 0.8548 and 0.4085\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.86506 and 0.38034025\n",
            "Val acc and loss are 0.8551 and 0.40762645\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.86538 and 0.3791822\n",
            "Val acc and loss are 0.8556 and 0.4067577\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.86588 and 0.37801063\n",
            "Val acc and loss are 0.8562 and 0.40591887\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.86654 and 0.37686056\n",
            "Val acc and loss are 0.8566 and 0.40509742\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.8669 and 0.3757468\n",
            "Val acc and loss are 0.857 and 0.40425196\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.86692 and 0.37471014\n",
            "Val acc and loss are 0.8572 and 0.4034739\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.86718 and 0.37369132\n",
            "Val acc and loss are 0.8571 and 0.4027168\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.86768 and 0.3726885\n",
            "Val acc and loss are 0.8574 and 0.40197402\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.86796 and 0.37168357\n",
            "Val acc and loss are 0.8576 and 0.40126395\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.86826 and 0.37062585\n",
            "Val acc and loss are 0.8584 and 0.40054232\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.86844 and 0.36956117\n",
            "Val acc and loss are 0.8593 and 0.39980283\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.86866 and 0.36850828\n",
            "Val acc and loss are 0.8598 and 0.3990833\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.86894 and 0.36750665\n",
            "Val acc and loss are 0.8603 and 0.39844334\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.86906 and 0.36658674\n",
            "Val acc and loss are 0.8604 and 0.39788774\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.8697 and 0.3657372\n",
            "Val acc and loss are 0.8611 and 0.39737442\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.86982 and 0.36495733\n",
            "Val acc and loss are 0.8613 and 0.39690724\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.87008 and 0.3641693\n",
            "Val acc and loss are 0.8613 and 0.3964364\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.8704 and 0.3633511\n",
            "Val acc and loss are 0.8618 and 0.39591128\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.87066 and 0.36248714\n",
            "Val acc and loss are 0.8619 and 0.39535862\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.87088 and 0.36160022\n",
            "Val acc and loss are 0.8623 and 0.39483327\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.87084 and 0.36065274\n",
            "Val acc and loss are 0.8621 and 0.39423963\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.87134 and 0.35972163\n",
            "Val acc and loss are 0.8619 and 0.39358068\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.87164 and 0.3588245\n",
            "Val acc and loss are 0.8624 and 0.39296713\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.8722 and 0.35799015\n",
            "Val acc and loss are 0.8633 and 0.39239386\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.87232 and 0.35718626\n",
            "Val acc and loss are 0.8637 and 0.3917672\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.87274 and 0.35638595\n",
            "Val acc and loss are 0.863 and 0.39117816\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.87268 and 0.3555405\n",
            "Val acc and loss are 0.8631 and 0.39059302\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.87286 and 0.35466915\n",
            "Val acc and loss are 0.8632 and 0.3900416\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.87332 and 0.3537425\n",
            "Val acc and loss are 0.8638 and 0.38947365\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.87348 and 0.3528639\n",
            "Val acc and loss are 0.8638 and 0.38896352\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.87378 and 0.3520771\n",
            "Val acc and loss are 0.8641 and 0.3884813\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.87396 and 0.35133472\n",
            "Val acc and loss are 0.8635 and 0.38800502\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.87412 and 0.35062125\n",
            "Val acc and loss are 0.8635 and 0.38752115\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.8744 and 0.3498841\n",
            "Val acc and loss are 0.8637 and 0.38704184\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.8748 and 0.34911266\n",
            "Val acc and loss are 0.8638 and 0.3865083\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.8751 and 0.34839085\n",
            "Val acc and loss are 0.8638 and 0.38604823\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.87546 and 0.34767184\n",
            "Val acc and loss are 0.8641 and 0.38561064\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.87566 and 0.34693214\n",
            "Val acc and loss are 0.8644 and 0.3851477\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.87596 and 0.34624532\n",
            "Val acc and loss are 0.8645 and 0.3847394\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.87614 and 0.34558782\n",
            "Val acc and loss are 0.8645 and 0.38434902\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.87656 and 0.3449726\n",
            "Val acc and loss are 0.8652 and 0.38400152\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.87712 and 0.3444125\n",
            "Val acc and loss are 0.865 and 0.38364625\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.8772 and 0.3437832\n",
            "Val acc and loss are 0.8651 and 0.38324073\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.87758 and 0.3431028\n",
            "Val acc and loss are 0.8654 and 0.38284388\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.87764 and 0.3423464\n",
            "Val acc and loss are 0.8653 and 0.3823762\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.87774 and 0.341556\n",
            "Val acc and loss are 0.8653 and 0.38185826\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.8781 and 0.34083992\n",
            "Val acc and loss are 0.8654 and 0.38144127\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.87832 and 0.34018\n",
            "Val acc and loss are 0.8656 and 0.38108957\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.87858 and 0.33953574\n",
            "Val acc and loss are 0.865 and 0.38073266\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.87884 and 0.3388772\n",
            "Val acc and loss are 0.8652 and 0.38029084\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.87882 and 0.33811387\n",
            "Val acc and loss are 0.8654 and 0.37966958\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.87916 and 0.337349\n",
            "Val acc and loss are 0.8658 and 0.37907225\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.87956 and 0.33662742\n",
            "Val acc and loss are 0.8667 and 0.37858778\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.87952 and 0.3359466\n",
            "Val acc and loss are 0.8663 and 0.37822086\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.87974 and 0.33537528\n",
            "Val acc and loss are 0.8663 and 0.37803143\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.8799 and 0.3349557\n",
            "Val acc and loss are 0.8669 and 0.37803352\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.88004 and 0.33452234\n",
            "Val acc and loss are 0.8666 and 0.37792802\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.88054 and 0.33394793\n",
            "Val acc and loss are 0.8662 and 0.37754756\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.88108 and 0.3332785\n",
            "Val acc and loss are 0.8666 and 0.37700382\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.88142 and 0.33267695\n",
            "Val acc and loss are 0.8672 and 0.37666002\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.88124 and 0.33203882\n",
            "Val acc and loss are 0.8667 and 0.37637353\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.88156 and 0.33135644\n",
            "Val acc and loss are 0.867 and 0.3760084\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.88174 and 0.33071047\n",
            "Val acc and loss are 0.8664 and 0.37564918\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.8818 and 0.33014888\n",
            "Val acc and loss are 0.8668 and 0.37531963\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.88206 and 0.32961792\n",
            "Val acc and loss are 0.8676 and 0.37492537\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.88228 and 0.32901376\n",
            "Val acc and loss are 0.8675 and 0.37448058\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.88244 and 0.32841292\n",
            "Val acc and loss are 0.8678 and 0.37414628\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.88286 and 0.32781264\n",
            "Val acc and loss are 0.8676 and 0.37382203\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.88316 and 0.32735032\n",
            "Val acc and loss are 0.8673 and 0.37363696\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.88302 and 0.32689553\n",
            "Val acc and loss are 0.8671 and 0.37348443\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.88304 and 0.32638538\n",
            "Val acc and loss are 0.8675 and 0.37326846\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.88356 and 0.3258304\n",
            "Val acc and loss are 0.868 and 0.3729536\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.8838 and 0.32526317\n",
            "Val acc and loss are 0.8681 and 0.37260947\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.884 and 0.32464764\n",
            "Val acc and loss are 0.8675 and 0.3722828\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.88416 and 0.3240092\n",
            "Val acc and loss are 0.8674 and 0.37188092\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.88438 and 0.32343176\n",
            "Val acc and loss are 0.8678 and 0.37158817\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.88462 and 0.3228508\n",
            "Val acc and loss are 0.8682 and 0.37115112\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.88502 and 0.32227302\n",
            "Val acc and loss are 0.8686 and 0.37066418\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.88546 and 0.32172227\n",
            "Val acc and loss are 0.8689 and 0.37027106\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.88556 and 0.32123715\n",
            "Val acc and loss are 0.8686 and 0.37004313\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.88562 and 0.32069626\n",
            "Val acc and loss are 0.8686 and 0.36986005\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.88596 and 0.32011285\n",
            "Val acc and loss are 0.8686 and 0.3696459\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.88626 and 0.31955263\n",
            "Val acc and loss are 0.8685 and 0.36948514\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.88626 and 0.31905985\n",
            "Val acc and loss are 0.8685 and 0.3693172\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.88638 and 0.31857905\n",
            "Val acc and loss are 0.8683 and 0.3690725\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.88662 and 0.31807396\n",
            "Val acc and loss are 0.8687 and 0.36878526\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.88664 and 0.31754348\n",
            "Val acc and loss are 0.8695 and 0.3684164\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.88654 and 0.31692296\n",
            "Val acc and loss are 0.8694 and 0.36801282\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.88678 and 0.31625324\n",
            "Val acc and loss are 0.87 and 0.3675794\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.88692 and 0.3156506\n",
            "Val acc and loss are 0.8703 and 0.36726436\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.8874 and 0.31508362\n",
            "Val acc and loss are 0.8709 and 0.36693624\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.88758 and 0.31456113\n",
            "Val acc and loss are 0.8713 and 0.36660597\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.88774 and 0.3141422\n",
            "Val acc and loss are 0.8706 and 0.36633423\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.88786 and 0.31378618\n",
            "Val acc and loss are 0.8705 and 0.36615217\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.88806 and 0.31347588\n",
            "Val acc and loss are 0.8697 and 0.36604598\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.8882 and 0.31301117\n",
            "Val acc and loss are 0.87 and 0.36579537\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.88828 and 0.3125547\n",
            "Val acc and loss are 0.8699 and 0.36555043\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.8885 and 0.31206745\n",
            "Val acc and loss are 0.8702 and 0.36530575\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.88882 and 0.31154007\n",
            "Val acc and loss are 0.8703 and 0.36500064\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.88922 and 0.31109083\n",
            "Val acc and loss are 0.8706 and 0.36485797\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.88914 and 0.31064108\n",
            "Val acc and loss are 0.8709 and 0.3646599\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.8894 and 0.31019452\n",
            "Val acc and loss are 0.871 and 0.36449903\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.88958 and 0.3097434\n",
            "Val acc and loss are 0.871 and 0.36438563\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.88984 and 0.30920064\n",
            "Val acc and loss are 0.8711 and 0.36412674\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.88994 and 0.30861259\n",
            "Val acc and loss are 0.8712 and 0.36370596\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.88994 and 0.30811542\n",
            "Val acc and loss are 0.8713 and 0.36340353\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.89002 and 0.30770567\n",
            "Val acc and loss are 0.8715 and 0.3632843\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.89024 and 0.3074154\n",
            "Val acc and loss are 0.8711 and 0.36328796\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.89026 and 0.30703738\n",
            "Val acc and loss are 0.8704 and 0.3630645\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.89038 and 0.3066288\n",
            "Val acc and loss are 0.8707 and 0.36280358\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.89038 and 0.30622032\n",
            "Val acc and loss are 0.8707 and 0.36253765\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.8909 and 0.30570903\n",
            "Val acc and loss are 0.8708 and 0.36228022\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.89088 and 0.30525866\n",
            "Val acc and loss are 0.8711 and 0.36218515\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.8909 and 0.304785\n",
            "Val acc and loss are 0.8711 and 0.36200622\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.8913 and 0.3042579\n",
            "Val acc and loss are 0.871 and 0.36166844\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.89134 and 0.3037331\n",
            "Val acc and loss are 0.8706 and 0.36122346\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.89164 and 0.30323133\n",
            "Val acc and loss are 0.8704 and 0.36074483\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.8918 and 0.30269197\n",
            "Val acc and loss are 0.8713 and 0.36036482\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.89212 and 0.30226436\n",
            "Val acc and loss are 0.8714 and 0.36034653\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.89244 and 0.30188543\n",
            "Val acc and loss are 0.8716 and 0.3604358\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.89266 and 0.30144882\n",
            "Val acc and loss are 0.8721 and 0.36036903\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.89266 and 0.30112618\n",
            "Val acc and loss are 0.8727 and 0.3603207\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.89254 and 0.30083895\n",
            "Val acc and loss are 0.8724 and 0.3603858\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.89298 and 0.30051345\n",
            "Val acc and loss are 0.872 and 0.36029837\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.89316 and 0.30020028\n",
            "Val acc and loss are 0.8715 and 0.36015543\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.89314 and 0.29973063\n",
            "Val acc and loss are 0.8715 and 0.35980868\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.89306 and 0.29926172\n",
            "Val acc and loss are 0.8716 and 0.35951596\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.89328 and 0.29882565\n",
            "Val acc and loss are 0.8725 and 0.35933223\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.89328 and 0.29836044\n",
            "Val acc and loss are 0.8727 and 0.3590839\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.89324 and 0.2978977\n",
            "Val acc and loss are 0.873 and 0.3587881\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.8938 and 0.29748088\n",
            "Val acc and loss are 0.8727 and 0.35846522\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.89406 and 0.29705706\n",
            "Val acc and loss are 0.8721 and 0.35827008\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.89438 and 0.29665318\n",
            "Val acc and loss are 0.872 and 0.35811692\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.89474 and 0.29622844\n",
            "Val acc and loss are 0.8721 and 0.35800734\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.89488 and 0.29582423\n",
            "Val acc and loss are 0.8719 and 0.3579627\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.89506 and 0.29537067\n",
            "Val acc and loss are 0.8727 and 0.35791436\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.8951 and 0.29489782\n",
            "Val acc and loss are 0.8731 and 0.3577324\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.89512 and 0.29452798\n",
            "Val acc and loss are 0.8735 and 0.35763934\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.8951 and 0.29409707\n",
            "Val acc and loss are 0.8733 and 0.35749972\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.89518 and 0.2936163\n",
            "Val acc and loss are 0.873 and 0.35729972\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.89542 and 0.29323366\n",
            "Val acc and loss are 0.8729 and 0.35713965\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.89594 and 0.29280454\n",
            "Val acc and loss are 0.8728 and 0.35689822\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.8959 and 0.29236126\n",
            "Val acc and loss are 0.8732 and 0.35659492\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.89604 and 0.2920377\n",
            "Val acc and loss are 0.8733 and 0.35646632\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.89614 and 0.2917242\n",
            "Val acc and loss are 0.8737 and 0.35634816\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.89642 and 0.29135424\n",
            "Val acc and loss are 0.8735 and 0.3562378\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.8965 and 0.2909496\n",
            "Val acc and loss are 0.8737 and 0.35596552\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.89664 and 0.29052043\n",
            "Val acc and loss are 0.8736 and 0.355577\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.89684 and 0.29017317\n",
            "Val acc and loss are 0.8736 and 0.35534245\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.89682 and 0.289823\n",
            "Val acc and loss are 0.8741 and 0.3552078\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.89694 and 0.2894051\n",
            "Val acc and loss are 0.8745 and 0.35502633\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.89684 and 0.2891881\n",
            "Val acc and loss are 0.8743 and 0.35523605\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.89674 and 0.28909546\n",
            "Val acc and loss are 0.8732 and 0.355535\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.89678 and 0.28879866\n",
            "Val acc and loss are 0.8735 and 0.3554584\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.897 and 0.28833252\n",
            "Val acc and loss are 0.8742 and 0.35513642\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.89726 and 0.28775373\n",
            "Val acc and loss are 0.8746 and 0.35465726\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.89754 and 0.28728122\n",
            "Val acc and loss are 0.874 and 0.3543855\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.89772 and 0.28683093\n",
            "Val acc and loss are 0.874 and 0.3541486\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.89772 and 0.2863826\n",
            "Val acc and loss are 0.874 and 0.3539688\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.8979 and 0.28599617\n",
            "Val acc and loss are 0.8743 and 0.35388812\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.89812 and 0.2856905\n",
            "Val acc and loss are 0.8743 and 0.35386342\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.89804 and 0.2854813\n",
            "Val acc and loss are 0.8742 and 0.3538933\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.89824 and 0.2851325\n",
            "Val acc and loss are 0.8744 and 0.35371974\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.89844 and 0.2846888\n",
            "Val acc and loss are 0.8745 and 0.35348275\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.8985 and 0.28438628\n",
            "Val acc and loss are 0.8752 and 0.3533727\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.8985 and 0.2840247\n",
            "Val acc and loss are 0.8748 and 0.3532405\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.89864 and 0.28359425\n",
            "Val acc and loss are 0.8745 and 0.35312325\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.89876 and 0.28326228\n",
            "Val acc and loss are 0.8747 and 0.35318002\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.8989 and 0.28300664\n",
            "Val acc and loss are 0.8738 and 0.3533875\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.899 and 0.28263628\n",
            "Val acc and loss are 0.8741 and 0.35324323\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.89896 and 0.2821499\n",
            "Val acc and loss are 0.8751 and 0.35273442\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.89918 and 0.2816378\n",
            "Val acc and loss are 0.8755 and 0.35206416\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.89926 and 0.28128406\n",
            "Val acc and loss are 0.8755 and 0.35160476\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.8995 and 0.280964\n",
            "Val acc and loss are 0.8752 and 0.3514854\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.89944 and 0.28075683\n",
            "Val acc and loss are 0.8759 and 0.35171276\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.89954 and 0.2805489\n",
            "Val acc and loss are 0.8756 and 0.35191774\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.89958 and 0.2802103\n",
            "Val acc and loss are 0.8751 and 0.35177284\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.90004 and 0.27988875\n",
            "Val acc and loss are 0.8755 and 0.35162437\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.9004 and 0.2794427\n",
            "Val acc and loss are 0.8765 and 0.351348\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.90028 and 0.27895114\n",
            "Val acc and loss are 0.8769 and 0.351179\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.90034 and 0.27879357\n",
            "Val acc and loss are 0.8757 and 0.35147756\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.9005 and 0.27837867\n",
            "Val acc and loss are 0.8757 and 0.35136148\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.9009 and 0.2778314\n",
            "Val acc and loss are 0.8761 and 0.35101277\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.90076 and 0.27757365\n",
            "Val acc and loss are 0.8763 and 0.35089028\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.9008 and 0.27739182\n",
            "Val acc and loss are 0.8766 and 0.35096723\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.9009 and 0.27723056\n",
            "Val acc and loss are 0.8767 and 0.35109845\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.90092 and 0.27692488\n",
            "Val acc and loss are 0.8762 and 0.35096842\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.90138 and 0.27638763\n",
            "Val acc and loss are 0.8761 and 0.35054663\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.90164 and 0.27589214\n",
            "Val acc and loss are 0.8764 and 0.35015282\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.9014 and 0.27545422\n",
            "Val acc and loss are 0.8763 and 0.35002318\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.90166 and 0.27518648\n",
            "Val acc and loss are 0.8763 and 0.35017017\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.90188 and 0.27487978\n",
            "Val acc and loss are 0.8761 and 0.35016096\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.902 and 0.27448648\n",
            "Val acc and loss are 0.8765 and 0.34992695\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.90204 and 0.2741074\n",
            "Val acc and loss are 0.8763 and 0.3497259\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.90232 and 0.27372164\n",
            "Val acc and loss are 0.8766 and 0.34953418\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.90236 and 0.27348033\n",
            "Val acc and loss are 0.876 and 0.3495823\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.90252 and 0.27333444\n",
            "Val acc and loss are 0.8761 and 0.34972882\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.90282 and 0.2730587\n",
            "Val acc and loss are 0.8759 and 0.3496066\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.90258 and 0.272707\n",
            "Val acc and loss are 0.876 and 0.3493267\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.9029 and 0.2723809\n",
            "Val acc and loss are 0.8764 and 0.34917736\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.90304 and 0.2720345\n",
            "Val acc and loss are 0.8762 and 0.34915167\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.903 and 0.27176788\n",
            "Val acc and loss are 0.8761 and 0.34929493\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.90286 and 0.2715798\n",
            "Val acc and loss are 0.8761 and 0.34943423\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.90306 and 0.27129382\n",
            "Val acc and loss are 0.8765 and 0.34938022\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.90332 and 0.27110648\n",
            "Val acc and loss are 0.8766 and 0.34941855\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.90342 and 0.2707518\n",
            "Val acc and loss are 0.8761 and 0.3492994\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.9034 and 0.27037182\n",
            "Val acc and loss are 0.8762 and 0.34924608\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.90358 and 0.27001944\n",
            "Val acc and loss are 0.8763 and 0.3491311\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.90394 and 0.2695796\n",
            "Val acc and loss are 0.8769 and 0.34881213\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.90402 and 0.26917416\n",
            "Val acc and loss are 0.8772 and 0.34843618\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.9041 and 0.26892143\n",
            "Val acc and loss are 0.8773 and 0.34831294\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.90424 and 0.26884\n",
            "Val acc and loss are 0.8772 and 0.34852797\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.90426 and 0.2686979\n",
            "Val acc and loss are 0.8774 and 0.34863448\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.90436 and 0.26844206\n",
            "Val acc and loss are 0.8774 and 0.3485012\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.90426 and 0.2680797\n",
            "Val acc and loss are 0.8779 and 0.34817678\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.9047 and 0.26760706\n",
            "Val acc and loss are 0.8778 and 0.34773028\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.9046 and 0.26719767\n",
            "Val acc and loss are 0.8781 and 0.3475205\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.90476 and 0.266861\n",
            "Val acc and loss are 0.8773 and 0.3476375\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.90474 and 0.26668456\n",
            "Val acc and loss are 0.8765 and 0.3479747\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.90478 and 0.2663848\n",
            "Val acc and loss are 0.8771 and 0.34799728\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.90488 and 0.26576054\n",
            "Val acc and loss are 0.877 and 0.34746242\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.90542 and 0.2653306\n",
            "Val acc and loss are 0.8777 and 0.34695923\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.90534 and 0.26505333\n",
            "Val acc and loss are 0.8773 and 0.34677652\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.90542 and 0.2647514\n",
            "Val acc and loss are 0.8779 and 0.3468354\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.90534 and 0.26478562\n",
            "Val acc and loss are 0.8771 and 0.3474082\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.9053 and 0.26460102\n",
            "Val acc and loss are 0.8772 and 0.34749565\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.90566 and 0.26407245\n",
            "Val acc and loss are 0.8778 and 0.3469194\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.90594 and 0.2637742\n",
            "Val acc and loss are 0.8785 and 0.34661102\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.90598 and 0.26344058\n",
            "Val acc and loss are 0.8784 and 0.34654593\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.90598 and 0.26330245\n",
            "Val acc and loss are 0.8778 and 0.34695566\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9062 and 0.26309842\n",
            "Val acc and loss are 0.8778 and 0.34717798\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.90612 and 0.2627032\n",
            "Val acc and loss are 0.8778 and 0.34703273\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.90634 and 0.26227558\n",
            "Val acc and loss are 0.8781 and 0.3466583\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.90658 and 0.26197618\n",
            "Val acc and loss are 0.8783 and 0.34633067\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.90642 and 0.2617881\n",
            "Val acc and loss are 0.8782 and 0.34632725\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.90656 and 0.2615988\n",
            "Val acc and loss are 0.8775 and 0.34649774\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.90688 and 0.26115423\n",
            "Val acc and loss are 0.8777 and 0.34623402\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.9067 and 0.26092967\n",
            "Val acc and loss are 0.8777 and 0.34619817\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.90694 and 0.26064706\n",
            "Val acc and loss are 0.878 and 0.34607455\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.90702 and 0.2602181\n",
            "Val acc and loss are 0.8784 and 0.3460512\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.90728 and 0.26011553\n",
            "Val acc and loss are 0.8773 and 0.34631166\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.90728 and 0.26000935\n",
            "Val acc and loss are 0.8767 and 0.34649867\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.90744 and 0.25949335\n",
            "Val acc and loss are 0.8775 and 0.34596616\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.90746 and 0.25929952\n",
            "Val acc and loss are 0.8782 and 0.34570116\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.90756 and 0.25904456\n",
            "Val acc and loss are 0.878 and 0.3456083\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.90786 and 0.2589165\n",
            "Val acc and loss are 0.8782 and 0.34577715\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.90788 and 0.2586269\n",
            "Val acc and loss are 0.8785 and 0.34581676\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.9078 and 0.2580326\n",
            "Val acc and loss are 0.8784 and 0.34551874\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.9077 and 0.2575304\n",
            "Val acc and loss are 0.8789 and 0.34525442\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.90766 and 0.2572788\n",
            "Val acc and loss are 0.8795 and 0.3451008\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.90782 and 0.25694817\n",
            "Val acc and loss are 0.879 and 0.3451258\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.90824 and 0.25696987\n",
            "Val acc and loss are 0.8791 and 0.34547108\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.90852 and 0.2569392\n",
            "Val acc and loss are 0.8791 and 0.34564814\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.90868 and 0.25647867\n",
            "Val acc and loss are 0.8783 and 0.34516576\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.90858 and 0.25627765\n",
            "Val acc and loss are 0.8788 and 0.34498015\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.90892 and 0.25578886\n",
            "Val acc and loss are 0.8784 and 0.34489867\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.90868 and 0.25555086\n",
            "Val acc and loss are 0.8782 and 0.3453184\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.90862 and 0.2554365\n",
            "Val acc and loss are 0.8782 and 0.34569556\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.90878 and 0.25477692\n",
            "Val acc and loss are 0.8785 and 0.34511194\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.90906 and 0.25447136\n",
            "Val acc and loss are 0.8794 and 0.34474578\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.909 and 0.25422502\n",
            "Val acc and loss are 0.879 and 0.34469897\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.90902 and 0.25404835\n",
            "Val acc and loss are 0.878 and 0.34482345\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.90896 and 0.25404325\n",
            "Val acc and loss are 0.8778 and 0.34514213\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.90906 and 0.25381318\n",
            "Val acc and loss are 0.8779 and 0.34504023\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.90912 and 0.25338086\n",
            "Val acc and loss are 0.8785 and 0.34464228\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.90954 and 0.25301525\n",
            "Val acc and loss are 0.8784 and 0.34430337\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.9096 and 0.25262222\n",
            "Val acc and loss are 0.8788 and 0.34426707\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.90976 and 0.25245792\n",
            "Val acc and loss are 0.8797 and 0.34458378\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.90992 and 0.25234035\n",
            "Val acc and loss are 0.8793 and 0.3448146\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.9097 and 0.25201297\n",
            "Val acc and loss are 0.8788 and 0.3444778\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.90986 and 0.25171223\n",
            "Val acc and loss are 0.8791 and 0.34410337\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.91014 and 0.25146687\n",
            "Val acc and loss are 0.8799 and 0.34400082\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.91012 and 0.25139764\n",
            "Val acc and loss are 0.8796 and 0.3441375\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.91028 and 0.2510183\n",
            "Val acc and loss are 0.879 and 0.34384722\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.91062 and 0.25078055\n",
            "Val acc and loss are 0.8797 and 0.34388912\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.9103 and 0.2506798\n",
            "Val acc and loss are 0.879 and 0.34421518\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.9106 and 0.25051996\n",
            "Val acc and loss are 0.8783 and 0.34437987\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.91044 and 0.2501571\n",
            "Val acc and loss are 0.8794 and 0.34425515\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.91066 and 0.24969648\n",
            "Val acc and loss are 0.8804 and 0.34374335\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.91082 and 0.24935006\n",
            "Val acc and loss are 0.881 and 0.34342378\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.911 and 0.24908842\n",
            "Val acc and loss are 0.8807 and 0.3432399\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.91082 and 0.2489191\n",
            "Val acc and loss are 0.8804 and 0.34326786\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.91084 and 0.24872981\n",
            "Val acc and loss are 0.879 and 0.3432522\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.91116 and 0.24836785\n",
            "Val acc and loss are 0.879 and 0.34298086\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.91148 and 0.24787986\n",
            "Val acc and loss are 0.8801 and 0.3426471\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.91144 and 0.24758053\n",
            "Val acc and loss are 0.8796 and 0.34273207\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.91176 and 0.2473823\n",
            "Val acc and loss are 0.8801 and 0.34277546\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.91176 and 0.24721117\n",
            "Val acc and loss are 0.8801 and 0.34271517\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.91176 and 0.24701306\n",
            "Val acc and loss are 0.8796 and 0.3427882\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.91174 and 0.2467551\n",
            "Val acc and loss are 0.8803 and 0.3428056\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.91192 and 0.24661791\n",
            "Val acc and loss are 0.8804 and 0.3429707\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.91184 and 0.24646863\n",
            "Val acc and loss are 0.8804 and 0.34311157\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.91192 and 0.24617253\n",
            "Val acc and loss are 0.8801 and 0.34283966\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.9121 and 0.24595293\n",
            "Val acc and loss are 0.8798 and 0.3427427\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.91236 and 0.24571696\n",
            "Val acc and loss are 0.8792 and 0.34272403\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.91236 and 0.24553318\n",
            "Val acc and loss are 0.8797 and 0.34283668\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.91256 and 0.24515665\n",
            "Val acc and loss are 0.8802 and 0.3426174\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.91268 and 0.24460617\n",
            "Val acc and loss are 0.8802 and 0.34215653\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.91274 and 0.24423417\n",
            "Val acc and loss are 0.88 and 0.3420833\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.91252 and 0.2440865\n",
            "Val acc and loss are 0.8795 and 0.3423646\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.9126 and 0.24383865\n",
            "Val acc and loss are 0.8796 and 0.34234864\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.91284 and 0.24352714\n",
            "Val acc and loss are 0.8798 and 0.34210214\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.91324 and 0.24319845\n",
            "Val acc and loss are 0.8796 and 0.3418453\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.91322 and 0.24294129\n",
            "Val acc and loss are 0.8804 and 0.3418206\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.91316 and 0.24291801\n",
            "Val acc and loss are 0.8804 and 0.3422539\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.91298 and 0.24320848\n",
            "Val acc and loss are 0.8799 and 0.34300074\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.91292 and 0.24302527\n",
            "Val acc and loss are 0.8801 and 0.3430049\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.91318 and 0.24265581\n",
            "Val acc and loss are 0.8803 and 0.34258384\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.91348 and 0.24228165\n",
            "Val acc and loss are 0.88 and 0.34214517\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.91354 and 0.24184582\n",
            "Val acc and loss are 0.8803 and 0.34191167\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.9134 and 0.2414625\n",
            "Val acc and loss are 0.8811 and 0.34189498\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.91366 and 0.24123874\n",
            "Val acc and loss are 0.8803 and 0.34203288\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.91372 and 0.2407938\n",
            "Val acc and loss are 0.8799 and 0.34175643\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.9139 and 0.24048878\n",
            "Val acc and loss are 0.8799 and 0.34143302\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.91398 and 0.24031344\n",
            "Val acc and loss are 0.8809 and 0.34125584\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.91402 and 0.24017063\n",
            "Val acc and loss are 0.8809 and 0.34132153\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.91398 and 0.24006435\n",
            "Val acc and loss are 0.8795 and 0.34158593\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.91418 and 0.23989475\n",
            "Val acc and loss are 0.8795 and 0.341682\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.9142 and 0.23961112\n",
            "Val acc and loss are 0.8802 and 0.34148178\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.9143 and 0.23937164\n",
            "Val acc and loss are 0.8811 and 0.3412437\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.91442 and 0.23918843\n",
            "Val acc and loss are 0.8808 and 0.34120634\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.91446 and 0.23908707\n",
            "Val acc and loss are 0.8806 and 0.34127775\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.91454 and 0.23906699\n",
            "Val acc and loss are 0.8803 and 0.3414256\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.91468 and 0.23884334\n",
            "Val acc and loss are 0.8806 and 0.34136266\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.91484 and 0.23852232\n",
            "Val acc and loss are 0.8818 and 0.34137812\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.91482 and 0.23813653\n",
            "Val acc and loss are 0.8814 and 0.34117728\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.9152 and 0.23766549\n",
            "Val acc and loss are 0.881 and 0.34083897\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.91516 and 0.23738082\n",
            "Val acc and loss are 0.8805 and 0.34094766\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.91526 and 0.23744074\n",
            "Val acc and loss are 0.8797 and 0.3415883\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.91554 and 0.23737891\n",
            "Val acc and loss are 0.8798 and 0.3419393\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.91546 and 0.23682722\n",
            "Val acc and loss are 0.8806 and 0.34155503\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.91568 and 0.23648979\n",
            "Val acc and loss are 0.8809 and 0.34126318\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.91576 and 0.23626488\n",
            "Val acc and loss are 0.8809 and 0.3411898\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.91582 and 0.2360889\n",
            "Val acc and loss are 0.8812 and 0.34154248\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.9156 and 0.23643854\n",
            "Val acc and loss are 0.8804 and 0.34241486\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.9157 and 0.2360554\n",
            "Val acc and loss are 0.8804 and 0.34207168\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.91588 and 0.23549865\n",
            "Val acc and loss are 0.8803 and 0.34128383\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.91634 and 0.23522753\n",
            "Val acc and loss are 0.8809 and 0.34091625\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.91624 and 0.23494312\n",
            "Val acc and loss are 0.8807 and 0.34106478\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.91634 and 0.23494539\n",
            "Val acc and loss are 0.8817 and 0.3415505\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.91622 and 0.2349453\n",
            "Val acc and loss are 0.881 and 0.3417929\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.91646 and 0.2344618\n",
            "Val acc and loss are 0.8807 and 0.34123725\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.9168 and 0.2339038\n",
            "Val acc and loss are 0.8811 and 0.3406347\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.91674 and 0.23363735\n",
            "Val acc and loss are 0.8812 and 0.34065792\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.91672 and 0.23336738\n",
            "Val acc and loss are 0.8811 and 0.34081295\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.91702 and 0.23319186\n",
            "Val acc and loss are 0.8806 and 0.34098163\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.91694 and 0.23312695\n",
            "Val acc and loss are 0.8802 and 0.3409751\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.9168 and 0.23298086\n",
            "Val acc and loss are 0.8803 and 0.3409319\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.91688 and 0.23275875\n",
            "Val acc and loss are 0.8805 and 0.34099132\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.91684 and 0.23265816\n",
            "Val acc and loss are 0.8812 and 0.3411433\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.91718 and 0.23227333\n",
            "Val acc and loss are 0.8815 and 0.340837\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.91708 and 0.23201774\n",
            "Val acc and loss are 0.8812 and 0.3406118\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.9169 and 0.23196195\n",
            "Val acc and loss are 0.8807 and 0.34079584\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.91748 and 0.23188059\n",
            "Val acc and loss are 0.8808 and 0.34091324\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.91764 and 0.23172325\n",
            "Val acc and loss are 0.8807 and 0.34109068\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.91758 and 0.23136078\n",
            "Val acc and loss are 0.8804 and 0.34102213\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.91744 and 0.23116106\n",
            "Val acc and loss are 0.8805 and 0.34088665\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.91708 and 0.2311532\n",
            "Val acc and loss are 0.8808 and 0.3410529\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.91726 and 0.23108798\n",
            "Val acc and loss are 0.8801 and 0.3412684\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.91718 and 0.23132454\n",
            "Val acc and loss are 0.8794 and 0.34203717\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.91748 and 0.23078094\n",
            "Val acc and loss are 0.8802 and 0.34150547\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.91762 and 0.23017648\n",
            "Val acc and loss are 0.8804 and 0.34061608\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.9175 and 0.22998631\n",
            "Val acc and loss are 0.8803 and 0.34046224\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.9178 and 0.22972152\n",
            "Val acc and loss are 0.8809 and 0.34066185\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.91794 and 0.22997054\n",
            "Val acc and loss are 0.8813 and 0.34133932\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.91782 and 0.22973904\n",
            "Val acc and loss are 0.881 and 0.3412715\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.91806 and 0.22895809\n",
            "Val acc and loss are 0.8816 and 0.340275\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.91834 and 0.22867757\n",
            "Val acc and loss are 0.8809 and 0.33989307\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.91828 and 0.22829965\n",
            "Val acc and loss are 0.8807 and 0.3398691\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.91844 and 0.22818847\n",
            "Val acc and loss are 0.8811 and 0.34018436\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.91864 and 0.2282823\n",
            "Val acc and loss are 0.8814 and 0.34043682\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.91864 and 0.22795051\n",
            "Val acc and loss are 0.881 and 0.3400954\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.9189 and 0.22748457\n",
            "Val acc and loss are 0.8805 and 0.33960158\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.91906 and 0.22721736\n",
            "Val acc and loss are 0.8814 and 0.33942276\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.91876 and 0.22709471\n",
            "Val acc and loss are 0.8813 and 0.3399042\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.91878 and 0.2270175\n",
            "Val acc and loss are 0.8816 and 0.34028223\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.91858 and 0.22677325\n",
            "Val acc and loss are 0.8814 and 0.3400189\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.91878 and 0.22658136\n",
            "Val acc and loss are 0.881 and 0.3397916\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.9189 and 0.22627856\n",
            "Val acc and loss are 0.8811 and 0.33982143\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.91916 and 0.22603896\n",
            "Val acc and loss are 0.8813 and 0.33967984\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.9192 and 0.22568765\n",
            "Val acc and loss are 0.882 and 0.33932826\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.9197 and 0.22532059\n",
            "Val acc and loss are 0.8817 and 0.33903423\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.91968 and 0.22521903\n",
            "Val acc and loss are 0.8818 and 0.33916157\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.91954 and 0.22523062\n",
            "Val acc and loss are 0.8816 and 0.33954707\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.91962 and 0.22510616\n",
            "Val acc and loss are 0.8819 and 0.33962834\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.91984 and 0.22476418\n",
            "Val acc and loss are 0.8822 and 0.33930683\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.9199 and 0.2244662\n",
            "Val acc and loss are 0.8818 and 0.33891168\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.91986 and 0.22431193\n",
            "Val acc and loss are 0.8813 and 0.33872706\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.91972 and 0.22423397\n",
            "Val acc and loss are 0.881 and 0.33913282\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.91978 and 0.22412711\n",
            "Val acc and loss are 0.8818 and 0.3396738\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.92002 and 0.22392981\n",
            "Val acc and loss are 0.882 and 0.33986303\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.92006 and 0.22339368\n",
            "Val acc and loss are 0.882 and 0.3393899\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.92018 and 0.22300962\n",
            "Val acc and loss are 0.8819 and 0.33888277\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.92036 and 0.22287098\n",
            "Val acc and loss are 0.882 and 0.3390184\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.92056 and 0.22290248\n",
            "Val acc and loss are 0.882 and 0.33935046\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.92028 and 0.22281988\n",
            "Val acc and loss are 0.8816 and 0.33953115\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.92036 and 0.22248973\n",
            "Val acc and loss are 0.8821 and 0.33929393\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.92066 and 0.22235101\n",
            "Val acc and loss are 0.882 and 0.33920604\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.92052 and 0.22220059\n",
            "Val acc and loss are 0.8818 and 0.33938134\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.92038 and 0.22227855\n",
            "Val acc and loss are 0.8816 and 0.33997777\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.92064 and 0.22218047\n",
            "Val acc and loss are 0.8817 and 0.34008276\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.92102 and 0.22152454\n",
            "Val acc and loss are 0.8814 and 0.3392549\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.92134 and 0.22129536\n",
            "Val acc and loss are 0.8816 and 0.33899644\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.92116 and 0.22106399\n",
            "Val acc and loss are 0.8817 and 0.33899567\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.9211 and 0.22093447\n",
            "Val acc and loss are 0.8819 and 0.3394348\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.92154 and 0.22084191\n",
            "Val acc and loss are 0.882 and 0.33968735\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.9215 and 0.22045189\n",
            "Val acc and loss are 0.8819 and 0.33930597\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.92154 and 0.2202697\n",
            "Val acc and loss are 0.8815 and 0.33917284\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.92148 and 0.22008158\n",
            "Val acc and loss are 0.8817 and 0.3394291\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.92168 and 0.21993367\n",
            "Val acc and loss are 0.8813 and 0.33960158\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.92176 and 0.21969402\n",
            "Val acc and loss are 0.8818 and 0.3395999\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.92182 and 0.2194591\n",
            "Val acc and loss are 0.8814 and 0.33948994\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.92168 and 0.21931241\n",
            "Val acc and loss are 0.8812 and 0.33977482\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.92184 and 0.21910402\n",
            "Val acc and loss are 0.8811 and 0.3401479\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.92184 and 0.21906391\n",
            "Val acc and loss are 0.8818 and 0.3405204\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.92174 and 0.2188018\n",
            "Val acc and loss are 0.8816 and 0.3403448\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.92226 and 0.21856292\n",
            "Val acc and loss are 0.8817 and 0.3399192\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.92246 and 0.21835455\n",
            "Val acc and loss are 0.8822 and 0.3394432\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.9226 and 0.21800219\n",
            "Val acc and loss are 0.8824 and 0.3393269\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.92218 and 0.21787843\n",
            "Val acc and loss are 0.8824 and 0.33981067\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.9223 and 0.21776387\n",
            "Val acc and loss are 0.8822 and 0.34000483\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.92264 and 0.21727993\n",
            "Val acc and loss are 0.8818 and 0.3394429\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.92292 and 0.21707353\n",
            "Val acc and loss are 0.8821 and 0.3388775\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.92288 and 0.2169402\n",
            "Val acc and loss are 0.8821 and 0.3386264\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.92262 and 0.21680745\n",
            "Val acc and loss are 0.8821 and 0.33881187\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.92236 and 0.21688771\n",
            "Val acc and loss are 0.882 and 0.33930475\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.92264 and 0.2167087\n",
            "Val acc and loss are 0.8819 and 0.33941007\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.92278 and 0.21627578\n",
            "Val acc and loss are 0.8815 and 0.3391292\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.9232 and 0.21606904\n",
            "Val acc and loss are 0.8813 and 0.33895233\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.9235 and 0.21582809\n",
            "Val acc and loss are 0.8822 and 0.3387107\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.92344 and 0.21556695\n",
            "Val acc and loss are 0.8826 and 0.33863857\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.92298 and 0.21562041\n",
            "Val acc and loss are 0.8826 and 0.3388646\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.92316 and 0.21545632\n",
            "Val acc and loss are 0.8815 and 0.33872715\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.92352 and 0.21530797\n",
            "Val acc and loss are 0.8817 and 0.33864808\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.92364 and 0.21516818\n",
            "Val acc and loss are 0.883 and 0.33857653\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.92366 and 0.21481305\n",
            "Val acc and loss are 0.8834 and 0.3382866\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.92396 and 0.2143766\n",
            "Val acc and loss are 0.8816 and 0.33783922\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.92386 and 0.2142309\n",
            "Val acc and loss are 0.8813 and 0.33778873\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.92382 and 0.21413605\n",
            "Val acc and loss are 0.8814 and 0.33804497\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.92394 and 0.21398312\n",
            "Val acc and loss are 0.8823 and 0.33798715\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.92388 and 0.21400169\n",
            "Val acc and loss are 0.8819 and 0.3380304\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.92398 and 0.21371736\n",
            "Val acc and loss are 0.8823 and 0.33772907\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.92392 and 0.21336812\n",
            "Val acc and loss are 0.882 and 0.33744535\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.92392 and 0.2132158\n",
            "Val acc and loss are 0.8818 and 0.33770323\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.92426 and 0.2130987\n",
            "Val acc and loss are 0.8818 and 0.3379335\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.92436 and 0.21290492\n",
            "Val acc and loss are 0.8823 and 0.33806247\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.92418 and 0.21280012\n",
            "Val acc and loss are 0.8821 and 0.33821565\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.92432 and 0.2124919\n",
            "Val acc and loss are 0.8816 and 0.33806506\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.92464 and 0.21218322\n",
            "Val acc and loss are 0.8824 and 0.33825448\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.92436 and 0.21233793\n",
            "Val acc and loss are 0.8814 and 0.33904415\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.92434 and 0.21221526\n",
            "Val acc and loss are 0.8812 and 0.33927915\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.92456 and 0.21211816\n",
            "Val acc and loss are 0.8819 and 0.33923268\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.92478 and 0.21229415\n",
            "Val acc and loss are 0.8812 and 0.33947536\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.925 and 0.2120579\n",
            "Val acc and loss are 0.8821 and 0.33950204\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.9248 and 0.21173094\n",
            "Val acc and loss are 0.8813 and 0.33959764\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.92492 and 0.2114075\n",
            "Val acc and loss are 0.8817 and 0.33948934\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.925 and 0.21083649\n",
            "Val acc and loss are 0.8812 and 0.33902565\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.92518 and 0.21050973\n",
            "Val acc and loss are 0.881 and 0.33899036\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.92538 and 0.21048719\n",
            "Val acc and loss are 0.8813 and 0.33930016\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.92534 and 0.21033949\n",
            "Val acc and loss are 0.8806 and 0.33948854\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.92538 and 0.21003383\n",
            "Val acc and loss are 0.8808 and 0.33925396\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.92566 and 0.20994018\n",
            "Val acc and loss are 0.8811 and 0.33901897\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.92546 and 0.2098623\n",
            "Val acc and loss are 0.8808 and 0.33906168\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.92552 and 0.2098446\n",
            "Val acc and loss are 0.8812 and 0.33919707\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.92568 and 0.20966828\n",
            "Val acc and loss are 0.8813 and 0.33907026\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.92574 and 0.2093027\n",
            "Val acc and loss are 0.8815 and 0.33895287\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.92574 and 0.208966\n",
            "Val acc and loss are 0.8813 and 0.3388591\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.9259 and 0.20875281\n",
            "Val acc and loss are 0.8819 and 0.33893082\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.92602 and 0.20844494\n",
            "Val acc and loss are 0.8813 and 0.3385949\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.92592 and 0.20834254\n",
            "Val acc and loss are 0.881 and 0.33880666\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.92612 and 0.20824635\n",
            "Val acc and loss are 0.8809 and 0.3389669\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.92602 and 0.20804113\n",
            "Val acc and loss are 0.8809 and 0.33895203\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.92596 and 0.2077711\n",
            "Val acc and loss are 0.8814 and 0.33881968\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.9259 and 0.20755686\n",
            "Val acc and loss are 0.8814 and 0.3387835\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.92616 and 0.20740892\n",
            "Val acc and loss are 0.881 and 0.33886728\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.9261 and 0.20727059\n",
            "Val acc and loss are 0.8815 and 0.33899096\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.9263 and 0.20719254\n",
            "Val acc and loss are 0.8815 and 0.33915502\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.92628 and 0.20714194\n",
            "Val acc and loss are 0.8812 and 0.3391905\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.92618 and 0.2070332\n",
            "Val acc and loss are 0.882 and 0.33922112\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.9263 and 0.20696644\n",
            "Val acc and loss are 0.8813 and 0.33952662\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.92646 and 0.20658489\n",
            "Val acc and loss are 0.8816 and 0.33930272\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.92626 and 0.20616613\n",
            "Val acc and loss are 0.8818 and 0.33887348\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.9267 and 0.20611793\n",
            "Val acc and loss are 0.8825 and 0.3389306\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.92678 and 0.20591016\n",
            "Val acc and loss are 0.8822 and 0.33897212\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.92664 and 0.20571852\n",
            "Val acc and loss are 0.8827 and 0.33909887\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.92684 and 0.2056612\n",
            "Val acc and loss are 0.882 and 0.3393707\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.92694 and 0.20532367\n",
            "Val acc and loss are 0.8821 and 0.3389866\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.92696 and 0.2050966\n",
            "Val acc and loss are 0.8818 and 0.3387719\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.927 and 0.20497668\n",
            "Val acc and loss are 0.8813 and 0.33873755\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.92706 and 0.20494711\n",
            "Val acc and loss are 0.8814 and 0.33906683\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.927 and 0.20472677\n",
            "Val acc and loss are 0.8819 and 0.33912906\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.92722 and 0.20459034\n",
            "Val acc and loss are 0.8822 and 0.33911118\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.92752 and 0.20408277\n",
            "Val acc and loss are 0.8817 and 0.3383112\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.92736 and 0.20394988\n",
            "Val acc and loss are 0.8809 and 0.33825362\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.92694 and 0.20398867\n",
            "Val acc and loss are 0.8815 and 0.33877978\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.9271 and 0.20408645\n",
            "Val acc and loss are 0.8818 and 0.3392867\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.9273 and 0.20388891\n",
            "Val acc and loss are 0.8812 and 0.33916894\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.92764 and 0.20342994\n",
            "Val acc and loss are 0.8819 and 0.3383504\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.92752 and 0.2032659\n",
            "Val acc and loss are 0.8824 and 0.33825433\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.92732 and 0.20322226\n",
            "Val acc and loss are 0.8822 and 0.3389878\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.92736 and 0.20328511\n",
            "Val acc and loss are 0.8822 and 0.33961046\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.92754 and 0.20315348\n",
            "Val acc and loss are 0.8818 and 0.33975515\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.92764 and 0.20280619\n",
            "Val acc and loss are 0.8814 and 0.33919573\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.92768 and 0.20269516\n",
            "Val acc and loss are 0.8819 and 0.33895043\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.92766 and 0.20258556\n",
            "Val acc and loss are 0.8819 and 0.33932233\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.92778 and 0.20266637\n",
            "Val acc and loss are 0.8817 and 0.3398386\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.928 and 0.20250185\n",
            "Val acc and loss are 0.8823 and 0.33969268\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.928 and 0.20224111\n",
            "Val acc and loss are 0.8817 and 0.3391733\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.92842 and 0.20161347\n",
            "Val acc and loss are 0.8818 and 0.33830863\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.92846 and 0.2012402\n",
            "Val acc and loss are 0.8824 and 0.33792344\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.92878 and 0.20098114\n",
            "Val acc and loss are 0.8826 and 0.33789706\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.92876 and 0.2008841\n",
            "Val acc and loss are 0.8818 and 0.3379594\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.92866 and 0.20090877\n",
            "Val acc and loss are 0.8821 and 0.3383706\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.92864 and 0.20094277\n",
            "Val acc and loss are 0.882 and 0.33883426\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.92856 and 0.20066279\n",
            "Val acc and loss are 0.8821 and 0.33851805\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.92866 and 0.20039657\n",
            "Val acc and loss are 0.8823 and 0.3381747\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.9286 and 0.20016477\n",
            "Val acc and loss are 0.8822 and 0.33791786\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.92892 and 0.19998677\n",
            "Val acc and loss are 0.8822 and 0.33817947\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.92896 and 0.19989415\n",
            "Val acc and loss are 0.8823 and 0.3386704\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.92894 and 0.1996897\n",
            "Val acc and loss are 0.8827 and 0.33866525\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.92912 and 0.19949116\n",
            "Val acc and loss are 0.8836 and 0.3384839\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.92892 and 0.19948918\n",
            "Val acc and loss are 0.8833 and 0.33862317\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.92898 and 0.19934131\n",
            "Val acc and loss are 0.8832 and 0.3387393\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.92912 and 0.19910239\n",
            "Val acc and loss are 0.8831 and 0.33879775\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.92892 and 0.19881138\n",
            "Val acc and loss are 0.8828 and 0.33858857\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.92908 and 0.19860093\n",
            "Val acc and loss are 0.8828 and 0.3384542\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.92932 and 0.19834453\n",
            "Val acc and loss are 0.8829 and 0.338269\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.92932 and 0.19826363\n",
            "Val acc and loss are 0.8832 and 0.3384294\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.92926 and 0.19809057\n",
            "Val acc and loss are 0.8832 and 0.33829623\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.9292 and 0.19802848\n",
            "Val acc and loss are 0.8827 and 0.33835116\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.9292 and 0.1977639\n",
            "Val acc and loss are 0.8831 and 0.33824718\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.92926 and 0.19745496\n",
            "Val acc and loss are 0.8829 and 0.33808303\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.92908 and 0.19732945\n",
            "Val acc and loss are 0.8821 and 0.33817577\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.92948 and 0.19724394\n",
            "Val acc and loss are 0.882 and 0.33842388\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.92996 and 0.1971653\n",
            "Val acc and loss are 0.8813 and 0.33863443\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.93008 and 0.19699195\n",
            "Val acc and loss are 0.8815 and 0.33872804\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.9301 and 0.19661029\n",
            "Val acc and loss are 0.8822 and 0.33833593\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.93012 and 0.1965859\n",
            "Val acc and loss are 0.8832 and 0.33835357\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.93004 and 0.19641894\n",
            "Val acc and loss are 0.8831 and 0.338175\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.93016 and 0.19627611\n",
            "Val acc and loss are 0.8831 and 0.33816078\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.93046 and 0.19624789\n",
            "Val acc and loss are 0.8829 and 0.33842394\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.9305 and 0.19597183\n",
            "Val acc and loss are 0.8824 and 0.33840033\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.93078 and 0.19557442\n",
            "Val acc and loss are 0.8828 and 0.33804664\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.9306 and 0.19541891\n",
            "Val acc and loss are 0.883 and 0.3380126\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.93094 and 0.1951616\n",
            "Val acc and loss are 0.8829 and 0.33774567\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.93104 and 0.19506219\n",
            "Val acc and loss are 0.8833 and 0.33785418\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.93078 and 0.1952106\n",
            "Val acc and loss are 0.8825 and 0.3387174\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.93042 and 0.19547871\n",
            "Val acc and loss are 0.883 and 0.3397483\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.93044 and 0.1950265\n",
            "Val acc and loss are 0.8827 and 0.33941644\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.93072 and 0.19469003\n",
            "Val acc and loss are 0.882 and 0.3389186\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.93074 and 0.19471203\n",
            "Val acc and loss are 0.8816 and 0.33903497\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.93088 and 0.19453745\n",
            "Val acc and loss are 0.8815 and 0.3396577\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.93062 and 0.19475296\n",
            "Val acc and loss are 0.8817 and 0.34032616\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.9309 and 0.19422325\n",
            "Val acc and loss are 0.8818 and 0.33950752\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.93108 and 0.19385996\n",
            "Val acc and loss are 0.8825 and 0.33848965\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.93152 and 0.19350532\n",
            "Val acc and loss are 0.8827 and 0.33816144\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.93124 and 0.19334377\n",
            "Val acc and loss are 0.8824 and 0.33868524\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.93116 and 0.19353165\n",
            "Val acc and loss are 0.8832 and 0.33937046\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.93128 and 0.19321035\n",
            "Val acc and loss are 0.8823 and 0.33904526\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.93138 and 0.19314376\n",
            "Val acc and loss are 0.8832 and 0.33872506\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.93158 and 0.19310912\n",
            "Val acc and loss are 0.8832 and 0.33867022\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.93144 and 0.19325624\n",
            "Val acc and loss are 0.8827 and 0.33919215\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.9314 and 0.19352578\n",
            "Val acc and loss are 0.8826 and 0.33968908\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.9315 and 0.19298355\n",
            "Val acc and loss are 0.8822 and 0.3390779\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.9317 and 0.19258617\n",
            "Val acc and loss are 0.8817 and 0.33865502\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.93178 and 0.19200648\n",
            "Val acc and loss are 0.8825 and 0.33854005\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.9319 and 0.19186343\n",
            "Val acc and loss are 0.8835 and 0.33881733\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.9316 and 0.19166102\n",
            "Val acc and loss are 0.8838 and 0.33871314\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.93232 and 0.19120967\n",
            "Val acc and loss are 0.8827 and 0.33788916\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.93232 and 0.1911119\n",
            "Val acc and loss are 0.8822 and 0.33752334\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.93214 and 0.1911914\n",
            "Val acc and loss are 0.8822 and 0.33767125\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.93182 and 0.19127563\n",
            "Val acc and loss are 0.8829 and 0.3382525\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.932 and 0.19149348\n",
            "Val acc and loss are 0.8834 and 0.33886865\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.93206 and 0.19118795\n",
            "Val acc and loss are 0.8838 and 0.3385106\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.93242 and 0.1908171\n",
            "Val acc and loss are 0.883 and 0.3380268\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.9327 and 0.19058727\n",
            "Val acc and loss are 0.8831 and 0.33780405\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.9325 and 0.19042508\n",
            "Val acc and loss are 0.8833 and 0.33813837\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.93212 and 0.19061969\n",
            "Val acc and loss are 0.8832 and 0.33891732\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.93236 and 0.1904268\n",
            "Val acc and loss are 0.8834 and 0.3388337\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.93236 and 0.18987353\n",
            "Val acc and loss are 0.884 and 0.33782396\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.93266 and 0.18959045\n",
            "Val acc and loss are 0.8837 and 0.3373284\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.93268 and 0.18940748\n",
            "Val acc and loss are 0.8833 and 0.33748838\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.93288 and 0.18955809\n",
            "Val acc and loss are 0.8829 and 0.33832988\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.93256 and 0.18959847\n",
            "Val acc and loss are 0.882 and 0.33872208\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.9329 and 0.18937199\n",
            "Val acc and loss are 0.8818 and 0.33862135\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.93312 and 0.18903875\n",
            "Val acc and loss are 0.8823 and 0.33820045\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.93314 and 0.18894687\n",
            "Val acc and loss are 0.8829 and 0.3383363\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.93316 and 0.18905613\n",
            "Val acc and loss are 0.8841 and 0.3391149\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.93284 and 0.18926223\n",
            "Val acc and loss are 0.8841 and 0.33990285\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.9334 and 0.18870972\n",
            "Val acc and loss are 0.8832 and 0.33937204\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.93346 and 0.18836027\n",
            "Val acc and loss are 0.8829 and 0.33900678\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.93356 and 0.18792957\n",
            "Val acc and loss are 0.8839 and 0.33870664\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.93358 and 0.1878747\n",
            "Val acc and loss are 0.8838 and 0.33914885\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9335 and 0.18774101\n",
            "Val acc and loss are 0.8842 and 0.33909374\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.93362 and 0.18749942\n",
            "Val acc and loss are 0.8836 and 0.33878422\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.93374 and 0.18732972\n",
            "Val acc and loss are 0.8834 and 0.33857235\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.93396 and 0.18713075\n",
            "Val acc and loss are 0.8832 and 0.33850005\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.93384 and 0.18722348\n",
            "Val acc and loss are 0.8828 and 0.3387831\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.93406 and 0.1870711\n",
            "Val acc and loss are 0.8829 and 0.33854812\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.93398 and 0.18702674\n",
            "Val acc and loss are 0.8825 and 0.3384993\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.9342 and 0.18687569\n",
            "Val acc and loss are 0.882 and 0.33863094\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.93422 and 0.18671046\n",
            "Val acc and loss are 0.8823 and 0.33885202\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.93386 and 0.18661344\n",
            "Val acc and loss are 0.8832 and 0.33906797\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.93416 and 0.18637227\n",
            "Val acc and loss are 0.8834 and 0.33901355\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.93408 and 0.18619394\n",
            "Val acc and loss are 0.8836 and 0.33897564\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.93404 and 0.1859472\n",
            "Val acc and loss are 0.8832 and 0.338926\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.93438 and 0.18566425\n",
            "Val acc and loss are 0.8835 and 0.33890972\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.93456 and 0.18549496\n",
            "Val acc and loss are 0.884 and 0.3390564\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.93458 and 0.18545681\n",
            "Val acc and loss are 0.8833 and 0.33929855\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.93456 and 0.1853945\n",
            "Val acc and loss are 0.8835 and 0.33946234\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.9344 and 0.18516713\n",
            "Val acc and loss are 0.8832 and 0.33923346\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.93458 and 0.18487945\n",
            "Val acc and loss are 0.8831 and 0.33885533\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.93464 and 0.18473789\n",
            "Val acc and loss are 0.8833 and 0.33885694\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.93504 and 0.18460298\n",
            "Val acc and loss are 0.8834 and 0.3389706\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.9347 and 0.18461844\n",
            "Val acc and loss are 0.8839 and 0.33944154\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.9348 and 0.18467157\n",
            "Val acc and loss are 0.8828 and 0.3399266\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.93478 and 0.18442152\n",
            "Val acc and loss are 0.8827 and 0.33977696\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.93498 and 0.18418512\n",
            "Val acc and loss are 0.8823 and 0.3395412\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.93508 and 0.18392813\n",
            "Val acc and loss are 0.8827 and 0.3393711\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.93512 and 0.18380953\n",
            "Val acc and loss are 0.8834 and 0.33949476\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.93492 and 0.18382344\n",
            "Val acc and loss are 0.8835 and 0.3397357\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.93516 and 0.18376887\n",
            "Val acc and loss are 0.8841 and 0.33965057\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.93542 and 0.18358524\n",
            "Val acc and loss are 0.8839 and 0.33917376\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.93554 and 0.18357636\n",
            "Val acc and loss are 0.883 and 0.33920056\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.93508 and 0.1834576\n",
            "Val acc and loss are 0.8835 and 0.33936185\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.93512 and 0.18316676\n",
            "Val acc and loss are 0.8845 and 0.33927983\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.93556 and 0.18295676\n",
            "Val acc and loss are 0.8845 and 0.33930895\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.93594 and 0.18285492\n",
            "Val acc and loss are 0.8844 and 0.3393609\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.93598 and 0.18282336\n",
            "Val acc and loss are 0.8843 and 0.33939052\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.93594 and 0.18273066\n",
            "Val acc and loss are 0.884 and 0.339654\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.93624 and 0.18241668\n",
            "Val acc and loss are 0.8835 and 0.3395082\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.93612 and 0.18218338\n",
            "Val acc and loss are 0.8835 and 0.3393785\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.93604 and 0.18184757\n",
            "Val acc and loss are 0.8831 and 0.3394006\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.93604 and 0.18170114\n",
            "Val acc and loss are 0.884 and 0.3397486\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.9363 and 0.18159129\n",
            "Val acc and loss are 0.884 and 0.33990166\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.9363 and 0.18157242\n",
            "Val acc and loss are 0.8837 and 0.34008044\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.9364 and 0.18152566\n",
            "Val acc and loss are 0.8828 and 0.3400231\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.93648 and 0.18142113\n",
            "Val acc and loss are 0.8828 and 0.3400076\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.93612 and 0.18127628\n",
            "Val acc and loss are 0.8832 and 0.33976588\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.93612 and 0.18103258\n",
            "Val acc and loss are 0.8831 and 0.33975965\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.93628 and 0.18087988\n",
            "Val acc and loss are 0.8844 and 0.33995023\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.93626 and 0.18067236\n",
            "Val acc and loss are 0.8844 and 0.33984107\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.9366 and 0.18041296\n",
            "Val acc and loss are 0.8845 and 0.33938128\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.93662 and 0.18029554\n",
            "Val acc and loss are 0.8849 and 0.33920488\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.93664 and 0.18022549\n",
            "Val acc and loss are 0.8848 and 0.3394277\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.93658 and 0.18013239\n",
            "Val acc and loss are 0.8845 and 0.33956763\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.93658 and 0.18003672\n",
            "Val acc and loss are 0.8844 and 0.33972663\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.93668 and 0.17982066\n",
            "Val acc and loss are 0.8834 and 0.3395827\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.9366 and 0.17969015\n",
            "Val acc and loss are 0.8836 and 0.3395593\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.9372 and 0.17953852\n",
            "Val acc and loss are 0.8836 and 0.33969784\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.93694 and 0.17939574\n",
            "Val acc and loss are 0.8836 and 0.33998722\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.9371 and 0.17911436\n",
            "Val acc and loss are 0.8839 and 0.33970267\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.93688 and 0.17897566\n",
            "Val acc and loss are 0.8836 and 0.3396876\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.9369 and 0.17883141\n",
            "Val acc and loss are 0.8836 and 0.33970925\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.93688 and 0.17878516\n",
            "Val acc and loss are 0.884 and 0.33990473\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.93724 and 0.1787039\n",
            "Val acc and loss are 0.8842 and 0.33988583\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.93736 and 0.17848301\n",
            "Val acc and loss are 0.8843 and 0.3395569\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.93726 and 0.17830054\n",
            "Val acc and loss are 0.8839 and 0.33945292\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.93724 and 0.17810482\n",
            "Val acc and loss are 0.8849 and 0.33949536\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.93738 and 0.1780088\n",
            "Val acc and loss are 0.885 and 0.33989415\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.93744 and 0.1780016\n",
            "Val acc and loss are 0.8848 and 0.34034422\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.93754 and 0.17766418\n",
            "Val acc and loss are 0.8849 and 0.34005743\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.93754 and 0.17731239\n",
            "Val acc and loss are 0.8837 and 0.33956152\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.9379 and 0.1770907\n",
            "Val acc and loss are 0.884 and 0.3396958\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.93752 and 0.17702626\n",
            "Val acc and loss are 0.8834 and 0.3399167\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.93796 and 0.17686895\n",
            "Val acc and loss are 0.8838 and 0.3397817\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.93786 and 0.17686683\n",
            "Val acc and loss are 0.8841 and 0.33988613\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.93798 and 0.17694153\n",
            "Val acc and loss are 0.8843 and 0.34007323\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.93804 and 0.17700118\n",
            "Val acc and loss are 0.884 and 0.3404082\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.93832 and 0.17672773\n",
            "Val acc and loss are 0.8848 and 0.34003848\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.93846 and 0.17641662\n",
            "Val acc and loss are 0.8845 and 0.3395687\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.93816 and 0.17627387\n",
            "Val acc and loss are 0.8843 and 0.33952025\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.93812 and 0.17619585\n",
            "Val acc and loss are 0.8838 and 0.3398189\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.93794 and 0.17617549\n",
            "Val acc and loss are 0.884 and 0.34030166\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.93816 and 0.1760445\n",
            "Val acc and loss are 0.8843 and 0.34036398\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.93838 and 0.17566924\n",
            "Val acc and loss are 0.885 and 0.33990094\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.93846 and 0.1754072\n",
            "Val acc and loss are 0.8846 and 0.33948794\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.93844 and 0.17532855\n",
            "Val acc and loss are 0.8847 and 0.3393931\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.9386 and 0.17533515\n",
            "Val acc and loss are 0.8849 and 0.33996555\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.9382 and 0.17540805\n",
            "Val acc and loss are 0.8854 and 0.3403812\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.93822 and 0.17533672\n",
            "Val acc and loss are 0.8856 and 0.3404383\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.93844 and 0.17499457\n",
            "Val acc and loss are 0.8847 and 0.33991998\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.9386 and 0.17469952\n",
            "Val acc and loss are 0.8844 and 0.3397617\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.93902 and 0.17445527\n",
            "Val acc and loss are 0.8846 and 0.3397514\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.93934 and 0.17441107\n",
            "Val acc and loss are 0.8851 and 0.33990216\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.9391 and 0.17450662\n",
            "Val acc and loss are 0.8852 and 0.34003904\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.93868 and 0.17445262\n",
            "Val acc and loss are 0.8857 and 0.3399639\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.9382 and 0.17462133\n",
            "Val acc and loss are 0.8858 and 0.3404394\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.9384 and 0.1745107\n",
            "Val acc and loss are 0.8849 and 0.3407536\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.93856 and 0.1741682\n",
            "Val acc and loss are 0.8847 and 0.34051597\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.93856 and 0.17367093\n",
            "Val acc and loss are 0.8847 and 0.33969384\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.93868 and 0.17342532\n",
            "Val acc and loss are 0.8842 and 0.3391159\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.9388 and 0.17351243\n",
            "Val acc and loss are 0.8843 and 0.33974752\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.93896 and 0.17359386\n",
            "Val acc and loss are 0.8845 and 0.34017348\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.93916 and 0.1732846\n",
            "Val acc and loss are 0.8846 and 0.3398229\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.93884 and 0.17292516\n",
            "Val acc and loss are 0.8837 and 0.33904386\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.9387 and 0.17283851\n",
            "Val acc and loss are 0.8845 and 0.33927128\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.93862 and 0.17291752\n",
            "Val acc and loss are 0.8853 and 0.33978555\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.9389 and 0.17306925\n",
            "Val acc and loss are 0.8843 and 0.34064478\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.93914 and 0.1728243\n",
            "Val acc and loss are 0.8842 and 0.340536\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.93906 and 0.17263098\n",
            "Val acc and loss are 0.8842 and 0.34001577\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.93918 and 0.1724668\n",
            "Val acc and loss are 0.8844 and 0.3399037\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.93938 and 0.17232664\n",
            "Val acc and loss are 0.8852 and 0.34012422\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.93932 and 0.17260183\n",
            "Val acc and loss are 0.8856 and 0.3409238\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.9396 and 0.17217615\n",
            "Val acc and loss are 0.8854 and 0.3404459\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.93948 and 0.17184395\n",
            "Val acc and loss are 0.8847 and 0.33987057\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.93958 and 0.17160214\n",
            "Val acc and loss are 0.8846 and 0.33979413\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.94018 and 0.17135848\n",
            "Val acc and loss are 0.8849 and 0.33995813\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.94032 and 0.17120919\n",
            "Val acc and loss are 0.8853 and 0.34043133\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.9402 and 0.17099811\n",
            "Val acc and loss are 0.8848 and 0.34045756\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.94016 and 0.17079277\n",
            "Val acc and loss are 0.8849 and 0.34036776\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.9404 and 0.17066869\n",
            "Val acc and loss are 0.8841 and 0.34053397\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.94034 and 0.1705766\n",
            "Val acc and loss are 0.8843 and 0.34066457\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.94044 and 0.17050444\n",
            "Val acc and loss are 0.8848 and 0.34077772\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.94 and 0.17055304\n",
            "Val acc and loss are 0.8853 and 0.3408351\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.9399 and 0.17078316\n",
            "Val acc and loss are 0.8848 and 0.34103173\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.93986 and 0.17054383\n",
            "Val acc and loss are 0.8841 and 0.3406019\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.94024 and 0.1702835\n",
            "Val acc and loss are 0.8845 and 0.34025043\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.94028 and 0.17002778\n",
            "Val acc and loss are 0.8853 and 0.34030956\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.94042 and 0.16966633\n",
            "Val acc and loss are 0.8852 and 0.3402297\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.94044 and 0.16944617\n",
            "Val acc and loss are 0.8848 and 0.34010735\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.94046 and 0.16919962\n",
            "Val acc and loss are 0.885 and 0.33998194\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.94054 and 0.16899757\n",
            "Val acc and loss are 0.8849 and 0.3399649\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.94054 and 0.16902937\n",
            "Val acc and loss are 0.8848 and 0.3402249\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.9404 and 0.16909441\n",
            "Val acc and loss are 0.8839 and 0.34035692\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.94054 and 0.1690789\n",
            "Val acc and loss are 0.8835 and 0.3402232\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.94068 and 0.168942\n",
            "Val acc and loss are 0.8842 and 0.34014943\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.94074 and 0.16883336\n",
            "Val acc and loss are 0.8847 and 0.34057632\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.9408 and 0.16861346\n",
            "Val acc and loss are 0.8851 and 0.3407204\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.94086 and 0.16840015\n",
            "Val acc and loss are 0.8853 and 0.34073186\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.9412 and 0.16817003\n",
            "Val acc and loss are 0.8842 and 0.34050116\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.9411 and 0.16804992\n",
            "Val acc and loss are 0.884 and 0.3403463\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.94134 and 0.16792902\n",
            "Val acc and loss are 0.884 and 0.34027573\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.94118 and 0.16786465\n",
            "Val acc and loss are 0.8848 and 0.34044808\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.94104 and 0.16770299\n",
            "Val acc and loss are 0.8851 and 0.34014904\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.94112 and 0.16754912\n",
            "Val acc and loss are 0.8847 and 0.33984816\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.94122 and 0.16753127\n",
            "Val acc and loss are 0.8847 and 0.33998746\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.94124 and 0.16750754\n",
            "Val acc and loss are 0.8846 and 0.340431\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.9412 and 0.16739358\n",
            "Val acc and loss are 0.8846 and 0.3404622\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.94124 and 0.16719773\n",
            "Val acc and loss are 0.885 and 0.34020934\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.94126 and 0.16704355\n",
            "Val acc and loss are 0.8844 and 0.33963826\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.9415 and 0.16706459\n",
            "Val acc and loss are 0.885 and 0.3401001\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.9414 and 0.16727996\n",
            "Val acc and loss are 0.8847 and 0.34082308\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.94166 and 0.16706352\n",
            "Val acc and loss are 0.885 and 0.34082422\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.94174 and 0.16651736\n",
            "Val acc and loss are 0.8851 and 0.3399318\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.94186 and 0.16619949\n",
            "Val acc and loss are 0.8858 and 0.33997157\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.9415 and 0.16619438\n",
            "Val acc and loss are 0.8855 and 0.34088778\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.94144 and 0.16629942\n",
            "Val acc and loss are 0.885 and 0.3415068\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.94154 and 0.16613701\n",
            "Val acc and loss are 0.8847 and 0.34121466\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.94188 and 0.1660925\n",
            "Val acc and loss are 0.8852 and 0.34086952\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.94172 and 0.16590211\n",
            "Val acc and loss are 0.8852 and 0.34038436\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.94202 and 0.16568121\n",
            "Val acc and loss are 0.8852 and 0.3406316\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.94184 and 0.16559443\n",
            "Val acc and loss are 0.886 and 0.34113002\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.94222 and 0.1652734\n",
            "Val acc and loss are 0.8849 and 0.34090656\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.9426 and 0.16486754\n",
            "Val acc and loss are 0.8842 and 0.34042054\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.94256 and 0.16476059\n",
            "Val acc and loss are 0.8843 and 0.3400248\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.94264 and 0.16469792\n",
            "Val acc and loss are 0.8845 and 0.34011802\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.94218 and 0.16491933\n",
            "Val acc and loss are 0.8856 and 0.34043902\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.9421 and 0.16532251\n",
            "Val acc and loss are 0.8866 and 0.34087008\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.9422 and 0.16508614\n",
            "Val acc and loss are 0.8855 and 0.34057495\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.94204 and 0.16482942\n",
            "Val acc and loss are 0.885 and 0.34023836\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.9419 and 0.16475175\n",
            "Val acc and loss are 0.8851 and 0.34068993\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.94224 and 0.16454437\n",
            "Val acc and loss are 0.8854 and 0.34130016\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.94222 and 0.16459523\n",
            "Val acc and loss are 0.8859 and 0.341813\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.94252 and 0.16421832\n",
            "Val acc and loss are 0.8857 and 0.34110218\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.94254 and 0.16380395\n",
            "Val acc and loss are 0.8846 and 0.3405433\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.9423 and 0.16388863\n",
            "Val acc and loss are 0.8852 and 0.340621\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.94224 and 0.16385326\n",
            "Val acc and loss are 0.8853 and 0.34103665\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.9426 and 0.1636589\n",
            "Val acc and loss are 0.8865 and 0.3408598\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.94296 and 0.1634438\n",
            "Val acc and loss are 0.8871 and 0.3404749\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.943 and 0.16293873\n",
            "Val acc and loss are 0.886 and 0.34015167\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.9431 and 0.16280892\n",
            "Val acc and loss are 0.8845 and 0.34073797\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.94298 and 0.16294615\n",
            "Val acc and loss are 0.8848 and 0.34141827\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.94308 and 0.16286619\n",
            "Val acc and loss are 0.8849 and 0.34131235\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.94322 and 0.16280058\n",
            "Val acc and loss are 0.8845 and 0.34074572\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.94282 and 0.16288169\n",
            "Val acc and loss are 0.8847 and 0.34077522\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.94296 and 0.16268276\n",
            "Val acc and loss are 0.8842 and 0.3404745\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.94278 and 0.16273801\n",
            "Val acc and loss are 0.8846 and 0.3406877\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.94286 and 0.16278769\n",
            "Val acc and loss are 0.8848 and 0.34100702\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.94306 and 0.16252366\n",
            "Val acc and loss are 0.8847 and 0.34078643\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.9431 and 0.16214857\n",
            "Val acc and loss are 0.8852 and 0.34026673\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.94324 and 0.16168539\n",
            "Val acc and loss are 0.8857 and 0.33977008\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.94322 and 0.16156667\n",
            "Val acc and loss are 0.8858 and 0.33997294\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.94284 and 0.16188075\n",
            "Val acc and loss are 0.8851 and 0.3408494\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.94312 and 0.16193463\n",
            "Val acc and loss are 0.8842 and 0.3411893\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.9431 and 0.16191325\n",
            "Val acc and loss are 0.8848 and 0.3410101\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.94306 and 0.161705\n",
            "Val acc and loss are 0.8855 and 0.34068885\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.9432 and 0.16156274\n",
            "Val acc and loss are 0.8854 and 0.34095988\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.94356 and 0.16171344\n",
            "Val acc and loss are 0.8857 and 0.34190258\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.94332 and 0.16149753\n",
            "Val acc and loss are 0.8857 and 0.34228617\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.94356 and 0.16105552\n",
            "Val acc and loss are 0.8847 and 0.34200832\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.94342 and 0.16081826\n",
            "Val acc and loss are 0.8855 and 0.34165162\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.94352 and 0.16061573\n",
            "Val acc and loss are 0.8861 and 0.341397\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.94358 and 0.1604499\n",
            "Val acc and loss are 0.8874 and 0.34108198\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.94374 and 0.16043943\n",
            "Val acc and loss are 0.8867 and 0.34111908\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.94388 and 0.16042085\n",
            "Val acc and loss are 0.8855 and 0.3411342\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.94368 and 0.16018215\n",
            "Val acc and loss are 0.885 and 0.34091914\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.944 and 0.15974632\n",
            "Val acc and loss are 0.8861 and 0.34068358\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.94416 and 0.15978977\n",
            "Val acc and loss are 0.8866 and 0.34161675\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.9436 and 0.16002567\n",
            "Val acc and loss are 0.887 and 0.34243578\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.94404 and 0.15980886\n",
            "Val acc and loss are 0.8867 and 0.34212703\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.94408 and 0.1595989\n",
            "Val acc and loss are 0.8857 and 0.34127873\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.94414 and 0.15957029\n",
            "Val acc and loss are 0.8853 and 0.34131068\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.94464 and 0.15946513\n",
            "Val acc and loss are 0.8849 and 0.3420233\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.94444 and 0.15954973\n",
            "Val acc and loss are 0.8854 and 0.3425905\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.94454 and 0.15910833\n",
            "Val acc and loss are 0.8849 and 0.3419516\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.94442 and 0.15898576\n",
            "Val acc and loss are 0.8857 and 0.34164274\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.94438 and 0.15894926\n",
            "Val acc and loss are 0.8851 and 0.34199727\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.9442 and 0.15899937\n",
            "Val acc and loss are 0.8853 and 0.34215003\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.94442 and 0.15874007\n",
            "Val acc and loss are 0.8852 and 0.34163353\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.94438 and 0.15834552\n",
            "Val acc and loss are 0.8859 and 0.340762\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.94442 and 0.1581018\n",
            "Val acc and loss are 0.8856 and 0.3405275\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.9447 and 0.15815197\n",
            "Val acc and loss are 0.8851 and 0.34105888\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.9447 and 0.15835045\n",
            "Val acc and loss are 0.8852 and 0.34167105\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.94446 and 0.15840887\n",
            "Val acc and loss are 0.8847 and 0.34196287\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.94476 and 0.15782975\n",
            "Val acc and loss are 0.885 and 0.34104472\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.94506 and 0.15752387\n",
            "Val acc and loss are 0.8857 and 0.34048685\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.94486 and 0.15747555\n",
            "Val acc and loss are 0.8851 and 0.3411714\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.94502 and 0.15759279\n",
            "Val acc and loss are 0.8849 and 0.3419481\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.94504 and 0.15737703\n",
            "Val acc and loss are 0.8851 and 0.34197173\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.94532 and 0.15695833\n",
            "Val acc and loss are 0.8853 and 0.3412154\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.94506 and 0.15685676\n",
            "Val acc and loss are 0.8851 and 0.34097743\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.9452 and 0.15684648\n",
            "Val acc and loss are 0.8854 and 0.34137562\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.94534 and 0.15681662\n",
            "Val acc and loss are 0.8855 and 0.34178466\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.94518 and 0.15668851\n",
            "Val acc and loss are 0.8852 and 0.34177846\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.94512 and 0.1565275\n",
            "Val acc and loss are 0.8852 and 0.34152642\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.94546 and 0.15631263\n",
            "Val acc and loss are 0.8856 and 0.3411429\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.94522 and 0.15626341\n",
            "Val acc and loss are 0.8858 and 0.34140632\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.94504 and 0.1562253\n",
            "Val acc and loss are 0.8863 and 0.34154093\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.94534 and 0.15633391\n",
            "Val acc and loss are 0.8857 and 0.3418813\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.9453 and 0.1563414\n",
            "Val acc and loss are 0.8858 and 0.34194708\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.94542 and 0.15589102\n",
            "Val acc and loss are 0.8862 and 0.34141538\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.94572 and 0.1555874\n",
            "Val acc and loss are 0.8853 and 0.34134594\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.94576 and 0.15549535\n",
            "Val acc and loss are 0.8857 and 0.3418781\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.94578 and 0.1555013\n",
            "Val acc and loss are 0.8867 and 0.34255904\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.94602 and 0.15524366\n",
            "Val acc and loss are 0.886 and 0.34246346\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.94618 and 0.15492742\n",
            "Val acc and loss are 0.8865 and 0.34216112\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.94582 and 0.15492637\n",
            "Val acc and loss are 0.8864 and 0.34223962\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.94562 and 0.15517071\n",
            "Val acc and loss are 0.886 and 0.3430283\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.94556 and 0.15528265\n",
            "Val acc and loss are 0.8861 and 0.3434526\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.94584 and 0.15508166\n",
            "Val acc and loss are 0.8862 and 0.3429073\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.94576 and 0.15474992\n",
            "Val acc and loss are 0.887 and 0.34240717\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.94604 and 0.15455966\n",
            "Val acc and loss are 0.8876 and 0.34223783\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.94612 and 0.15457956\n",
            "Val acc and loss are 0.887 and 0.3426757\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.94618 and 0.15460888\n",
            "Val acc and loss are 0.887 and 0.34293514\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.94604 and 0.15432967\n",
            "Val acc and loss are 0.886 and 0.3423807\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.94638 and 0.15408593\n",
            "Val acc and loss are 0.8859 and 0.34194556\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.94642 and 0.15406322\n",
            "Val acc and loss are 0.8865 and 0.3421735\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.94628 and 0.15404895\n",
            "Val acc and loss are 0.8863 and 0.34266025\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.94618 and 0.15411046\n",
            "Val acc and loss are 0.886 and 0.34299487\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.94652 and 0.15368421\n",
            "Val acc and loss are 0.8867 and 0.34222308\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.9466 and 0.1533114\n",
            "Val acc and loss are 0.8869 and 0.34190556\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.94644 and 0.15319474\n",
            "Val acc and loss are 0.8869 and 0.34225267\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.9465 and 0.15336287\n",
            "Val acc and loss are 0.8862 and 0.34296328\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.94672 and 0.15342258\n",
            "Val acc and loss are 0.8856 and 0.34288123\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.94642 and 0.15332592\n",
            "Val acc and loss are 0.8862 and 0.34232923\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.94684 and 0.15293793\n",
            "Val acc and loss are 0.8861 and 0.34194908\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.94652 and 0.15315834\n",
            "Val acc and loss are 0.8865 and 0.34293053\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.94642 and 0.15334138\n",
            "Val acc and loss are 0.8871 and 0.3435962\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.94662 and 0.15289028\n",
            "Val acc and loss are 0.8864 and 0.34314653\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.94672 and 0.15278025\n",
            "Val acc and loss are 0.8863 and 0.34284768\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.94676 and 0.15251434\n",
            "Val acc and loss are 0.8858 and 0.34314632\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.94702 and 0.15252928\n",
            "Val acc and loss are 0.8856 and 0.34397042\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.94698 and 0.15273382\n",
            "Val acc and loss are 0.8859 and 0.3445784\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.94718 and 0.15239333\n",
            "Val acc and loss are 0.8854 and 0.34392\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.94762 and 0.15190108\n",
            "Val acc and loss are 0.8871 and 0.34275004\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.94762 and 0.15160605\n",
            "Val acc and loss are 0.8872 and 0.3420717\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.9473 and 0.15162979\n",
            "Val acc and loss are 0.8862 and 0.34225318\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.94716 and 0.15173574\n",
            "Val acc and loss are 0.8867 and 0.34282622\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.94766 and 0.15138449\n",
            "Val acc and loss are 0.8865 and 0.34237975\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.94756 and 0.15144742\n",
            "Val acc and loss are 0.8861 and 0.34227902\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.94764 and 0.1514085\n",
            "Val acc and loss are 0.8859 and 0.34230617\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.94748 and 0.15133722\n",
            "Val acc and loss are 0.8866 and 0.34269395\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.94768 and 0.1513015\n",
            "Val acc and loss are 0.8864 and 0.34305775\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.94744 and 0.15110266\n",
            "Val acc and loss are 0.8856 and 0.3428819\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.94752 and 0.15075545\n",
            "Val acc and loss are 0.8855 and 0.3423536\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.94768 and 0.15062335\n",
            "Val acc and loss are 0.8861 and 0.34233144\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.94764 and 0.15070158\n",
            "Val acc and loss are 0.8864 and 0.34294125\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.94752 and 0.15109313\n",
            "Val acc and loss are 0.886 and 0.3439314\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.94756 and 0.15111561\n",
            "Val acc and loss are 0.8862 and 0.3440047\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.94766 and 0.15059581\n",
            "Val acc and loss are 0.8865 and 0.34336007\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.94804 and 0.15014754\n",
            "Val acc and loss are 0.8865 and 0.34277695\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.94802 and 0.15003692\n",
            "Val acc and loss are 0.8867 and 0.3432654\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.94784 and 0.15001574\n",
            "Val acc and loss are 0.887 and 0.34362265\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.9477 and 0.15006909\n",
            "Val acc and loss are 0.8866 and 0.34357405\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.9479 and 0.14978951\n",
            "Val acc and loss are 0.8872 and 0.34292483\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.94776 and 0.14967176\n",
            "Val acc and loss are 0.8873 and 0.34256852\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.94806 and 0.1495756\n",
            "Val acc and loss are 0.8872 and 0.34271866\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.94794 and 0.14960454\n",
            "Val acc and loss are 0.8869 and 0.3434488\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.94804 and 0.14962393\n",
            "Val acc and loss are 0.8866 and 0.3437773\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.94828 and 0.14917952\n",
            "Val acc and loss are 0.8859 and 0.3430174\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.94846 and 0.14882237\n",
            "Val acc and loss are 0.8865 and 0.34240377\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.9483 and 0.1487383\n",
            "Val acc and loss are 0.8864 and 0.3422373\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.94836 and 0.14871024\n",
            "Val acc and loss are 0.886 and 0.34261894\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.94848 and 0.14862905\n",
            "Val acc and loss are 0.8862 and 0.34316137\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.94868 and 0.14837623\n",
            "Val acc and loss are 0.8859 and 0.34319118\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.94862 and 0.14808989\n",
            "Val acc and loss are 0.8861 and 0.34320062\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.94868 and 0.14802931\n",
            "Val acc and loss are 0.8858 and 0.34348944\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.94864 and 0.14827122\n",
            "Val acc and loss are 0.8857 and 0.34402192\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.94858 and 0.1484893\n",
            "Val acc and loss are 0.8867 and 0.34427723\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.94856 and 0.14838432\n",
            "Val acc and loss are 0.8865 and 0.34411767\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.94844 and 0.14816873\n",
            "Val acc and loss are 0.8859 and 0.34374723\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.94848 and 0.14799652\n",
            "Val acc and loss are 0.8859 and 0.34353232\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.94868 and 0.14778908\n",
            "Val acc and loss are 0.8859 and 0.34332412\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.94884 and 0.1476699\n",
            "Val acc and loss are 0.8868 and 0.34332678\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.94882 and 0.14758337\n",
            "Val acc and loss are 0.8873 and 0.34351578\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.94878 and 0.14738967\n",
            "Val acc and loss are 0.8877 and 0.34342167\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.94886 and 0.14712672\n",
            "Val acc and loss are 0.8865 and 0.3433512\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.94868 and 0.14715022\n",
            "Val acc and loss are 0.8862 and 0.34391612\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.94894 and 0.14719693\n",
            "Val acc and loss are 0.8858 and 0.34427142\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.9486 and 0.14723542\n",
            "Val acc and loss are 0.8859 and 0.3444273\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.94878 and 0.14703031\n",
            "Val acc and loss are 0.8867 and 0.34425956\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.94872 and 0.14677174\n",
            "Val acc and loss are 0.8871 and 0.34404016\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.94858 and 0.14679019\n",
            "Val acc and loss are 0.8872 and 0.34430507\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.94848 and 0.14688212\n",
            "Val acc and loss are 0.8868 and 0.3444741\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.94876 and 0.14678986\n",
            "Val acc and loss are 0.8867 and 0.34437528\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.9488 and 0.14653061\n",
            "Val acc and loss are 0.8862 and 0.34425724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_poglivUvxB",
        "outputId": "f03739b1-1e7f-45c3-c7e2-12b3924c0eed"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.8877 at epoch 990 with a corresponding training accuracy of 0.94878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RU5L3K1npiM"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "8p3Zrga9nppx",
        "outputId": "6648f87f-d527-4e33-8d83-7b27b708ee2d"
      },
      "source": [
        "# Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.94894 achieved at epoch: 992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1X0S_rnpvK",
        "outputId": "64df899b-bc5e-4b39-e1c5-cbac0f7e9427"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4396,   14,   51,  110,   13,    1,  374,    0,   18,    0],\n",
              "       [   9, 4950,    2,   44,    3,    1,    3,    0,    0,    0],\n",
              "       [  45,    4, 4235,   35,  401,    0,  262,    1,    9,    0],\n",
              "       [  86,   20,   25, 4621,  140,    0,   72,    1,   14,    0],\n",
              "       [  12,    6,  302,  121, 4281,    0,  214,    0,   13,    1],\n",
              "       [   0,    2,    1,    0,    0, 4902,    0,   72,    5,   22],\n",
              "       [ 480,    7,  283,   93,  264,    2, 3883,    0,   18,    0],\n",
              "       [   0,    0,    0,    0,    0,   67,    0, 4868,    3,  107],\n",
              "       [   4,    2,    8,   17,    6,    1,   19,    2, 4973,    0],\n",
              "       [   1,    0,    0,    0,    0,   19,    0,  101,    2, 4856]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "N-RKqmWdnpnM",
        "outputId": "443edf1d-c58c-43f3-8880-bd36aa797ab0"
      },
      "source": [
        "# Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8877 achieved at epoch: 989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7BfCHmEnplS",
        "outputId": "785e3bbe-9f10-469d-9a25-c2efceee1baf"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[858,   0,  12,  36,   3,   1, 106,   0,   7,   0],\n",
              "       [  1, 962,   1,  16,   3,   0,   3,   0,   2,   0],\n",
              "       [ 13,   1, 819,   8,  95,   0,  66,   0,   6,   0],\n",
              "       [ 25,   3,   6, 923,  42,   0,  20,   0,   2,   0],\n",
              "       [  1,   2,  74,  23, 881,   0,  64,   0,   5,   0],\n",
              "       [  0,   0,   0,   0,   0, 943,   0,  31,   7,  15],\n",
              "       [121,   2,  76,  18,  75,   0, 661,   0,  16,   1],\n",
              "       [  0,   0,   1,   0,   0,  16,   0, 904,   1,  33],\n",
              "       [  3,   0,   6,   3,   2,   1,  11,   3, 937,   2],\n",
              "       [  0,   0,   0,   0,   0,   6,   0,  41,   0, 974]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F73HB7KQnpfT",
        "outputId": "f4fa9f79-d8cf-42fc-8d82-ec0b17424bdb"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37203148\n",
            "0.877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpxKrW-dnywV",
        "outputId": "a1597ef8-0587-429e-f8ea-bc6196be984c"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[830,   2,   9,  33,   3,   1, 112,   0,  10,   0],\n",
              "       [  3, 963,   1,  23,   4,   0,   5,   0,   1,   0],\n",
              "       [ 15,   1, 786,  17, 103,   0,  74,   1,   3,   0],\n",
              "       [ 28,   5,   9, 885,  34,   1,  32,   0,   6,   0],\n",
              "       [  0,   1,  87,  31, 822,   1,  54,   0,   4,   0],\n",
              "       [  1,   0,   0,   1,   0, 945,   0,  28,   3,  22],\n",
              "       [122,   2,  88,  28,  72,   0, 672,   0,  16,   0],\n",
              "       [  0,   0,   0,   0,   0,  20,   0, 952,   1,  27],\n",
              "       [  4,   1,   3,   6,   3,   5,  15,   5, 958,   0],\n",
              "       [  0,   0,   0,   1,   0,   7,   1,  34,   0, 957]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnwYQ-9Wn54B"
      },
      "source": [
        "# **Test 4** *(Revised from Test 3: activation = ReLU)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTHK5Yq2n6Eb"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 128, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"relu\", reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOidhDW5n6MT",
        "outputId": "7c8895bf-b543-4a23-b723-c3486c576d3f"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.34516 and 1.9681259\n",
            "Val acc and loss are 0.3448 and 1.956796\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.5066 and 1.60675\n",
            "Val acc and loss are 0.5086 and 1.596198\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.6057 and 1.3525919\n",
            "Val acc and loss are 0.6083 and 1.3419358\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.65348 and 1.1726726\n",
            "Val acc and loss are 0.6554 and 1.1623123\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.67944 and 1.0461428\n",
            "Val acc and loss are 0.687 and 1.0366586\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.69746 and 0.9573981\n",
            "Val acc and loss are 0.7041 and 0.9490344\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.70994 and 0.8923657\n",
            "Val acc and loss are 0.7169 and 0.885102\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.72242 and 0.840733\n",
            "Val acc and loss are 0.7281 and 0.8345993\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.73318 and 0.797135\n",
            "Val acc and loss are 0.7362 and 0.791959\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.7428 and 0.75904804\n",
            "Val acc and loss are 0.7461 and 0.75475156\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.7514 and 0.7262337\n",
            "Val acc and loss are 0.7523 and 0.72262436\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.7581 and 0.6982209\n",
            "Val acc and loss are 0.7592 and 0.69545037\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.76306 and 0.6746697\n",
            "Val acc and loss are 0.7652 and 0.67265975\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.76836 and 0.6548299\n",
            "Val acc and loss are 0.7698 and 0.65347874\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.77298 and 0.63785607\n",
            "Val acc and loss are 0.7748 and 0.6369578\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.77694 and 0.6230688\n",
            "Val acc and loss are 0.7783 and 0.622571\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.78044 and 0.61013293\n",
            "Val acc and loss are 0.7819 and 0.6099428\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.7839 and 0.5986492\n",
            "Val acc and loss are 0.784 and 0.5988271\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.7879 and 0.58833045\n",
            "Val acc and loss are 0.7872 and 0.58888906\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.79156 and 0.5789167\n",
            "Val acc and loss are 0.7913 and 0.5799431\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.79456 and 0.57019633\n",
            "Val acc and loss are 0.7942 and 0.5717633\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.79716 and 0.5619688\n",
            "Val acc and loss are 0.7965 and 0.56400293\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.79938 and 0.553933\n",
            "Val acc and loss are 0.7985 and 0.5564217\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.8022 and 0.5461949\n",
            "Val acc and loss are 0.8018 and 0.54921573\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.80532 and 0.538595\n",
            "Val acc and loss are 0.8038 and 0.5421574\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.80846 and 0.5312708\n",
            "Val acc and loss are 0.8068 and 0.5353797\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.81112 and 0.52437603\n",
            "Val acc and loss are 0.8086 and 0.52899534\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.8142 and 0.51799726\n",
            "Val acc and loss are 0.8103 and 0.52308196\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.81668 and 0.5121515\n",
            "Val acc and loss are 0.8138 and 0.517649\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.81872 and 0.50675744\n",
            "Val acc and loss are 0.8158 and 0.51264864\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.82072 and 0.5017165\n",
            "Val acc and loss are 0.8176 and 0.50799847\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.82272 and 0.49696046\n",
            "Val acc and loss are 0.8191 and 0.50365627\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.82412 and 0.49247113\n",
            "Val acc and loss are 0.8205 and 0.4995909\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.82562 and 0.4881861\n",
            "Val acc and loss are 0.822 and 0.49571314\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.82654 and 0.4841407\n",
            "Val acc and loss are 0.823 and 0.49210718\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.8278 and 0.48026085\n",
            "Val acc and loss are 0.8241 and 0.48869833\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.8292 and 0.47652066\n",
            "Val acc and loss are 0.8259 and 0.4854161\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.83054 and 0.472916\n",
            "Val acc and loss are 0.8274 and 0.482279\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.8317 and 0.46932715\n",
            "Val acc and loss are 0.8283 and 0.4791545\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.83302 and 0.46573454\n",
            "Val acc and loss are 0.8293 and 0.47604218\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.83446 and 0.462115\n",
            "Val acc and loss are 0.83 and 0.47289452\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.8355 and 0.45862085\n",
            "Val acc and loss are 0.8306 and 0.4698318\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.83708 and 0.45524707\n",
            "Val acc and loss are 0.8311 and 0.4668553\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.83826 and 0.45202446\n",
            "Val acc and loss are 0.8316 and 0.46399394\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.8395 and 0.44894534\n",
            "Val acc and loss are 0.8325 and 0.46128136\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.84052 and 0.4460782\n",
            "Val acc and loss are 0.8332 and 0.45875454\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.84134 and 0.44334063\n",
            "Val acc and loss are 0.8343 and 0.45640698\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.84238 and 0.440696\n",
            "Val acc and loss are 0.8354 and 0.45417714\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.84346 and 0.43812242\n",
            "Val acc and loss are 0.8362 and 0.45204884\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.84458 and 0.43564025\n",
            "Val acc and loss are 0.8367 and 0.4500259\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.84548 and 0.4332844\n",
            "Val acc and loss are 0.8376 and 0.44811803\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.84638 and 0.4309849\n",
            "Val acc and loss are 0.8388 and 0.4462191\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.84698 and 0.42880067\n",
            "Val acc and loss are 0.839 and 0.44437978\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.84746 and 0.4266908\n",
            "Val acc and loss are 0.839 and 0.44256788\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.84808 and 0.42462444\n",
            "Val acc and loss are 0.8393 and 0.44077364\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.8482 and 0.42258886\n",
            "Val acc and loss are 0.84 and 0.4390089\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.84858 and 0.420605\n",
            "Val acc and loss are 0.8405 and 0.43730184\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.84962 and 0.4186184\n",
            "Val acc and loss are 0.8406 and 0.43559146\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.8507 and 0.4166565\n",
            "Val acc and loss are 0.8408 and 0.43390864\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.8512 and 0.41475347\n",
            "Val acc and loss are 0.8415 and 0.43227354\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.8518 and 0.41288632\n",
            "Val acc and loss are 0.8418 and 0.43063593\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.85234 and 0.4110519\n",
            "Val acc and loss are 0.8424 and 0.42901552\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.85286 and 0.40924555\n",
            "Val acc and loss are 0.8431 and 0.42742547\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.85354 and 0.40748554\n",
            "Val acc and loss are 0.8434 and 0.42586514\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.85396 and 0.4057486\n",
            "Val acc and loss are 0.844 and 0.42438057\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.85462 and 0.40398782\n",
            "Val acc and loss are 0.845 and 0.42288652\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.85548 and 0.402219\n",
            "Val acc and loss are 0.8464 and 0.42140132\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.85626 and 0.4005101\n",
            "Val acc and loss are 0.8473 and 0.4200158\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.85694 and 0.39885554\n",
            "Val acc and loss are 0.8479 and 0.41869995\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.85762 and 0.397252\n",
            "Val acc and loss are 0.8482 and 0.41741064\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.85842 and 0.3957068\n",
            "Val acc and loss are 0.8485 and 0.4161751\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.85894 and 0.39416617\n",
            "Val acc and loss are 0.8482 and 0.41495997\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.85914 and 0.3926618\n",
            "Val acc and loss are 0.8484 and 0.41379198\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.85938 and 0.3911787\n",
            "Val acc and loss are 0.8489 and 0.4126655\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.8597 and 0.38976672\n",
            "Val acc and loss are 0.8488 and 0.41162592\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.86052 and 0.38825864\n",
            "Val acc and loss are 0.8491 and 0.41047075\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.86094 and 0.38677913\n",
            "Val acc and loss are 0.8496 and 0.40934607\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.8614 and 0.3853322\n",
            "Val acc and loss are 0.8504 and 0.40823615\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.8617 and 0.38396457\n",
            "Val acc and loss are 0.8511 and 0.40719527\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.86222 and 0.38265574\n",
            "Val acc and loss are 0.8515 and 0.40622216\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.86262 and 0.38135594\n",
            "Val acc and loss are 0.8512 and 0.4052956\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.86336 and 0.38010406\n",
            "Val acc and loss are 0.8514 and 0.40442985\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.86382 and 0.37892106\n",
            "Val acc and loss are 0.8512 and 0.40360352\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.86436 and 0.37771204\n",
            "Val acc and loss are 0.8522 and 0.40271556\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.86464 and 0.37646344\n",
            "Val acc and loss are 0.8527 and 0.40175238\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.86506 and 0.3751985\n",
            "Val acc and loss are 0.853 and 0.40076602\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.86538 and 0.37391317\n",
            "Val acc and loss are 0.8535 and 0.39975503\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.86578 and 0.37263918\n",
            "Val acc and loss are 0.8541 and 0.39875743\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.86596 and 0.37139532\n",
            "Val acc and loss are 0.8551 and 0.39783007\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.8666 and 0.3702189\n",
            "Val acc and loss are 0.8552 and 0.3969462\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.86708 and 0.3690902\n",
            "Val acc and loss are 0.8559 and 0.39613754\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.8674 and 0.36801532\n",
            "Val acc and loss are 0.8564 and 0.395381\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.86756 and 0.36698657\n",
            "Val acc and loss are 0.8564 and 0.39465496\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.8677 and 0.365935\n",
            "Val acc and loss are 0.856 and 0.393888\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.86818 and 0.36482754\n",
            "Val acc and loss are 0.856 and 0.39310044\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.86878 and 0.36369142\n",
            "Val acc and loss are 0.8567 and 0.3922949\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.86904 and 0.36247772\n",
            "Val acc and loss are 0.8571 and 0.39144135\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.86956 and 0.3613014\n",
            "Val acc and loss are 0.8575 and 0.39063373\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.87006 and 0.36015305\n",
            "Val acc and loss are 0.8581 and 0.3898333\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.87044 and 0.35907188\n",
            "Val acc and loss are 0.8581 and 0.3890429\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.87068 and 0.358041\n",
            "Val acc and loss are 0.8583 and 0.38828048\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.87096 and 0.35701773\n",
            "Val acc and loss are 0.8584 and 0.38752425\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.87126 and 0.356043\n",
            "Val acc and loss are 0.8584 and 0.38684106\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.8716 and 0.35508543\n",
            "Val acc and loss are 0.8586 and 0.38620287\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.8719 and 0.35407087\n",
            "Val acc and loss are 0.8586 and 0.38549834\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.87202 and 0.35298488\n",
            "Val acc and loss are 0.8585 and 0.38476023\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.87274 and 0.35189617\n",
            "Val acc and loss are 0.8591 and 0.38399905\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.87328 and 0.35086116\n",
            "Val acc and loss are 0.8593 and 0.38331556\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.87386 and 0.3499081\n",
            "Val acc and loss are 0.8596 and 0.38276273\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.87428 and 0.34902176\n",
            "Val acc and loss are 0.8602 and 0.38226137\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.87456 and 0.34818932\n",
            "Val acc and loss are 0.8607 and 0.38178664\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.87478 and 0.34742516\n",
            "Val acc and loss are 0.8606 and 0.38131136\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.87486 and 0.3466082\n",
            "Val acc and loss are 0.8602 and 0.38079947\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.8751 and 0.34569648\n",
            "Val acc and loss are 0.8601 and 0.3801944\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.87532 and 0.34473148\n",
            "Val acc and loss are 0.8605 and 0.37949008\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.87564 and 0.34377843\n",
            "Val acc and loss are 0.8609 and 0.37878895\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.876 and 0.3428642\n",
            "Val acc and loss are 0.8608 and 0.37817237\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.8762 and 0.34197438\n",
            "Val acc and loss are 0.861 and 0.37760484\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.87634 and 0.3411198\n",
            "Val acc and loss are 0.8614 and 0.37713242\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.87674 and 0.34030122\n",
            "Val acc and loss are 0.8622 and 0.37667286\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.877 and 0.3394393\n",
            "Val acc and loss are 0.8617 and 0.37609455\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.8773 and 0.33856413\n",
            "Val acc and loss are 0.862 and 0.3754265\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.8773 and 0.3376939\n",
            "Val acc and loss are 0.862 and 0.37471431\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.87748 and 0.3368699\n",
            "Val acc and loss are 0.8623 and 0.37405935\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.87772 and 0.3360442\n",
            "Val acc and loss are 0.863 and 0.37347487\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.87802 and 0.33522955\n",
            "Val acc and loss are 0.8626 and 0.3729983\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.87856 and 0.3344664\n",
            "Val acc and loss are 0.8635 and 0.37257612\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.87858 and 0.3337778\n",
            "Val acc and loss are 0.863 and 0.37221357\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.87896 and 0.3330839\n",
            "Val acc and loss are 0.8627 and 0.37178883\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.87898 and 0.3323882\n",
            "Val acc and loss are 0.8626 and 0.37137887\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.87928 and 0.33167937\n",
            "Val acc and loss are 0.8633 and 0.37089586\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.87948 and 0.3309285\n",
            "Val acc and loss are 0.863 and 0.3703711\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.87966 and 0.33012655\n",
            "Val acc and loss are 0.8634 and 0.36980805\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.88014 and 0.32920083\n",
            "Val acc and loss are 0.8637 and 0.3691351\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.88044 and 0.3283015\n",
            "Val acc and loss are 0.8641 and 0.3685394\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.88076 and 0.3274953\n",
            "Val acc and loss are 0.864 and 0.3679899\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.88124 and 0.3267818\n",
            "Val acc and loss are 0.8651 and 0.36749166\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.8815 and 0.32616726\n",
            "Val acc and loss are 0.8648 and 0.36708117\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.88162 and 0.32557678\n",
            "Val acc and loss are 0.8642 and 0.36665982\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.88184 and 0.32488558\n",
            "Val acc and loss are 0.8644 and 0.36617538\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.88198 and 0.32410508\n",
            "Val acc and loss are 0.8648 and 0.3656173\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.88212 and 0.32332692\n",
            "Val acc and loss are 0.865 and 0.36512107\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.8823 and 0.3225372\n",
            "Val acc and loss are 0.8656 and 0.36467493\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.88262 and 0.32177895\n",
            "Val acc and loss are 0.8655 and 0.36429796\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.88298 and 0.32105657\n",
            "Val acc and loss are 0.866 and 0.3639492\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.88316 and 0.32034743\n",
            "Val acc and loss are 0.8663 and 0.36358392\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.8833 and 0.31967032\n",
            "Val acc and loss are 0.8662 and 0.36323833\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.8835 and 0.31899437\n",
            "Val acc and loss are 0.8667 and 0.3628336\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.88368 and 0.31832477\n",
            "Val acc and loss are 0.8669 and 0.3624125\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.88376 and 0.31767172\n",
            "Val acc and loss are 0.8667 and 0.36197224\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.88378 and 0.3169893\n",
            "Val acc and loss are 0.8673 and 0.3615346\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.88408 and 0.3162541\n",
            "Val acc and loss are 0.8673 and 0.3610857\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.88432 and 0.31550357\n",
            "Val acc and loss are 0.8674 and 0.36064377\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.88468 and 0.31478754\n",
            "Val acc and loss are 0.8674 and 0.36023292\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.88506 and 0.31408828\n",
            "Val acc and loss are 0.8676 and 0.35983115\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.88544 and 0.31343895\n",
            "Val acc and loss are 0.867 and 0.35948378\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.88558 and 0.3127908\n",
            "Val acc and loss are 0.8673 and 0.3591607\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.88606 and 0.31215885\n",
            "Val acc and loss are 0.8676 and 0.35882157\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.88636 and 0.31153315\n",
            "Val acc and loss are 0.8678 and 0.35847276\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.88638 and 0.31094405\n",
            "Val acc and loss are 0.8682 and 0.35820377\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.88634 and 0.3103539\n",
            "Val acc and loss are 0.8685 and 0.35789573\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.88674 and 0.30978695\n",
            "Val acc and loss are 0.8692 and 0.3575895\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.88714 and 0.30921534\n",
            "Val acc and loss are 0.8697 and 0.35730714\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.8873 and 0.30870804\n",
            "Val acc and loss are 0.8692 and 0.35704872\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.88754 and 0.3081306\n",
            "Val acc and loss are 0.8695 and 0.35671502\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.88778 and 0.30743837\n",
            "Val acc and loss are 0.8695 and 0.35629213\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.88794 and 0.30667818\n",
            "Val acc and loss are 0.8695 and 0.35578877\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.88818 and 0.30593\n",
            "Val acc and loss are 0.8695 and 0.3553119\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.88854 and 0.30527177\n",
            "Val acc and loss are 0.8693 and 0.35489884\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.88872 and 0.3046835\n",
            "Val acc and loss are 0.8697 and 0.35454988\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.88852 and 0.304192\n",
            "Val acc and loss are 0.8692 and 0.35433742\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.88868 and 0.30371916\n",
            "Val acc and loss are 0.8696 and 0.35418835\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.88886 and 0.30315498\n",
            "Val acc and loss are 0.8699 and 0.35395315\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.88914 and 0.3024516\n",
            "Val acc and loss are 0.8698 and 0.35355675\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.88942 and 0.3017705\n",
            "Val acc and loss are 0.8702 and 0.35314164\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.8896 and 0.3010994\n",
            "Val acc and loss are 0.8709 and 0.35273862\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.89004 and 0.30040446\n",
            "Val acc and loss are 0.8706 and 0.3523111\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.89016 and 0.2997071\n",
            "Val acc and loss are 0.8706 and 0.35186467\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.89046 and 0.29904023\n",
            "Val acc and loss are 0.8707 and 0.35143375\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.8906 and 0.29838878\n",
            "Val acc and loss are 0.8718 and 0.35102725\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.89078 and 0.29774398\n",
            "Val acc and loss are 0.8721 and 0.3506441\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.89096 and 0.29708567\n",
            "Val acc and loss are 0.872 and 0.35024628\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.89132 and 0.2965097\n",
            "Val acc and loss are 0.8724 and 0.3499482\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.89164 and 0.29607117\n",
            "Val acc and loss are 0.8725 and 0.3498239\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.89166 and 0.2955686\n",
            "Val acc and loss are 0.873 and 0.34960166\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.8917 and 0.2949776\n",
            "Val acc and loss are 0.8725 and 0.34929472\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.89202 and 0.2943475\n",
            "Val acc and loss are 0.8725 and 0.34892547\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.8924 and 0.2937088\n",
            "Val acc and loss are 0.8732 and 0.34851325\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.89218 and 0.29316944\n",
            "Val acc and loss are 0.8736 and 0.34816375\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.89244 and 0.29269844\n",
            "Val acc and loss are 0.8738 and 0.34790716\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.89272 and 0.29231614\n",
            "Val acc and loss are 0.874 and 0.34778386\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.89272 and 0.29194203\n",
            "Val acc and loss are 0.8743 and 0.3476837\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.89284 and 0.291581\n",
            "Val acc and loss are 0.874 and 0.34758252\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.89324 and 0.29113385\n",
            "Val acc and loss are 0.8743 and 0.34744728\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.89354 and 0.290635\n",
            "Val acc and loss are 0.874 and 0.34728652\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.8938 and 0.29004896\n",
            "Val acc and loss are 0.874 and 0.3470445\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.89394 and 0.28943223\n",
            "Val acc and loss are 0.8736 and 0.3467843\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.89432 and 0.28882667\n",
            "Val acc and loss are 0.8735 and 0.34653008\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.89458 and 0.28823\n",
            "Val acc and loss are 0.8738 and 0.34614336\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.89476 and 0.2876513\n",
            "Val acc and loss are 0.874 and 0.34572536\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.89506 and 0.2870371\n",
            "Val acc and loss are 0.8743 and 0.3453089\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.89508 and 0.28645644\n",
            "Val acc and loss are 0.8748 and 0.3449105\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.89528 and 0.28590384\n",
            "Val acc and loss are 0.8749 and 0.34450963\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.89534 and 0.28543508\n",
            "Val acc and loss are 0.8743 and 0.34427586\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.89548 and 0.28498796\n",
            "Val acc and loss are 0.8742 and 0.34405708\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.89584 and 0.28439888\n",
            "Val acc and loss are 0.8753 and 0.3436413\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.89606 and 0.28382382\n",
            "Val acc and loss are 0.8755 and 0.3432321\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.89616 and 0.28330824\n",
            "Val acc and loss are 0.8751 and 0.34290516\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.89628 and 0.28285423\n",
            "Val acc and loss are 0.8751 and 0.3426962\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.89644 and 0.28236747\n",
            "Val acc and loss are 0.8748 and 0.34245986\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.89672 and 0.28184447\n",
            "Val acc and loss are 0.8756 and 0.34220123\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.89686 and 0.2812467\n",
            "Val acc and loss are 0.8754 and 0.3418517\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.89704 and 0.28058907\n",
            "Val acc and loss are 0.8754 and 0.34141827\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.89746 and 0.2800155\n",
            "Val acc and loss are 0.8761 and 0.34114105\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.89766 and 0.27947813\n",
            "Val acc and loss are 0.8763 and 0.34091908\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.8977 and 0.27903453\n",
            "Val acc and loss are 0.8763 and 0.34080446\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.89794 and 0.27871722\n",
            "Val acc and loss are 0.8759 and 0.34081528\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.89814 and 0.27832794\n",
            "Val acc and loss are 0.8754 and 0.3407515\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.89804 and 0.27781656\n",
            "Val acc and loss are 0.8759 and 0.34057403\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.8981 and 0.27723125\n",
            "Val acc and loss are 0.876 and 0.3402613\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.8985 and 0.27655423\n",
            "Val acc and loss are 0.8761 and 0.339886\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.89876 and 0.2759252\n",
            "Val acc and loss are 0.8768 and 0.33955878\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.89906 and 0.27540994\n",
            "Val acc and loss are 0.8771 and 0.33930072\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.89944 and 0.27497497\n",
            "Val acc and loss are 0.8766 and 0.33906782\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.89972 and 0.27462468\n",
            "Val acc and loss are 0.8757 and 0.33890888\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.9001 and 0.27424946\n",
            "Val acc and loss are 0.8768 and 0.33880416\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.90028 and 0.27378175\n",
            "Val acc and loss are 0.8768 and 0.33859497\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.90046 and 0.27331552\n",
            "Val acc and loss are 0.8775 and 0.33841273\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.90064 and 0.27286398\n",
            "Val acc and loss are 0.877 and 0.3383155\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.90066 and 0.27234617\n",
            "Val acc and loss are 0.8772 and 0.33815122\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.90096 and 0.2718432\n",
            "Val acc and loss are 0.8772 and 0.33800104\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.90094 and 0.27134442\n",
            "Val acc and loss are 0.8769 and 0.3377791\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.9011 and 0.27084872\n",
            "Val acc and loss are 0.8777 and 0.3374945\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.90124 and 0.27030703\n",
            "Val acc and loss are 0.8779 and 0.33707917\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.90142 and 0.26975265\n",
            "Val acc and loss are 0.878 and 0.33666417\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.90172 and 0.26914117\n",
            "Val acc and loss are 0.8785 and 0.33620167\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.90192 and 0.26856625\n",
            "Val acc and loss are 0.8786 and 0.3358865\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.90218 and 0.26815677\n",
            "Val acc and loss are 0.8793 and 0.33581537\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.9026 and 0.26782414\n",
            "Val acc and loss are 0.8795 and 0.33576825\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.90278 and 0.2675293\n",
            "Val acc and loss are 0.879 and 0.33576626\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.90274 and 0.26714808\n",
            "Val acc and loss are 0.8781 and 0.33566427\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.903 and 0.26665968\n",
            "Val acc and loss are 0.8785 and 0.335432\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.9032 and 0.2661282\n",
            "Val acc and loss are 0.8793 and 0.3352824\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.90338 and 0.26569355\n",
            "Val acc and loss are 0.8803 and 0.3352604\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.9036 and 0.26530114\n",
            "Val acc and loss are 0.8803 and 0.33526132\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.9038 and 0.26490024\n",
            "Val acc and loss are 0.8802 and 0.33514208\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.90354 and 0.2644832\n",
            "Val acc and loss are 0.8798 and 0.33490035\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.90362 and 0.2640837\n",
            "Val acc and loss are 0.8795 and 0.33464605\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.90374 and 0.2635738\n",
            "Val acc and loss are 0.8795 and 0.3343616\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.90406 and 0.26294369\n",
            "Val acc and loss are 0.88 and 0.33402345\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.90438 and 0.26229563\n",
            "Val acc and loss are 0.8803 and 0.33378232\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.90464 and 0.2617924\n",
            "Val acc and loss are 0.8804 and 0.33363917\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.90472 and 0.26135683\n",
            "Val acc and loss are 0.8806 and 0.3335152\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.90482 and 0.26091042\n",
            "Val acc and loss are 0.8802 and 0.33337703\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.90488 and 0.26040965\n",
            "Val acc and loss are 0.8803 and 0.33313724\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.90516 and 0.25990933\n",
            "Val acc and loss are 0.8799 and 0.33287257\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.90548 and 0.25933987\n",
            "Val acc and loss are 0.8801 and 0.3324732\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.9055 and 0.2588294\n",
            "Val acc and loss are 0.8803 and 0.33236104\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.90582 and 0.25843817\n",
            "Val acc and loss are 0.8805 and 0.33231217\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.90594 and 0.25804427\n",
            "Val acc and loss are 0.8807 and 0.3321758\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.90616 and 0.25767323\n",
            "Val acc and loss are 0.8804 and 0.33199662\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.90606 and 0.25729078\n",
            "Val acc and loss are 0.8804 and 0.33174732\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.90638 and 0.2567757\n",
            "Val acc and loss are 0.8807 and 0.33145005\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.90676 and 0.2562933\n",
            "Val acc and loss are 0.8805 and 0.33118886\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.90694 and 0.2559543\n",
            "Val acc and loss are 0.8808 and 0.3310565\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.90714 and 0.25571832\n",
            "Val acc and loss are 0.8807 and 0.33113062\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.90684 and 0.25542602\n",
            "Val acc and loss are 0.8808 and 0.3311797\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.90716 and 0.2549875\n",
            "Val acc and loss are 0.8806 and 0.33109134\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.9074 and 0.2544238\n",
            "Val acc and loss are 0.8809 and 0.33089933\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.9078 and 0.25373617\n",
            "Val acc and loss are 0.8817 and 0.3304965\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.90808 and 0.253129\n",
            "Val acc and loss are 0.8825 and 0.3300948\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.90844 and 0.25263247\n",
            "Val acc and loss are 0.8832 and 0.3298119\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.90816 and 0.25230584\n",
            "Val acc and loss are 0.8824 and 0.3296207\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.908 and 0.25212902\n",
            "Val acc and loss are 0.8827 and 0.32957837\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.90822 and 0.2518445\n",
            "Val acc and loss are 0.8826 and 0.32936484\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.9084 and 0.25143898\n",
            "Val acc and loss are 0.8827 and 0.32911548\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.90888 and 0.25086972\n",
            "Val acc and loss are 0.8826 and 0.32871085\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.90916 and 0.2501874\n",
            "Val acc and loss are 0.883 and 0.32830894\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.9094 and 0.24956615\n",
            "Val acc and loss are 0.883 and 0.32810038\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.90956 and 0.2490741\n",
            "Val acc and loss are 0.8833 and 0.3280418\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.90926 and 0.24876547\n",
            "Val acc and loss are 0.8831 and 0.32805404\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.9093 and 0.24855652\n",
            "Val acc and loss are 0.8833 and 0.3280693\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.9095 and 0.24819309\n",
            "Val acc and loss are 0.8824 and 0.3279699\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.90992 and 0.24786426\n",
            "Val acc and loss are 0.8826 and 0.32795212\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.91002 and 0.24733406\n",
            "Val acc and loss are 0.8825 and 0.3278257\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.9104 and 0.24685672\n",
            "Val acc and loss are 0.8825 and 0.3277336\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.91038 and 0.24649422\n",
            "Val acc and loss are 0.8827 and 0.32774654\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.91062 and 0.2460343\n",
            "Val acc and loss are 0.8829 and 0.32751822\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.9109 and 0.2455052\n",
            "Val acc and loss are 0.8832 and 0.3271531\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.91066 and 0.24517688\n",
            "Val acc and loss are 0.8839 and 0.32694083\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.91066 and 0.24485886\n",
            "Val acc and loss are 0.8837 and 0.3268308\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.91094 and 0.24433136\n",
            "Val acc and loss are 0.8835 and 0.32663935\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.91102 and 0.24386203\n",
            "Val acc and loss are 0.884 and 0.32658607\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.9115 and 0.24343799\n",
            "Val acc and loss are 0.8846 and 0.32654044\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.91186 and 0.24304852\n",
            "Val acc and loss are 0.8845 and 0.3263759\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.91192 and 0.24275447\n",
            "Val acc and loss are 0.884 and 0.3263529\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.9121 and 0.24252102\n",
            "Val acc and loss are 0.8839 and 0.32640678\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.91228 and 0.24200098\n",
            "Val acc and loss are 0.8845 and 0.3261979\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.91252 and 0.2413752\n",
            "Val acc and loss are 0.8848 and 0.32585534\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.91244 and 0.24089696\n",
            "Val acc and loss are 0.884 and 0.32562715\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.91268 and 0.24047269\n",
            "Val acc and loss are 0.8839 and 0.32553434\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.91274 and 0.24012437\n",
            "Val acc and loss are 0.8839 and 0.3254835\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.91288 and 0.23988093\n",
            "Val acc and loss are 0.8841 and 0.32544947\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.91278 and 0.23958015\n",
            "Val acc and loss are 0.8838 and 0.3253643\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.91292 and 0.23906834\n",
            "Val acc and loss are 0.8844 and 0.32509425\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.9132 and 0.23846312\n",
            "Val acc and loss are 0.8851 and 0.3247148\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.91356 and 0.2378687\n",
            "Val acc and loss are 0.8855 and 0.3243619\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.9134 and 0.23747045\n",
            "Val acc and loss are 0.8852 and 0.3241269\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.91334 and 0.23719715\n",
            "Val acc and loss are 0.8851 and 0.3239915\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.9135 and 0.23693144\n",
            "Val acc and loss are 0.8849 and 0.323988\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.91384 and 0.23644638\n",
            "Val acc and loss are 0.885 and 0.3238348\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.9141 and 0.2360755\n",
            "Val acc and loss are 0.8848 and 0.32381895\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.9141 and 0.23578309\n",
            "Val acc and loss are 0.8847 and 0.3238206\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9143 and 0.2354865\n",
            "Val acc and loss are 0.8843 and 0.32393217\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.9145 and 0.23510145\n",
            "Val acc and loss are 0.8841 and 0.32392052\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.91444 and 0.23480833\n",
            "Val acc and loss are 0.8838 and 0.32400656\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.91466 and 0.23438752\n",
            "Val acc and loss are 0.884 and 0.3239421\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.9146 and 0.23391369\n",
            "Val acc and loss are 0.8845 and 0.3236608\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.91482 and 0.23337108\n",
            "Val acc and loss are 0.885 and 0.32320544\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.91506 and 0.23292828\n",
            "Val acc and loss are 0.885 and 0.32293126\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.91516 and 0.23249026\n",
            "Val acc and loss are 0.885 and 0.3227318\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.91542 and 0.2321131\n",
            "Val acc and loss are 0.8851 and 0.32276288\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.91544 and 0.23180051\n",
            "Val acc and loss are 0.885 and 0.3229284\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.91576 and 0.23130532\n",
            "Val acc and loss are 0.8849 and 0.32295814\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.91648 and 0.23055375\n",
            "Val acc and loss are 0.8855 and 0.32253954\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.91688 and 0.22995374\n",
            "Val acc and loss are 0.8862 and 0.32214817\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.91714 and 0.2295172\n",
            "Val acc and loss are 0.8861 and 0.32194713\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.91722 and 0.22919445\n",
            "Val acc and loss are 0.8857 and 0.3219386\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.91734 and 0.22891021\n",
            "Val acc and loss are 0.8854 and 0.32202157\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.91736 and 0.2285463\n",
            "Val acc and loss are 0.8855 and 0.32207075\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.91758 and 0.22829066\n",
            "Val acc and loss are 0.8856 and 0.32215747\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.91784 and 0.22808683\n",
            "Val acc and loss are 0.8855 and 0.32242432\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.91768 and 0.22792616\n",
            "Val acc and loss are 0.8854 and 0.32282132\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.91782 and 0.22777413\n",
            "Val acc and loss are 0.8861 and 0.32322094\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.91784 and 0.22743656\n",
            "Val acc and loss are 0.8862 and 0.32336316\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.91852 and 0.22684018\n",
            "Val acc and loss are 0.8856 and 0.3232003\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.9186 and 0.22635317\n",
            "Val acc and loss are 0.8858 and 0.322992\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.91848 and 0.22586419\n",
            "Val acc and loss are 0.8849 and 0.3226271\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.91842 and 0.22538714\n",
            "Val acc and loss are 0.8848 and 0.32223532\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.91866 and 0.22486748\n",
            "Val acc and loss are 0.8855 and 0.32177544\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.91896 and 0.22443601\n",
            "Val acc and loss are 0.8863 and 0.32153693\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.91876 and 0.22418603\n",
            "Val acc and loss are 0.886 and 0.32152852\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.91888 and 0.22396915\n",
            "Val acc and loss are 0.8866 and 0.32184827\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.91902 and 0.22362173\n",
            "Val acc and loss are 0.8862 and 0.32208723\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.91972 and 0.22315209\n",
            "Val acc and loss are 0.8852 and 0.32211745\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.91986 and 0.22274254\n",
            "Val acc and loss are 0.8858 and 0.3221061\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.92034 and 0.22228444\n",
            "Val acc and loss are 0.8858 and 0.32203028\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.92012 and 0.22198172\n",
            "Val acc and loss are 0.8858 and 0.32194483\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.92002 and 0.22167403\n",
            "Val acc and loss are 0.8856 and 0.32177982\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.92022 and 0.22123846\n",
            "Val acc and loss are 0.8859 and 0.32148343\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.92062 and 0.22091815\n",
            "Val acc and loss are 0.8855 and 0.32134503\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.92106 and 0.2205204\n",
            "Val acc and loss are 0.8855 and 0.3213041\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.92096 and 0.22011408\n",
            "Val acc and loss are 0.8856 and 0.32129875\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.9212 and 0.2196952\n",
            "Val acc and loss are 0.8856 and 0.32122782\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.92128 and 0.21934722\n",
            "Val acc and loss are 0.8857 and 0.3210771\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.92142 and 0.21893418\n",
            "Val acc and loss are 0.8858 and 0.32094723\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.92212 and 0.21850812\n",
            "Val acc and loss are 0.8862 and 0.3206808\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.92252 and 0.21810183\n",
            "Val acc and loss are 0.8869 and 0.3204377\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.9225 and 0.2176762\n",
            "Val acc and loss are 0.8869 and 0.32021382\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.92248 and 0.21728528\n",
            "Val acc and loss are 0.8868 and 0.32009357\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.92224 and 0.2169866\n",
            "Val acc and loss are 0.8863 and 0.32007882\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.92216 and 0.2165453\n",
            "Val acc and loss are 0.8861 and 0.31989607\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.9221 and 0.21621707\n",
            "Val acc and loss are 0.8855 and 0.31972238\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.92222 and 0.21593885\n",
            "Val acc and loss are 0.8851 and 0.3196899\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.92248 and 0.21555026\n",
            "Val acc and loss are 0.8853 and 0.3197537\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.9223 and 0.21538156\n",
            "Val acc and loss are 0.885 and 0.32014385\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.92312 and 0.21492137\n",
            "Val acc and loss are 0.8862 and 0.32017517\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.9236 and 0.21438546\n",
            "Val acc and loss are 0.886 and 0.31984589\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.92342 and 0.21418244\n",
            "Val acc and loss are 0.8851 and 0.31972194\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.92358 and 0.21390328\n",
            "Val acc and loss are 0.8853 and 0.31973022\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.92378 and 0.21331033\n",
            "Val acc and loss are 0.8862 and 0.3196388\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.92368 and 0.21283244\n",
            "Val acc and loss are 0.8857 and 0.31961\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.92388 and 0.21233131\n",
            "Val acc and loss are 0.8858 and 0.31952745\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.92394 and 0.21197738\n",
            "Val acc and loss are 0.8859 and 0.3194391\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.92414 and 0.2118619\n",
            "Val acc and loss are 0.8853 and 0.31955883\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.92436 and 0.21141322\n",
            "Val acc and loss are 0.8855 and 0.31943092\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.9247 and 0.21083768\n",
            "Val acc and loss are 0.8851 and 0.31918865\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.92498 and 0.21042489\n",
            "Val acc and loss are 0.8855 and 0.31915066\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.9249 and 0.21024852\n",
            "Val acc and loss are 0.8856 and 0.3191933\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.92496 and 0.2102104\n",
            "Val acc and loss are 0.8857 and 0.3192029\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.92528 and 0.2098718\n",
            "Val acc and loss are 0.8862 and 0.31906703\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.9255 and 0.2093957\n",
            "Val acc and loss are 0.886 and 0.31883648\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.92564 and 0.2090018\n",
            "Val acc and loss are 0.8859 and 0.31888512\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.92546 and 0.20869103\n",
            "Val acc and loss are 0.8857 and 0.31900042\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.92566 and 0.20830804\n",
            "Val acc and loss are 0.8859 and 0.31906453\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.92586 and 0.20782275\n",
            "Val acc and loss are 0.886 and 0.31903136\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.92608 and 0.20744826\n",
            "Val acc and loss are 0.8864 and 0.3191059\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.92636 and 0.2071122\n",
            "Val acc and loss are 0.8864 and 0.319389\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.92644 and 0.20685135\n",
            "Val acc and loss are 0.8866 and 0.31974876\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.92658 and 0.20631887\n",
            "Val acc and loss are 0.8867 and 0.3197529\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.92662 and 0.20560043\n",
            "Val acc and loss are 0.8857 and 0.31953758\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.92664 and 0.20524365\n",
            "Val acc and loss are 0.8853 and 0.3195083\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.92678 and 0.20506755\n",
            "Val acc and loss are 0.8854 and 0.31967607\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.92706 and 0.20482844\n",
            "Val acc and loss are 0.8851 and 0.319894\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.92708 and 0.20477027\n",
            "Val acc and loss are 0.8864 and 0.32028276\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.9273 and 0.20444363\n",
            "Val acc and loss are 0.8862 and 0.32004452\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.92728 and 0.20404743\n",
            "Val acc and loss are 0.8864 and 0.3196656\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.9274 and 0.20350635\n",
            "Val acc and loss are 0.8862 and 0.31916478\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.9279 and 0.20289066\n",
            "Val acc and loss are 0.8864 and 0.31868187\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.9285 and 0.20227662\n",
            "Val acc and loss are 0.8863 and 0.3184466\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.92842 and 0.20197816\n",
            "Val acc and loss are 0.8868 and 0.31842142\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.9285 and 0.20173246\n",
            "Val acc and loss are 0.8869 and 0.31845626\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.9289 and 0.20136122\n",
            "Val acc and loss are 0.8867 and 0.31831542\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.92888 and 0.20099492\n",
            "Val acc and loss are 0.8871 and 0.31824\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.92912 and 0.20071551\n",
            "Val acc and loss are 0.8868 and 0.31846896\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.9288 and 0.20052165\n",
            "Val acc and loss are 0.887 and 0.31905597\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.92902 and 0.20020941\n",
            "Val acc and loss are 0.8873 and 0.31954733\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.92932 and 0.19984682\n",
            "Val acc and loss are 0.887 and 0.319669\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.92964 and 0.19934046\n",
            "Val acc and loss are 0.8866 and 0.3193584\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.92996 and 0.19892648\n",
            "Val acc and loss are 0.8869 and 0.31902885\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.9299 and 0.19856997\n",
            "Val acc and loss are 0.8873 and 0.31891403\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.92994 and 0.19859877\n",
            "Val acc and loss are 0.8882 and 0.31930715\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.92994 and 0.19819918\n",
            "Val acc and loss are 0.8877 and 0.31934708\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.9301 and 0.19777279\n",
            "Val acc and loss are 0.8871 and 0.3192107\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.9305 and 0.19736728\n",
            "Val acc and loss are 0.8876 and 0.3191216\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.9302 and 0.19724381\n",
            "Val acc and loss are 0.888 and 0.31914604\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.93028 and 0.19685304\n",
            "Val acc and loss are 0.8869 and 0.31885678\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.93084 and 0.19622883\n",
            "Val acc and loss are 0.8867 and 0.31834644\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.93086 and 0.19587652\n",
            "Val acc and loss are 0.8866 and 0.3181861\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.931 and 0.19569512\n",
            "Val acc and loss are 0.8871 and 0.3185108\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.93094 and 0.1956537\n",
            "Val acc and loss are 0.8869 and 0.3188796\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.93106 and 0.19505906\n",
            "Val acc and loss are 0.8877 and 0.31874985\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.931 and 0.19458397\n",
            "Val acc and loss are 0.8867 and 0.3186835\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.9313 and 0.19428079\n",
            "Val acc and loss are 0.8868 and 0.31876332\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.93144 and 0.19414134\n",
            "Val acc and loss are 0.8874 and 0.31901217\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.93126 and 0.19414037\n",
            "Val acc and loss are 0.8875 and 0.31908026\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.93126 and 0.1937948\n",
            "Val acc and loss are 0.8875 and 0.31890056\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.93152 and 0.19306959\n",
            "Val acc and loss are 0.8879 and 0.31857193\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.93172 and 0.19250886\n",
            "Val acc and loss are 0.8875 and 0.3184587\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.93178 and 0.19202954\n",
            "Val acc and loss are 0.888 and 0.31829536\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.93186 and 0.19165233\n",
            "Val acc and loss are 0.8883 and 0.3182649\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.93208 and 0.19163197\n",
            "Val acc and loss are 0.8885 and 0.31826207\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.93244 and 0.19139394\n",
            "Val acc and loss are 0.8881 and 0.31827587\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.93224 and 0.19072649\n",
            "Val acc and loss are 0.8884 and 0.31806913\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.93242 and 0.19021913\n",
            "Val acc and loss are 0.8884 and 0.31798378\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.93266 and 0.18987304\n",
            "Val acc and loss are 0.8885 and 0.31809753\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.9327 and 0.18961203\n",
            "Val acc and loss are 0.8873 and 0.3179641\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.93312 and 0.18934996\n",
            "Val acc and loss are 0.8874 and 0.31791398\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.93308 and 0.18879244\n",
            "Val acc and loss are 0.8878 and 0.31798282\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.93284 and 0.18867803\n",
            "Val acc and loss are 0.888 and 0.31827343\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.93292 and 0.18865038\n",
            "Val acc and loss are 0.8888 and 0.31880626\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.93318 and 0.18856144\n",
            "Val acc and loss are 0.8882 and 0.31920406\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.93304 and 0.18801922\n",
            "Val acc and loss are 0.8884 and 0.31877306\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.9335 and 0.18739212\n",
            "Val acc and loss are 0.8889 and 0.31817678\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.9334 and 0.18681417\n",
            "Val acc and loss are 0.8888 and 0.31781498\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.93362 and 0.18628727\n",
            "Val acc and loss are 0.8889 and 0.31775475\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.93418 and 0.18616602\n",
            "Val acc and loss are 0.8893 and 0.31820884\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.9342 and 0.18617451\n",
            "Val acc and loss are 0.8899 and 0.31875536\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.93424 and 0.18604551\n",
            "Val acc and loss are 0.8894 and 0.31896242\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.93456 and 0.18545148\n",
            "Val acc and loss are 0.8884 and 0.31848773\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.9345 and 0.18494572\n",
            "Val acc and loss are 0.8877 and 0.31821948\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.93458 and 0.18443616\n",
            "Val acc and loss are 0.888 and 0.31809205\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.93466 and 0.18429436\n",
            "Val acc and loss are 0.8889 and 0.3183607\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.93476 and 0.18398863\n",
            "Val acc and loss are 0.8893 and 0.31858683\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.93486 and 0.18358082\n",
            "Val acc and loss are 0.8901 and 0.31865045\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.93454 and 0.18322554\n",
            "Val acc and loss are 0.8895 and 0.31869376\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.93492 and 0.18304804\n",
            "Val acc and loss are 0.8891 and 0.31883\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.9356 and 0.18254879\n",
            "Val acc and loss are 0.8891 and 0.31869903\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.93598 and 0.1821788\n",
            "Val acc and loss are 0.889 and 0.31856418\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.93618 and 0.18182945\n",
            "Val acc and loss are 0.889 and 0.31819874\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.9358 and 0.18169878\n",
            "Val acc and loss are 0.8888 and 0.31820214\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.93576 and 0.18167293\n",
            "Val acc and loss are 0.8883 and 0.31876358\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.93562 and 0.18125598\n",
            "Val acc and loss are 0.8883 and 0.31909236\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.93628 and 0.18073212\n",
            "Val acc and loss are 0.8881 and 0.31896546\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.9365 and 0.18011683\n",
            "Val acc and loss are 0.8879 and 0.3183992\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.93642 and 0.1796481\n",
            "Val acc and loss are 0.889 and 0.31794092\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.93618 and 0.17942992\n",
            "Val acc and loss are 0.8892 and 0.31770587\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.93642 and 0.17910975\n",
            "Val acc and loss are 0.8891 and 0.31732705\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.93678 and 0.17901403\n",
            "Val acc and loss are 0.8887 and 0.31713873\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.93676 and 0.17884886\n",
            "Val acc and loss are 0.8886 and 0.31712982\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.93716 and 0.1786657\n",
            "Val acc and loss are 0.8889 and 0.31734648\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.93724 and 0.1784295\n",
            "Val acc and loss are 0.8881 and 0.31763124\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.93796 and 0.17807557\n",
            "Val acc and loss are 0.8887 and 0.31751475\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.93786 and 0.17779441\n",
            "Val acc and loss are 0.8887 and 0.31781894\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.9376 and 0.1775427\n",
            "Val acc and loss are 0.8886 and 0.31850755\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.93736 and 0.17727688\n",
            "Val acc and loss are 0.8894 and 0.31912455\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.93762 and 0.17675678\n",
            "Val acc and loss are 0.8891 and 0.31937206\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.93792 and 0.17650963\n",
            "Val acc and loss are 0.8887 and 0.31954712\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.93828 and 0.17591245\n",
            "Val acc and loss are 0.8887 and 0.31976086\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.93798 and 0.17568934\n",
            "Val acc and loss are 0.8898 and 0.3200315\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.93854 and 0.1752356\n",
            "Val acc and loss are 0.8897 and 0.31932664\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.93856 and 0.17496239\n",
            "Val acc and loss are 0.8888 and 0.31865573\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.93864 and 0.17475611\n",
            "Val acc and loss are 0.8888 and 0.3184961\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.93856 and 0.1745675\n",
            "Val acc and loss are 0.8884 and 0.31895548\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.93882 and 0.17419872\n",
            "Val acc and loss are 0.889 and 0.3191997\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.93932 and 0.1737709\n",
            "Val acc and loss are 0.8886 and 0.31924292\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.93972 and 0.17346637\n",
            "Val acc and loss are 0.8879 and 0.3192391\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.93938 and 0.17310081\n",
            "Val acc and loss are 0.8875 and 0.3190765\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.93958 and 0.1728963\n",
            "Val acc and loss are 0.8889 and 0.3191489\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.93968 and 0.17284715\n",
            "Val acc and loss are 0.8895 and 0.31962746\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.93976 and 0.17250168\n",
            "Val acc and loss are 0.8898 and 0.32009614\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.93988 and 0.171777\n",
            "Val acc and loss are 0.8896 and 0.32006073\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.94008 and 0.17143978\n",
            "Val acc and loss are 0.8882 and 0.32010913\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.94042 and 0.17115232\n",
            "Val acc and loss are 0.8881 and 0.32004097\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.94044 and 0.17095715\n",
            "Val acc and loss are 0.8894 and 0.3203073\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.93998 and 0.17137447\n",
            "Val acc and loss are 0.8904 and 0.32080635\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.9399 and 0.17108782\n",
            "Val acc and loss are 0.8903 and 0.32042813\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.94032 and 0.17028707\n",
            "Val acc and loss are 0.89 and 0.31945065\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.94074 and 0.16987103\n",
            "Val acc and loss are 0.8895 and 0.3189768\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.9408 and 0.16935915\n",
            "Val acc and loss are 0.8895 and 0.31935376\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.94116 and 0.16887707\n",
            "Val acc and loss are 0.8896 and 0.32006234\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.94128 and 0.16833574\n",
            "Val acc and loss are 0.8902 and 0.32025596\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.941 and 0.16793269\n",
            "Val acc and loss are 0.89 and 0.3200919\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.94076 and 0.1676513\n",
            "Val acc and loss are 0.8901 and 0.32033202\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.94142 and 0.16724028\n",
            "Val acc and loss are 0.89 and 0.32028267\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.9416 and 0.16687594\n",
            "Val acc and loss are 0.8901 and 0.32067475\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.9416 and 0.16646564\n",
            "Val acc and loss are 0.8899 and 0.3209945\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.94196 and 0.16624953\n",
            "Val acc and loss are 0.8895 and 0.3210917\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.94204 and 0.16616984\n",
            "Val acc and loss are 0.8896 and 0.32093373\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.94228 and 0.16596553\n",
            "Val acc and loss are 0.8894 and 0.32082516\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.94228 and 0.16532539\n",
            "Val acc and loss are 0.8904 and 0.32044053\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.94256 and 0.16495685\n",
            "Val acc and loss are 0.8898 and 0.32046708\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.94278 and 0.16474238\n",
            "Val acc and loss are 0.8905 and 0.32061815\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.94302 and 0.16469236\n",
            "Val acc and loss are 0.89 and 0.32079098\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.94286 and 0.16470715\n",
            "Val acc and loss are 0.8898 and 0.3208824\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.94292 and 0.16445531\n",
            "Val acc and loss are 0.8894 and 0.32074016\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.94304 and 0.16389044\n",
            "Val acc and loss are 0.8901 and 0.32036963\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.94338 and 0.16349085\n",
            "Val acc and loss are 0.8896 and 0.32026914\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.94334 and 0.16305003\n",
            "Val acc and loss are 0.8903 and 0.32042593\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.9435 and 0.16292486\n",
            "Val acc and loss are 0.8889 and 0.3212613\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.94356 and 0.16240649\n",
            "Val acc and loss are 0.8899 and 0.32138002\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.94398 and 0.16214168\n",
            "Val acc and loss are 0.8908 and 0.32149905\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.94398 and 0.16185902\n",
            "Val acc and loss are 0.8908 and 0.32120302\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.94376 and 0.16205694\n",
            "Val acc and loss are 0.8902 and 0.32163125\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.9439 and 0.16152863\n",
            "Val acc and loss are 0.8893 and 0.32136548\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.94464 and 0.16106926\n",
            "Val acc and loss are 0.8907 and 0.32116884\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.94478 and 0.160586\n",
            "Val acc and loss are 0.8903 and 0.3211584\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.94468 and 0.16050227\n",
            "Val acc and loss are 0.89 and 0.32155508\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.94476 and 0.16028164\n",
            "Val acc and loss are 0.8901 and 0.3218372\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.94528 and 0.15968715\n",
            "Val acc and loss are 0.8897 and 0.32204646\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.94492 and 0.15925173\n",
            "Val acc and loss are 0.8903 and 0.32246363\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.94482 and 0.1593316\n",
            "Val acc and loss are 0.8897 and 0.3231243\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.94512 and 0.1588491\n",
            "Val acc and loss are 0.8902 and 0.32246113\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.94548 and 0.15864608\n",
            "Val acc and loss are 0.8902 and 0.32188255\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.94552 and 0.15826577\n",
            "Val acc and loss are 0.89 and 0.321429\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.94522 and 0.15859562\n",
            "Val acc and loss are 0.8907 and 0.3222064\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.94556 and 0.1581412\n",
            "Val acc and loss are 0.8907 and 0.3222813\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.9458 and 0.15755357\n",
            "Val acc and loss are 0.8909 and 0.3216585\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.94596 and 0.1571146\n",
            "Val acc and loss are 0.8891 and 0.32138237\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.94614 and 0.15692954\n",
            "Val acc and loss are 0.8882 and 0.3216078\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.94576 and 0.15700129\n",
            "Val acc and loss are 0.8899 and 0.32201597\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.9459 and 0.15696648\n",
            "Val acc and loss are 0.8907 and 0.32216227\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.94616 and 0.15656838\n",
            "Val acc and loss are 0.8917 and 0.32172778\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.94652 and 0.15572454\n",
            "Val acc and loss are 0.8924 and 0.32095358\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.947 and 0.15546189\n",
            "Val acc and loss are 0.8899 and 0.32122263\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.94698 and 0.15506116\n",
            "Val acc and loss are 0.8891 and 0.32127672\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.94718 and 0.1541609\n",
            "Val acc and loss are 0.8903 and 0.32076755\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.94718 and 0.15418425\n",
            "Val acc and loss are 0.8907 and 0.3209383\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.94692 and 0.15428099\n",
            "Val acc and loss are 0.8898 and 0.32149404\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.94648 and 0.15460613\n",
            "Val acc and loss are 0.8894 and 0.32210487\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.94688 and 0.15393426\n",
            "Val acc and loss are 0.8903 and 0.32161292\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.94718 and 0.15325779\n",
            "Val acc and loss are 0.8914 and 0.32110563\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.94762 and 0.15292974\n",
            "Val acc and loss are 0.8908 and 0.32127896\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.94768 and 0.15279952\n",
            "Val acc and loss are 0.8902 and 0.321932\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.94796 and 0.15289302\n",
            "Val acc and loss are 0.89 and 0.32292032\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.94804 and 0.15239163\n",
            "Val acc and loss are 0.8905 and 0.32270405\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.94796 and 0.15219624\n",
            "Val acc and loss are 0.8903 and 0.32269052\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.9478 and 0.15178713\n",
            "Val acc and loss are 0.8907 and 0.3228109\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.9476 and 0.15177464\n",
            "Val acc and loss are 0.8905 and 0.3232184\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.94818 and 0.15100993\n",
            "Val acc and loss are 0.89 and 0.3225359\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.94856 and 0.15093993\n",
            "Val acc and loss are 0.8913 and 0.32314497\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.9483 and 0.15043403\n",
            "Val acc and loss are 0.8906 and 0.32359228\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.94806 and 0.15046875\n",
            "Val acc and loss are 0.8908 and 0.32412943\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.94872 and 0.14979616\n",
            "Val acc and loss are 0.8903 and 0.32322872\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.9489 and 0.14957821\n",
            "Val acc and loss are 0.8904 and 0.32267964\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.94872 and 0.14939225\n",
            "Val acc and loss are 0.8911 and 0.32272452\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.9487 and 0.14932805\n",
            "Val acc and loss are 0.891 and 0.32296655\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.9493 and 0.14888576\n",
            "Val acc and loss are 0.8914 and 0.32298142\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.94998 and 0.14845\n",
            "Val acc and loss are 0.8914 and 0.32299945\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.95022 and 0.14797755\n",
            "Val acc and loss are 0.8918 and 0.32297418\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.95028 and 0.14779061\n",
            "Val acc and loss are 0.8917 and 0.3228392\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.95018 and 0.14757772\n",
            "Val acc and loss are 0.8911 and 0.32303554\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.94986 and 0.14727475\n",
            "Val acc and loss are 0.8905 and 0.32360387\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.9499 and 0.14726515\n",
            "Val acc and loss are 0.8902 and 0.32467973\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.94932 and 0.14749314\n",
            "Val acc and loss are 0.89 and 0.32607833\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.94962 and 0.14688583\n",
            "Val acc and loss are 0.8905 and 0.32599902\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.95032 and 0.14617455\n",
            "Val acc and loss are 0.8913 and 0.3255476\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.95018 and 0.14568721\n",
            "Val acc and loss are 0.8922 and 0.32557902\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.95026 and 0.14543575\n",
            "Val acc and loss are 0.8911 and 0.325863\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.95072 and 0.14500777\n",
            "Val acc and loss are 0.8903 and 0.32580653\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.95122 and 0.14475247\n",
            "Val acc and loss are 0.8913 and 0.32617584\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.95112 and 0.144868\n",
            "Val acc and loss are 0.8903 and 0.32660386\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.95058 and 0.1449977\n",
            "Val acc and loss are 0.8891 and 0.326679\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.95082 and 0.14442134\n",
            "Val acc and loss are 0.8905 and 0.32579693\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.95072 and 0.1439444\n",
            "Val acc and loss are 0.8907 and 0.32546002\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.95112 and 0.14342447\n",
            "Val acc and loss are 0.8908 and 0.32537797\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.95124 and 0.14321445\n",
            "Val acc and loss are 0.8907 and 0.32552958\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.95182 and 0.14291485\n",
            "Val acc and loss are 0.8913 and 0.32602718\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.95232 and 0.14257723\n",
            "Val acc and loss are 0.8904 and 0.32615727\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.9524 and 0.14218833\n",
            "Val acc and loss are 0.8918 and 0.32577428\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.9522 and 0.14225344\n",
            "Val acc and loss are 0.891 and 0.32566276\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.95212 and 0.14270072\n",
            "Val acc and loss are 0.8914 and 0.3262921\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.95188 and 0.14215256\n",
            "Val acc and loss are 0.8914 and 0.32624823\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.95216 and 0.14166097\n",
            "Val acc and loss are 0.892 and 0.3265552\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.95268 and 0.14104246\n",
            "Val acc and loss are 0.891 and 0.3267258\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.95286 and 0.14089909\n",
            "Val acc and loss are 0.8908 and 0.32713202\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.95252 and 0.14114195\n",
            "Val acc and loss are 0.8909 and 0.32779816\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.95278 and 0.14035285\n",
            "Val acc and loss are 0.8906 and 0.3271296\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.95282 and 0.14026357\n",
            "Val acc and loss are 0.8905 and 0.32699165\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.95286 and 0.14002416\n",
            "Val acc and loss are 0.8917 and 0.3266176\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.9531 and 0.14016834\n",
            "Val acc and loss are 0.8909 and 0.32684207\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.95352 and 0.13916835\n",
            "Val acc and loss are 0.8901 and 0.32640117\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.95368 and 0.13856493\n",
            "Val acc and loss are 0.8912 and 0.32674766\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.95364 and 0.13832374\n",
            "Val acc and loss are 0.8919 and 0.32726923\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.95304 and 0.13872814\n",
            "Val acc and loss are 0.891 and 0.32797867\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.95332 and 0.13824348\n",
            "Val acc and loss are 0.8912 and 0.32693276\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.95352 and 0.13802041\n",
            "Val acc and loss are 0.8914 and 0.3262073\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.95376 and 0.13771982\n",
            "Val acc and loss are 0.8917 and 0.32631084\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.95354 and 0.13744636\n",
            "Val acc and loss are 0.8915 and 0.3272989\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.95358 and 0.13755803\n",
            "Val acc and loss are 0.8909 and 0.3282417\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.95428 and 0.13689297\n",
            "Val acc and loss are 0.8913 and 0.3273004\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.95408 and 0.13674153\n",
            "Val acc and loss are 0.8917 and 0.32599774\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.9539 and 0.136678\n",
            "Val acc and loss are 0.8915 and 0.3254357\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.95358 and 0.1368518\n",
            "Val acc and loss are 0.8914 and 0.32599038\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.95404 and 0.1365205\n",
            "Val acc and loss are 0.8915 and 0.3263881\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.95458 and 0.13588712\n",
            "Val acc and loss are 0.8914 and 0.32610002\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.95496 and 0.13561721\n",
            "Val acc and loss are 0.891 and 0.32636493\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.95472 and 0.13534987\n",
            "Val acc and loss are 0.8907 and 0.32717735\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.9549 and 0.13501413\n",
            "Val acc and loss are 0.8918 and 0.3275702\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.95522 and 0.13425025\n",
            "Val acc and loss are 0.8919 and 0.32708946\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.95548 and 0.13382275\n",
            "Val acc and loss are 0.8908 and 0.3269059\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.95508 and 0.13417149\n",
            "Val acc and loss are 0.8908 and 0.32746676\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.955 and 0.13425143\n",
            "Val acc and loss are 0.8919 and 0.32787713\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.95526 and 0.13380268\n",
            "Val acc and loss are 0.892 and 0.32772115\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.95548 and 0.13298358\n",
            "Val acc and loss are 0.8919 and 0.32699078\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.95596 and 0.13274595\n",
            "Val acc and loss are 0.8917 and 0.3271193\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.9559 and 0.13252023\n",
            "Val acc and loss are 0.8918 and 0.32790855\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.95596 and 0.13230029\n",
            "Val acc and loss are 0.8912 and 0.32883137\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.95602 and 0.13249332\n",
            "Val acc and loss are 0.8901 and 0.32983583\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.95588 and 0.13276333\n",
            "Val acc and loss are 0.8907 and 0.3302783\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.9561 and 0.13260219\n",
            "Val acc and loss are 0.8916 and 0.32929295\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.95684 and 0.13168047\n",
            "Val acc and loss are 0.8922 and 0.32748613\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.95684 and 0.13093378\n",
            "Val acc and loss are 0.8917 and 0.3267395\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.95658 and 0.13075127\n",
            "Val acc and loss are 0.892 and 0.32710186\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.9568 and 0.13085599\n",
            "Val acc and loss are 0.892 and 0.32763442\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.95696 and 0.13041714\n",
            "Val acc and loss are 0.8923 and 0.32777017\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.95742 and 0.13002278\n",
            "Val acc and loss are 0.892 and 0.32755816\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.95788 and 0.12986998\n",
            "Val acc and loss are 0.8923 and 0.32744026\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.95748 and 0.12983038\n",
            "Val acc and loss are 0.8917 and 0.3275934\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.95696 and 0.12947717\n",
            "Val acc and loss are 0.8921 and 0.32792392\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.95718 and 0.1291069\n",
            "Val acc and loss are 0.8915 and 0.32832637\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.95742 and 0.12892492\n",
            "Val acc and loss are 0.8912 and 0.32891074\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.9578 and 0.1283924\n",
            "Val acc and loss are 0.8908 and 0.32935673\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.95798 and 0.1279801\n",
            "Val acc and loss are 0.8913 and 0.33003268\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.958 and 0.12782331\n",
            "Val acc and loss are 0.8916 and 0.33073798\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.95812 and 0.12744185\n",
            "Val acc and loss are 0.892 and 0.33031952\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.95806 and 0.1271818\n",
            "Val acc and loss are 0.8925 and 0.32976654\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.95812 and 0.12706713\n",
            "Val acc and loss are 0.8919 and 0.33026427\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.95848 and 0.12665774\n",
            "Val acc and loss are 0.8919 and 0.3306798\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.958 and 0.1268\n",
            "Val acc and loss are 0.8918 and 0.33129767\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.95846 and 0.12648527\n",
            "Val acc and loss are 0.8912 and 0.3308697\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.95878 and 0.12638773\n",
            "Val acc and loss are 0.8908 and 0.33066425\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.9592 and 0.12580258\n",
            "Val acc and loss are 0.8918 and 0.33035323\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.95916 and 0.1257131\n",
            "Val acc and loss are 0.893 and 0.33048072\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.95862 and 0.12597434\n",
            "Val acc and loss are 0.8916 and 0.33117408\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.95924 and 0.12522237\n",
            "Val acc and loss are 0.8915 and 0.3308624\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.95938 and 0.12460895\n",
            "Val acc and loss are 0.8918 and 0.3303639\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.95928 and 0.12435258\n",
            "Val acc and loss are 0.8909 and 0.33035854\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.9589 and 0.12455485\n",
            "Val acc and loss are 0.8917 and 0.331275\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.95942 and 0.12388666\n",
            "Val acc and loss are 0.8915 and 0.3311539\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.95944 and 0.12352371\n",
            "Val acc and loss are 0.8919 and 0.3307763\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.95942 and 0.12360867\n",
            "Val acc and loss are 0.8918 and 0.33141348\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.95948 and 0.12355123\n",
            "Val acc and loss are 0.8907 and 0.33189458\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.95978 and 0.12349494\n",
            "Val acc and loss are 0.8903 and 0.3323935\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.95978 and 0.12305577\n",
            "Val acc and loss are 0.891 and 0.3319309\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.95996 and 0.122311905\n",
            "Val acc and loss are 0.8918 and 0.33055893\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.96004 and 0.12188029\n",
            "Val acc and loss are 0.8918 and 0.32997632\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.96012 and 0.12158945\n",
            "Val acc and loss are 0.8912 and 0.33027223\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.96018 and 0.12166965\n",
            "Val acc and loss are 0.8913 and 0.331221\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.96022 and 0.12191854\n",
            "Val acc and loss are 0.8911 and 0.33213577\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.9602 and 0.12164667\n",
            "Val acc and loss are 0.8904 and 0.3325697\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.96054 and 0.12133105\n",
            "Val acc and loss are 0.8904 and 0.33285096\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.9603 and 0.12091498\n",
            "Val acc and loss are 0.891 and 0.3334999\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.96062 and 0.12047839\n",
            "Val acc and loss are 0.8911 and 0.33399352\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.96072 and 0.11994715\n",
            "Val acc and loss are 0.8917 and 0.334136\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.96082 and 0.11959607\n",
            "Val acc and loss are 0.8921 and 0.3343501\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.961 and 0.11972316\n",
            "Val acc and loss are 0.8915 and 0.33460572\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.96086 and 0.11940871\n",
            "Val acc and loss are 0.8917 and 0.33471498\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.96162 and 0.118697636\n",
            "Val acc and loss are 0.8905 and 0.33383703\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.96204 and 0.11870178\n",
            "Val acc and loss are 0.8897 and 0.33386207\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.96174 and 0.11886801\n",
            "Val acc and loss are 0.8896 and 0.33432376\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.9611 and 0.11899955\n",
            "Val acc and loss are 0.8899 and 0.33501258\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.9613 and 0.11856875\n",
            "Val acc and loss are 0.8909 and 0.33497167\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.96198 and 0.11788424\n",
            "Val acc and loss are 0.8904 and 0.3342737\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.96186 and 0.1178767\n",
            "Val acc and loss are 0.8896 and 0.33430517\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.9618 and 0.11779352\n",
            "Val acc and loss are 0.8901 and 0.33438802\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.96198 and 0.1172363\n",
            "Val acc and loss are 0.8902 and 0.33400363\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.962 and 0.11723701\n",
            "Val acc and loss are 0.8909 and 0.33432135\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.962 and 0.117065996\n",
            "Val acc and loss are 0.8919 and 0.33420697\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.96234 and 0.11673568\n",
            "Val acc and loss are 0.8919 and 0.33350077\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.96282 and 0.11646504\n",
            "Val acc and loss are 0.8914 and 0.3329643\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.96278 and 0.11595835\n",
            "Val acc and loss are 0.8908 and 0.33297518\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.96266 and 0.11561496\n",
            "Val acc and loss are 0.8915 and 0.3331173\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.96272 and 0.115316056\n",
            "Val acc and loss are 0.8911 and 0.33305648\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.96276 and 0.114913926\n",
            "Val acc and loss are 0.8911 and 0.33286127\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.96286 and 0.11476187\n",
            "Val acc and loss are 0.8912 and 0.33318824\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.96306 and 0.114697516\n",
            "Val acc and loss are 0.8915 and 0.3339398\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.96338 and 0.11424044\n",
            "Val acc and loss are 0.8914 and 0.33443096\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.9636 and 0.113918886\n",
            "Val acc and loss are 0.8913 and 0.33504242\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.96344 and 0.11394025\n",
            "Val acc and loss are 0.8909 and 0.33592573\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.96342 and 0.114140496\n",
            "Val acc and loss are 0.8911 and 0.3371742\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.96336 and 0.11383987\n",
            "Val acc and loss are 0.8913 and 0.3372139\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.96354 and 0.11328471\n",
            "Val acc and loss are 0.891 and 0.33695087\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.96348 and 0.11304746\n",
            "Val acc and loss are 0.8907 and 0.33681968\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.96386 and 0.11292908\n",
            "Val acc and loss are 0.8908 and 0.33668777\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.96392 and 0.11256459\n",
            "Val acc and loss are 0.891 and 0.33699527\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.9641 and 0.112287335\n",
            "Val acc and loss are 0.8906 and 0.3370698\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.96394 and 0.11241187\n",
            "Val acc and loss are 0.8906 and 0.337217\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.96406 and 0.11213944\n",
            "Val acc and loss are 0.8913 and 0.33701453\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.96418 and 0.111628264\n",
            "Val acc and loss are 0.8914 and 0.3370609\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.96402 and 0.11126637\n",
            "Val acc and loss are 0.8918 and 0.33765385\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.96414 and 0.11097877\n",
            "Val acc and loss are 0.8917 and 0.33777457\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.964 and 0.11107076\n",
            "Val acc and loss are 0.8916 and 0.33826178\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.96442 and 0.11075523\n",
            "Val acc and loss are 0.8909 and 0.3383388\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.96496 and 0.11001826\n",
            "Val acc and loss are 0.8899 and 0.33836597\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.96476 and 0.10977755\n",
            "Val acc and loss are 0.8912 and 0.3390454\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.96492 and 0.10957423\n",
            "Val acc and loss are 0.8919 and 0.339486\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.9647 and 0.109996065\n",
            "Val acc and loss are 0.8916 and 0.3393898\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.96488 and 0.10963912\n",
            "Val acc and loss are 0.8906 and 0.33898017\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.96512 and 0.10938715\n",
            "Val acc and loss are 0.8919 and 0.3391958\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.96536 and 0.109515\n",
            "Val acc and loss are 0.8916 and 0.33989334\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.9653 and 0.10918898\n",
            "Val acc and loss are 0.8905 and 0.34003732\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.96496 and 0.10865283\n",
            "Val acc and loss are 0.8905 and 0.33967692\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.96526 and 0.108385466\n",
            "Val acc and loss are 0.891 and 0.3400933\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.96532 and 0.10801605\n",
            "Val acc and loss are 0.8916 and 0.34042206\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.96552 and 0.10792539\n",
            "Val acc and loss are 0.8915 and 0.3404564\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.96534 and 0.107973605\n",
            "Val acc and loss are 0.892 and 0.34006697\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.96564 and 0.10766123\n",
            "Val acc and loss are 0.8915 and 0.33913085\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.96612 and 0.10722472\n",
            "Val acc and loss are 0.8911 and 0.3384881\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.9656 and 0.10721522\n",
            "Val acc and loss are 0.8924 and 0.33935064\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.96518 and 0.10755121\n",
            "Val acc and loss are 0.8918 and 0.34089592\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.966 and 0.10662475\n",
            "Val acc and loss are 0.8917 and 0.34035796\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.96632 and 0.10618902\n",
            "Val acc and loss are 0.8912 and 0.34032246\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.9662 and 0.1057038\n",
            "Val acc and loss are 0.8914 and 0.34032777\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.96594 and 0.106009215\n",
            "Val acc and loss are 0.892 and 0.34108922\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.96634 and 0.10565767\n",
            "Val acc and loss are 0.8923 and 0.34106246\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.9659 and 0.1050855\n",
            "Val acc and loss are 0.8911 and 0.34055257\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.96588 and 0.104976565\n",
            "Val acc and loss are 0.8904 and 0.34099612\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.96658 and 0.104354\n",
            "Val acc and loss are 0.8918 and 0.34150824\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.96678 and 0.10432459\n",
            "Val acc and loss are 0.8919 and 0.3424856\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.96652 and 0.10429977\n",
            "Val acc and loss are 0.8912 and 0.3430506\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.9669 and 0.10410772\n",
            "Val acc and loss are 0.8916 and 0.3430228\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.96658 and 0.10450964\n",
            "Val acc and loss are 0.8914 and 0.34365323\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.96692 and 0.10405898\n",
            "Val acc and loss are 0.8916 and 0.34406415\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.96694 and 0.103574336\n",
            "Val acc and loss are 0.8908 and 0.3440762\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.96728 and 0.10363172\n",
            "Val acc and loss are 0.8913 and 0.3435312\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.96752 and 0.1036552\n",
            "Val acc and loss are 0.8917 and 0.34361044\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.96698 and 0.10332428\n",
            "Val acc and loss are 0.8898 and 0.34396031\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.96716 and 0.103082374\n",
            "Val acc and loss are 0.8906 and 0.34400997\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.96758 and 0.10257351\n",
            "Val acc and loss are 0.8904 and 0.34295782\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.96786 and 0.102105886\n",
            "Val acc and loss are 0.8911 and 0.34139252\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.96738 and 0.102314286\n",
            "Val acc and loss are 0.8913 and 0.3418884\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.96788 and 0.10159449\n",
            "Val acc and loss are 0.8926 and 0.3424148\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.96792 and 0.10148119\n",
            "Val acc and loss are 0.8923 and 0.34386417\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.96766 and 0.101501465\n",
            "Val acc and loss are 0.8916 and 0.34486157\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.96782 and 0.10157805\n",
            "Val acc and loss are 0.8915 and 0.3455439\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.96828 and 0.1006059\n",
            "Val acc and loss are 0.8916 and 0.34462842\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.96842 and 0.10029484\n",
            "Val acc and loss are 0.892 and 0.34430665\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.96814 and 0.10030913\n",
            "Val acc and loss are 0.8909 and 0.34465924\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.96842 and 0.09992683\n",
            "Val acc and loss are 0.8916 and 0.34479132\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.96848 and 0.09985891\n",
            "Val acc and loss are 0.8903 and 0.3452937\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.96856 and 0.09969784\n",
            "Val acc and loss are 0.8913 and 0.34600145\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.96808 and 0.099864766\n",
            "Val acc and loss are 0.8906 and 0.3466126\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.96812 and 0.09967861\n",
            "Val acc and loss are 0.8902 and 0.34653607\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.96818 and 0.098897666\n",
            "Val acc and loss are 0.8914 and 0.3453969\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.96814 and 0.09867213\n",
            "Val acc and loss are 0.8912 and 0.34514385\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.9686 and 0.098231345\n",
            "Val acc and loss are 0.8909 and 0.34578547\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.96892 and 0.09780497\n",
            "Val acc and loss are 0.8915 and 0.34619907\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.96942 and 0.09751687\n",
            "Val acc and loss are 0.8917 and 0.34600052\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.96958 and 0.09763789\n",
            "Val acc and loss are 0.8914 and 0.34580216\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.96962 and 0.097732715\n",
            "Val acc and loss are 0.8909 and 0.34618586\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.9697 and 0.09725088\n",
            "Val acc and loss are 0.8906 and 0.34676892\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.96994 and 0.09689046\n",
            "Val acc and loss are 0.8907 and 0.34758487\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.96988 and 0.09670803\n",
            "Val acc and loss are 0.8911 and 0.3482251\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.97 and 0.09647399\n",
            "Val acc and loss are 0.8903 and 0.34847045\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.96982 and 0.096405506\n",
            "Val acc and loss are 0.8903 and 0.34867892\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.9695 and 0.09641226\n",
            "Val acc and loss are 0.8902 and 0.3483967\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.96994 and 0.096454754\n",
            "Val acc and loss are 0.8907 and 0.34790814\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.97012 and 0.09610088\n",
            "Val acc and loss are 0.8897 and 0.34737685\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.97052 and 0.095537655\n",
            "Val acc and loss are 0.8891 and 0.3471266\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.97046 and 0.0953155\n",
            "Val acc and loss are 0.8896 and 0.34798655\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.97004 and 0.09541728\n",
            "Val acc and loss are 0.8909 and 0.3488323\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.97 and 0.0953971\n",
            "Val acc and loss are 0.8911 and 0.34900245\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.97002 and 0.095174305\n",
            "Val acc and loss are 0.8906 and 0.34869117\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.97028 and 0.09526918\n",
            "Val acc and loss are 0.8904 and 0.34901977\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.97052 and 0.09464476\n",
            "Val acc and loss are 0.8907 and 0.3490657\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.9708 and 0.093868114\n",
            "Val acc and loss are 0.8917 and 0.34907115\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.9707 and 0.09373379\n",
            "Val acc and loss are 0.8912 and 0.3492921\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.97098 and 0.09362758\n",
            "Val acc and loss are 0.8916 and 0.3493418\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.97086 and 0.09376125\n",
            "Val acc and loss are 0.8898 and 0.3500664\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.97068 and 0.09370082\n",
            "Val acc and loss are 0.8886 and 0.35090727\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.9711 and 0.09365797\n",
            "Val acc and loss are 0.8902 and 0.35126224\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.97118 and 0.093218476\n",
            "Val acc and loss are 0.8905 and 0.35044548\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.9717 and 0.09255189\n",
            "Val acc and loss are 0.8913 and 0.34944037\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.97168 and 0.09244193\n",
            "Val acc and loss are 0.8909 and 0.34953192\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.97174 and 0.09264705\n",
            "Val acc and loss are 0.89 and 0.35019928\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.9712 and 0.09249369\n",
            "Val acc and loss are 0.8906 and 0.35046464\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.9712 and 0.092211775\n",
            "Val acc and loss are 0.8903 and 0.34958944\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.97154 and 0.09170197\n",
            "Val acc and loss are 0.891 and 0.3485628\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.9715 and 0.09188384\n",
            "Val acc and loss are 0.8903 and 0.34899855\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.97202 and 0.091089636\n",
            "Val acc and loss are 0.8907 and 0.34979022\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.9722 and 0.09074049\n",
            "Val acc and loss are 0.8905 and 0.3517066\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.97236 and 0.09051018\n",
            "Val acc and loss are 0.8906 and 0.35283038\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.9725 and 0.09039408\n",
            "Val acc and loss are 0.891 and 0.35335755\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.97204 and 0.090233974\n",
            "Val acc and loss are 0.8918 and 0.35301504\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.97236 and 0.09028106\n",
            "Val acc and loss are 0.8921 and 0.35230553\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.97268 and 0.09019805\n",
            "Val acc and loss are 0.8914 and 0.35239783\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.97294 and 0.08941557\n",
            "Val acc and loss are 0.8911 and 0.351905\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.97264 and 0.08954221\n",
            "Val acc and loss are 0.8904 and 0.35256985\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.97262 and 0.08970547\n",
            "Val acc and loss are 0.8906 and 0.3523811\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.97284 and 0.0893099\n",
            "Val acc and loss are 0.8919 and 0.35241932\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.97282 and 0.08859543\n",
            "Val acc and loss are 0.8904 and 0.35280588\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.97282 and 0.08868556\n",
            "Val acc and loss are 0.8899 and 0.35361293\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.97302 and 0.08923931\n",
            "Val acc and loss are 0.8898 and 0.3542201\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.97322 and 0.08863401\n",
            "Val acc and loss are 0.8899 and 0.3529306\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.97344 and 0.088526785\n",
            "Val acc and loss are 0.8896 and 0.35246\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.97328 and 0.08807293\n",
            "Val acc and loss are 0.8921 and 0.3528867\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.97366 and 0.08798793\n",
            "Val acc and loss are 0.8922 and 0.3540439\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.9736 and 0.087617\n",
            "Val acc and loss are 0.8917 and 0.3543527\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.97322 and 0.087958194\n",
            "Val acc and loss are 0.8918 and 0.35477027\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.97286 and 0.08823254\n",
            "Val acc and loss are 0.8916 and 0.35508436\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.97332 and 0.08788994\n",
            "Val acc and loss are 0.8905 and 0.35482055\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.97334 and 0.08751914\n",
            "Val acc and loss are 0.8899 and 0.3542674\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.9736 and 0.087310165\n",
            "Val acc and loss are 0.8891 and 0.3533597\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.97388 and 0.08699385\n",
            "Val acc and loss are 0.8899 and 0.353217\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.97384 and 0.08640583\n",
            "Val acc and loss are 0.8895 and 0.35335487\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.97408 and 0.08589723\n",
            "Val acc and loss are 0.8898 and 0.35395285\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.9746 and 0.0855475\n",
            "Val acc and loss are 0.8895 and 0.3544413\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.97464 and 0.0853817\n",
            "Val acc and loss are 0.8899 and 0.3547925\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.97428 and 0.08559794\n",
            "Val acc and loss are 0.8895 and 0.35498172\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.97426 and 0.0857457\n",
            "Val acc and loss are 0.8901 and 0.35493523\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.9743 and 0.08547414\n",
            "Val acc and loss are 0.8903 and 0.35450765\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.97442 and 0.08496904\n",
            "Val acc and loss are 0.8919 and 0.35425514\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.97468 and 0.08454375\n",
            "Val acc and loss are 0.8921 and 0.3543277\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.97486 and 0.08443353\n",
            "Val acc and loss are 0.8924 and 0.35449567\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.97482 and 0.08439756\n",
            "Val acc and loss are 0.8921 and 0.35445404\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.9747 and 0.084228225\n",
            "Val acc and loss are 0.8914 and 0.3539728\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.97456 and 0.084232636\n",
            "Val acc and loss are 0.8911 and 0.35389277\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.97474 and 0.08387977\n",
            "Val acc and loss are 0.8908 and 0.3540544\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.9753 and 0.08369863\n",
            "Val acc and loss are 0.8908 and 0.35456824\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.97564 and 0.08320082\n",
            "Val acc and loss are 0.8913 and 0.35412645\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.9754 and 0.082957014\n",
            "Val acc and loss are 0.8908 and 0.35447395\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.975 and 0.08337793\n",
            "Val acc and loss are 0.8901 and 0.35583064\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.97474 and 0.08342668\n",
            "Val acc and loss are 0.8897 and 0.3575434\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.97526 and 0.082629256\n",
            "Val acc and loss are 0.891 and 0.35740033\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.97528 and 0.0821221\n",
            "Val acc and loss are 0.8917 and 0.35706615\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.9754 and 0.081890896\n",
            "Val acc and loss are 0.8918 and 0.35742182\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.97558 and 0.08185875\n",
            "Val acc and loss are 0.8911 and 0.35829625\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.97576 and 0.08212563\n",
            "Val acc and loss are 0.8908 and 0.35878682\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.97558 and 0.08202583\n",
            "Val acc and loss are 0.8906 and 0.3586367\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.97588 and 0.08186442\n",
            "Val acc and loss are 0.8903 and 0.3582732\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.97616 and 0.08168767\n",
            "Val acc and loss are 0.8901 and 0.3581267\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.97582 and 0.081615776\n",
            "Val acc and loss are 0.8896 and 0.35892987\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.97606 and 0.08127709\n",
            "Val acc and loss are 0.8901 and 0.3595506\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.97584 and 0.08116294\n",
            "Val acc and loss are 0.8914 and 0.3605116\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.9759 and 0.08099949\n",
            "Val acc and loss are 0.8901 and 0.36058357\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.97618 and 0.08063511\n",
            "Val acc and loss are 0.8914 and 0.35989603\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.97608 and 0.080435485\n",
            "Val acc and loss are 0.8918 and 0.35880738\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.97592 and 0.08041275\n",
            "Val acc and loss are 0.8914 and 0.35824847\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.97628 and 0.08028682\n",
            "Val acc and loss are 0.8911 and 0.35781598\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.97638 and 0.08033381\n",
            "Val acc and loss are 0.8918 and 0.3587944\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.97598 and 0.08076066\n",
            "Val acc and loss are 0.8911 and 0.360691\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.97666 and 0.07961568\n",
            "Val acc and loss are 0.8911 and 0.36020043\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.97694 and 0.07920362\n",
            "Val acc and loss are 0.8913 and 0.35999596\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.97724 and 0.07909455\n",
            "Val acc and loss are 0.8914 and 0.36018905\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.97718 and 0.07897754\n",
            "Val acc and loss are 0.8918 and 0.36103782\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.97692 and 0.07899523\n",
            "Val acc and loss are 0.8895 and 0.3614786\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.97646 and 0.079267815\n",
            "Val acc and loss are 0.8901 and 0.362381\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.97654 and 0.07904851\n",
            "Val acc and loss are 0.8909 and 0.3629721\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.97688 and 0.07843312\n",
            "Val acc and loss are 0.8917 and 0.3633086\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.97732 and 0.077916816\n",
            "Val acc and loss are 0.8925 and 0.3633217\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.97744 and 0.07805502\n",
            "Val acc and loss are 0.8915 and 0.36312565\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.97712 and 0.078054026\n",
            "Val acc and loss are 0.8901 and 0.36258844\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.97718 and 0.07785645\n",
            "Val acc and loss are 0.8909 and 0.36273992\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.97732 and 0.07760421\n",
            "Val acc and loss are 0.8903 and 0.36333406\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.97724 and 0.07788143\n",
            "Val acc and loss are 0.8905 and 0.36484355\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.97768 and 0.07764835\n",
            "Val acc and loss are 0.8899 and 0.36505032\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.97744 and 0.077584304\n",
            "Val acc and loss are 0.8896 and 0.3654283\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.97748 and 0.07721703\n",
            "Val acc and loss are 0.8891 and 0.3645982\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.97788 and 0.07657709\n",
            "Val acc and loss are 0.8915 and 0.3634604\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.97818 and 0.07632108\n",
            "Val acc and loss are 0.8922 and 0.36339462\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.97778 and 0.07677658\n",
            "Val acc and loss are 0.8895 and 0.36493015\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.9775 and 0.07697309\n",
            "Val acc and loss are 0.8909 and 0.36638123\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.9775 and 0.0767032\n",
            "Val acc and loss are 0.8915 and 0.3666738\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.97736 and 0.07654494\n",
            "Val acc and loss are 0.8905 and 0.3665247\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.97798 and 0.07609378\n",
            "Val acc and loss are 0.8913 and 0.36585882\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.9783 and 0.075819045\n",
            "Val acc and loss are 0.8909 and 0.3659697\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.97822 and 0.0755317\n",
            "Val acc and loss are 0.891 and 0.36621696\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.97802 and 0.07545808\n",
            "Val acc and loss are 0.891 and 0.3661518\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.97782 and 0.07526695\n",
            "Val acc and loss are 0.8906 and 0.3660204\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.9783 and 0.074869074\n",
            "Val acc and loss are 0.8907 and 0.365958\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.97822 and 0.07456659\n",
            "Val acc and loss are 0.8904 and 0.36616278\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.97844 and 0.07455576\n",
            "Val acc and loss are 0.8909 and 0.36645713\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.97858 and 0.074484475\n",
            "Val acc and loss are 0.8896 and 0.36565816\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.97832 and 0.07467329\n",
            "Val acc and loss are 0.8896 and 0.3645561\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.97838 and 0.074806556\n",
            "Val acc and loss are 0.8888 and 0.36473483\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.97816 and 0.07415805\n",
            "Val acc and loss are 0.8906 and 0.36515924\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.97842 and 0.07371672\n",
            "Val acc and loss are 0.8917 and 0.36603734\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.97874 and 0.07316968\n",
            "Val acc and loss are 0.8908 and 0.36549398\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.9788 and 0.073348045\n",
            "Val acc and loss are 0.8898 and 0.3654402\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.9784 and 0.073160216\n",
            "Val acc and loss are 0.8903 and 0.3658132\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.97852 and 0.07287327\n",
            "Val acc and loss are 0.8906 and 0.36623287\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.9789 and 0.07288766\n",
            "Val acc and loss are 0.8909 and 0.3660861\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.97868 and 0.072977215\n",
            "Val acc and loss are 0.8911 and 0.36512178\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.97898 and 0.072588764\n",
            "Val acc and loss are 0.8905 and 0.36468184\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.97918 and 0.072270334\n",
            "Val acc and loss are 0.8909 and 0.36515635\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.97898 and 0.072379015\n",
            "Val acc and loss are 0.8905 and 0.36642754\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.97938 and 0.07239754\n",
            "Val acc and loss are 0.8902 and 0.36720362\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.97934 and 0.07229159\n",
            "Val acc and loss are 0.8899 and 0.3671091\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.97926 and 0.07175552\n",
            "Val acc and loss are 0.8895 and 0.36673784\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.97948 and 0.071325816\n",
            "Val acc and loss are 0.8894 and 0.36749136\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.9797 and 0.07071846\n",
            "Val acc and loss are 0.8895 and 0.36866185\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.97986 and 0.070837095\n",
            "Val acc and loss are 0.8907 and 0.37013987\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.9795 and 0.07098764\n",
            "Val acc and loss are 0.8907 and 0.37024605\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.97924 and 0.07161436\n",
            "Val acc and loss are 0.8908 and 0.37060472\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.97998 and 0.07131573\n",
            "Val acc and loss are 0.891 and 0.36985385\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.9797 and 0.07063982\n",
            "Val acc and loss are 0.8908 and 0.36907265\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.97944 and 0.07075714\n",
            "Val acc and loss are 0.8901 and 0.36907277\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.98018 and 0.06971256\n",
            "Val acc and loss are 0.891 and 0.36848354\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.98064 and 0.06943298\n",
            "Val acc and loss are 0.8912 and 0.3685572\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.97994 and 0.070066705\n",
            "Val acc and loss are 0.8903 and 0.3695697\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.98012 and 0.0701915\n",
            "Val acc and loss are 0.8892 and 0.3707199\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.98048 and 0.0696087\n",
            "Val acc and loss are 0.8914 and 0.370594\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.98012 and 0.069292486\n",
            "Val acc and loss are 0.8914 and 0.3706498\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.9803 and 0.0694306\n",
            "Val acc and loss are 0.8907 and 0.3708635\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.98086 and 0.06897164\n",
            "Val acc and loss are 0.8911 and 0.3715316\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.9806 and 0.06892465\n",
            "Val acc and loss are 0.8899 and 0.37114394\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.98064 and 0.06904772\n",
            "Val acc and loss are 0.8902 and 0.37020895\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.98064 and 0.06936152\n",
            "Val acc and loss are 0.8902 and 0.36971796\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.9804 and 0.069096014\n",
            "Val acc and loss are 0.8895 and 0.36999595\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.98006 and 0.068831496\n",
            "Val acc and loss are 0.8898 and 0.37141654\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.9803 and 0.06858746\n",
            "Val acc and loss are 0.8901 and 0.37283853\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.9803 and 0.06812938\n",
            "Val acc and loss are 0.8901 and 0.37300354\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.98096 and 0.06778233\n",
            "Val acc and loss are 0.8907 and 0.37323666\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.98046 and 0.068072215\n",
            "Val acc and loss are 0.89 and 0.3747463\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.98042 and 0.067651585\n",
            "Val acc and loss are 0.8902 and 0.3749819\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.98136 and 0.06710805\n",
            "Val acc and loss are 0.8907 and 0.37480408\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.98124 and 0.06678228\n",
            "Val acc and loss are 0.8916 and 0.37412035\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.98078 and 0.06692582\n",
            "Val acc and loss are 0.8901 and 0.37352902\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.981 and 0.06659473\n",
            "Val acc and loss are 0.8921 and 0.37364724\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.98126 and 0.06647072\n",
            "Val acc and loss are 0.8905 and 0.37448356\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.98108 and 0.06676106\n",
            "Val acc and loss are 0.8903 and 0.37607533\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.98122 and 0.06647914\n",
            "Val acc and loss are 0.8904 and 0.37595057\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.98146 and 0.06618968\n",
            "Val acc and loss are 0.8913 and 0.37504774\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.98146 and 0.06629508\n",
            "Val acc and loss are 0.8913 and 0.37526676\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.98142 and 0.06614254\n",
            "Val acc and loss are 0.8899 and 0.37556314\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.98122 and 0.066002466\n",
            "Val acc and loss are 0.8897 and 0.37640333\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.98138 and 0.06597543\n",
            "Val acc and loss are 0.8912 and 0.377859\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.98112 and 0.06603696\n",
            "Val acc and loss are 0.8895 and 0.3780803\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.98124 and 0.065802\n",
            "Val acc and loss are 0.8901 and 0.37630787\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.98216 and 0.065567106\n",
            "Val acc and loss are 0.8898 and 0.37566578\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.98204 and 0.064897284\n",
            "Val acc and loss are 0.8901 and 0.37664905\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.98152 and 0.06537867\n",
            "Val acc and loss are 0.8902 and 0.37905145\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.98172 and 0.0651557\n",
            "Val acc and loss are 0.8901 and 0.3782735\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.98158 and 0.0650116\n",
            "Val acc and loss are 0.8911 and 0.37683493\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.982 and 0.064560145\n",
            "Val acc and loss are 0.8897 and 0.3766992\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.98206 and 0.064372174\n",
            "Val acc and loss are 0.8895 and 0.37748396\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.98224 and 0.06450731\n",
            "Val acc and loss are 0.8895 and 0.3785557\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.98234 and 0.06412875\n",
            "Val acc and loss are 0.8893 and 0.3781339\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.982 and 0.06436179\n",
            "Val acc and loss are 0.8915 and 0.37779325\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.98194 and 0.06389703\n",
            "Val acc and loss are 0.8914 and 0.37667572\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.98218 and 0.06355621\n",
            "Val acc and loss are 0.8925 and 0.37709862\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.98252 and 0.063214056\n",
            "Val acc and loss are 0.8918 and 0.37763464\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.98246 and 0.063189864\n",
            "Val acc and loss are 0.8913 and 0.377404\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.98246 and 0.06309492\n",
            "Val acc and loss are 0.89 and 0.37672725\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.9824 and 0.06286309\n",
            "Val acc and loss are 0.8918 and 0.3766241\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.982 and 0.062954776\n",
            "Val acc and loss are 0.8927 and 0.37889394\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.98216 and 0.062386837\n",
            "Val acc and loss are 0.893 and 0.38052738\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.98272 and 0.061987106\n",
            "Val acc and loss are 0.8913 and 0.3811305\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.98272 and 0.0620154\n",
            "Val acc and loss are 0.8921 and 0.38061616\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.98274 and 0.061961416\n",
            "Val acc and loss are 0.8907 and 0.38002723\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.98262 and 0.06193744\n",
            "Val acc and loss are 0.8891 and 0.3803093\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.98284 and 0.061876524\n",
            "Val acc and loss are 0.889 and 0.37984097\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.98268 and 0.06182775\n",
            "Val acc and loss are 0.8893 and 0.379932\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.98278 and 0.06155917\n",
            "Val acc and loss are 0.8905 and 0.3803072\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.9828 and 0.061743017\n",
            "Val acc and loss are 0.8904 and 0.38161546\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.98324 and 0.0616958\n",
            "Val acc and loss are 0.8913 and 0.38266352\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.98296 and 0.06141014\n",
            "Val acc and loss are 0.8909 and 0.3826512\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.98272 and 0.061710946\n",
            "Val acc and loss are 0.8904 and 0.38305235\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.98302 and 0.06088534\n",
            "Val acc and loss are 0.8905 and 0.38277677\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.9831 and 0.060755584\n",
            "Val acc and loss are 0.8901 and 0.38383788\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.98334 and 0.060517106\n",
            "Val acc and loss are 0.8907 and 0.3846977\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.98274 and 0.06090295\n",
            "Val acc and loss are 0.8904 and 0.3858306\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.983 and 0.0604937\n",
            "Val acc and loss are 0.8899 and 0.38526264\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.98362 and 0.060140558\n",
            "Val acc and loss are 0.8899 and 0.38529423\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.98324 and 0.06013494\n",
            "Val acc and loss are 0.8904 and 0.38462627\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.98352 and 0.060060795\n",
            "Val acc and loss are 0.8901 and 0.38357443\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.98374 and 0.059849367\n",
            "Val acc and loss are 0.8905 and 0.38231274\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.98388 and 0.05980162\n",
            "Val acc and loss are 0.8905 and 0.38204616\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.98338 and 0.059577834\n",
            "Val acc and loss are 0.8905 and 0.38269693\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.98314 and 0.059491955\n",
            "Val acc and loss are 0.891 and 0.38388336\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.98354 and 0.05911087\n",
            "Val acc and loss are 0.8921 and 0.3850945\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.98368 and 0.05884414\n",
            "Val acc and loss are 0.8909 and 0.3857995\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.98374 and 0.058636338\n",
            "Val acc and loss are 0.8912 and 0.38575336\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.98354 and 0.05940683\n",
            "Val acc and loss are 0.8913 and 0.38551134\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.98374 and 0.059345096\n",
            "Val acc and loss are 0.8914 and 0.38427526\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.98416 and 0.058569845\n",
            "Val acc and loss are 0.8901 and 0.38379812\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.98418 and 0.058481727\n",
            "Val acc and loss are 0.8911 and 0.3843364\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.98388 and 0.058297295\n",
            "Val acc and loss are 0.8908 and 0.38460246\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.98408 and 0.057994165\n",
            "Val acc and loss are 0.8909 and 0.38463286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJxogMPhUxhF",
        "outputId": "9dd5dd79-f070-4181-9ac9-36d30bbdface"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.893 at epoch 654 with a corresponding training accuracy of 0.95916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c51IPv7cn6SV"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "d-xe1auOn6P4",
        "outputId": "f314b7ff-645a-4d81-bad1-89435739c416"
      },
      "source": [
        "# Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.98418 achieved at epoch: 997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59xwAssxoK_I",
        "outputId": "25897ce4-e119-476a-d0d6-d24e2a452f3d"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4611,    4,   31,   65,    4,    0,  250,    0,   11,    1],\n",
              "       [   9, 4941,    3,   45,    7,    0,    4,    1,    2,    0],\n",
              "       [  37,    0, 4565,   25,  234,    0,  126,    1,    4,    0],\n",
              "       [  55,   16,   15, 4737,   91,    1,   52,    1,   11,    0],\n",
              "       [   5,    2,  136,   72, 4621,    1,  107,    0,    6,    0],\n",
              "       [   1,    1,    0,    2,    0, 4927,    1,   38,   13,   21],\n",
              "       [ 289,    1,  148,   73,  136,    1, 4370,    0,   11,    1],\n",
              "       [   0,    0,    2,    2,    0,   32,    0, 4949,    3,   57],\n",
              "       [   4,    4,   10,   14,   10,   13,   16,    5, 4952,    4],\n",
              "       [   0,    1,    0,    0,    1,   23,    3,   63,    1, 4887]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "U-ddPyNNoLHm",
        "outputId": "80e84c77-0315-4365-841c-1d30738309de"
      },
      "source": [
        "# Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.893 achieved at epoch: 653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM8pZduIoLMM",
        "outputId": "79988017-d364-4d37-b62e-a06bfc77e236"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[867,   1,  15,  30,   0,   4,  98,   0,   8,   0],\n",
              "       [  5, 960,   0,  17,   3,   0,   1,   0,   2,   0],\n",
              "       [ 12,   1, 850,   4,  77,   0,  57,   0,   7,   0],\n",
              "       [ 23,   6,   6, 923,  38,   0,  23,   0,   2,   0],\n",
              "       [  2,   1,  77,  29, 881,   1,  53,   0,   6,   0],\n",
              "       [  0,   0,   0,   0,   0, 955,   0,  24,   7,  10],\n",
              "       [114,   1,  85,  24,  70,   0, 660,   1,  14,   1],\n",
              "       [  0,   0,   0,   0,   0,  16,   0, 906,   0,  33],\n",
              "       [  4,   0,   5,   2,   3,   5,   9,   3, 934,   3],\n",
              "       [  0,   0,   0,   0,   0,  10,   0,  38,   0, 973]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igVUtcbCoLQw",
        "outputId": "9dfa2dc9-8578-4499-e71f-5fc8582f135d"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.40519166\n",
            "0.886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H45u7Mo_oLEV",
        "outputId": "dc57f816-143d-4cb9-babb-3824960d5cb7"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[836,   5,  14,  33,   1,   1,  99,   0,  11,   0],\n",
              "       [  4, 965,   2,  19,   4,   0,   5,   0,   1,   0],\n",
              "       [ 23,   1, 811,   8,  79,   1,  73,   1,   3,   0],\n",
              "       [ 22,   7,   8, 897,  26,   0,  33,   0,   7,   0],\n",
              "       [  0,   0,  86,  29, 839,   1,  43,   0,   2,   0],\n",
              "       [  0,   0,   0,   0,   0, 962,   0,  20,   1,  17],\n",
              "       [120,   1,  94,  27,  66,   1, 681,   0,  10,   0],\n",
              "       [  0,   0,   0,   0,   0,  17,   0, 954,   1,  28],\n",
              "       [  6,   1,   2,   7,   3,   7,   8,   5, 961,   0],\n",
              "       [  0,   0,   0,   0,   0,   7,   1,  38,   0, 954]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8lJiCXIoWti"
      },
      "source": [
        "# **Test 5** *(Revised from Test 4: optimiser = SGD)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sumq2YM0oLCf",
        "outputId": "ffac0c5f-cf3a-4d3a-b6fd-83bd29e70683"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 128, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"relu\", optimizer_name = 'RMSProp', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M4E2Cpzofg-",
        "outputId": "889cf714-ed51-4c75-aa97-f304eff39da2"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.07336 and 2.6310225\n",
            "Val acc and loss are 0.0746 and 2.6167364\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.0747 and 2.6207678\n",
            "Val acc and loss are 0.0757 and 2.6065948\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.07614 and 2.6101146\n",
            "Val acc and loss are 0.0764 and 2.5960617\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.07754 and 2.5989912\n",
            "Val acc and loss are 0.0783 and 2.585068\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.07908 and 2.5874717\n",
            "Val acc and loss are 0.0799 and 2.5736814\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.0808 and 2.5755563\n",
            "Val acc and loss are 0.0817 and 2.5619037\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.0822 and 2.5632656\n",
            "Val acc and loss are 0.0839 and 2.5497499\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.0842 and 2.5504725\n",
            "Val acc and loss are 0.0868 and 2.5370965\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.08618 and 2.5371966\n",
            "Val acc and loss are 0.0887 and 2.5239637\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.08848 and 2.5235\n",
            "Val acc and loss are 0.0913 and 2.5104098\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.09108 and 2.509287\n",
            "Val acc and loss are 0.0939 and 2.4963422\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.09392 and 2.494626\n",
            "Val acc and loss are 0.0966 and 2.481823\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.09702 and 2.4795043\n",
            "Val acc and loss are 0.1003 and 2.4668524\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.10082 and 2.4639194\n",
            "Val acc and loss are 0.1027 and 2.4514143\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.1046 and 2.4478045\n",
            "Val acc and loss are 0.1078 and 2.4354508\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.10802 and 2.431183\n",
            "Val acc and loss are 0.1122 and 2.4189868\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.11202 and 2.4140997\n",
            "Val acc and loss are 0.116 and 2.4020667\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.11698 and 2.3966274\n",
            "Val acc and loss are 0.1213 and 2.3847504\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.12192 and 2.3785627\n",
            "Val acc and loss are 0.1264 and 2.366843\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.12722 and 2.3600354\n",
            "Val acc and loss are 0.1333 and 2.3484793\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.13354 and 2.341049\n",
            "Val acc and loss are 0.139 and 2.3296492\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.14052 and 2.3216293\n",
            "Val acc and loss are 0.1471 and 2.310394\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.1473 and 2.301727\n",
            "Val acc and loss are 0.1547 and 2.2906501\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.15508 and 2.2812166\n",
            "Val acc and loss are 0.1617 and 2.2703109\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.16346 and 2.2602975\n",
            "Val acc and loss are 0.1713 and 2.249555\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.1724 and 2.2389638\n",
            "Val acc and loss are 0.1786 and 2.228384\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.18146 and 2.217142\n",
            "Val acc and loss are 0.1873 and 2.2067347\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.19198 and 2.1948526\n",
            "Val acc and loss are 0.198 and 2.1846137\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.20284 and 2.172076\n",
            "Val acc and loss are 0.2074 and 2.1619954\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.21432 and 2.14886\n",
            "Val acc and loss are 0.2179 and 2.1389484\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.22642 and 2.1250842\n",
            "Val acc and loss are 0.2291 and 2.1153429\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.23886 and 2.1009686\n",
            "Val acc and loss are 0.243 and 2.0913987\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.25186 and 2.0764813\n",
            "Val acc and loss are 0.257 and 2.0670872\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.266 and 2.05155\n",
            "Val acc and loss are 0.2728 and 2.042347\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.28038 and 2.0261972\n",
            "Val acc and loss are 0.2883 and 2.0171864\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.29532 and 2.00049\n",
            "Val acc and loss are 0.3013 and 1.9916754\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.31168 and 1.9744775\n",
            "Val acc and loss are 0.3173 and 1.9658586\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.3279 and 1.9481006\n",
            "Val acc and loss are 0.3331 and 1.9396838\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.3455 and 1.9213599\n",
            "Val acc and loss are 0.3536 and 1.9131504\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.36368 and 1.894335\n",
            "Val acc and loss are 0.373 and 1.8863195\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.38208 and 1.8669696\n",
            "Val acc and loss are 0.3905 and 1.8591611\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.4005 and 1.8392147\n",
            "Val acc and loss are 0.4104 and 1.8316152\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.41798 and 1.8114371\n",
            "Val acc and loss are 0.4263 and 1.8040252\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.43616 and 1.7832153\n",
            "Val acc and loss are 0.4435 and 1.7760096\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.4531 and 1.7549561\n",
            "Val acc and loss are 0.4617 and 1.747941\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.4695 and 1.7264625\n",
            "Val acc and loss are 0.4774 and 1.7196139\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.48476 and 1.6977903\n",
            "Val acc and loss are 0.4914 and 1.6910737\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.49966 and 1.6691575\n",
            "Val acc and loss are 0.505 and 1.6625582\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.51218 and 1.6404587\n",
            "Val acc and loss are 0.5176 and 1.6339653\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.52418 and 1.6116835\n",
            "Val acc and loss are 0.5295 and 1.6053213\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.53546 and 1.5829134\n",
            "Val acc and loss are 0.5425 and 1.5766896\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.54694 and 1.5541971\n",
            "Val acc and loss are 0.555 and 1.548115\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.5571 and 1.525535\n",
            "Val acc and loss are 0.5645 and 1.5195928\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.56684 and 1.496949\n",
            "Val acc and loss are 0.5745 and 1.4911081\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.57556 and 1.4686664\n",
            "Val acc and loss are 0.5841 and 1.4629008\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.58374 and 1.4404817\n",
            "Val acc and loss are 0.5917 and 1.4347793\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.59132 and 1.4127572\n",
            "Val acc and loss are 0.599 and 1.4070638\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.5983 and 1.3851457\n",
            "Val acc and loss are 0.6062 and 1.3794771\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.606 and 1.3578272\n",
            "Val acc and loss are 0.6129 and 1.3521553\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.6135 and 1.33098\n",
            "Val acc and loss are 0.6198 and 1.3253273\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.61984 and 1.3045353\n",
            "Val acc and loss are 0.6262 and 1.2988931\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.62632 and 1.2784954\n",
            "Val acc and loss are 0.6321 and 1.2728534\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.63196 and 1.252831\n",
            "Val acc and loss are 0.639 and 1.2471731\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.6385 and 1.2275884\n",
            "Val acc and loss are 0.6451 and 1.2219044\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.64426 and 1.2026623\n",
            "Val acc and loss are 0.649 and 1.1969777\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.6498 and 1.1780807\n",
            "Val acc and loss are 0.6538 and 1.1724478\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.65558 and 1.1538967\n",
            "Val acc and loss are 0.6594 and 1.1482898\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.66052 and 1.1306517\n",
            "Val acc and loss are 0.6633 and 1.1250566\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.6655 and 1.1077603\n",
            "Val acc and loss are 0.6686 and 1.102153\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.66994 and 1.0853823\n",
            "Val acc and loss are 0.6738 and 1.0797781\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.6745 and 1.0635909\n",
            "Val acc and loss are 0.6775 and 1.0580225\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.67948 and 1.0423684\n",
            "Val acc and loss are 0.6816 and 1.0368444\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.68418 and 1.0215975\n",
            "Val acc and loss are 0.6863 and 1.0161265\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.6884 and 1.0013925\n",
            "Val acc and loss are 0.6919 and 0.9959518\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.69238 and 0.9819974\n",
            "Val acc and loss are 0.6958 and 0.9765994\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.69638 and 0.96316236\n",
            "Val acc and loss are 0.6988 and 0.9578678\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.70028 and 0.944732\n",
            "Val acc and loss are 0.7027 and 0.9395837\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.70442 and 0.9268225\n",
            "Val acc and loss are 0.7059 and 0.9218432\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.70776 and 0.90944374\n",
            "Val acc and loss are 0.7085 and 0.9046281\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.712 and 0.89227283\n",
            "Val acc and loss are 0.7118 and 0.8876424\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.7162 and 0.87572217\n",
            "Val acc and loss are 0.7147 and 0.87123513\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.71994 and 0.86012125\n",
            "Val acc and loss are 0.7181 and 0.8558337\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.72336 and 0.84476\n",
            "Val acc and loss are 0.7217 and 0.8406504\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.72772 and 0.8295723\n",
            "Val acc and loss are 0.7247 and 0.8256662\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.73114 and 0.815311\n",
            "Val acc and loss are 0.7284 and 0.8116311\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.73438 and 0.80147815\n",
            "Val acc and loss are 0.7325 and 0.7980319\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.73752 and 0.78781354\n",
            "Val acc and loss are 0.7351 and 0.7846138\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.74094 and 0.77465296\n",
            "Val acc and loss are 0.7403 and 0.771606\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.74408 and 0.76202106\n",
            "Val acc and loss are 0.7428 and 0.7591255\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.74734 and 0.7492259\n",
            "Val acc and loss are 0.7458 and 0.746688\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.75002 and 0.7371264\n",
            "Val acc and loss are 0.7497 and 0.7348151\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.75282 and 0.7254986\n",
            "Val acc and loss are 0.7523 and 0.7233955\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.756 and 0.71443313\n",
            "Val acc and loss are 0.7557 and 0.7125009\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.75902 and 0.7033784\n",
            "Val acc and loss are 0.758 and 0.70169836\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.76158 and 0.6931771\n",
            "Val acc and loss are 0.7612 and 0.69161355\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.76438 and 0.68265575\n",
            "Val acc and loss are 0.7643 and 0.6813747\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.76712 and 0.6732437\n",
            "Val acc and loss are 0.7678 and 0.6720916\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.77 and 0.66355455\n",
            "Val acc and loss are 0.7709 and 0.66265666\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.77274 and 0.6537276\n",
            "Val acc and loss are 0.7727 and 0.65321636\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.77456 and 0.64504415\n",
            "Val acc and loss are 0.7754 and 0.64471835\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.77682 and 0.63656515\n",
            "Val acc and loss are 0.7774 and 0.6362558\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.77886 and 0.62821853\n",
            "Val acc and loss are 0.7805 and 0.6281394\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.78126 and 0.6197812\n",
            "Val acc and loss are 0.7808 and 0.62007755\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.78364 and 0.61262023\n",
            "Val acc and loss are 0.7837 and 0.61312175\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.78602 and 0.6041279\n",
            "Val acc and loss are 0.7855 and 0.60508835\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.78836 and 0.5971205\n",
            "Val acc and loss are 0.7874 and 0.59818166\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.79034 and 0.590044\n",
            "Val acc and loss are 0.789 and 0.5914126\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.79294 and 0.5822455\n",
            "Val acc and loss are 0.7909 and 0.58406824\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.79488 and 0.57609266\n",
            "Val acc and loss are 0.7937 and 0.5781389\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.79742 and 0.5687484\n",
            "Val acc and loss are 0.7964 and 0.5713644\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.79938 and 0.56269765\n",
            "Val acc and loss are 0.7984 and 0.56566685\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.80096 and 0.55678\n",
            "Val acc and loss are 0.7995 and 0.5600232\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.80492 and 0.54953784\n",
            "Val acc and loss are 0.8023 and 0.55326056\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.80536 and 0.54420525\n",
            "Val acc and loss are 0.8023 and 0.5482216\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.80772 and 0.53870624\n",
            "Val acc and loss are 0.8055 and 0.54304\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.80846 and 0.53427815\n",
            "Val acc and loss are 0.8039 and 0.53933567\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.81054 and 0.5306654\n",
            "Val acc and loss are 0.8073 and 0.53587085\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.80758 and 0.5323345\n",
            "Val acc and loss are 0.8024 and 0.5381844\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.80462 and 0.53822255\n",
            "Val acc and loss are 0.8029 and 0.5442312\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.80188 and 0.544105\n",
            "Val acc and loss are 0.7986 and 0.5505765\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.80104 and 0.5433547\n",
            "Val acc and loss are 0.7992 and 0.5503241\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.8126 and 0.52488357\n",
            "Val acc and loss are 0.8065 and 0.53265464\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.80546 and 0.52815014\n",
            "Val acc and loss are 0.8052 and 0.53522134\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.8213 and 0.50400656\n",
            "Val acc and loss are 0.8142 and 0.51300555\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.80798 and 0.515947\n",
            "Val acc and loss are 0.8086 and 0.52336955\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.82514 and 0.49408805\n",
            "Val acc and loss are 0.8169 and 0.5038638\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.81376 and 0.5025598\n",
            "Val acc and loss are 0.8116 and 0.51115793\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.82826 and 0.48477125\n",
            "Val acc and loss are 0.8203 and 0.49449354\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.8199 and 0.4898308\n",
            "Val acc and loss are 0.8176 and 0.49882978\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.8289 and 0.47826782\n",
            "Val acc and loss are 0.822 and 0.48857862\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.82034 and 0.48693487\n",
            "Val acc and loss are 0.8177 and 0.49716264\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.82694 and 0.48130563\n",
            "Val acc and loss are 0.82 and 0.49185088\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.81876 and 0.49148512\n",
            "Val acc and loss are 0.8144 and 0.5028469\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.82516 and 0.48146194\n",
            "Val acc and loss are 0.8197 and 0.4922272\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.82366 and 0.4806558\n",
            "Val acc and loss are 0.8202 and 0.49309346\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.82862 and 0.4709793\n",
            "Val acc and loss are 0.8234 and 0.48243648\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.83146 and 0.4649143\n",
            "Val acc and loss are 0.8264 and 0.47825792\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.8309 and 0.46232423\n",
            "Val acc and loss are 0.8246 and 0.4741019\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.83744 and 0.4537747\n",
            "Val acc and loss are 0.8307 and 0.46851274\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.82772 and 0.46543986\n",
            "Val acc and loss are 0.8229 and 0.47755572\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.8391 and 0.44888738\n",
            "Val acc and loss are 0.8316 and 0.46454424\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.8284 and 0.4596783\n",
            "Val acc and loss are 0.8232 and 0.47209868\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.84314 and 0.44081616\n",
            "Val acc and loss are 0.836 and 0.45692274\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.83382 and 0.44730347\n",
            "Val acc and loss are 0.8279 and 0.46052724\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.84566 and 0.43565476\n",
            "Val acc and loss are 0.8374 and 0.4529208\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.83702 and 0.4396437\n",
            "Val acc and loss are 0.83 and 0.4545479\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.84206 and 0.44059914\n",
            "Val acc and loss are 0.834 and 0.45854872\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.83636 and 0.4433\n",
            "Val acc and loss are 0.8308 and 0.45910707\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.83594 and 0.451825\n",
            "Val acc and loss are 0.8285 and 0.4708664\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.83954 and 0.44023234\n",
            "Val acc and loss are 0.8307 and 0.4573261\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.83694 and 0.44779313\n",
            "Val acc and loss are 0.8283 and 0.46743146\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.84466 and 0.432825\n",
            "Val acc and loss are 0.8366 and 0.45066005\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.8382 and 0.4403416\n",
            "Val acc and loss are 0.8307 and 0.45958263\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.84956 and 0.4185465\n",
            "Val acc and loss are 0.8414 and 0.43720537\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.84274 and 0.42487398\n",
            "Val acc and loss are 0.8365 and 0.44309962\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.85394 and 0.40915564\n",
            "Val acc and loss are 0.8442 and 0.42864332\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.84462 and 0.42066652\n",
            "Val acc and loss are 0.8392 and 0.43870386\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.85336 and 0.40727165\n",
            "Val acc and loss are 0.8455 and 0.42781934\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.84628 and 0.42016602\n",
            "Val acc and loss are 0.8397 and 0.43868184\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.85454 and 0.40314716\n",
            "Val acc and loss are 0.845 and 0.42503622\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.85038 and 0.4109941\n",
            "Val acc and loss are 0.8433 and 0.43061748\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.85652 and 0.39732328\n",
            "Val acc and loss are 0.8465 and 0.41950953\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.85384 and 0.40097573\n",
            "Val acc and loss are 0.8467 and 0.42085516\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.85664 and 0.39534652\n",
            "Val acc and loss are 0.8452 and 0.41920117\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.85332 and 0.40162456\n",
            "Val acc and loss are 0.8425 and 0.42169702\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.84804 and 0.41372\n",
            "Val acc and loss are 0.8385 and 0.43942645\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.83212 and 0.44724968\n",
            "Val acc and loss are 0.8244 and 0.4664728\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.83162 and 0.45884836\n",
            "Val acc and loss are 0.8217 and 0.48554415\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.84734 and 0.4117882\n",
            "Val acc and loss are 0.8394 and 0.4333665\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.85698 and 0.39394906\n",
            "Val acc and loss are 0.8491 and 0.4176791\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.85988 and 0.38755292\n",
            "Val acc and loss are 0.8483 and 0.41116846\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.85358 and 0.39602184\n",
            "Val acc and loss are 0.847 and 0.41926622\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.85542 and 0.39668024\n",
            "Val acc and loss are 0.8437 and 0.42222783\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.84902 and 0.40298608\n",
            "Val acc and loss are 0.8446 and 0.42638043\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.85762 and 0.39108276\n",
            "Val acc and loss are 0.8459 and 0.41600692\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.85736 and 0.38580337\n",
            "Val acc and loss are 0.8517 and 0.40982407\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.86186 and 0.38146985\n",
            "Val acc and loss are 0.8509 and 0.40662506\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.8617 and 0.37729055\n",
            "Val acc and loss are 0.8544 and 0.40239638\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.86278 and 0.37815544\n",
            "Val acc and loss are 0.8514 and 0.40408778\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.86332 and 0.37543124\n",
            "Val acc and loss are 0.8547 and 0.4010429\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.86056 and 0.37957382\n",
            "Val acc and loss are 0.8475 and 0.4062793\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.8583 and 0.38635933\n",
            "Val acc and loss are 0.8498 and 0.41264194\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.85314 and 0.39364153\n",
            "Val acc and loss are 0.8442 and 0.4215314\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.857 and 0.39149624\n",
            "Val acc and loss are 0.8486 and 0.41964078\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.85592 and 0.38727173\n",
            "Val acc and loss are 0.8449 and 0.41469157\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.86078 and 0.3809897\n",
            "Val acc and loss are 0.8496 and 0.41092142\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.8581 and 0.3853475\n",
            "Val acc and loss are 0.8483 and 0.4115625\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.85878 and 0.38424665\n",
            "Val acc and loss are 0.8465 and 0.415374\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.85932 and 0.38469684\n",
            "Val acc and loss are 0.8535 and 0.41099444\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.86058 and 0.37830147\n",
            "Val acc and loss are 0.8488 and 0.40875626\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.86768 and 0.3648643\n",
            "Val acc and loss are 0.8581 and 0.3937808\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.86046 and 0.3740871\n",
            "Val acc and loss are 0.8515 and 0.4027825\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.86836 and 0.36424905\n",
            "Val acc and loss are 0.8571 and 0.39548722\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.85982 and 0.37587586\n",
            "Val acc and loss are 0.8507 and 0.4043332\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.86778 and 0.3625446\n",
            "Val acc and loss are 0.8572 and 0.3948808\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.86126 and 0.3721941\n",
            "Val acc and loss are 0.8514 and 0.40224102\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.867 and 0.36303413\n",
            "Val acc and loss are 0.8547 and 0.39500087\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.86186 and 0.3712225\n",
            "Val acc and loss are 0.8534 and 0.40266034\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.86208 and 0.37192523\n",
            "Val acc and loss are 0.8479 and 0.4023473\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.85946 and 0.3812583\n",
            "Val acc and loss are 0.8496 and 0.415012\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.8594 and 0.376946\n",
            "Val acc and loss are 0.8468 and 0.40688837\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.86964 and 0.35817063\n",
            "Val acc and loss are 0.8591 and 0.39229652\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.8687 and 0.35674188\n",
            "Val acc and loss are 0.8548 and 0.3878585\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.8722 and 0.35124505\n",
            "Val acc and loss are 0.8601 and 0.38616055\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.86864 and 0.35542262\n",
            "Val acc and loss are 0.8556 and 0.38678843\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.87202 and 0.35085487\n",
            "Val acc and loss are 0.8596 and 0.38751197\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.86574 and 0.36011323\n",
            "Val acc and loss are 0.8539 and 0.39089316\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.8702 and 0.35626644\n",
            "Val acc and loss are 0.8592 and 0.39441994\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.86316 and 0.36106652\n",
            "Val acc and loss are 0.8522 and 0.39248705\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.86978 and 0.3575426\n",
            "Val acc and loss are 0.8587 and 0.39447275\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.86786 and 0.353558\n",
            "Val acc and loss are 0.8548 and 0.38741025\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.8695 and 0.3570354\n",
            "Val acc and loss are 0.8575 and 0.39330077\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.86976 and 0.35124046\n",
            "Val acc and loss are 0.8559 and 0.38720903\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.87118 and 0.35173672\n",
            "Val acc and loss are 0.8591 and 0.38846588\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.8702 and 0.34938726\n",
            "Val acc and loss are 0.8552 and 0.38498846\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.86962 and 0.3518216\n",
            "Val acc and loss are 0.8573 and 0.39025474\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.8639 and 0.36272976\n",
            "Val acc and loss are 0.8508 and 0.3965249\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.86384 and 0.36489114\n",
            "Val acc and loss are 0.8509 and 0.40454057\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.86576 and 0.36336118\n",
            "Val acc and loss are 0.8535 and 0.3973496\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.8697 and 0.35273108\n",
            "Val acc and loss are 0.8592 and 0.39243716\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.8739 and 0.34574372\n",
            "Val acc and loss are 0.8575 and 0.38153368\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.8721 and 0.34373125\n",
            "Val acc and loss are 0.862 and 0.38288665\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.87742 and 0.3368991\n",
            "Val acc and loss are 0.861 and 0.37399542\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.87298 and 0.34162968\n",
            "Val acc and loss are 0.8625 and 0.38092968\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.87802 and 0.33415493\n",
            "Val acc and loss are 0.8627 and 0.3722231\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.87198 and 0.342549\n",
            "Val acc and loss are 0.8611 and 0.38262603\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.875 and 0.34044853\n",
            "Val acc and loss are 0.8582 and 0.37817153\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.86896 and 0.35275656\n",
            "Val acc and loss are 0.857 and 0.39389586\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.8687 and 0.35196945\n",
            "Val acc and loss are 0.8528 and 0.38998148\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.87056 and 0.35026774\n",
            "Val acc and loss are 0.8587 and 0.39158216\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.87078 and 0.34553632\n",
            "Val acc and loss are 0.8557 and 0.38396245\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.87766 and 0.3350082\n",
            "Val acc and loss are 0.8674 and 0.37582648\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.872 and 0.34093094\n",
            "Val acc and loss are 0.8578 and 0.380088\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.88022 and 0.33075425\n",
            "Val acc and loss are 0.867 and 0.37184817\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.87302 and 0.3394215\n",
            "Val acc and loss are 0.8593 and 0.3795313\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.8813 and 0.3275317\n",
            "Val acc and loss are 0.8693 and 0.36799443\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.87504 and 0.33416712\n",
            "Val acc and loss are 0.8614 and 0.3758599\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.8809 and 0.32657534\n",
            "Val acc and loss are 0.8669 and 0.36759052\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.87374 and 0.33703977\n",
            "Val acc and loss are 0.8604 and 0.38042897\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.87776 and 0.33334976\n",
            "Val acc and loss are 0.8637 and 0.3742032\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.86966 and 0.34974933\n",
            "Val acc and loss are 0.8554 and 0.39443427\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.87358 and 0.34014747\n",
            "Val acc and loss are 0.8597 and 0.38089576\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.87512 and 0.34078625\n",
            "Val acc and loss are 0.8576 and 0.38615412\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.87218 and 0.33841133\n",
            "Val acc and loss are 0.8601 and 0.3793393\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.88112 and 0.32616022\n",
            "Val acc and loss are 0.8637 and 0.37167308\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.87328 and 0.33482113\n",
            "Val acc and loss are 0.8601 and 0.37756935\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.88358 and 0.32145458\n",
            "Val acc and loss are 0.8659 and 0.36588377\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.87652 and 0.32810485\n",
            "Val acc and loss are 0.8654 and 0.37252554\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.88426 and 0.3184226\n",
            "Val acc and loss are 0.8687 and 0.36238322\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.87866 and 0.32328552\n",
            "Val acc and loss are 0.8675 and 0.36938208\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.88372 and 0.31876972\n",
            "Val acc and loss are 0.8674 and 0.36312008\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.8785 and 0.3242742\n",
            "Val acc and loss are 0.8669 and 0.3710348\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.87604 and 0.33437547\n",
            "Val acc and loss are 0.8597 and 0.37760687\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.87328 and 0.33642662\n",
            "Val acc and loss are 0.8612 and 0.38349363\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.87452 and 0.33857992\n",
            "Val acc and loss are 0.8581 and 0.38198268\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.87714 and 0.33001593\n",
            "Val acc and loss are 0.8629 and 0.37723482\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.8821 and 0.32196805\n",
            "Val acc and loss are 0.863 and 0.36719513\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.8807 and 0.32189423\n",
            "Val acc and loss are 0.8677 and 0.3688996\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.88482 and 0.31367627\n",
            "Val acc and loss are 0.8671 and 0.36047402\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.88344 and 0.31638062\n",
            "Val acc and loss are 0.8702 and 0.36349866\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.88444 and 0.31280962\n",
            "Val acc and loss are 0.8666 and 0.36067206\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.88256 and 0.31912765\n",
            "Val acc and loss are 0.8705 and 0.36710557\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.87756 and 0.32411578\n",
            "Val acc and loss are 0.8615 and 0.37161022\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.88054 and 0.324127\n",
            "Val acc and loss are 0.8669 and 0.37328216\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.87798 and 0.3226866\n",
            "Val acc and loss are 0.8607 and 0.36975023\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.88348 and 0.3178087\n",
            "Val acc and loss are 0.8676 and 0.3673655\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.8845 and 0.3123789\n",
            "Val acc and loss are 0.8645 and 0.3596849\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.88224 and 0.31608006\n",
            "Val acc and loss are 0.8666 and 0.36711177\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.88468 and 0.3128153\n",
            "Val acc and loss are 0.8652 and 0.36112106\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.8794 and 0.32037145\n",
            "Val acc and loss are 0.8656 and 0.37180758\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.88122 and 0.32080257\n",
            "Val acc and loss are 0.8626 and 0.37038368\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.87772 and 0.32406083\n",
            "Val acc and loss are 0.8641 and 0.37542263\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.87916 and 0.3276699\n",
            "Val acc and loss are 0.8615 and 0.37639943\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.8796 and 0.32245523\n",
            "Val acc and loss are 0.8626 and 0.37555102\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.8833 and 0.31695655\n",
            "Val acc and loss are 0.8656 and 0.36549237\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.88254 and 0.31616458\n",
            "Val acc and loss are 0.864 and 0.36924633\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.8827 and 0.3147445\n",
            "Val acc and loss are 0.8682 and 0.3639084\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.88388 and 0.31673652\n",
            "Val acc and loss are 0.8636 and 0.37189007\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.87954 and 0.3237992\n",
            "Val acc and loss are 0.8643 and 0.373528\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.88594 and 0.31262976\n",
            "Val acc and loss are 0.8662 and 0.36781186\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.88524 and 0.30643415\n",
            "Val acc and loss are 0.8726 and 0.35562408\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.88956 and 0.30283442\n",
            "Val acc and loss are 0.87 and 0.35629222\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.88708 and 0.30042562\n",
            "Val acc and loss are 0.8738 and 0.3519309\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.88966 and 0.30325046\n",
            "Val acc and loss are 0.8689 and 0.35674\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.88504 and 0.30395994\n",
            "Val acc and loss are 0.8706 and 0.3566624\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.88784 and 0.30630067\n",
            "Val acc and loss are 0.8695 and 0.3604211\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.8818 and 0.30796582\n",
            "Val acc and loss are 0.8675 and 0.36136153\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.88834 and 0.3054935\n",
            "Val acc and loss are 0.8697 and 0.35960606\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.88494 and 0.3040567\n",
            "Val acc and loss are 0.87 and 0.35790762\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.8911 and 0.29892617\n",
            "Val acc and loss are 0.872 and 0.35385764\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.88522 and 0.30503717\n",
            "Val acc and loss are 0.8671 and 0.35815856\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.8865 and 0.30883175\n",
            "Val acc and loss are 0.8702 and 0.36518267\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.87998 and 0.32002297\n",
            "Val acc and loss are 0.8634 and 0.3736449\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.88442 and 0.31152934\n",
            "Val acc and loss are 0.8659 and 0.36736703\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.88856 and 0.30259115\n",
            "Val acc and loss are 0.8715 and 0.35782146\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.88542 and 0.30550796\n",
            "Val acc and loss are 0.8674 and 0.36277306\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.89022 and 0.30402562\n",
            "Val acc and loss are 0.8714 and 0.36001104\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.88314 and 0.3102284\n",
            "Val acc and loss are 0.8637 and 0.36802006\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.89046 and 0.30151916\n",
            "Val acc and loss are 0.8724 and 0.35678\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.8863 and 0.30293167\n",
            "Val acc and loss are 0.8658 and 0.36095265\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.89072 and 0.29821134\n",
            "Val acc and loss are 0.872 and 0.35319906\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.88814 and 0.3008734\n",
            "Val acc and loss are 0.8667 and 0.35953188\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.88746 and 0.30313078\n",
            "Val acc and loss are 0.87 and 0.35865763\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.88926 and 0.30019015\n",
            "Val acc and loss are 0.8672 and 0.35920328\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.88564 and 0.30366752\n",
            "Val acc and loss are 0.8676 and 0.35989073\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.89236 and 0.29553348\n",
            "Val acc and loss are 0.871 and 0.35554722\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.8872 and 0.29892856\n",
            "Val acc and loss are 0.8702 and 0.35581154\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.8941 and 0.29277754\n",
            "Val acc and loss are 0.8726 and 0.3530806\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.88646 and 0.29818007\n",
            "Val acc and loss are 0.8702 and 0.35621825\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.89276 and 0.29361823\n",
            "Val acc and loss are 0.8724 and 0.35454872\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.88734 and 0.2958617\n",
            "Val acc and loss are 0.8694 and 0.35576597\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.89248 and 0.29414824\n",
            "Val acc and loss are 0.8715 and 0.35506278\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.88892 and 0.2932269\n",
            "Val acc and loss are 0.8697 and 0.35317785\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.8926 and 0.2938753\n",
            "Val acc and loss are 0.8716 and 0.35510087\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.89216 and 0.28972858\n",
            "Val acc and loss are 0.8716 and 0.35028824\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.89052 and 0.29673582\n",
            "Val acc and loss are 0.87 and 0.35789856\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.89454 and 0.28693712\n",
            "Val acc and loss are 0.8725 and 0.3487784\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.88916 and 0.29635283\n",
            "Val acc and loss are 0.8711 and 0.35659456\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.89692 and 0.2853038\n",
            "Val acc and loss are 0.8734 and 0.348166\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.88902 and 0.29656357\n",
            "Val acc and loss are 0.8698 and 0.35669437\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.89378 and 0.2949905\n",
            "Val acc and loss are 0.8691 and 0.3609507\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.8845 and 0.31875467\n",
            "Val acc and loss are 0.8674 and 0.3779713\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.88628 and 0.30826572\n",
            "Val acc and loss are 0.8634 and 0.37679714\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.89396 and 0.29154727\n",
            "Val acc and loss are 0.8734 and 0.35228097\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.88686 and 0.29957402\n",
            "Val acc and loss are 0.8685 and 0.36541057\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.89236 and 0.2933958\n",
            "Val acc and loss are 0.8686 and 0.35414565\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.88808 and 0.2986899\n",
            "Val acc and loss are 0.8695 and 0.363941\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.89024 and 0.29604492\n",
            "Val acc and loss are 0.8687 and 0.3565938\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.893 and 0.28748757\n",
            "Val acc and loss are 0.8743 and 0.35121268\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.89554 and 0.28428456\n",
            "Val acc and loss are 0.8724 and 0.34665376\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.89486 and 0.28324917\n",
            "Val acc and loss are 0.8754 and 0.34738335\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.89718 and 0.2815369\n",
            "Val acc and loss are 0.8722 and 0.34493896\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.89468 and 0.28283587\n",
            "Val acc and loss are 0.8746 and 0.34756193\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.89768 and 0.2792692\n",
            "Val acc and loss are 0.8725 and 0.34389398\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.89426 and 0.28267068\n",
            "Val acc and loss are 0.8737 and 0.34801674\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.898 and 0.27966458\n",
            "Val acc and loss are 0.8726 and 0.344564\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.89468 and 0.28233778\n",
            "Val acc and loss are 0.8734 and 0.34762314\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.89838 and 0.27728176\n",
            "Val acc and loss are 0.8743 and 0.34326783\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.89448 and 0.2811805\n",
            "Val acc and loss are 0.873 and 0.34674484\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.89622 and 0.283419\n",
            "Val acc and loss are 0.8733 and 0.35140714\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.88606 and 0.29795644\n",
            "Val acc and loss are 0.8665 and 0.36256307\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.88772 and 0.3064934\n",
            "Val acc and loss are 0.8645 and 0.3763429\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.88708 and 0.29714683\n",
            "Val acc and loss are 0.8668 and 0.36024886\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.89732 and 0.28018025\n",
            "Val acc and loss are 0.8754 and 0.3481281\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.89734 and 0.27762485\n",
            "Val acc and loss are 0.8722 and 0.34336865\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.8992 and 0.2738192\n",
            "Val acc and loss are 0.8785 and 0.34092525\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.89796 and 0.27661967\n",
            "Val acc and loss are 0.8751 and 0.34323114\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.89748 and 0.27703905\n",
            "Val acc and loss are 0.8749 and 0.34443027\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.896 and 0.28336856\n",
            "Val acc and loss are 0.8739 and 0.35080978\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.88978 and 0.29322875\n",
            "Val acc and loss are 0.8683 and 0.36080712\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.89246 and 0.29365638\n",
            "Val acc and loss are 0.8679 and 0.36032045\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.89026 and 0.2901751\n",
            "Val acc and loss are 0.8686 and 0.36003575\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.89932 and 0.27681094\n",
            "Val acc and loss are 0.8748 and 0.34215024\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.8976 and 0.2763702\n",
            "Val acc and loss are 0.8733 and 0.34677327\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.90028 and 0.27177954\n",
            "Val acc and loss are 0.8774 and 0.33856812\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.89904 and 0.27451447\n",
            "Val acc and loss are 0.8737 and 0.3453875\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.89802 and 0.2751098\n",
            "Val acc and loss are 0.8767 and 0.34271646\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.89682 and 0.2789125\n",
            "Val acc and loss are 0.8709 and 0.34980363\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.89396 and 0.282835\n",
            "Val acc and loss are 0.8706 and 0.35145932\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.89722 and 0.27833563\n",
            "Val acc and loss are 0.8721 and 0.34912804\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.89554 and 0.27887043\n",
            "Val acc and loss are 0.8729 and 0.3488085\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.90102 and 0.27064377\n",
            "Val acc and loss are 0.8753 and 0.34104878\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.89866 and 0.27224228\n",
            "Val acc and loss are 0.8746 and 0.34354708\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.90042 and 0.27166715\n",
            "Val acc and loss are 0.8736 and 0.34190524\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.89584 and 0.28034073\n",
            "Val acc and loss are 0.8717 and 0.35372797\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.89234 and 0.28845534\n",
            "Val acc and loss are 0.867 and 0.35682255\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.89062 and 0.29006776\n",
            "Val acc and loss are 0.8701 and 0.36481065\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.89584 and 0.28396782\n",
            "Val acc and loss are 0.87 and 0.35272074\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.89608 and 0.27854502\n",
            "Val acc and loss are 0.8719 and 0.35295665\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.90254 and 0.26840827\n",
            "Val acc and loss are 0.8765 and 0.3388461\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.89768 and 0.27298608\n",
            "Val acc and loss are 0.8748 and 0.34723744\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.90346 and 0.26861823\n",
            "Val acc and loss are 0.8775 and 0.34001243\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.89436 and 0.277326\n",
            "Val acc and loss are 0.8707 and 0.35032213\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.902 and 0.27142945\n",
            "Val acc and loss are 0.8766 and 0.34319642\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.89494 and 0.27716696\n",
            "Val acc and loss are 0.8694 and 0.35056517\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.90204 and 0.26928365\n",
            "Val acc and loss are 0.8793 and 0.34116697\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.89952 and 0.26983944\n",
            "Val acc and loss are 0.8739 and 0.34264377\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.9019 and 0.26641357\n",
            "Val acc and loss are 0.8782 and 0.3391702\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.90044 and 0.26871312\n",
            "Val acc and loss are 0.8745 and 0.34359375\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.90022 and 0.269402\n",
            "Val acc and loss are 0.8753 and 0.3428421\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.89958 and 0.26961854\n",
            "Val acc and loss are 0.8748 and 0.34484512\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.89882 and 0.27207476\n",
            "Val acc and loss are 0.8709 and 0.34583086\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.90142 and 0.26603413\n",
            "Val acc and loss are 0.8774 and 0.3417679\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.90192 and 0.26475173\n",
            "Val acc and loss are 0.8741 and 0.34018773\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.90276 and 0.26477742\n",
            "Val acc and loss are 0.8784 and 0.3420813\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.89996 and 0.26756665\n",
            "Val acc and loss are 0.8754 and 0.3426399\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.89824 and 0.27555385\n",
            "Val acc and loss are 0.8731 and 0.35401168\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.89538 and 0.27633026\n",
            "Val acc and loss are 0.8722 and 0.3508639\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.89952 and 0.27389908\n",
            "Val acc and loss are 0.8744 and 0.3530729\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.8983 and 0.2732182\n",
            "Val acc and loss are 0.8728 and 0.34752628\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.90236 and 0.26560196\n",
            "Val acc and loss are 0.8764 and 0.34403875\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.90152 and 0.2669961\n",
            "Val acc and loss are 0.8741 and 0.34240264\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.90404 and 0.26205865\n",
            "Val acc and loss are 0.8755 and 0.3412873\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.90196 and 0.26822922\n",
            "Val acc and loss are 0.8765 and 0.3431004\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.89562 and 0.27595872\n",
            "Val acc and loss are 0.8688 and 0.35602146\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.90068 and 0.27151698\n",
            "Val acc and loss are 0.875 and 0.3462917\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.89734 and 0.2708877\n",
            "Val acc and loss are 0.8725 and 0.34890696\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.9064 and 0.2591243\n",
            "Val acc and loss are 0.8809 and 0.33300343\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.90376 and 0.2593206\n",
            "Val acc and loss are 0.8775 and 0.33877602\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.90562 and 0.2586962\n",
            "Val acc and loss are 0.8792 and 0.3345372\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.90308 and 0.26158077\n",
            "Val acc and loss are 0.8788 and 0.34169632\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.90152 and 0.26733458\n",
            "Val acc and loss are 0.8727 and 0.34514755\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.89686 and 0.27504823\n",
            "Val acc and loss are 0.8721 and 0.3547809\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.89674 and 0.27381405\n",
            "Val acc and loss are 0.8688 and 0.3515523\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.89656 and 0.27328578\n",
            "Val acc and loss are 0.8715 and 0.35215756\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.90476 and 0.25923583\n",
            "Val acc and loss are 0.8771 and 0.3360777\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.90546 and 0.25621763\n",
            "Val acc and loss are 0.8796 and 0.33596274\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.90908 and 0.25151515\n",
            "Val acc and loss are 0.8802 and 0.3303045\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.9067 and 0.2530184\n",
            "Val acc and loss are 0.8809 and 0.33405277\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.90912 and 0.25093368\n",
            "Val acc and loss are 0.8794 and 0.33085948\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.90436 and 0.2570967\n",
            "Val acc and loss are 0.8766 and 0.33972728\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.90766 and 0.25415462\n",
            "Val acc and loss are 0.8787 and 0.3346185\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.89934 and 0.26710835\n",
            "Val acc and loss are 0.8725 and 0.35184467\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.90226 and 0.2664187\n",
            "Val acc and loss are 0.8753 and 0.34663546\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.89412 and 0.27753323\n",
            "Val acc and loss are 0.8684 and 0.36170655\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.90352 and 0.2642529\n",
            "Val acc and loss are 0.8755 and 0.34174496\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.89998 and 0.26537856\n",
            "Val acc and loss are 0.8726 and 0.34802184\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.90408 and 0.25844163\n",
            "Val acc and loss are 0.8768 and 0.33573273\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.90552 and 0.25680578\n",
            "Val acc and loss are 0.8777 and 0.34046245\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.9022 and 0.2617842\n",
            "Val acc and loss are 0.8726 and 0.34035516\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.90338 and 0.26165032\n",
            "Val acc and loss are 0.8792 and 0.3460197\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.89924 and 0.26710862\n",
            "Val acc and loss are 0.871 and 0.34604868\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.90596 and 0.25679648\n",
            "Val acc and loss are 0.8786 and 0.34107903\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.90386 and 0.2561299\n",
            "Val acc and loss are 0.8766 and 0.33656058\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.90894 and 0.24976304\n",
            "Val acc and loss are 0.8813 and 0.3328841\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.90638 and 0.2514495\n",
            "Val acc and loss are 0.8782 and 0.33304405\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.90936 and 0.24910416\n",
            "Val acc and loss are 0.8829 and 0.33136934\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.90644 and 0.25327513\n",
            "Val acc and loss are 0.8781 and 0.33588317\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.9089 and 0.25066558\n",
            "Val acc and loss are 0.8814 and 0.33268082\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.90566 and 0.25494933\n",
            "Val acc and loss are 0.8773 and 0.33917174\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.90708 and 0.25167802\n",
            "Val acc and loss are 0.8795 and 0.3337769\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.90558 and 0.25409922\n",
            "Val acc and loss are 0.8775 and 0.33952174\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.90764 and 0.25215378\n",
            "Val acc and loss are 0.8775 and 0.33501753\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.905 and 0.25348574\n",
            "Val acc and loss are 0.8776 and 0.33977306\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.90486 and 0.2588025\n",
            "Val acc and loss are 0.8759 and 0.34119618\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.89804 and 0.2685676\n",
            "Val acc and loss are 0.8729 and 0.35585582\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.89532 and 0.2767497\n",
            "Val acc and loss are 0.8687 and 0.3593031\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.89498 and 0.27718174\n",
            "Val acc and loss are 0.8676 and 0.3621622\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.90396 and 0.2602197\n",
            "Val acc and loss are 0.8764 and 0.3421722\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.90618 and 0.254544\n",
            "Val acc and loss are 0.8764 and 0.34002373\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.91088 and 0.24520187\n",
            "Val acc and loss are 0.879 and 0.329201\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.91184 and 0.24289177\n",
            "Val acc and loss are 0.881 and 0.32853246\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.9127 and 0.24050736\n",
            "Val acc and loss are 0.8802 and 0.32613516\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.91252 and 0.24036185\n",
            "Val acc and loss are 0.8822 and 0.32762814\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.9125 and 0.23888433\n",
            "Val acc and loss are 0.8801 and 0.3241605\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.91266 and 0.23965698\n",
            "Val acc and loss are 0.8814 and 0.32776016\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.91172 and 0.24037579\n",
            "Val acc and loss are 0.8789 and 0.32646778\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.91044 and 0.24450654\n",
            "Val acc and loss are 0.8811 and 0.3348102\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.90604 and 0.25100332\n",
            "Val acc and loss are 0.8739 and 0.33687922\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.90124 and 0.26380235\n",
            "Val acc and loss are 0.8757 and 0.35585624\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.89672 and 0.26897958\n",
            "Val acc and loss are 0.8665 and 0.35303184\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.90586 and 0.2552113\n",
            "Val acc and loss are 0.8761 and 0.34600127\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.90722 and 0.24921562\n",
            "Val acc and loss are 0.8767 and 0.33448866\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.91032 and 0.24366562\n",
            "Val acc and loss are 0.878 and 0.33318502\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.90816 and 0.24806285\n",
            "Val acc and loss are 0.879 and 0.33427793\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.9029 and 0.255912\n",
            "Val acc and loss are 0.8745 and 0.34545472\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.90364 and 0.2609437\n",
            "Val acc and loss are 0.8754 and 0.34644097\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.90126 and 0.2603643\n",
            "Val acc and loss are 0.8729 and 0.34925342\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.91152 and 0.246136\n",
            "Val acc and loss are 0.8805 and 0.33179054\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.90974 and 0.24294025\n",
            "Val acc and loss are 0.8803 and 0.33159187\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.9128 and 0.24054503\n",
            "Val acc and loss are 0.8825 and 0.32751676\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.90718 and 0.24701\n",
            "Val acc and loss are 0.8764 and 0.33754987\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.9064 and 0.251512\n",
            "Val acc and loss are 0.8771 and 0.33918688\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.90174 and 0.2602026\n",
            "Val acc and loss are 0.8736 and 0.35066825\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.90656 and 0.2519264\n",
            "Val acc and loss are 0.8769 and 0.3386626\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.9077 and 0.24673578\n",
            "Val acc and loss are 0.8788 and 0.3370709\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.9106 and 0.24312282\n",
            "Val acc and loss are 0.8782 and 0.3316795\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.9109 and 0.24032012\n",
            "Val acc and loss are 0.8814 and 0.3319417\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.91238 and 0.23788261\n",
            "Val acc and loss are 0.8805 and 0.32753876\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.9122 and 0.23693308\n",
            "Val acc and loss are 0.8824 and 0.3286154\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.91266 and 0.23876902\n",
            "Val acc and loss are 0.8818 and 0.3312235\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.90878 and 0.24366808\n",
            "Val acc and loss are 0.8781 and 0.3352735\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.90742 and 0.25036564\n",
            "Val acc and loss are 0.8788 and 0.34357387\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.90238 and 0.255981\n",
            "Val acc and loss are 0.8732 and 0.3461058\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.91138 and 0.24513707\n",
            "Val acc and loss are 0.8822 and 0.33799112\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.90966 and 0.24171866\n",
            "Val acc and loss are 0.8786 and 0.33218974\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.91224 and 0.23921067\n",
            "Val acc and loss are 0.8816 and 0.33396846\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.91018 and 0.24230695\n",
            "Val acc and loss are 0.8784 and 0.33402726\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.90896 and 0.24492641\n",
            "Val acc and loss are 0.8768 and 0.34079155\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.90698 and 0.24971633\n",
            "Val acc and loss are 0.8771 and 0.3393571\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.9039 and 0.25591788\n",
            "Val acc and loss are 0.8728 and 0.35322547\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.90948 and 0.24620777\n",
            "Val acc and loss are 0.8798 and 0.3350939\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.90964 and 0.24224082\n",
            "Val acc and loss are 0.8772 and 0.33908546\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.91576 and 0.23272075\n",
            "Val acc and loss are 0.8835 and 0.3235465\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.91392 and 0.23308785\n",
            "Val acc and loss are 0.8802 and 0.32901534\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.91672 and 0.23068498\n",
            "Val acc and loss are 0.8841 and 0.32385147\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.9111 and 0.23849589\n",
            "Val acc and loss are 0.8784 and 0.33642003\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.9104 and 0.24305928\n",
            "Val acc and loss are 0.8784 and 0.33782446\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.90196 and 0.25770435\n",
            "Val acc and loss are 0.8723 and 0.35604528\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.9053 and 0.2530549\n",
            "Val acc and loss are 0.8736 and 0.34561753\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.90932 and 0.24370466\n",
            "Val acc and loss are 0.8782 and 0.3378791\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.91188 and 0.2394054\n",
            "Val acc and loss are 0.8769 and 0.33364096\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.91498 and 0.23169075\n",
            "Val acc and loss are 0.8835 and 0.3260876\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.91522 and 0.23150052\n",
            "Val acc and loss are 0.8809 and 0.32807875\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.91656 and 0.22772524\n",
            "Val acc and loss are 0.8843 and 0.32382667\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.91518 and 0.230368\n",
            "Val acc and loss are 0.8812 and 0.32811013\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.91552 and 0.23092678\n",
            "Val acc and loss are 0.8833 and 0.32777354\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.91162 and 0.23806512\n",
            "Val acc and loss are 0.8803 and 0.3375911\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.90972 and 0.24319293\n",
            "Val acc and loss are 0.8755 and 0.33947086\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.9073 and 0.25040317\n",
            "Val acc and loss are 0.8768 and 0.3521946\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.90692 and 0.24731466\n",
            "Val acc and loss are 0.8732 and 0.34193605\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.91228 and 0.23816608\n",
            "Val acc and loss are 0.8801 and 0.33950254\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.91216 and 0.23658384\n",
            "Val acc and loss are 0.8775 and 0.33137584\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.91602 and 0.23039187\n",
            "Val acc and loss are 0.8827 and 0.32998708\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.91484 and 0.2313758\n",
            "Val acc and loss are 0.8791 and 0.32789072\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.91708 and 0.22705598\n",
            "Val acc and loss are 0.885 and 0.32607278\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.91078 and 0.2369039\n",
            "Val acc and loss are 0.8794 and 0.3347879\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.91202 and 0.24116492\n",
            "Val acc and loss are 0.8793 and 0.3402837\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.90146 and 0.25810364\n",
            "Val acc and loss are 0.8729 and 0.35429087\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.91404 and 0.23725711\n",
            "Val acc and loss are 0.8816 and 0.33287477\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.91338 and 0.23306172\n",
            "Val acc and loss are 0.8799 and 0.33473447\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.91662 and 0.22821699\n",
            "Val acc and loss are 0.8841 and 0.3252375\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.91628 and 0.22738834\n",
            "Val acc and loss are 0.8805 and 0.32835692\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.918 and 0.22619365\n",
            "Val acc and loss are 0.884 and 0.32478812\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.9117 and 0.23492469\n",
            "Val acc and loss are 0.8774 and 0.3383327\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.90794 and 0.24596657\n",
            "Val acc and loss are 0.8756 and 0.3437707\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.90116 and 0.2592563\n",
            "Val acc and loss are 0.8702 and 0.36239803\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.90912 and 0.24544387\n",
            "Val acc and loss are 0.8768 and 0.34275648\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.9137 and 0.2319244\n",
            "Val acc and loss are 0.878 and 0.33284765\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.91744 and 0.22739492\n",
            "Val acc and loss are 0.8827 and 0.32758507\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.91788 and 0.22375442\n",
            "Val acc and loss are 0.882 and 0.32508022\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.91786 and 0.22403355\n",
            "Val acc and loss are 0.8832 and 0.32554471\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.91778 and 0.22497332\n",
            "Val acc and loss are 0.8821 and 0.32676739\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.91496 and 0.22889498\n",
            "Val acc and loss are 0.8815 and 0.3336956\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.91402 and 0.23224129\n",
            "Val acc and loss are 0.8811 and 0.3335732\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.91194 and 0.23715514\n",
            "Val acc and loss are 0.8789 and 0.34394762\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.91292 and 0.23485318\n",
            "Val acc and loss are 0.8778 and 0.33536974\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.91422 and 0.232478\n",
            "Val acc and loss are 0.8811 and 0.33911112\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.91554 and 0.22857925\n",
            "Val acc and loss are 0.8814 and 0.32744482\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.91616 and 0.2267927\n",
            "Val acc and loss are 0.881 and 0.332308\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.91638 and 0.22760363\n",
            "Val acc and loss are 0.8831 and 0.32741615\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.91098 and 0.23520623\n",
            "Val acc and loss are 0.8783 and 0.33925372\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.91382 and 0.23580395\n",
            "Val acc and loss are 0.8802 and 0.33738494\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.90744 and 0.24127215\n",
            "Val acc and loss are 0.8781 and 0.34363994\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.91726 and 0.22863425\n",
            "Val acc and loss are 0.8835 and 0.3304335\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.91454 and 0.22694497\n",
            "Val acc and loss are 0.8797 and 0.32982758\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.91756 and 0.22505681\n",
            "Val acc and loss are 0.8853 and 0.32880268\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.91392 and 0.22947882\n",
            "Val acc and loss are 0.8779 and 0.33471197\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.91428 and 0.23207515\n",
            "Val acc and loss are 0.8808 and 0.3356212\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.91072 and 0.23668872\n",
            "Val acc and loss are 0.877 and 0.3439203\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.91444 and 0.23152427\n",
            "Val acc and loss are 0.8789 and 0.333053\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.91394 and 0.23180786\n",
            "Val acc and loss are 0.8793 and 0.34107295\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.91646 and 0.22692649\n",
            "Val acc and loss are 0.879 and 0.32904258\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.91724 and 0.22566035\n",
            "Val acc and loss are 0.8823 and 0.33585513\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.91758 and 0.22403222\n",
            "Val acc and loss are 0.8804 and 0.3275011\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.91682 and 0.22565112\n",
            "Val acc and loss are 0.8807 and 0.33597252\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.91642 and 0.22451547\n",
            "Val acc and loss are 0.8788 and 0.32926017\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.91774 and 0.22331701\n",
            "Val acc and loss are 0.8827 and 0.33388966\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.91608 and 0.2254632\n",
            "Val acc and loss are 0.8782 and 0.33149713\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.91872 and 0.22116584\n",
            "Val acc and loss are 0.8845 and 0.32904825\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.91512 and 0.22795661\n",
            "Val acc and loss are 0.8783 and 0.333272\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.91886 and 0.22119977\n",
            "Val acc and loss are 0.8831 and 0.32938027\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.91612 and 0.22434512\n",
            "Val acc and loss are 0.8813 and 0.33086303\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.91594 and 0.2258082\n",
            "Val acc and loss are 0.8801 and 0.3342059\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.9156 and 0.22837508\n",
            "Val acc and loss are 0.881 and 0.33481294\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.90804 and 0.24180266\n",
            "Val acc and loss are 0.8734 and 0.34944752\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.91714 and 0.22884332\n",
            "Val acc and loss are 0.8827 and 0.33399794\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.91386 and 0.22872247\n",
            "Val acc and loss are 0.8778 and 0.33647785\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.92036 and 0.2210623\n",
            "Val acc and loss are 0.8839 and 0.32855138\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.91748 and 0.22353117\n",
            "Val acc and loss are 0.8796 and 0.33568433\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.91992 and 0.2207233\n",
            "Val acc and loss are 0.882 and 0.32819352\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.9152 and 0.2264923\n",
            "Val acc and loss are 0.8809 and 0.33997458\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.91614 and 0.22654797\n",
            "Val acc and loss are 0.879 and 0.33249012\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.91596 and 0.22524098\n",
            "Val acc and loss are 0.8815 and 0.3375509\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.91696 and 0.22521383\n",
            "Val acc and loss are 0.8804 and 0.3323361\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.91788 and 0.22221032\n",
            "Val acc and loss are 0.8804 and 0.3332815\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.91786 and 0.22398828\n",
            "Val acc and loss are 0.882 and 0.333943\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.91532 and 0.22595616\n",
            "Val acc and loss are 0.8787 and 0.33668983\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.9171 and 0.22601739\n",
            "Val acc and loss are 0.8829 and 0.33660328\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.91224 and 0.23026331\n",
            "Val acc and loss are 0.8775 and 0.33951923\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.91846 and 0.22397858\n",
            "Val acc and loss are 0.8832 and 0.3344465\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.91772 and 0.22133033\n",
            "Val acc and loss are 0.8809 and 0.33155423\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.92158 and 0.21563643\n",
            "Val acc and loss are 0.8851 and 0.32762685\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.92034 and 0.21625786\n",
            "Val acc and loss are 0.884 and 0.3274002\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.92238 and 0.21362123\n",
            "Val acc and loss are 0.8867 and 0.32583576\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.91764 and 0.21984048\n",
            "Val acc and loss are 0.8818 and 0.33163115\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.92052 and 0.21770276\n",
            "Val acc and loss are 0.883 and 0.328896\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.9124 and 0.2301392\n",
            "Val acc and loss are 0.8774 and 0.34235692\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.91674 and 0.22598031\n",
            "Val acc and loss are 0.8791 and 0.3347653\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.91166 and 0.23250578\n",
            "Val acc and loss are 0.8782 and 0.34860525\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.91908 and 0.22262289\n",
            "Val acc and loss are 0.8817 and 0.33113706\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.91518 and 0.22806971\n",
            "Val acc and loss are 0.8772 and 0.34868369\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.92154 and 0.2176354\n",
            "Val acc and loss are 0.8791 and 0.32635015\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.9193 and 0.21811305\n",
            "Val acc and loss are 0.8822 and 0.33581027\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.92112 and 0.21836615\n",
            "Val acc and loss are 0.8806 and 0.3291147\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.92012 and 0.21571262\n",
            "Val acc and loss are 0.8843 and 0.33068818\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.92106 and 0.21775363\n",
            "Val acc and loss are 0.8825 and 0.33014387\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.9162 and 0.2232028\n",
            "Val acc and loss are 0.878 and 0.3383631\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.91806 and 0.22438031\n",
            "Val acc and loss are 0.8814 and 0.33810294\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.9105 and 0.23371463\n",
            "Val acc and loss are 0.8745 and 0.3467089\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.92048 and 0.22010222\n",
            "Val acc and loss are 0.8842 and 0.3334515\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.92032 and 0.21477863\n",
            "Val acc and loss are 0.8813 and 0.32978374\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.9248 and 0.20965073\n",
            "Val acc and loss are 0.8867 and 0.32422602\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.92302 and 0.20985886\n",
            "Val acc and loss are 0.8847 and 0.32603377\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.92464 and 0.20952657\n",
            "Val acc and loss are 0.8869 and 0.3249523\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.92162 and 0.21195121\n",
            "Val acc and loss are 0.883 and 0.32870364\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.92426 and 0.20997314\n",
            "Val acc and loss are 0.8853 and 0.32703274\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.92002 and 0.21372901\n",
            "Val acc and loss are 0.8813 and 0.32938033\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.92372 and 0.2111024\n",
            "Val acc and loss are 0.886 and 0.3303095\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.91776 and 0.21839225\n",
            "Val acc and loss are 0.8775 and 0.33326262\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.9168 and 0.22301981\n",
            "Val acc and loss are 0.8817 and 0.34312615\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.91356 and 0.22985312\n",
            "Val acc and loss are 0.8739 and 0.34281796\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.90988 and 0.23690619\n",
            "Val acc and loss are 0.8748 and 0.358151\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.91858 and 0.2247696\n",
            "Val acc and loss are 0.8808 and 0.33541793\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.91716 and 0.22309992\n",
            "Val acc and loss are 0.8771 and 0.34474742\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.92132 and 0.21537308\n",
            "Val acc and loss are 0.8824 and 0.32843712\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.9208 and 0.2134784\n",
            "Val acc and loss are 0.8813 and 0.33231434\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.92244 and 0.21112593\n",
            "Val acc and loss are 0.8832 and 0.3288646\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.92258 and 0.2096708\n",
            "Val acc and loss are 0.8837 and 0.32751477\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.9221 and 0.21175031\n",
            "Val acc and loss are 0.8823 and 0.33023193\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.92158 and 0.2115909\n",
            "Val acc and loss are 0.8835 and 0.33134818\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.91952 and 0.21636617\n",
            "Val acc and loss are 0.8785 and 0.3340108\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.92282 and 0.2135495\n",
            "Val acc and loss are 0.8837 and 0.3326016\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.91614 and 0.22077596\n",
            "Val acc and loss are 0.8786 and 0.33802867\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.92334 and 0.2124675\n",
            "Val acc and loss are 0.8845 and 0.33235446\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.92068 and 0.2123248\n",
            "Val acc and loss are 0.8798 and 0.3314783\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.92572 and 0.20614141\n",
            "Val acc and loss are 0.8865 and 0.32621846\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.92262 and 0.20831071\n",
            "Val acc and loss are 0.8816 and 0.3272872\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.92402 and 0.20843719\n",
            "Val acc and loss are 0.8854 and 0.329777\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.92104 and 0.21115121\n",
            "Val acc and loss are 0.8815 and 0.33201897\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.92106 and 0.21428773\n",
            "Val acc and loss are 0.8841 and 0.3380872\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.91942 and 0.21523754\n",
            "Val acc and loss are 0.8798 and 0.33762103\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.92158 and 0.21365286\n",
            "Val acc and loss are 0.8835 and 0.33398375\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.91798 and 0.21786512\n",
            "Val acc and loss are 0.8777 and 0.3408769\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.92202 and 0.21446384\n",
            "Val acc and loss are 0.885 and 0.33337453\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.91848 and 0.21762243\n",
            "Val acc and loss are 0.8761 and 0.34237105\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.92214 and 0.2123633\n",
            "Val acc and loss are 0.883 and 0.33229765\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.92024 and 0.2159897\n",
            "Val acc and loss are 0.8771 and 0.34575948\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.92384 and 0.21076915\n",
            "Val acc and loss are 0.8802 and 0.33143982\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.92034 and 0.21496469\n",
            "Val acc and loss are 0.8793 and 0.3436276\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.9228 and 0.21087016\n",
            "Val acc and loss are 0.8786 and 0.33047333\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.91972 and 0.2141931\n",
            "Val acc and loss are 0.8812 and 0.34029847\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.9229 and 0.21123017\n",
            "Val acc and loss are 0.8807 and 0.33140263\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.91762 and 0.21755181\n",
            "Val acc and loss are 0.8787 and 0.33985344\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.92376 and 0.20914562\n",
            "Val acc and loss are 0.8833 and 0.32892364\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.92164 and 0.20978576\n",
            "Val acc and loss are 0.8808 and 0.3319396\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.92652 and 0.20343848\n",
            "Val acc and loss are 0.8865 and 0.3253481\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.92564 and 0.20306742\n",
            "Val acc and loss are 0.8837 and 0.3259188\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.92832 and 0.19933489\n",
            "Val acc and loss are 0.8882 and 0.32293448\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.9262 and 0.2019776\n",
            "Val acc and loss are 0.8833 and 0.326525\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.92772 and 0.19979882\n",
            "Val acc and loss are 0.8885 and 0.32502398\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.92406 and 0.20675299\n",
            "Val acc and loss are 0.8817 and 0.3317669\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.92396 and 0.20793778\n",
            "Val acc and loss are 0.8844 and 0.33375192\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.91932 and 0.2153241\n",
            "Val acc and loss are 0.8788 and 0.3411469\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.92592 and 0.2044589\n",
            "Val acc and loss are 0.8857 and 0.32885107\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.92434 and 0.20407648\n",
            "Val acc and loss are 0.8829 and 0.32956305\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.92758 and 0.20090798\n",
            "Val acc and loss are 0.886 and 0.32795292\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.92162 and 0.21118301\n",
            "Val acc and loss are 0.879 and 0.33709317\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.91804 and 0.22598974\n",
            "Val acc and loss are 0.8794 and 0.3532565\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.9157 and 0.2230061\n",
            "Val acc and loss are 0.8764 and 0.35032347\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.92568 and 0.20578046\n",
            "Val acc and loss are 0.8843 and 0.33164307\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.92352 and 0.2066123\n",
            "Val acc and loss are 0.8812 and 0.33509043\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.92522 and 0.20837313\n",
            "Val acc and loss are 0.8809 and 0.32995415\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.91998 and 0.21628377\n",
            "Val acc and loss are 0.8773 and 0.34981105\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.92378 and 0.20960149\n",
            "Val acc and loss are 0.8832 and 0.33101797\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.92596 and 0.20143649\n",
            "Val acc and loss are 0.883 and 0.33091944\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.9287 and 0.19744554\n",
            "Val acc and loss are 0.8868 and 0.32312655\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.92842 and 0.19665538\n",
            "Val acc and loss are 0.8865 and 0.3251936\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.92724 and 0.19936322\n",
            "Val acc and loss are 0.8854 and 0.3266285\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.92408 and 0.20563\n",
            "Val acc and loss are 0.8845 and 0.3341304\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.91744 and 0.21693617\n",
            "Val acc and loss are 0.8769 and 0.3437519\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.92158 and 0.21227728\n",
            "Val acc and loss are 0.8806 and 0.3388289\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.9212 and 0.209906\n",
            "Val acc and loss are 0.879 and 0.3367563\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.92698 and 0.20098238\n",
            "Val acc and loss are 0.8864 and 0.3281761\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.92518 and 0.20251891\n",
            "Val acc and loss are 0.8807 and 0.33115724\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.92504 and 0.20391119\n",
            "Val acc and loss are 0.8865 and 0.33205572\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.9195 and 0.21300057\n",
            "Val acc and loss are 0.8791 and 0.3413691\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.92214 and 0.21163309\n",
            "Val acc and loss are 0.8833 and 0.3405976\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.92156 and 0.20773226\n",
            "Val acc and loss are 0.8812 and 0.33536142\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.92914 and 0.19733758\n",
            "Val acc and loss are 0.8864 and 0.32438\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.92704 and 0.19655207\n",
            "Val acc and loss are 0.8853 and 0.32548803\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.9309 and 0.19377369\n",
            "Val acc and loss are 0.8849 and 0.32222113\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.9255 and 0.19875509\n",
            "Val acc and loss are 0.8853 and 0.3291914\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.93002 and 0.19580159\n",
            "Val acc and loss are 0.8843 and 0.32356837\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.92258 and 0.20624961\n",
            "Val acc and loss are 0.8805 and 0.34062567\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.92472 and 0.20774515\n",
            "Val acc and loss are 0.88 and 0.33399835\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9158 and 0.22447291\n",
            "Val acc and loss are 0.8736 and 0.36640486\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.92344 and 0.2110287\n",
            "Val acc and loss are 0.8785 and 0.33558187\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.92604 and 0.20163494\n",
            "Val acc and loss are 0.8842 and 0.3366829\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.92704 and 0.1980041\n",
            "Val acc and loss are 0.8831 and 0.32787257\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.9303 and 0.19375972\n",
            "Val acc and loss are 0.8881 and 0.32593495\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.92636 and 0.19869603\n",
            "Val acc and loss are 0.8835 and 0.3287728\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.93022 and 0.19453532\n",
            "Val acc and loss are 0.8891 and 0.32699883\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.9251 and 0.19982558\n",
            "Val acc and loss are 0.8815 and 0.3301445\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.92894 and 0.19665743\n",
            "Val acc and loss are 0.8868 and 0.3295016\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.92266 and 0.20362353\n",
            "Val acc and loss are 0.8795 and 0.3342803\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.92438 and 0.20450957\n",
            "Val acc and loss are 0.8835 and 0.33476698\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.92184 and 0.20780942\n",
            "Val acc and loss are 0.8796 and 0.3417267\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.92302 and 0.20673312\n",
            "Val acc and loss are 0.8819 and 0.33559483\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.92688 and 0.20001164\n",
            "Val acc and loss are 0.8837 and 0.3349572\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.9273 and 0.19804116\n",
            "Val acc and loss are 0.8831 and 0.32880646\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.93032 and 0.19283453\n",
            "Val acc and loss are 0.8865 and 0.32817718\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.92866 and 0.19597787\n",
            "Val acc and loss are 0.8845 and 0.3269927\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.93004 and 0.19268848\n",
            "Val acc and loss are 0.886 and 0.3276146\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.92838 and 0.19529957\n",
            "Val acc and loss are 0.8852 and 0.32952657\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.93 and 0.19433631\n",
            "Val acc and loss are 0.8852 and 0.3295294\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.92394 and 0.20448196\n",
            "Val acc and loss are 0.882 and 0.3395115\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.92352 and 0.20977204\n",
            "Val acc and loss are 0.8806 and 0.34383535\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.9152 and 0.22182234\n",
            "Val acc and loss are 0.8765 and 0.35498428\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.9265 and 0.20349044\n",
            "Val acc and loss are 0.8844 and 0.33302817\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.923 and 0.20354539\n",
            "Val acc and loss are 0.8807 and 0.3390034\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.92756 and 0.20008758\n",
            "Val acc and loss are 0.8838 and 0.3312754\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.92558 and 0.20035246\n",
            "Val acc and loss are 0.8816 and 0.3354657\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.92994 and 0.19484188\n",
            "Val acc and loss are 0.8827 and 0.32663658\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.92882 and 0.1925295\n",
            "Val acc and loss are 0.8853 and 0.32830763\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.9325 and 0.18978673\n",
            "Val acc and loss are 0.8847 and 0.32329085\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.9296 and 0.19133882\n",
            "Val acc and loss are 0.8853 and 0.32752123\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.93114 and 0.19118777\n",
            "Val acc and loss are 0.8853 and 0.32478487\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.92782 and 0.19467501\n",
            "Val acc and loss are 0.8846 and 0.33449578\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.92636 and 0.19955307\n",
            "Val acc and loss are 0.8817 and 0.33459806\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.9242 and 0.20239086\n",
            "Val acc and loss are 0.8799 and 0.3394919\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.9209 and 0.20854405\n",
            "Val acc and loss are 0.8803 and 0.34292796\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.9254 and 0.20068519\n",
            "Val acc and loss are 0.8785 and 0.33308858\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.92502 and 0.19942145\n",
            "Val acc and loss are 0.8827 and 0.33567202\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.93052 and 0.19415414\n",
            "Val acc and loss are 0.8839 and 0.32554042\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.92744 and 0.19661587\n",
            "Val acc and loss are 0.8808 and 0.338805\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.9309 and 0.19316688\n",
            "Val acc and loss are 0.8837 and 0.32657558\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.9274 and 0.19657\n",
            "Val acc and loss are 0.8821 and 0.33991796\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.93284 and 0.18823972\n",
            "Val acc and loss are 0.8851 and 0.32286978\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.9303 and 0.18920086\n",
            "Val acc and loss are 0.8862 and 0.32792765\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.93398 and 0.18522297\n",
            "Val acc and loss are 0.887 and 0.3218742\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.93044 and 0.19086082\n",
            "Val acc and loss are 0.8847 and 0.3307847\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.93126 and 0.18952031\n",
            "Val acc and loss are 0.887 and 0.32913008\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.92544 and 0.19880605\n",
            "Val acc and loss are 0.8811 and 0.33714527\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.9257 and 0.20203215\n",
            "Val acc and loss are 0.8829 and 0.34304237\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.9153 and 0.21952829\n",
            "Val acc and loss are 0.8732 and 0.35535133\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.92202 and 0.21113285\n",
            "Val acc and loss are 0.8803 and 0.34971896\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.92454 and 0.20202658\n",
            "Val acc and loss are 0.8788 and 0.34011492\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.92856 and 0.19611521\n",
            "Val acc and loss are 0.8861 and 0.33168644\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.92982 and 0.19157332\n",
            "Val acc and loss are 0.8835 and 0.33352122\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.93112 and 0.19068946\n",
            "Val acc and loss are 0.8855 and 0.32668844\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.93022 and 0.19058406\n",
            "Val acc and loss are 0.8844 and 0.33364332\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.931 and 0.19161141\n",
            "Val acc and loss are 0.8839 and 0.32933483\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.92786 and 0.1941236\n",
            "Val acc and loss are 0.8821 and 0.33900052\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.93072 and 0.19167407\n",
            "Val acc and loss are 0.885 and 0.32967266\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.92788 and 0.19217785\n",
            "Val acc and loss are 0.8854 and 0.33506876\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.9327 and 0.18933667\n",
            "Val acc and loss are 0.8848 and 0.32786906\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.92484 and 0.19834459\n",
            "Val acc and loss are 0.8819 and 0.34010798\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.92996 and 0.19426031\n",
            "Val acc and loss are 0.8827 and 0.33125204\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.92184 and 0.20443434\n",
            "Val acc and loss are 0.8789 and 0.3442432\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.9296 and 0.19247696\n",
            "Val acc and loss are 0.8846 and 0.3286647\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.92952 and 0.19073714\n",
            "Val acc and loss are 0.8844 and 0.32928938\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.93218 and 0.18611757\n",
            "Val acc and loss are 0.8858 and 0.3249326\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.9328 and 0.18492375\n",
            "Val acc and loss are 0.8868 and 0.3250521\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.9302 and 0.18851371\n",
            "Val acc and loss are 0.8829 and 0.32635537\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.93132 and 0.18816094\n",
            "Val acc and loss are 0.8854 and 0.329938\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.92796 and 0.19311392\n",
            "Val acc and loss are 0.8819 and 0.33257753\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.92934 and 0.19467789\n",
            "Val acc and loss are 0.8849 and 0.33874902\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.92454 and 0.20046735\n",
            "Val acc and loss are 0.8803 and 0.34326997\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.92848 and 0.1961425\n",
            "Val acc and loss are 0.8848 and 0.3404409\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.92992 and 0.19243109\n",
            "Val acc and loss are 0.8823 and 0.33568466\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.93248 and 0.18730742\n",
            "Val acc and loss are 0.887 and 0.33451647\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.93214 and 0.18627124\n",
            "Val acc and loss are 0.8827 and 0.3277402\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.93426 and 0.18221943\n",
            "Val acc and loss are 0.8874 and 0.3289803\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.93268 and 0.18526243\n",
            "Val acc and loss are 0.8822 and 0.32947204\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.9321 and 0.1861771\n",
            "Val acc and loss are 0.8859 and 0.33287808\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.92972 and 0.1911179\n",
            "Val acc and loss are 0.8817 and 0.33403453\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.92828 and 0.1929052\n",
            "Val acc and loss are 0.8832 and 0.33727103\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.92318 and 0.20249574\n",
            "Val acc and loss are 0.8786 and 0.34640306\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.92832 and 0.1962802\n",
            "Val acc and loss are 0.8813 and 0.3364673\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.92326 and 0.2059958\n",
            "Val acc and loss are 0.8767 and 0.3554846\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.92376 and 0.20827031\n",
            "Val acc and loss are 0.8776 and 0.345351\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.92496 and 0.20076668\n",
            "Val acc and loss are 0.8787 and 0.35026416\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.93124 and 0.19011734\n",
            "Val acc and loss are 0.8855 and 0.33119312\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.93078 and 0.18578483\n",
            "Val acc and loss are 0.8826 and 0.32959774\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.93466 and 0.18211696\n",
            "Val acc and loss are 0.8882 and 0.326059\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.93276 and 0.18250775\n",
            "Val acc and loss are 0.8832 and 0.32491463\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.9352 and 0.18084659\n",
            "Val acc and loss are 0.8894 and 0.32680386\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.93236 and 0.18357591\n",
            "Val acc and loss are 0.8831 and 0.32739913\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.93394 and 0.1840014\n",
            "Val acc and loss are 0.8881 and 0.33101213\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.93056 and 0.18688059\n",
            "Val acc and loss are 0.8835 and 0.33458427\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.93344 and 0.18666545\n",
            "Val acc and loss are 0.8875 and 0.33338028\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.9296 and 0.18935926\n",
            "Val acc and loss are 0.883 and 0.3369471\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.93502 and 0.18418688\n",
            "Val acc and loss are 0.8887 and 0.33016065\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.93154 and 0.18506597\n",
            "Val acc and loss are 0.8835 and 0.3321781\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.93544 and 0.18186705\n",
            "Val acc and loss are 0.8883 and 0.32879505\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.93026 and 0.18827015\n",
            "Val acc and loss are 0.8831 and 0.3374644\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.93308 and 0.18677461\n",
            "Val acc and loss are 0.8842 and 0.33362764\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.92518 and 0.19810754\n",
            "Val acc and loss are 0.8807 and 0.34938964\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.93156 and 0.18904673\n",
            "Val acc and loss are 0.8827 and 0.3346273\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.9249 and 0.1967355\n",
            "Val acc and loss are 0.8833 and 0.34607685\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.92824 and 0.1922534\n",
            "Val acc and loss are 0.8804 and 0.338095\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.9296 and 0.18854965\n",
            "Val acc and loss are 0.8879 and 0.336066\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.93312 and 0.1821086\n",
            "Val acc and loss are 0.8828 and 0.3295521\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.93572 and 0.1795\n",
            "Val acc and loss are 0.8886 and 0.3258831\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.93374 and 0.1798512\n",
            "Val acc and loss are 0.8847 and 0.33084947\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.9365 and 0.17967738\n",
            "Val acc and loss are 0.8889 and 0.3279728\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.92928 and 0.18784511\n",
            "Val acc and loss are 0.8818 and 0.34231368\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.93124 and 0.19106248\n",
            "Val acc and loss are 0.8841 and 0.3381392\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.92522 and 0.19966255\n",
            "Val acc and loss are 0.8782 and 0.3582597\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.92976 and 0.19368465\n",
            "Val acc and loss are 0.8789 and 0.33884087\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.93126 and 0.18697314\n",
            "Val acc and loss are 0.8855 and 0.34159333\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.93624 and 0.17801055\n",
            "Val acc and loss are 0.8848 and 0.32482824\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.93694 and 0.17469211\n",
            "Val acc and loss are 0.8899 and 0.32601514\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.9374 and 0.1748943\n",
            "Val acc and loss are 0.8859 and 0.32437035\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.93738 and 0.17446293\n",
            "Val acc and loss are 0.8888 and 0.32634506\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.93394 and 0.18123399\n",
            "Val acc and loss are 0.8846 and 0.33123925\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.93444 and 0.18197414\n",
            "Val acc and loss are 0.8877 and 0.33411244\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.92848 and 0.19092606\n",
            "Val acc and loss are 0.8816 and 0.3401517\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.93216 and 0.18650937\n",
            "Val acc and loss are 0.8831 and 0.3367854\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.92436 and 0.19931453\n",
            "Val acc and loss are 0.8771 and 0.3466783\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.92912 and 0.19043209\n",
            "Val acc and loss are 0.8818 and 0.33874926\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.93044 and 0.18749829\n",
            "Val acc and loss are 0.8848 and 0.3359355\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.9332 and 0.18205889\n",
            "Val acc and loss are 0.8846 and 0.33107203\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.93438 and 0.17980836\n",
            "Val acc and loss are 0.889 and 0.33059013\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.9335 and 0.18078527\n",
            "Val acc and loss are 0.8833 and 0.3310985\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.93496 and 0.17849535\n",
            "Val acc and loss are 0.8889 and 0.32994235\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.93416 and 0.17946172\n",
            "Val acc and loss are 0.8825 and 0.33099627\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.93594 and 0.17916036\n",
            "Val acc and loss are 0.8903 and 0.3324761\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.93106 and 0.18434635\n",
            "Val acc and loss are 0.882 and 0.3375483\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.93232 and 0.18531129\n",
            "Val acc and loss are 0.8882 and 0.33999467\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.9282 and 0.19052015\n",
            "Val acc and loss are 0.8797 and 0.34300515\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.9309 and 0.18933472\n",
            "Val acc and loss are 0.8843 and 0.344164\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.93172 and 0.18368469\n",
            "Val acc and loss are 0.8838 and 0.3388262\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.93538 and 0.18158089\n",
            "Val acc and loss are 0.8855 and 0.33386844\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.92992 and 0.1896092\n",
            "Val acc and loss are 0.8795 and 0.35116985\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.9309 and 0.19365269\n",
            "Val acc and loss are 0.8797 and 0.34364387\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.92872 and 0.19365656\n",
            "Val acc and loss are 0.8807 and 0.3547344\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.93716 and 0.17694028\n",
            "Val acc and loss are 0.8834 and 0.32588655\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.9378 and 0.17278984\n",
            "Val acc and loss are 0.8887 and 0.32623595\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.94114 and 0.16771321\n",
            "Val acc and loss are 0.8877 and 0.3213661\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.93952 and 0.16967715\n",
            "Val acc and loss are 0.8896 and 0.32483584\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.93882 and 0.1700523\n",
            "Val acc and loss are 0.8878 and 0.32687762\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.93366 and 0.18013361\n",
            "Val acc and loss are 0.8843 and 0.3361317\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.92998 and 0.18629758\n",
            "Val acc and loss are 0.8807 and 0.34120065\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.92468 and 0.19892506\n",
            "Val acc and loss are 0.8777 and 0.35279083\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.92948 and 0.18807654\n",
            "Val acc and loss are 0.8803 and 0.34065145\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.93508 and 0.17885503\n",
            "Val acc and loss are 0.8855 and 0.33032402\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.93744 and 0.17388931\n",
            "Val acc and loss are 0.8883 and 0.32733434\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.9362 and 0.176085\n",
            "Val acc and loss are 0.8864 and 0.33187258\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.93772 and 0.17546313\n",
            "Val acc and loss are 0.8866 and 0.3310136\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.93262 and 0.17990023\n",
            "Val acc and loss are 0.8853 and 0.33883423\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.93704 and 0.17627361\n",
            "Val acc and loss are 0.8855 and 0.33289972\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.93182 and 0.18206434\n",
            "Val acc and loss are 0.8845 and 0.34132946\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.93424 and 0.18215789\n",
            "Val acc and loss are 0.8854 and 0.33779064\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.92804 and 0.19023953\n",
            "Val acc and loss are 0.8791 and 0.34716973\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.93288 and 0.18468246\n",
            "Val acc and loss are 0.8847 and 0.34059033\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.93278 and 0.18195817\n",
            "Val acc and loss are 0.8821 and 0.33804896\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.93594 and 0.17756283\n",
            "Val acc and loss are 0.888 and 0.33352277\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.93632 and 0.17482828\n",
            "Val acc and loss are 0.8845 and 0.3341564\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.93598 and 0.17629443\n",
            "Val acc and loss are 0.886 and 0.33238804\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.93654 and 0.17454854\n",
            "Val acc and loss are 0.8871 and 0.33526865\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.93784 and 0.17323269\n",
            "Val acc and loss are 0.8855 and 0.3290309\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.93672 and 0.17393342\n",
            "Val acc and loss are 0.8874 and 0.3373852\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.93806 and 0.17417261\n",
            "Val acc and loss are 0.8827 and 0.3310638\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.93412 and 0.17895353\n",
            "Val acc and loss are 0.8847 and 0.34399423\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.93654 and 0.17656435\n",
            "Val acc and loss are 0.8825 and 0.33270606\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.9328 and 0.18167464\n",
            "Val acc and loss are 0.8828 and 0.34903625\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.93444 and 0.17908387\n",
            "Val acc and loss are 0.8808 and 0.33320972\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.92914 and 0.18793398\n",
            "Val acc and loss are 0.8824 and 0.35108605\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.93294 and 0.18150008\n",
            "Val acc and loss are 0.8824 and 0.3363979\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.9328 and 0.18125889\n",
            "Val acc and loss are 0.8827 and 0.34015208\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.93858 and 0.17209023\n",
            "Val acc and loss are 0.8852 and 0.33057174\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.93676 and 0.1731911\n",
            "Val acc and loss are 0.8866 and 0.33257413\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.94016 and 0.16932984\n",
            "Val acc and loss are 0.8873 and 0.33058074\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.9383 and 0.16967687\n",
            "Val acc and loss are 0.888 and 0.330463\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.93996 and 0.16864729\n",
            "Val acc and loss are 0.8867 and 0.3314046\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.9382 and 0.16946465\n",
            "Val acc and loss are 0.8882 and 0.33130762\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.93704 and 0.17245391\n",
            "Val acc and loss are 0.8861 and 0.33514288\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.93296 and 0.17832027\n",
            "Val acc and loss are 0.8835 and 0.33880338\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.93414 and 0.17950489\n",
            "Val acc and loss are 0.8838 and 0.3408764\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.93074 and 0.18276614\n",
            "Val acc and loss are 0.8817 and 0.34302467\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.9369 and 0.17815863\n",
            "Val acc and loss are 0.8869 and 0.33560342\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.93378 and 0.17847624\n",
            "Val acc and loss are 0.8827 and 0.34178123\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.93674 and 0.17763266\n",
            "Val acc and loss are 0.8881 and 0.33884174\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.93036 and 0.18517968\n",
            "Val acc and loss are 0.8827 and 0.3483753\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.93792 and 0.1742045\n",
            "Val acc and loss are 0.887 and 0.33452612\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.93728 and 0.17129238\n",
            "Val acc and loss are 0.8866 and 0.3318886\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.94152 and 0.16655827\n",
            "Val acc and loss are 0.888 and 0.32764328\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.93778 and 0.16999207\n",
            "Val acc and loss are 0.8873 and 0.3303348\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.94008 and 0.16772366\n",
            "Val acc and loss are 0.8867 and 0.332122\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.93476 and 0.17634735\n",
            "Val acc and loss are 0.8846 and 0.33704937\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.93742 and 0.17232399\n",
            "Val acc and loss are 0.8864 and 0.33676374\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.93392 and 0.17917597\n",
            "Val acc and loss are 0.881 and 0.3393322\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.93616 and 0.17522456\n",
            "Val acc and loss are 0.8847 and 0.34062594\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.93558 and 0.17566697\n",
            "Val acc and loss are 0.8823 and 0.3355064\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.93806 and 0.17104149\n",
            "Val acc and loss are 0.8868 and 0.33340272\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.93592 and 0.17461605\n",
            "Val acc and loss are 0.8843 and 0.33756042\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.934 and 0.18002248\n",
            "Val acc and loss are 0.8816 and 0.33894503\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.93016 and 0.18791397\n",
            "Val acc and loss are 0.8806 and 0.35950714\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.92998 and 0.19053324\n",
            "Val acc and loss are 0.8769 and 0.34628296\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.93298 and 0.18071304\n",
            "Val acc and loss are 0.8801 and 0.3539459\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.94064 and 0.16720797\n",
            "Val acc and loss are 0.8847 and 0.32617813\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.94018 and 0.16536531\n",
            "Val acc and loss are 0.8879 and 0.32988724\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.93782 and 0.17039524\n",
            "Val acc and loss are 0.8831 and 0.33246025\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.9364 and 0.17394742\n",
            "Val acc and loss are 0.8852 and 0.3379744\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.93224 and 0.17958027\n",
            "Val acc and loss are 0.8792 and 0.34388638\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.93788 and 0.17220908\n",
            "Val acc and loss are 0.8878 and 0.33565035\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.93796 and 0.16960093\n",
            "Val acc and loss are 0.8838 and 0.33606142\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.94066 and 0.1676259\n",
            "Val acc and loss are 0.8892 and 0.330871\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.93952 and 0.16691582\n",
            "Val acc and loss are 0.8864 and 0.3355094\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.94036 and 0.16800557\n",
            "Val acc and loss are 0.8885 and 0.33367032\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.93562 and 0.17329718\n",
            "Val acc and loss are 0.8855 and 0.34207556\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.93684 and 0.17648625\n",
            "Val acc and loss are 0.8839 and 0.34188065\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.9306 and 0.18328434\n",
            "Val acc and loss are 0.8824 and 0.34969786\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.94156 and 0.16656701\n",
            "Val acc and loss are 0.887 and 0.33174726\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.93882 and 0.16641282\n",
            "Val acc and loss are 0.8869 and 0.33083996\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.9439 and 0.16056515\n",
            "Val acc and loss are 0.8886 and 0.32571232\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.93902 and 0.16641884\n",
            "Val acc and loss are 0.8874 and 0.33403736\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.9391 and 0.16713914\n",
            "Val acc and loss are 0.8855 and 0.3340457\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.93278 and 0.17900762\n",
            "Val acc and loss are 0.8841 and 0.34753725\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.93176 and 0.18014619\n",
            "Val acc and loss are 0.8805 and 0.3461058\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.93538 and 0.17547484\n",
            "Val acc and loss are 0.8848 and 0.34175664\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.939 and 0.16704714\n",
            "Val acc and loss are 0.8868 and 0.33186573\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.94264 and 0.16209185\n",
            "Val acc and loss are 0.8872 and 0.32907894\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.94222 and 0.15961204\n",
            "Val acc and loss are 0.8892 and 0.3301276\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.94332 and 0.16153213\n",
            "Val acc and loss are 0.8868 and 0.32891434\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.93954 and 0.16631132\n",
            "Val acc and loss are 0.8883 and 0.34222126\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.93684 and 0.17637034\n",
            "Val acc and loss are 0.8791 and 0.34352782\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.93062 and 0.1875345\n",
            "Val acc and loss are 0.8796 and 0.371329\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.93804 and 0.17543063\n",
            "Val acc and loss are 0.8793 and 0.3404304\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.93918 and 0.1656625\n",
            "Val acc and loss are 0.8851 and 0.34100407\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.94264 and 0.16145204\n",
            "Val acc and loss are 0.8868 and 0.329051\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.93532 and 0.17326415\n",
            "Val acc and loss are 0.8839 and 0.3423437\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.93928 and 0.16947086\n",
            "Val acc and loss are 0.8849 and 0.33671445\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.933 and 0.17624138\n",
            "Val acc and loss are 0.8829 and 0.34240308\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.94174 and 0.16414899\n",
            "Val acc and loss are 0.8878 and 0.3307082\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.93948 and 0.16757679\n",
            "Val acc and loss are 0.8851 and 0.3355626\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.94058 and 0.1660614\n",
            "Val acc and loss are 0.8893 and 0.3354148\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.93622 and 0.17302254\n",
            "Val acc and loss are 0.8826 and 0.3423586\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.9384 and 0.17118695\n",
            "Val acc and loss are 0.8858 and 0.34171388\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.93566 and 0.1731915\n",
            "Val acc and loss are 0.8815 and 0.34121063\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.94022 and 0.16671333\n",
            "Val acc and loss are 0.8875 and 0.3363281\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.93922 and 0.16623332\n",
            "Val acc and loss are 0.8845 and 0.3369369\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.9397 and 0.16583177\n",
            "Val acc and loss are 0.8864 and 0.3355772\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.94074 and 0.16400507\n",
            "Val acc and loss are 0.8842 and 0.3360489\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.93998 and 0.16603386\n",
            "Val acc and loss are 0.8878 and 0.33589447\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.9415 and 0.16144419\n",
            "Val acc and loss are 0.8872 and 0.33424664\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.94188 and 0.16173717\n",
            "Val acc and loss are 0.8879 and 0.33394444\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.94298 and 0.15815957\n",
            "Val acc and loss are 0.8883 and 0.3329736\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.94264 and 0.1605691\n",
            "Val acc and loss are 0.8902 and 0.3336532\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.94112 and 0.16216286\n",
            "Val acc and loss are 0.8862 and 0.33761224\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.94104 and 0.16524215\n",
            "Val acc and loss are 0.8883 and 0.33860192\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.93594 and 0.17213777\n",
            "Val acc and loss are 0.8828 and 0.34863684\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.93656 and 0.17575906\n",
            "Val acc and loss are 0.8855 and 0.34787032\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.9282 and 0.18925844\n",
            "Val acc and loss are 0.877 and 0.36738282\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.9354 and 0.17868462\n",
            "Val acc and loss are 0.8798 and 0.34563315\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.935 and 0.17551355\n",
            "Val acc and loss are 0.8837 and 0.35166723\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.94188 and 0.16427113\n",
            "Val acc and loss are 0.8841 and 0.33189908\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.93956 and 0.1641209\n",
            "Val acc and loss are 0.8876 and 0.33862203\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.94156 and 0.16210712\n",
            "Val acc and loss are 0.8851 and 0.33122578\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.9426 and 0.15941818\n",
            "Val acc and loss are 0.8888 and 0.33330175\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.9427 and 0.15918681\n",
            "Val acc and loss are 0.8855 and 0.33171293\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.94396 and 0.15693487\n",
            "Val acc and loss are 0.89 and 0.33201063\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.94314 and 0.15787223\n",
            "Val acc and loss are 0.8855 and 0.33167133\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.94372 and 0.15723468\n",
            "Val acc and loss are 0.8906 and 0.33450085\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.93994 and 0.16498707\n",
            "Val acc and loss are 0.8831 and 0.3395119\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.93644 and 0.17277536\n",
            "Val acc and loss are 0.8863 and 0.3543253\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.9328 and 0.17950438\n",
            "Val acc and loss are 0.8787 and 0.35385096\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.93876 and 0.16961977\n",
            "Val acc and loss are 0.8844 and 0.34892544\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.9398 and 0.16472498\n",
            "Val acc and loss are 0.8855 and 0.33959347\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.94466 and 0.1569057\n",
            "Val acc and loss are 0.888 and 0.3330287\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.94376 and 0.15778825\n",
            "Val acc and loss are 0.8881 and 0.336436\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.94502 and 0.15515463\n",
            "Val acc and loss are 0.8896 and 0.33062598\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.94004 and 0.16328068\n",
            "Val acc and loss are 0.8868 and 0.34209594\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.9405 and 0.1638355\n",
            "Val acc and loss are 0.8838 and 0.3383409\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.93488 and 0.17404178\n",
            "Val acc and loss are 0.8835 and 0.34953398\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.93866 and 0.1677684\n",
            "Val acc and loss are 0.8826 and 0.34134537\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.9371 and 0.16765426\n",
            "Val acc and loss are 0.8857 and 0.34445053\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.94176 and 0.16245984\n",
            "Val acc and loss are 0.8866 and 0.33802423\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.94112 and 0.1650282\n",
            "Val acc and loss are 0.8859 and 0.3435305\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.94108 and 0.16129111\n",
            "Val acc and loss are 0.888 and 0.3387081\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.94366 and 0.15829208\n",
            "Val acc and loss are 0.8886 and 0.33645746\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.93982 and 0.1632514\n",
            "Val acc and loss are 0.8852 and 0.34314254\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.93968 and 0.16951576\n",
            "Val acc and loss are 0.8837 and 0.34561294\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.93078 and 0.18224613\n",
            "Val acc and loss are 0.8782 and 0.36948064\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.9385 and 0.17459543\n",
            "Val acc and loss are 0.8813 and 0.3467916\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.9404 and 0.16386795\n",
            "Val acc and loss are 0.884 and 0.34776595\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.9477 and 0.15359892\n",
            "Val acc and loss are 0.888 and 0.32674846\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.9465 and 0.1511748\n",
            "Val acc and loss are 0.8893 and 0.33143708\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.94744 and 0.15086082\n",
            "Val acc and loss are 0.8884 and 0.32795942\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.94656 and 0.15071376\n",
            "Val acc and loss are 0.8904 and 0.33240917\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.94426 and 0.15546046\n",
            "Val acc and loss are 0.8853 and 0.33147413\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.94324 and 0.15718108\n",
            "Val acc and loss are 0.8901 and 0.33959875\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.93878 and 0.16389695\n",
            "Val acc and loss are 0.8833 and 0.33857632\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.9422 and 0.16007231\n",
            "Val acc and loss are 0.8878 and 0.34317237\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.9387 and 0.16406955\n",
            "Val acc and loss are 0.8835 and 0.3397308\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.94286 and 0.158476\n",
            "Val acc and loss are 0.8874 and 0.3379272\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.93664 and 0.16965455\n",
            "Val acc and loss are 0.8817 and 0.34848237\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.93718 and 0.16930598\n",
            "Val acc and loss are 0.8804 and 0.34672168\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.93544 and 0.17129846\n",
            "Val acc and loss are 0.8849 and 0.35122192\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.94022 and 0.16361937\n",
            "Val acc and loss are 0.8832 and 0.33872524\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.94272 and 0.15824218\n",
            "Val acc and loss are 0.8888 and 0.33787465\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.94466 and 0.15582255\n",
            "Val acc and loss are 0.8862 and 0.33412474\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.94338 and 0.15713622\n",
            "Val acc and loss are 0.89 and 0.33732393\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.94284 and 0.15896198\n",
            "Val acc and loss are 0.8849 and 0.34286597\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.94282 and 0.16033497\n",
            "Val acc and loss are 0.8899 and 0.3434576\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.94166 and 0.16065621\n",
            "Val acc and loss are 0.8842 and 0.34497052\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.94432 and 0.15781543\n",
            "Val acc and loss are 0.8899 and 0.3416001\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.94042 and 0.16095679\n",
            "Val acc and loss are 0.8856 and 0.3448086\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.94594 and 0.15389055\n",
            "Val acc and loss are 0.8885 and 0.33717\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.94204 and 0.15728918\n",
            "Val acc and loss are 0.8875 and 0.34014884\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.94674 and 0.15282431\n",
            "Val acc and loss are 0.8886 and 0.335956\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.94006 and 0.16229463\n",
            "Val acc and loss are 0.8857 and 0.35218358\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.94354 and 0.16080455\n",
            "Val acc and loss are 0.8858 and 0.34215635\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.93976 and 0.16191868\n",
            "Val acc and loss are 0.8843 and 0.35435352\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.94216 and 0.15942232\n",
            "Val acc and loss are 0.8856 and 0.33865854\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.94002 and 0.16345792\n",
            "Val acc and loss are 0.8852 and 0.34944886\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.937 and 0.16860789\n",
            "Val acc and loss are 0.8803 and 0.35032538\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.93468 and 0.17761442\n",
            "Val acc and loss are 0.8811 and 0.3643972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H--d5hvrUzh7",
        "outputId": "51b56aff-5882-403b-fb4a-3faf8de8852f"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.7703 at epoch 999 with a corresponding training accuracy of 0.77362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHQrDDsHofpl"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "x-YDOCDaofuZ",
        "outputId": "f5d9531e-3430-4ccc-ac59-d1fc55fecd8b"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.9477 achieved at epoch: 968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tByOYlQxof7x",
        "outputId": "63f9be71-b5b9-4a0d-e473-ef4bdc57a65c"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2795,  313,  339,  482,  255,  245,  299,   40,  197,   12],\n",
              "       [ 205, 3869,   82,  604,   85,   53,   71,   25,   17,    1],\n",
              "       [ 301,  118, 2076,  162, 1036,  236,  601,   51,  377,   34],\n",
              "       [ 407,  927,  122, 2706,  261,  197,  191,   48,  105,   15],\n",
              "       [ 221,  194,  994,  379, 2078,  103,  607,   37,  315,   22],\n",
              "       [  56,   75,   66,   36,   26, 2575,   89, 1262,  241,  578],\n",
              "       [ 850,  263,  979,  308,  939,  356,  778,   77,  437,   43],\n",
              "       [   3,   11,   13,   13,    8,  829,   21, 3498,  141,  508],\n",
              "       [ 123,   33,  145,  101,  180,  243,  316,  250, 3374,  267],\n",
              "       [  38,   18,   52,   34,   40,  321,   50,  506,  221, 3699]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "s65-YhivogBG",
        "outputId": "abea26bd-bcac-42f3-8801-19ccd0402b2f"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8906 achieved at epoch: 945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDoiAD3WogGp",
        "outputId": "172c6a7f-88bc-4c4e-cd9d-c567385c8685"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[800,  38,  26,  63,  16,  48,   0,   0,  32,   0],\n",
              "       [ 16, 928,   9,  26,   4,   3,   0,   0,   2,   0],\n",
              "       [ 25,  15, 564,  10, 250,  75,  24,   1,  44,   0],\n",
              "       [ 54,  95,   6, 788,  38,  29,   3,   0,   8,   0],\n",
              "       [ 25,  40, 117,  56, 752,  22,  22,   0,  15,   1],\n",
              "       [  0,   2,   0,   0,   1, 761,   2, 137,   6,  87],\n",
              "       [214,  25, 178,  33, 266, 105,  85,   0,  62,   2],\n",
              "       [  0,   0,   0,   0,   0, 116,   0, 757,   1,  81],\n",
              "       [  5,   2,  11,  16,  12,  47,   6,  16, 844,   9],\n",
              "       [  1,   1,   4,   1,   2,  26,   4,  42,   0, 940]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QQoFSgogNJ",
        "outputId": "4d0e7b6d-4e4c-463d-dc02-e891de5050f4"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9696291\n",
            "0.7116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FPer1xpofmL",
        "outputId": "2cb01332-10a1-47f1-f40f-d0edea9f07f5"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[758,  43,  35,  63,  19,  52,   4,   1,  23,   2],\n",
              "       [ 25, 923,   9,  29,   8,   2,   0,   0,   4,   0],\n",
              "       [ 28,  30, 561,   7, 255,  42,  30,   0,  45,   2],\n",
              "       [ 56, 106,  12, 733,  44,  35,   4,   0,  10,   0],\n",
              "       [ 14,  28, 122,  53, 702,  30,  27,   0,  23,   1],\n",
              "       [  0,   1,   1,   1,   0, 768,   1, 125,   5,  98],\n",
              "       [225,  27, 187,  49, 276,  94,  72,   1,  66,   3],\n",
              "       [  0,   0,   0,   0,   0,  92,   0, 823,   0,  85],\n",
              "       [  7,   3,  14,  16,   9,  46,   7,  16, 874,   8],\n",
              "       [  2,   0,   2,   1,   3,  32,   3,  53,   2, 902]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sFlwlctopvL"
      },
      "source": [
        "# **Test 6** *(Revised from Test 5: epoch = 5000)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsHTzfEJofj7"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 128, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"relu\", optimizer_name = 'SGD', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbQxN4o7owpw",
        "outputId": "55f55849-2220-41b2-b03e-8741ab0c7f4a"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 5000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training acc and loss are 0.77966 and 0.62699556\n",
            "Val acc and loss are 0.77 and 0.63102746\n",
            "Processing Epoch 3335\n",
            "Training acc and loss are 0.77964 and 0.6269512\n",
            "Val acc and loss are 0.77 and 0.6309842\n",
            "Processing Epoch 3336\n",
            "Training acc and loss are 0.77966 and 0.62690383\n",
            "Val acc and loss are 0.7699 and 0.63093835\n",
            "Processing Epoch 3337\n",
            "Training acc and loss are 0.77968 and 0.62685573\n",
            "Val acc and loss are 0.77 and 0.630891\n",
            "Processing Epoch 3338\n",
            "Training acc and loss are 0.7797 and 0.62680495\n",
            "Val acc and loss are 0.77 and 0.63084143\n",
            "Processing Epoch 3339\n",
            "Training acc and loss are 0.7797 and 0.62675333\n",
            "Val acc and loss are 0.77 and 0.6307921\n",
            "Processing Epoch 3340\n",
            "Training acc and loss are 0.77972 and 0.6267079\n",
            "Val acc and loss are 0.77 and 0.630748\n",
            "Processing Epoch 3341\n",
            "Training acc and loss are 0.77976 and 0.62665904\n",
            "Val acc and loss are 0.77 and 0.6307003\n",
            "Processing Epoch 3342\n",
            "Training acc and loss are 0.77974 and 0.62661463\n",
            "Val acc and loss are 0.7701 and 0.63065726\n",
            "Processing Epoch 3343\n",
            "Training acc and loss are 0.77976 and 0.6265684\n",
            "Val acc and loss are 0.7701 and 0.6306123\n",
            "Processing Epoch 3344\n",
            "Training acc and loss are 0.77974 and 0.6265187\n",
            "Val acc and loss are 0.7701 and 0.63056403\n",
            "Processing Epoch 3345\n",
            "Training acc and loss are 0.77976 and 0.6264703\n",
            "Val acc and loss are 0.7702 and 0.6305169\n",
            "Processing Epoch 3346\n",
            "Training acc and loss are 0.77978 and 0.6264236\n",
            "Val acc and loss are 0.7702 and 0.63047117\n",
            "Processing Epoch 3347\n",
            "Training acc and loss are 0.7798 and 0.62637603\n",
            "Val acc and loss are 0.7702 and 0.6304252\n",
            "Processing Epoch 3348\n",
            "Training acc and loss are 0.77982 and 0.6263261\n",
            "Val acc and loss are 0.7702 and 0.6303769\n",
            "Processing Epoch 3349\n",
            "Training acc and loss are 0.77986 and 0.6262812\n",
            "Val acc and loss are 0.7702 and 0.6303332\n",
            "Processing Epoch 3350\n",
            "Training acc and loss are 0.77988 and 0.62623537\n",
            "Val acc and loss are 0.7702 and 0.6302882\n",
            "Processing Epoch 3351\n",
            "Training acc and loss are 0.7799 and 0.6261898\n",
            "Val acc and loss are 0.7702 and 0.63024443\n",
            "Processing Epoch 3352\n",
            "Training acc and loss are 0.7799 and 0.6261433\n",
            "Val acc and loss are 0.7702 and 0.63019955\n",
            "Processing Epoch 3353\n",
            "Training acc and loss are 0.7799 and 0.62609524\n",
            "Val acc and loss are 0.7702 and 0.630153\n",
            "Processing Epoch 3354\n",
            "Training acc and loss are 0.77994 and 0.6260504\n",
            "Val acc and loss are 0.7702 and 0.6301098\n",
            "Processing Epoch 3355\n",
            "Training acc and loss are 0.77996 and 0.62600166\n",
            "Val acc and loss are 0.7703 and 0.6300628\n",
            "Processing Epoch 3356\n",
            "Training acc and loss are 0.77994 and 0.6259547\n",
            "Val acc and loss are 0.7703 and 0.6300169\n",
            "Processing Epoch 3357\n",
            "Training acc and loss are 0.78 and 0.62590456\n",
            "Val acc and loss are 0.7703 and 0.62996787\n",
            "Processing Epoch 3358\n",
            "Training acc and loss are 0.78 and 0.6258573\n",
            "Val acc and loss are 0.7704 and 0.6299223\n",
            "Processing Epoch 3359\n",
            "Training acc and loss are 0.78004 and 0.6258129\n",
            "Val acc and loss are 0.7704 and 0.6298789\n",
            "Processing Epoch 3360\n",
            "Training acc and loss are 0.78006 and 0.62576586\n",
            "Val acc and loss are 0.7704 and 0.6298331\n",
            "Processing Epoch 3361\n",
            "Training acc and loss are 0.78008 and 0.6257187\n",
            "Val acc and loss are 0.7704 and 0.6297868\n",
            "Processing Epoch 3362\n",
            "Training acc and loss are 0.78012 and 0.6256726\n",
            "Val acc and loss are 0.7704 and 0.62974143\n",
            "Processing Epoch 3363\n",
            "Training acc and loss are 0.78014 and 0.6256251\n",
            "Val acc and loss are 0.7704 and 0.6296954\n",
            "Processing Epoch 3364\n",
            "Training acc and loss are 0.7802 and 0.62557983\n",
            "Val acc and loss are 0.7703 and 0.6296518\n",
            "Processing Epoch 3365\n",
            "Training acc and loss are 0.78018 and 0.6255298\n",
            "Val acc and loss are 0.7704 and 0.62960255\n",
            "Processing Epoch 3366\n",
            "Training acc and loss are 0.7802 and 0.6254854\n",
            "Val acc and loss are 0.7703 and 0.6295603\n",
            "Processing Epoch 3367\n",
            "Training acc and loss are 0.7802 and 0.6254412\n",
            "Val acc and loss are 0.7705 and 0.629517\n",
            "Processing Epoch 3368\n",
            "Training acc and loss are 0.7802 and 0.62539417\n",
            "Val acc and loss are 0.7705 and 0.6294715\n",
            "Processing Epoch 3369\n",
            "Training acc and loss are 0.7802 and 0.62534654\n",
            "Val acc and loss are 0.7705 and 0.62942475\n",
            "Processing Epoch 3370\n",
            "Training acc and loss are 0.7802 and 0.62530094\n",
            "Val acc and loss are 0.7705 and 0.6293812\n",
            "Processing Epoch 3371\n",
            "Training acc and loss are 0.78022 and 0.62525374\n",
            "Val acc and loss are 0.7705 and 0.6293355\n",
            "Processing Epoch 3372\n",
            "Training acc and loss are 0.78024 and 0.62520826\n",
            "Val acc and loss are 0.7705 and 0.6292913\n",
            "Processing Epoch 3373\n",
            "Training acc and loss are 0.78022 and 0.6251591\n",
            "Val acc and loss are 0.7705 and 0.6292425\n",
            "Processing Epoch 3374\n",
            "Training acc and loss are 0.78022 and 0.625113\n",
            "Val acc and loss are 0.7705 and 0.6291981\n",
            "Processing Epoch 3375\n",
            "Training acc and loss are 0.78024 and 0.6250661\n",
            "Val acc and loss are 0.7705 and 0.62915236\n",
            "Processing Epoch 3376\n",
            "Training acc and loss are 0.78028 and 0.6250201\n",
            "Val acc and loss are 0.7705 and 0.62910795\n",
            "Processing Epoch 3377\n",
            "Training acc and loss are 0.7803 and 0.6249748\n",
            "Val acc and loss are 0.7705 and 0.62906414\n",
            "Processing Epoch 3378\n",
            "Training acc and loss are 0.7803 and 0.6249245\n",
            "Val acc and loss are 0.7705 and 0.6290146\n",
            "Processing Epoch 3379\n",
            "Training acc and loss are 0.78032 and 0.6248774\n",
            "Val acc and loss are 0.7705 and 0.62896943\n",
            "Processing Epoch 3380\n",
            "Training acc and loss are 0.78032 and 0.62483376\n",
            "Val acc and loss are 0.7705 and 0.62892735\n",
            "Processing Epoch 3381\n",
            "Training acc and loss are 0.78032 and 0.62478495\n",
            "Val acc and loss are 0.7706 and 0.6288798\n",
            "Processing Epoch 3382\n",
            "Training acc and loss are 0.78036 and 0.6247396\n",
            "Val acc and loss are 0.7706 and 0.62883544\n",
            "Processing Epoch 3383\n",
            "Training acc and loss are 0.78038 and 0.6246911\n",
            "Val acc and loss are 0.7706 and 0.6287887\n",
            "Processing Epoch 3384\n",
            "Training acc and loss are 0.7804 and 0.62464744\n",
            "Val acc and loss are 0.7706 and 0.62874705\n",
            "Processing Epoch 3385\n",
            "Training acc and loss are 0.7804 and 0.6246007\n",
            "Val acc and loss are 0.7706 and 0.6287022\n",
            "Processing Epoch 3386\n",
            "Training acc and loss are 0.7804 and 0.6245537\n",
            "Val acc and loss are 0.7706 and 0.62865627\n",
            "Processing Epoch 3387\n",
            "Training acc and loss are 0.7804 and 0.6245046\n",
            "Val acc and loss are 0.7706 and 0.628609\n",
            "Processing Epoch 3388\n",
            "Training acc and loss are 0.78042 and 0.62445986\n",
            "Val acc and loss are 0.7706 and 0.6285654\n",
            "Processing Epoch 3389\n",
            "Training acc and loss are 0.78044 and 0.62441397\n",
            "Val acc and loss are 0.7706 and 0.6285211\n",
            "Processing Epoch 3390\n",
            "Training acc and loss are 0.78046 and 0.624366\n",
            "Val acc and loss are 0.7706 and 0.628474\n",
            "Processing Epoch 3391\n",
            "Training acc and loss are 0.78048 and 0.6243188\n",
            "Val acc and loss are 0.7707 and 0.6284278\n",
            "Processing Epoch 3392\n",
            "Training acc and loss are 0.7805 and 0.62427235\n",
            "Val acc and loss are 0.7707 and 0.6283825\n",
            "Processing Epoch 3393\n",
            "Training acc and loss are 0.78048 and 0.62422717\n",
            "Val acc and loss are 0.7707 and 0.6283384\n",
            "Processing Epoch 3394\n",
            "Training acc and loss are 0.78056 and 0.62417734\n",
            "Val acc and loss are 0.7707 and 0.6282893\n",
            "Processing Epoch 3395\n",
            "Training acc and loss are 0.78056 and 0.62413\n",
            "Val acc and loss are 0.7708 and 0.62824446\n",
            "Processing Epoch 3396\n",
            "Training acc and loss are 0.78056 and 0.62408197\n",
            "Val acc and loss are 0.7708 and 0.62819743\n",
            "Processing Epoch 3397\n",
            "Training acc and loss are 0.78058 and 0.6240401\n",
            "Val acc and loss are 0.7708 and 0.62815714\n",
            "Processing Epoch 3398\n",
            "Training acc and loss are 0.78062 and 0.62399226\n",
            "Val acc and loss are 0.7708 and 0.62811047\n",
            "Processing Epoch 3399\n",
            "Training acc and loss are 0.78062 and 0.6239471\n",
            "Val acc and loss are 0.7709 and 0.6280664\n",
            "Processing Epoch 3400\n",
            "Training acc and loss are 0.78066 and 0.6239038\n",
            "Val acc and loss are 0.7709 and 0.62802464\n",
            "Processing Epoch 3401\n",
            "Training acc and loss are 0.78066 and 0.62386024\n",
            "Val acc and loss are 0.771 and 0.6279822\n",
            "Processing Epoch 3402\n",
            "Training acc and loss are 0.78068 and 0.6238154\n",
            "Val acc and loss are 0.7709 and 0.62793857\n",
            "Processing Epoch 3403\n",
            "Training acc and loss are 0.78068 and 0.6237682\n",
            "Val acc and loss are 0.771 and 0.6278925\n",
            "Processing Epoch 3404\n",
            "Training acc and loss are 0.7807 and 0.62372375\n",
            "Val acc and loss are 0.7713 and 0.62784934\n",
            "Processing Epoch 3405\n",
            "Training acc and loss are 0.7807 and 0.62367815\n",
            "Val acc and loss are 0.7713 and 0.627805\n",
            "Processing Epoch 3406\n",
            "Training acc and loss are 0.78072 and 0.62363476\n",
            "Val acc and loss are 0.7713 and 0.6277628\n",
            "Processing Epoch 3407\n",
            "Training acc and loss are 0.78072 and 0.6235894\n",
            "Val acc and loss are 0.7713 and 0.62771964\n",
            "Processing Epoch 3408\n",
            "Training acc and loss are 0.78074 and 0.6235445\n",
            "Val acc and loss are 0.7713 and 0.6276759\n",
            "Processing Epoch 3409\n",
            "Training acc and loss are 0.78078 and 0.62350196\n",
            "Val acc and loss are 0.7713 and 0.6276349\n",
            "Processing Epoch 3410\n",
            "Training acc and loss are 0.78076 and 0.6234578\n",
            "Val acc and loss are 0.7714 and 0.6275928\n",
            "Processing Epoch 3411\n",
            "Training acc and loss are 0.78078 and 0.62341315\n",
            "Val acc and loss are 0.7714 and 0.62754905\n",
            "Processing Epoch 3412\n",
            "Training acc and loss are 0.78078 and 0.62336737\n",
            "Val acc and loss are 0.7714 and 0.62750477\n",
            "Processing Epoch 3413\n",
            "Training acc and loss are 0.78078 and 0.6233208\n",
            "Val acc and loss are 0.7715 and 0.627459\n",
            "Processing Epoch 3414\n",
            "Training acc and loss are 0.78076 and 0.6232754\n",
            "Val acc and loss are 0.7715 and 0.6274149\n",
            "Processing Epoch 3415\n",
            "Training acc and loss are 0.78076 and 0.6232306\n",
            "Val acc and loss are 0.7715 and 0.6273711\n",
            "Processing Epoch 3416\n",
            "Training acc and loss are 0.78078 and 0.6231831\n",
            "Val acc and loss are 0.7716 and 0.62732446\n",
            "Processing Epoch 3417\n",
            "Training acc and loss are 0.78076 and 0.62313795\n",
            "Val acc and loss are 0.7716 and 0.62728065\n",
            "Processing Epoch 3418\n",
            "Training acc and loss are 0.7808 and 0.62309295\n",
            "Val acc and loss are 0.7716 and 0.62723714\n",
            "Processing Epoch 3419\n",
            "Training acc and loss are 0.78078 and 0.6230526\n",
            "Val acc and loss are 0.7716 and 0.62719846\n",
            "Processing Epoch 3420\n",
            "Training acc and loss are 0.7808 and 0.62301135\n",
            "Val acc and loss are 0.7716 and 0.6271585\n",
            "Processing Epoch 3421\n",
            "Training acc and loss are 0.7808 and 0.6229684\n",
            "Val acc and loss are 0.7716 and 0.627117\n",
            "Processing Epoch 3422\n",
            "Training acc and loss are 0.78082 and 0.6229209\n",
            "Val acc and loss are 0.7717 and 0.6270713\n",
            "Processing Epoch 3423\n",
            "Training acc and loss are 0.78082 and 0.6228756\n",
            "Val acc and loss are 0.7717 and 0.62702763\n",
            "Processing Epoch 3424\n",
            "Training acc and loss are 0.78084 and 0.62283146\n",
            "Val acc and loss are 0.7718 and 0.6269852\n",
            "Processing Epoch 3425\n",
            "Training acc and loss are 0.78088 and 0.6227832\n",
            "Val acc and loss are 0.7718 and 0.6269387\n",
            "Processing Epoch 3426\n",
            "Training acc and loss are 0.78086 and 0.6227408\n",
            "Val acc and loss are 0.7718 and 0.62689763\n",
            "Processing Epoch 3427\n",
            "Training acc and loss are 0.7809 and 0.6226942\n",
            "Val acc and loss are 0.7719 and 0.6268523\n",
            "Processing Epoch 3428\n",
            "Training acc and loss are 0.7809 and 0.62264967\n",
            "Val acc and loss are 0.7719 and 0.6268095\n",
            "Processing Epoch 3429\n",
            "Training acc and loss are 0.78092 and 0.6226055\n",
            "Val acc and loss are 0.7719 and 0.6267659\n",
            "Processing Epoch 3430\n",
            "Training acc and loss are 0.78096 and 0.62255865\n",
            "Val acc and loss are 0.772 and 0.62672\n",
            "Processing Epoch 3431\n",
            "Training acc and loss are 0.78102 and 0.62251073\n",
            "Val acc and loss are 0.7719 and 0.62667334\n",
            "Processing Epoch 3432\n",
            "Training acc and loss are 0.78106 and 0.6224659\n",
            "Val acc and loss are 0.772 and 0.62663\n",
            "Processing Epoch 3433\n",
            "Training acc and loss are 0.78108 and 0.62241954\n",
            "Val acc and loss are 0.772 and 0.6265852\n",
            "Processing Epoch 3434\n",
            "Training acc and loss are 0.78108 and 0.6223743\n",
            "Val acc and loss are 0.772 and 0.6265415\n",
            "Processing Epoch 3435\n",
            "Training acc and loss are 0.78112 and 0.6223278\n",
            "Val acc and loss are 0.772 and 0.6264971\n",
            "Processing Epoch 3436\n",
            "Training acc and loss are 0.78116 and 0.62228185\n",
            "Val acc and loss are 0.7719 and 0.62645215\n",
            "Processing Epoch 3437\n",
            "Training acc and loss are 0.78116 and 0.62223476\n",
            "Val acc and loss are 0.7719 and 0.6264061\n",
            "Processing Epoch 3438\n",
            "Training acc and loss are 0.7812 and 0.6221889\n",
            "Val acc and loss are 0.7719 and 0.6263613\n",
            "Processing Epoch 3439\n",
            "Training acc and loss are 0.7812 and 0.6221453\n",
            "Val acc and loss are 0.7719 and 0.6263191\n",
            "Processing Epoch 3440\n",
            "Training acc and loss are 0.78124 and 0.6220986\n",
            "Val acc and loss are 0.7719 and 0.62627417\n",
            "Processing Epoch 3441\n",
            "Training acc and loss are 0.78126 and 0.62205184\n",
            "Val acc and loss are 0.7719 and 0.62622863\n",
            "Processing Epoch 3442\n",
            "Training acc and loss are 0.7813 and 0.6220062\n",
            "Val acc and loss are 0.7719 and 0.62618464\n",
            "Processing Epoch 3443\n",
            "Training acc and loss are 0.78132 and 0.621963\n",
            "Val acc and loss are 0.7719 and 0.6261428\n",
            "Processing Epoch 3444\n",
            "Training acc and loss are 0.78132 and 0.621922\n",
            "Val acc and loss are 0.7721 and 0.6261029\n",
            "Processing Epoch 3445\n",
            "Training acc and loss are 0.78132 and 0.6218787\n",
            "Val acc and loss are 0.7721 and 0.62606096\n",
            "Processing Epoch 3446\n",
            "Training acc and loss are 0.78132 and 0.62183446\n",
            "Val acc and loss are 0.7722 and 0.62601787\n",
            "Processing Epoch 3447\n",
            "Training acc and loss are 0.78132 and 0.6217914\n",
            "Val acc and loss are 0.7722 and 0.62597656\n",
            "Processing Epoch 3448\n",
            "Training acc and loss are 0.78132 and 0.62174386\n",
            "Val acc and loss are 0.7722 and 0.6259301\n",
            "Processing Epoch 3449\n",
            "Training acc and loss are 0.7813 and 0.62169826\n",
            "Val acc and loss are 0.7722 and 0.62588555\n",
            "Processing Epoch 3450\n",
            "Training acc and loss are 0.78134 and 0.62164974\n",
            "Val acc and loss are 0.7721 and 0.62583864\n",
            "Processing Epoch 3451\n",
            "Training acc and loss are 0.78134 and 0.6216036\n",
            "Val acc and loss are 0.7721 and 0.6257938\n",
            "Processing Epoch 3452\n",
            "Training acc and loss are 0.78134 and 0.62155944\n",
            "Val acc and loss are 0.7721 and 0.62575144\n",
            "Processing Epoch 3453\n",
            "Training acc and loss are 0.78134 and 0.6215144\n",
            "Val acc and loss are 0.7721 and 0.6257074\n",
            "Processing Epoch 3454\n",
            "Training acc and loss are 0.78136 and 0.62146914\n",
            "Val acc and loss are 0.7721 and 0.6256637\n",
            "Processing Epoch 3455\n",
            "Training acc and loss are 0.78136 and 0.6214241\n",
            "Val acc and loss are 0.7721 and 0.62561935\n",
            "Processing Epoch 3456\n",
            "Training acc and loss are 0.78142 and 0.62138\n",
            "Val acc and loss are 0.7721 and 0.6255767\n",
            "Processing Epoch 3457\n",
            "Training acc and loss are 0.7814 and 0.62133497\n",
            "Val acc and loss are 0.7721 and 0.6255334\n",
            "Processing Epoch 3458\n",
            "Training acc and loss are 0.7814 and 0.62129027\n",
            "Val acc and loss are 0.7721 and 0.62549007\n",
            "Processing Epoch 3459\n",
            "Training acc and loss are 0.7814 and 0.62124646\n",
            "Val acc and loss are 0.7721 and 0.6254473\n",
            "Processing Epoch 3460\n",
            "Training acc and loss are 0.7814 and 0.6212032\n",
            "Val acc and loss are 0.7722 and 0.62540495\n",
            "Processing Epoch 3461\n",
            "Training acc and loss are 0.78142 and 0.62115824\n",
            "Val acc and loss are 0.7722 and 0.62536144\n",
            "Processing Epoch 3462\n",
            "Training acc and loss are 0.78142 and 0.6211162\n",
            "Val acc and loss are 0.7722 and 0.625321\n",
            "Processing Epoch 3463\n",
            "Training acc and loss are 0.78144 and 0.6210722\n",
            "Val acc and loss are 0.7722 and 0.6252774\n",
            "Processing Epoch 3464\n",
            "Training acc and loss are 0.78146 and 0.6210268\n",
            "Val acc and loss are 0.7722 and 0.6252336\n",
            "Processing Epoch 3465\n",
            "Training acc and loss are 0.78146 and 0.62098217\n",
            "Val acc and loss are 0.7722 and 0.625191\n",
            "Processing Epoch 3466\n",
            "Training acc and loss are 0.78146 and 0.6209379\n",
            "Val acc and loss are 0.7722 and 0.6251474\n",
            "Processing Epoch 3467\n",
            "Training acc and loss are 0.78146 and 0.62089944\n",
            "Val acc and loss are 0.7722 and 0.62511086\n",
            "Processing Epoch 3468\n",
            "Training acc and loss are 0.7815 and 0.6208553\n",
            "Val acc and loss are 0.7723 and 0.62506896\n",
            "Processing Epoch 3469\n",
            "Training acc and loss are 0.78148 and 0.62081075\n",
            "Val acc and loss are 0.7723 and 0.6250256\n",
            "Processing Epoch 3470\n",
            "Training acc and loss are 0.78148 and 0.6207674\n",
            "Val acc and loss are 0.7723 and 0.6249834\n",
            "Processing Epoch 3471\n",
            "Training acc and loss are 0.7815 and 0.6207243\n",
            "Val acc and loss are 0.7724 and 0.624942\n",
            "Processing Epoch 3472\n",
            "Training acc and loss are 0.7815 and 0.62068343\n",
            "Val acc and loss are 0.7726 and 0.6249028\n",
            "Processing Epoch 3473\n",
            "Training acc and loss are 0.7815 and 0.6206407\n",
            "Val acc and loss are 0.7727 and 0.6248615\n",
            "Processing Epoch 3474\n",
            "Training acc and loss are 0.78154 and 0.6205966\n",
            "Val acc and loss are 0.7727 and 0.62481844\n",
            "Processing Epoch 3475\n",
            "Training acc and loss are 0.78156 and 0.6205526\n",
            "Val acc and loss are 0.7727 and 0.62477636\n",
            "Processing Epoch 3476\n",
            "Training acc and loss are 0.78158 and 0.62050825\n",
            "Val acc and loss are 0.7727 and 0.6247333\n",
            "Processing Epoch 3477\n",
            "Training acc and loss are 0.78158 and 0.6204642\n",
            "Val acc and loss are 0.7727 and 0.624691\n",
            "Processing Epoch 3478\n",
            "Training acc and loss are 0.78158 and 0.6204191\n",
            "Val acc and loss are 0.7727 and 0.62464744\n",
            "Processing Epoch 3479\n",
            "Training acc and loss are 0.78158 and 0.6203703\n",
            "Val acc and loss are 0.7727 and 0.6246002\n",
            "Processing Epoch 3480\n",
            "Training acc and loss are 0.78162 and 0.6203277\n",
            "Val acc and loss are 0.7727 and 0.6245591\n",
            "Processing Epoch 3481\n",
            "Training acc and loss are 0.78166 and 0.6202816\n",
            "Val acc and loss are 0.7727 and 0.6245148\n",
            "Processing Epoch 3482\n",
            "Training acc and loss are 0.7817 and 0.62023604\n",
            "Val acc and loss are 0.7727 and 0.6244701\n",
            "Processing Epoch 3483\n",
            "Training acc and loss are 0.78172 and 0.6201926\n",
            "Val acc and loss are 0.7727 and 0.6244282\n",
            "Processing Epoch 3484\n",
            "Training acc and loss are 0.78174 and 0.62014955\n",
            "Val acc and loss are 0.7727 and 0.62438655\n",
            "Processing Epoch 3485\n",
            "Training acc and loss are 0.78176 and 0.6201066\n",
            "Val acc and loss are 0.7727 and 0.6243445\n",
            "Processing Epoch 3486\n",
            "Training acc and loss are 0.7818 and 0.62006164\n",
            "Val acc and loss are 0.7728 and 0.6243018\n",
            "Processing Epoch 3487\n",
            "Training acc and loss are 0.78182 and 0.62001663\n",
            "Val acc and loss are 0.7727 and 0.62425804\n",
            "Processing Epoch 3488\n",
            "Training acc and loss are 0.78182 and 0.61997265\n",
            "Val acc and loss are 0.7728 and 0.6242153\n",
            "Processing Epoch 3489\n",
            "Training acc and loss are 0.78182 and 0.6199288\n",
            "Val acc and loss are 0.7728 and 0.62417316\n",
            "Processing Epoch 3490\n",
            "Training acc and loss are 0.78188 and 0.6198868\n",
            "Val acc and loss are 0.7728 and 0.6241328\n",
            "Processing Epoch 3491\n",
            "Training acc and loss are 0.78194 and 0.6198432\n",
            "Val acc and loss are 0.7728 and 0.6240908\n",
            "Processing Epoch 3492\n",
            "Training acc and loss are 0.78194 and 0.6197977\n",
            "Val acc and loss are 0.7728 and 0.62404686\n",
            "Processing Epoch 3493\n",
            "Training acc and loss are 0.78198 and 0.6197521\n",
            "Val acc and loss are 0.7728 and 0.62400186\n",
            "Processing Epoch 3494\n",
            "Training acc and loss are 0.78196 and 0.6197063\n",
            "Val acc and loss are 0.7728 and 0.6239582\n",
            "Processing Epoch 3495\n",
            "Training acc and loss are 0.78202 and 0.61966056\n",
            "Val acc and loss are 0.7728 and 0.62391394\n",
            "Processing Epoch 3496\n",
            "Training acc and loss are 0.78204 and 0.6196164\n",
            "Val acc and loss are 0.7728 and 0.6238709\n",
            "Processing Epoch 3497\n",
            "Training acc and loss are 0.78206 and 0.6195695\n",
            "Val acc and loss are 0.7728 and 0.6238255\n",
            "Processing Epoch 3498\n",
            "Training acc and loss are 0.78206 and 0.61952674\n",
            "Val acc and loss are 0.7728 and 0.623784\n",
            "Processing Epoch 3499\n",
            "Training acc and loss are 0.78208 and 0.6194807\n",
            "Val acc and loss are 0.7728 and 0.62373906\n",
            "Processing Epoch 3500\n",
            "Training acc and loss are 0.78208 and 0.619433\n",
            "Val acc and loss are 0.7727 and 0.623693\n",
            "Processing Epoch 3501\n",
            "Training acc and loss are 0.78208 and 0.6193904\n",
            "Val acc and loss are 0.7728 and 0.62365174\n",
            "Processing Epoch 3502\n",
            "Training acc and loss are 0.78208 and 0.61934495\n",
            "Val acc and loss are 0.7728 and 0.62360764\n",
            "Processing Epoch 3503\n",
            "Training acc and loss are 0.78208 and 0.61930144\n",
            "Val acc and loss are 0.7728 and 0.6235659\n",
            "Processing Epoch 3504\n",
            "Training acc and loss are 0.78208 and 0.61926186\n",
            "Val acc and loss are 0.7728 and 0.6235274\n",
            "Processing Epoch 3505\n",
            "Training acc and loss are 0.7821 and 0.6192198\n",
            "Val acc and loss are 0.7728 and 0.6234853\n",
            "Processing Epoch 3506\n",
            "Training acc and loss are 0.78214 and 0.61917335\n",
            "Val acc and loss are 0.7728 and 0.62344\n",
            "Processing Epoch 3507\n",
            "Training acc and loss are 0.7822 and 0.6191265\n",
            "Val acc and loss are 0.7727 and 0.6233944\n",
            "Processing Epoch 3508\n",
            "Training acc and loss are 0.78222 and 0.61908364\n",
            "Val acc and loss are 0.7728 and 0.62335354\n",
            "Processing Epoch 3509\n",
            "Training acc and loss are 0.78222 and 0.61904114\n",
            "Val acc and loss are 0.7728 and 0.6233123\n",
            "Processing Epoch 3510\n",
            "Training acc and loss are 0.78226 and 0.61899924\n",
            "Val acc and loss are 0.7728 and 0.62327147\n",
            "Processing Epoch 3511\n",
            "Training acc and loss are 0.78226 and 0.61895454\n",
            "Val acc and loss are 0.7729 and 0.6232285\n",
            "Processing Epoch 3512\n",
            "Training acc and loss are 0.78228 and 0.61891115\n",
            "Val acc and loss are 0.7729 and 0.6231866\n",
            "Processing Epoch 3513\n",
            "Training acc and loss are 0.78228 and 0.6188667\n",
            "Val acc and loss are 0.7729 and 0.62314343\n",
            "Processing Epoch 3514\n",
            "Training acc and loss are 0.7823 and 0.61882526\n",
            "Val acc and loss are 0.7729 and 0.6231038\n",
            "Processing Epoch 3515\n",
            "Training acc and loss are 0.7823 and 0.6187819\n",
            "Val acc and loss are 0.7729 and 0.62306225\n",
            "Processing Epoch 3516\n",
            "Training acc and loss are 0.7823 and 0.61873525\n",
            "Val acc and loss are 0.7729 and 0.6230171\n",
            "Processing Epoch 3517\n",
            "Training acc and loss are 0.78232 and 0.6186923\n",
            "Val acc and loss are 0.7729 and 0.6229747\n",
            "Processing Epoch 3518\n",
            "Training acc and loss are 0.78232 and 0.6186473\n",
            "Val acc and loss are 0.7729 and 0.62293077\n",
            "Processing Epoch 3519\n",
            "Training acc and loss are 0.78232 and 0.6186049\n",
            "Val acc and loss are 0.7729 and 0.622889\n",
            "Processing Epoch 3520\n",
            "Training acc and loss are 0.78236 and 0.6185657\n",
            "Val acc and loss are 0.773 and 0.62285054\n",
            "Processing Epoch 3521\n",
            "Training acc and loss are 0.78238 and 0.61852026\n",
            "Val acc and loss are 0.773 and 0.6228063\n",
            "Processing Epoch 3522\n",
            "Training acc and loss are 0.78238 and 0.6184769\n",
            "Val acc and loss are 0.773 and 0.6227646\n",
            "Processing Epoch 3523\n",
            "Training acc and loss are 0.78238 and 0.61843026\n",
            "Val acc and loss are 0.7729 and 0.62271917\n",
            "Processing Epoch 3524\n",
            "Training acc and loss are 0.78238 and 0.6183833\n",
            "Val acc and loss are 0.7728 and 0.62267345\n",
            "Processing Epoch 3525\n",
            "Training acc and loss are 0.78244 and 0.61834174\n",
            "Val acc and loss are 0.7729 and 0.6226336\n",
            "Processing Epoch 3526\n",
            "Training acc and loss are 0.78244 and 0.61829907\n",
            "Val acc and loss are 0.7729 and 0.6225925\n",
            "Processing Epoch 3527\n",
            "Training acc and loss are 0.78242 and 0.6182529\n",
            "Val acc and loss are 0.7729 and 0.62254834\n",
            "Processing Epoch 3528\n",
            "Training acc and loss are 0.78242 and 0.61821115\n",
            "Val acc and loss are 0.7729 and 0.6225072\n",
            "Processing Epoch 3529\n",
            "Training acc and loss are 0.78246 and 0.6181683\n",
            "Val acc and loss are 0.7729 and 0.62246567\n",
            "Processing Epoch 3530\n",
            "Training acc and loss are 0.78248 and 0.6181244\n",
            "Val acc and loss are 0.7729 and 0.62242323\n",
            "Processing Epoch 3531\n",
            "Training acc and loss are 0.78252 and 0.6180801\n",
            "Val acc and loss are 0.7729 and 0.62238\n",
            "Processing Epoch 3532\n",
            "Training acc and loss are 0.78252 and 0.6180393\n",
            "Val acc and loss are 0.7729 and 0.6223398\n",
            "Processing Epoch 3533\n",
            "Training acc and loss are 0.78254 and 0.61799675\n",
            "Val acc and loss are 0.7729 and 0.6222987\n",
            "Processing Epoch 3534\n",
            "Training acc and loss are 0.78252 and 0.61795235\n",
            "Val acc and loss are 0.7728 and 0.62225544\n",
            "Processing Epoch 3535\n",
            "Training acc and loss are 0.78252 and 0.6179101\n",
            "Val acc and loss are 0.7729 and 0.6222144\n",
            "Processing Epoch 3536\n",
            "Training acc and loss are 0.78252 and 0.61786896\n",
            "Val acc and loss are 0.7731 and 0.6221743\n",
            "Processing Epoch 3537\n",
            "Training acc and loss are 0.78254 and 0.617826\n",
            "Val acc and loss are 0.7731 and 0.6221327\n",
            "Processing Epoch 3538\n",
            "Training acc and loss are 0.78254 and 0.61778444\n",
            "Val acc and loss are 0.7731 and 0.62209284\n",
            "Processing Epoch 3539\n",
            "Training acc and loss are 0.78256 and 0.61774445\n",
            "Val acc and loss are 0.7732 and 0.62205404\n",
            "Processing Epoch 3540\n",
            "Training acc and loss are 0.78256 and 0.6177019\n",
            "Val acc and loss are 0.7731 and 0.6220133\n",
            "Processing Epoch 3541\n",
            "Training acc and loss are 0.78258 and 0.6176602\n",
            "Val acc and loss are 0.7733 and 0.6219733\n",
            "Processing Epoch 3542\n",
            "Training acc and loss are 0.78262 and 0.6176172\n",
            "Val acc and loss are 0.7734 and 0.6219317\n",
            "Processing Epoch 3543\n",
            "Training acc and loss are 0.78266 and 0.6175743\n",
            "Val acc and loss are 0.7734 and 0.62188977\n",
            "Processing Epoch 3544\n",
            "Training acc and loss are 0.7827 and 0.61752975\n",
            "Val acc and loss are 0.7734 and 0.6218467\n",
            "Processing Epoch 3545\n",
            "Training acc and loss are 0.7827 and 0.6174852\n",
            "Val acc and loss are 0.7734 and 0.6218034\n",
            "Processing Epoch 3546\n",
            "Training acc and loss are 0.78274 and 0.61743885\n",
            "Val acc and loss are 0.7734 and 0.6217578\n",
            "Processing Epoch 3547\n",
            "Training acc and loss are 0.78274 and 0.6173999\n",
            "Val acc and loss are 0.7734 and 0.62171966\n",
            "Processing Epoch 3548\n",
            "Training acc and loss are 0.78274 and 0.6173567\n",
            "Val acc and loss are 0.7734 and 0.62167805\n",
            "Processing Epoch 3549\n",
            "Training acc and loss are 0.78276 and 0.6173145\n",
            "Val acc and loss are 0.7734 and 0.6216378\n",
            "Processing Epoch 3550\n",
            "Training acc and loss are 0.78276 and 0.61726964\n",
            "Val acc and loss are 0.7734 and 0.6215942\n",
            "Processing Epoch 3551\n",
            "Training acc and loss are 0.78274 and 0.6172242\n",
            "Val acc and loss are 0.7734 and 0.62154996\n",
            "Processing Epoch 3552\n",
            "Training acc and loss are 0.78276 and 0.61717623\n",
            "Val acc and loss are 0.7735 and 0.6215031\n",
            "Processing Epoch 3553\n",
            "Training acc and loss are 0.78276 and 0.6171346\n",
            "Val acc and loss are 0.7735 and 0.62146336\n",
            "Processing Epoch 3554\n",
            "Training acc and loss are 0.78278 and 0.6170917\n",
            "Val acc and loss are 0.7735 and 0.62142146\n",
            "Processing Epoch 3555\n",
            "Training acc and loss are 0.7828 and 0.6170483\n",
            "Val acc and loss are 0.7735 and 0.6213796\n",
            "Processing Epoch 3556\n",
            "Training acc and loss are 0.78282 and 0.6170035\n",
            "Val acc and loss are 0.7735 and 0.62133586\n",
            "Processing Epoch 3557\n",
            "Training acc and loss are 0.78284 and 0.61695796\n",
            "Val acc and loss are 0.7735 and 0.6212915\n",
            "Processing Epoch 3558\n",
            "Training acc and loss are 0.78286 and 0.61691326\n",
            "Val acc and loss are 0.7734 and 0.62124795\n",
            "Processing Epoch 3559\n",
            "Training acc and loss are 0.78292 and 0.6168695\n",
            "Val acc and loss are 0.7736 and 0.62120557\n",
            "Processing Epoch 3560\n",
            "Training acc and loss are 0.78292 and 0.61682796\n",
            "Val acc and loss are 0.7736 and 0.62116545\n",
            "Processing Epoch 3561\n",
            "Training acc and loss are 0.78292 and 0.6167839\n",
            "Val acc and loss are 0.7736 and 0.6211226\n",
            "Processing Epoch 3562\n",
            "Training acc and loss are 0.78296 and 0.6167429\n",
            "Val acc and loss are 0.7736 and 0.6210834\n",
            "Processing Epoch 3563\n",
            "Training acc and loss are 0.78298 and 0.61669445\n",
            "Val acc and loss are 0.7736 and 0.6210366\n",
            "Processing Epoch 3564\n",
            "Training acc and loss are 0.783 and 0.6166534\n",
            "Val acc and loss are 0.7736 and 0.62099624\n",
            "Processing Epoch 3565\n",
            "Training acc and loss are 0.783 and 0.6166104\n",
            "Val acc and loss are 0.7736 and 0.62095505\n",
            "Processing Epoch 3566\n",
            "Training acc and loss are 0.78304 and 0.6165676\n",
            "Val acc and loss are 0.7737 and 0.6209138\n",
            "Processing Epoch 3567\n",
            "Training acc and loss are 0.78306 and 0.61652607\n",
            "Val acc and loss are 0.7737 and 0.6208744\n",
            "Processing Epoch 3568\n",
            "Training acc and loss are 0.7831 and 0.61648357\n",
            "Val acc and loss are 0.7737 and 0.620834\n",
            "Processing Epoch 3569\n",
            "Training acc and loss are 0.78308 and 0.6164384\n",
            "Val acc and loss are 0.7737 and 0.62079054\n",
            "Processing Epoch 3570\n",
            "Training acc and loss are 0.78308 and 0.61639655\n",
            "Val acc and loss are 0.7737 and 0.62075007\n",
            "Processing Epoch 3571\n",
            "Training acc and loss are 0.78308 and 0.6163519\n",
            "Val acc and loss are 0.7737 and 0.62070674\n",
            "Processing Epoch 3572\n",
            "Training acc and loss are 0.78308 and 0.61631006\n",
            "Val acc and loss are 0.7737 and 0.62066644\n",
            "Processing Epoch 3573\n",
            "Training acc and loss are 0.7831 and 0.61626667\n",
            "Val acc and loss are 0.7737 and 0.62062365\n",
            "Processing Epoch 3574\n",
            "Training acc and loss are 0.78312 and 0.61622363\n",
            "Val acc and loss are 0.7737 and 0.62058216\n",
            "Processing Epoch 3575\n",
            "Training acc and loss are 0.78312 and 0.61618185\n",
            "Val acc and loss are 0.7737 and 0.62054193\n",
            "Processing Epoch 3576\n",
            "Training acc and loss are 0.78316 and 0.6161368\n",
            "Val acc and loss are 0.7737 and 0.62049794\n",
            "Processing Epoch 3577\n",
            "Training acc and loss are 0.78318 and 0.6160968\n",
            "Val acc and loss are 0.7739 and 0.6204592\n",
            "Processing Epoch 3578\n",
            "Training acc and loss are 0.7832 and 0.6160549\n",
            "Val acc and loss are 0.7739 and 0.6204182\n",
            "Processing Epoch 3579\n",
            "Training acc and loss are 0.7832 and 0.6160123\n",
            "Val acc and loss are 0.7739 and 0.6203768\n",
            "Processing Epoch 3580\n",
            "Training acc and loss are 0.7832 and 0.61597\n",
            "Val acc and loss are 0.7739 and 0.6203358\n",
            "Processing Epoch 3581\n",
            "Training acc and loss are 0.78322 and 0.61592805\n",
            "Val acc and loss are 0.7739 and 0.62029505\n",
            "Processing Epoch 3582\n",
            "Training acc and loss are 0.78322 and 0.6158861\n",
            "Val acc and loss are 0.7739 and 0.6202546\n",
            "Processing Epoch 3583\n",
            "Training acc and loss are 0.78324 and 0.6158451\n",
            "Val acc and loss are 0.774 and 0.6202158\n",
            "Processing Epoch 3584\n",
            "Training acc and loss are 0.78328 and 0.61580294\n",
            "Val acc and loss are 0.774 and 0.6201749\n",
            "Processing Epoch 3585\n",
            "Training acc and loss are 0.7833 and 0.6157588\n",
            "Val acc and loss are 0.7738 and 0.6201315\n",
            "Processing Epoch 3586\n",
            "Training acc and loss are 0.78332 and 0.61571443\n",
            "Val acc and loss are 0.7738 and 0.6200886\n",
            "Processing Epoch 3587\n",
            "Training acc and loss are 0.78334 and 0.6156724\n",
            "Val acc and loss are 0.7738 and 0.62004757\n",
            "Processing Epoch 3588\n",
            "Training acc and loss are 0.78336 and 0.6156321\n",
            "Val acc and loss are 0.7739 and 0.6200089\n",
            "Processing Epoch 3589\n",
            "Training acc and loss are 0.78336 and 0.61559045\n",
            "Val acc and loss are 0.7739 and 0.61996925\n",
            "Processing Epoch 3590\n",
            "Training acc and loss are 0.7834 and 0.6155492\n",
            "Val acc and loss are 0.7739 and 0.6199293\n",
            "Processing Epoch 3591\n",
            "Training acc and loss are 0.78344 and 0.61551\n",
            "Val acc and loss are 0.7739 and 0.6198918\n",
            "Processing Epoch 3592\n",
            "Training acc and loss are 0.78342 and 0.6154659\n",
            "Val acc and loss are 0.774 and 0.61984915\n",
            "Processing Epoch 3593\n",
            "Training acc and loss are 0.78344 and 0.61542714\n",
            "Val acc and loss are 0.774 and 0.61981195\n",
            "Processing Epoch 3594\n",
            "Training acc and loss are 0.78348 and 0.6153835\n",
            "Val acc and loss are 0.774 and 0.61977\n",
            "Processing Epoch 3595\n",
            "Training acc and loss are 0.78346 and 0.6153436\n",
            "Val acc and loss are 0.774 and 0.61973137\n",
            "Processing Epoch 3596\n",
            "Training acc and loss are 0.78348 and 0.61530083\n",
            "Val acc and loss are 0.7741 and 0.61969006\n",
            "Processing Epoch 3597\n",
            "Training acc and loss are 0.78354 and 0.615257\n",
            "Val acc and loss are 0.7741 and 0.6196473\n",
            "Processing Epoch 3598\n",
            "Training acc and loss are 0.78358 and 0.6152156\n",
            "Val acc and loss are 0.7741 and 0.6196072\n",
            "Processing Epoch 3599\n",
            "Training acc and loss are 0.78358 and 0.6151761\n",
            "Val acc and loss are 0.7741 and 0.6195694\n",
            "Processing Epoch 3600\n",
            "Training acc and loss are 0.78362 and 0.6151363\n",
            "Val acc and loss are 0.7742 and 0.6195311\n",
            "Processing Epoch 3601\n",
            "Training acc and loss are 0.7836 and 0.6150974\n",
            "Val acc and loss are 0.7742 and 0.61949295\n",
            "Processing Epoch 3602\n",
            "Training acc and loss are 0.78366 and 0.6150554\n",
            "Val acc and loss are 0.7742 and 0.6194521\n",
            "Processing Epoch 3603\n",
            "Training acc and loss are 0.78368 and 0.61501366\n",
            "Val acc and loss are 0.7742 and 0.6194121\n",
            "Processing Epoch 3604\n",
            "Training acc and loss are 0.78368 and 0.6149708\n",
            "Val acc and loss are 0.7742 and 0.61937106\n",
            "Processing Epoch 3605\n",
            "Training acc and loss are 0.78366 and 0.6149282\n",
            "Val acc and loss are 0.7742 and 0.61932915\n",
            "Processing Epoch 3606\n",
            "Training acc and loss are 0.78368 and 0.6148828\n",
            "Val acc and loss are 0.7742 and 0.6192854\n",
            "Processing Epoch 3607\n",
            "Training acc and loss are 0.7837 and 0.6148444\n",
            "Val acc and loss are 0.7742 and 0.61924773\n",
            "Processing Epoch 3608\n",
            "Training acc and loss are 0.78372 and 0.6147994\n",
            "Val acc and loss are 0.7742 and 0.619204\n",
            "Processing Epoch 3609\n",
            "Training acc and loss are 0.7837 and 0.61475974\n",
            "Val acc and loss are 0.7742 and 0.61916596\n",
            "Processing Epoch 3610\n",
            "Training acc and loss are 0.78372 and 0.6147176\n",
            "Val acc and loss are 0.7742 and 0.6191252\n",
            "Processing Epoch 3611\n",
            "Training acc and loss are 0.78374 and 0.61467296\n",
            "Val acc and loss are 0.7742 and 0.6190817\n",
            "Processing Epoch 3612\n",
            "Training acc and loss are 0.78376 and 0.6146314\n",
            "Val acc and loss are 0.7742 and 0.6190421\n",
            "Processing Epoch 3613\n",
            "Training acc and loss are 0.78378 and 0.61458766\n",
            "Val acc and loss are 0.7742 and 0.6189999\n",
            "Processing Epoch 3614\n",
            "Training acc and loss are 0.7838 and 0.61454654\n",
            "Val acc and loss are 0.7742 and 0.61895984\n",
            "Processing Epoch 3615\n",
            "Training acc and loss are 0.78382 and 0.6145025\n",
            "Val acc and loss are 0.7742 and 0.6189177\n",
            "Processing Epoch 3616\n",
            "Training acc and loss are 0.78384 and 0.6144635\n",
            "Val acc and loss are 0.7742 and 0.61888\n",
            "Processing Epoch 3617\n",
            "Training acc and loss are 0.78386 and 0.61442006\n",
            "Val acc and loss are 0.7742 and 0.61883837\n",
            "Processing Epoch 3618\n",
            "Training acc and loss are 0.78386 and 0.61437917\n",
            "Val acc and loss are 0.7742 and 0.6187994\n",
            "Processing Epoch 3619\n",
            "Training acc and loss are 0.78388 and 0.6143367\n",
            "Val acc and loss are 0.7742 and 0.6187581\n",
            "Processing Epoch 3620\n",
            "Training acc and loss are 0.78388 and 0.6142947\n",
            "Val acc and loss are 0.7744 and 0.6187173\n",
            "Processing Epoch 3621\n",
            "Training acc and loss are 0.7839 and 0.6142531\n",
            "Val acc and loss are 0.7744 and 0.61867666\n",
            "Processing Epoch 3622\n",
            "Training acc and loss are 0.78388 and 0.6142107\n",
            "Val acc and loss are 0.7744 and 0.61863476\n",
            "Processing Epoch 3623\n",
            "Training acc and loss are 0.7839 and 0.61417043\n",
            "Val acc and loss are 0.7744 and 0.6185963\n",
            "Processing Epoch 3624\n",
            "Training acc and loss are 0.7839 and 0.614129\n",
            "Val acc and loss are 0.7744 and 0.6185565\n",
            "Processing Epoch 3625\n",
            "Training acc and loss are 0.78396 and 0.61408734\n",
            "Val acc and loss are 0.7744 and 0.6185162\n",
            "Processing Epoch 3626\n",
            "Training acc and loss are 0.78398 and 0.6140438\n",
            "Val acc and loss are 0.7744 and 0.6184741\n",
            "Processing Epoch 3627\n",
            "Training acc and loss are 0.78402 and 0.61400014\n",
            "Val acc and loss are 0.7744 and 0.6184316\n",
            "Processing Epoch 3628\n",
            "Training acc and loss are 0.78402 and 0.6139601\n",
            "Val acc and loss are 0.7744 and 0.6183929\n",
            "Processing Epoch 3629\n",
            "Training acc and loss are 0.78404 and 0.6139168\n",
            "Val acc and loss are 0.7744 and 0.6183505\n",
            "Processing Epoch 3630\n",
            "Training acc and loss are 0.78406 and 0.6138721\n",
            "Val acc and loss are 0.7744 and 0.6183071\n",
            "Processing Epoch 3631\n",
            "Training acc and loss are 0.78406 and 0.61383086\n",
            "Val acc and loss are 0.7744 and 0.6182673\n",
            "Processing Epoch 3632\n",
            "Training acc and loss are 0.78406 and 0.6137898\n",
            "Val acc and loss are 0.7744 and 0.618228\n",
            "Processing Epoch 3633\n",
            "Training acc and loss are 0.78408 and 0.6137483\n",
            "Val acc and loss are 0.7744 and 0.6181879\n",
            "Processing Epoch 3634\n",
            "Training acc and loss are 0.7841 and 0.61370826\n",
            "Val acc and loss are 0.7744 and 0.61814964\n",
            "Processing Epoch 3635\n",
            "Training acc and loss are 0.78412 and 0.6136653\n",
            "Val acc and loss are 0.7744 and 0.61810786\n",
            "Processing Epoch 3636\n",
            "Training acc and loss are 0.78412 and 0.6136221\n",
            "Val acc and loss are 0.7744 and 0.61806583\n",
            "Processing Epoch 3637\n",
            "Training acc and loss are 0.78416 and 0.6135815\n",
            "Val acc and loss are 0.7744 and 0.6180267\n",
            "Processing Epoch 3638\n",
            "Training acc and loss are 0.78414 and 0.61353624\n",
            "Val acc and loss are 0.7744 and 0.61798257\n",
            "Processing Epoch 3639\n",
            "Training acc and loss are 0.78414 and 0.6134942\n",
            "Val acc and loss are 0.7744 and 0.6179424\n",
            "Processing Epoch 3640\n",
            "Training acc and loss are 0.78414 and 0.613456\n",
            "Val acc and loss are 0.7744 and 0.61790526\n",
            "Processing Epoch 3641\n",
            "Training acc and loss are 0.78414 and 0.6134139\n",
            "Val acc and loss are 0.7745 and 0.61786443\n",
            "Processing Epoch 3642\n",
            "Training acc and loss are 0.78414 and 0.6133763\n",
            "Val acc and loss are 0.7745 and 0.6178288\n",
            "Processing Epoch 3643\n",
            "Training acc and loss are 0.78416 and 0.6133351\n",
            "Val acc and loss are 0.7746 and 0.6177891\n",
            "Processing Epoch 3644\n",
            "Training acc and loss are 0.7842 and 0.61329377\n",
            "Val acc and loss are 0.7746 and 0.6177487\n",
            "Processing Epoch 3645\n",
            "Training acc and loss are 0.78424 and 0.61325306\n",
            "Val acc and loss are 0.7746 and 0.61770904\n",
            "Processing Epoch 3646\n",
            "Training acc and loss are 0.78424 and 0.61320704\n",
            "Val acc and loss are 0.7746 and 0.61766434\n",
            "Processing Epoch 3647\n",
            "Training acc and loss are 0.78428 and 0.6131701\n",
            "Val acc and loss are 0.7747 and 0.617629\n",
            "Processing Epoch 3648\n",
            "Training acc and loss are 0.7843 and 0.6131312\n",
            "Val acc and loss are 0.7747 and 0.6175918\n",
            "Processing Epoch 3649\n",
            "Training acc and loss are 0.78432 and 0.6130928\n",
            "Val acc and loss are 0.7747 and 0.6175549\n",
            "Processing Epoch 3650\n",
            "Training acc and loss are 0.78432 and 0.6130528\n",
            "Val acc and loss are 0.7747 and 0.6175164\n",
            "Processing Epoch 3651\n",
            "Training acc and loss are 0.78434 and 0.61300915\n",
            "Val acc and loss are 0.7748 and 0.6174739\n",
            "Processing Epoch 3652\n",
            "Training acc and loss are 0.78436 and 0.6129664\n",
            "Val acc and loss are 0.7748 and 0.61743206\n",
            "Processing Epoch 3653\n",
            "Training acc and loss are 0.78438 and 0.6129244\n",
            "Val acc and loss are 0.7748 and 0.6173919\n",
            "Processing Epoch 3654\n",
            "Training acc and loss are 0.78438 and 0.6128848\n",
            "Val acc and loss are 0.7749 and 0.6173535\n",
            "Processing Epoch 3655\n",
            "Training acc and loss are 0.78438 and 0.6128458\n",
            "Val acc and loss are 0.7749 and 0.61731565\n",
            "Processing Epoch 3656\n",
            "Training acc and loss are 0.7844 and 0.612804\n",
            "Val acc and loss are 0.7749 and 0.61727554\n",
            "Processing Epoch 3657\n",
            "Training acc and loss are 0.7844 and 0.6127619\n",
            "Val acc and loss are 0.7749 and 0.61723495\n",
            "Processing Epoch 3658\n",
            "Training acc and loss are 0.7844 and 0.61272055\n",
            "Val acc and loss are 0.775 and 0.6171952\n",
            "Processing Epoch 3659\n",
            "Training acc and loss are 0.78444 and 0.6126754\n",
            "Val acc and loss are 0.7751 and 0.6171508\n",
            "Processing Epoch 3660\n",
            "Training acc and loss are 0.78444 and 0.6126316\n",
            "Val acc and loss are 0.7751 and 0.61710817\n",
            "Processing Epoch 3661\n",
            "Training acc and loss are 0.78444 and 0.6125943\n",
            "Val acc and loss are 0.7751 and 0.6170726\n",
            "Processing Epoch 3662\n",
            "Training acc and loss are 0.78444 and 0.61255413\n",
            "Val acc and loss are 0.7751 and 0.6170334\n",
            "Processing Epoch 3663\n",
            "Training acc and loss are 0.78444 and 0.6125137\n",
            "Val acc and loss are 0.7752 and 0.6169945\n",
            "Processing Epoch 3664\n",
            "Training acc and loss are 0.78444 and 0.61247474\n",
            "Val acc and loss are 0.7752 and 0.616957\n",
            "Processing Epoch 3665\n",
            "Training acc and loss are 0.78446 and 0.61243296\n",
            "Val acc and loss are 0.7752 and 0.61691654\n",
            "Processing Epoch 3666\n",
            "Training acc and loss are 0.78446 and 0.61238956\n",
            "Val acc and loss are 0.7752 and 0.6168739\n",
            "Processing Epoch 3667\n",
            "Training acc and loss are 0.78444 and 0.61234874\n",
            "Val acc and loss are 0.7752 and 0.6168343\n",
            "Processing Epoch 3668\n",
            "Training acc and loss are 0.78444 and 0.61230624\n",
            "Val acc and loss are 0.7753 and 0.6167931\n",
            "Processing Epoch 3669\n",
            "Training acc and loss are 0.78446 and 0.6122653\n",
            "Val acc and loss are 0.7753 and 0.6167548\n",
            "Processing Epoch 3670\n",
            "Training acc and loss are 0.78446 and 0.6122258\n",
            "Val acc and loss are 0.7754 and 0.6167164\n",
            "Processing Epoch 3671\n",
            "Training acc and loss are 0.7845 and 0.61218643\n",
            "Val acc and loss are 0.7754 and 0.6166787\n",
            "Processing Epoch 3672\n",
            "Training acc and loss are 0.7845 and 0.6121445\n",
            "Val acc and loss are 0.7754 and 0.6166385\n",
            "Processing Epoch 3673\n",
            "Training acc and loss are 0.7845 and 0.612107\n",
            "Val acc and loss are 0.7754 and 0.6166024\n",
            "Processing Epoch 3674\n",
            "Training acc and loss are 0.7845 and 0.6120631\n",
            "Val acc and loss are 0.7754 and 0.61656004\n",
            "Processing Epoch 3675\n",
            "Training acc and loss are 0.78452 and 0.6120223\n",
            "Val acc and loss are 0.7754 and 0.61652076\n",
            "Processing Epoch 3676\n",
            "Training acc and loss are 0.78452 and 0.61197805\n",
            "Val acc and loss are 0.7754 and 0.61647725\n",
            "Processing Epoch 3677\n",
            "Training acc and loss are 0.78456 and 0.6119392\n",
            "Val acc and loss are 0.7754 and 0.6164405\n",
            "Processing Epoch 3678\n",
            "Training acc and loss are 0.78458 and 0.61189985\n",
            "Val acc and loss are 0.7754 and 0.6164031\n",
            "Processing Epoch 3679\n",
            "Training acc and loss are 0.7846 and 0.61186016\n",
            "Val acc and loss are 0.7754 and 0.61636454\n",
            "Processing Epoch 3680\n",
            "Training acc and loss are 0.7846 and 0.6118172\n",
            "Val acc and loss are 0.7754 and 0.61632293\n",
            "Processing Epoch 3681\n",
            "Training acc and loss are 0.7846 and 0.61177677\n",
            "Val acc and loss are 0.7754 and 0.6162844\n",
            "Processing Epoch 3682\n",
            "Training acc and loss are 0.7846 and 0.61173373\n",
            "Val acc and loss are 0.7754 and 0.6162429\n",
            "Processing Epoch 3683\n",
            "Training acc and loss are 0.7846 and 0.6116918\n",
            "Val acc and loss are 0.7754 and 0.61620253\n",
            "Processing Epoch 3684\n",
            "Training acc and loss are 0.7846 and 0.6116497\n",
            "Val acc and loss are 0.7754 and 0.61616147\n",
            "Processing Epoch 3685\n",
            "Training acc and loss are 0.78458 and 0.6116069\n",
            "Val acc and loss are 0.7755 and 0.6161201\n",
            "Processing Epoch 3686\n",
            "Training acc and loss are 0.7846 and 0.6115677\n",
            "Val acc and loss are 0.7755 and 0.61608195\n",
            "Processing Epoch 3687\n",
            "Training acc and loss are 0.7846 and 0.6115284\n",
            "Val acc and loss are 0.7755 and 0.6160434\n",
            "Processing Epoch 3688\n",
            "Training acc and loss are 0.78462 and 0.6114889\n",
            "Val acc and loss are 0.7755 and 0.61600596\n",
            "Processing Epoch 3689\n",
            "Training acc and loss are 0.78464 and 0.61144865\n",
            "Val acc and loss are 0.7756 and 0.6159667\n",
            "Processing Epoch 3690\n",
            "Training acc and loss are 0.78464 and 0.6114113\n",
            "Val acc and loss are 0.7756 and 0.61592984\n",
            "Processing Epoch 3691\n",
            "Training acc and loss are 0.78464 and 0.61137193\n",
            "Val acc and loss are 0.7756 and 0.6158917\n",
            "Processing Epoch 3692\n",
            "Training acc and loss are 0.78464 and 0.61133033\n",
            "Val acc and loss are 0.7756 and 0.615852\n",
            "Processing Epoch 3693\n",
            "Training acc and loss are 0.78466 and 0.6112896\n",
            "Val acc and loss are 0.7756 and 0.6158126\n",
            "Processing Epoch 3694\n",
            "Training acc and loss are 0.78466 and 0.61124957\n",
            "Val acc and loss are 0.7756 and 0.61577326\n",
            "Processing Epoch 3695\n",
            "Training acc and loss are 0.78466 and 0.6112075\n",
            "Val acc and loss are 0.7756 and 0.61573327\n",
            "Processing Epoch 3696\n",
            "Training acc and loss are 0.78468 and 0.61116767\n",
            "Val acc and loss are 0.7757 and 0.61569446\n",
            "Processing Epoch 3697\n",
            "Training acc and loss are 0.78468 and 0.6111273\n",
            "Val acc and loss are 0.7756 and 0.6156554\n",
            "Processing Epoch 3698\n",
            "Training acc and loss are 0.7847 and 0.61109084\n",
            "Val acc and loss are 0.7756 and 0.6156196\n",
            "Processing Epoch 3699\n",
            "Training acc and loss are 0.78472 and 0.61105144\n",
            "Val acc and loss are 0.7758 and 0.6155818\n",
            "Processing Epoch 3700\n",
            "Training acc and loss are 0.78472 and 0.6110124\n",
            "Val acc and loss are 0.7758 and 0.615544\n",
            "Processing Epoch 3701\n",
            "Training acc and loss are 0.78474 and 0.6109708\n",
            "Val acc and loss are 0.7758 and 0.61550367\n",
            "Processing Epoch 3702\n",
            "Training acc and loss are 0.78476 and 0.61093056\n",
            "Val acc and loss are 0.7757 and 0.61546487\n",
            "Processing Epoch 3703\n",
            "Training acc and loss are 0.78476 and 0.61089045\n",
            "Val acc and loss are 0.7758 and 0.61542606\n",
            "Processing Epoch 3704\n",
            "Training acc and loss are 0.78474 and 0.61085093\n",
            "Val acc and loss are 0.7758 and 0.6153879\n",
            "Processing Epoch 3705\n",
            "Training acc and loss are 0.78476 and 0.61081254\n",
            "Val acc and loss are 0.7758 and 0.61535096\n",
            "Processing Epoch 3706\n",
            "Training acc and loss are 0.78476 and 0.610771\n",
            "Val acc and loss are 0.7758 and 0.6153112\n",
            "Processing Epoch 3707\n",
            "Training acc and loss are 0.78478 and 0.6107302\n",
            "Val acc and loss are 0.7759 and 0.6152711\n",
            "Processing Epoch 3708\n",
            "Training acc and loss are 0.7848 and 0.61068946\n",
            "Val acc and loss are 0.7759 and 0.61523163\n",
            "Processing Epoch 3709\n",
            "Training acc and loss are 0.78482 and 0.61064917\n",
            "Val acc and loss are 0.7759 and 0.61519253\n",
            "Processing Epoch 3710\n",
            "Training acc and loss are 0.78484 and 0.6106115\n",
            "Val acc and loss are 0.7759 and 0.61515635\n",
            "Processing Epoch 3711\n",
            "Training acc and loss are 0.78486 and 0.610571\n",
            "Val acc and loss are 0.7759 and 0.6151175\n",
            "Processing Epoch 3712\n",
            "Training acc and loss are 0.78486 and 0.6105315\n",
            "Val acc and loss are 0.7759 and 0.6150794\n",
            "Processing Epoch 3713\n",
            "Training acc and loss are 0.78486 and 0.6104889\n",
            "Val acc and loss are 0.7759 and 0.61503845\n",
            "Processing Epoch 3714\n",
            "Training acc and loss are 0.78486 and 0.6104483\n",
            "Val acc and loss are 0.7759 and 0.61499935\n",
            "Processing Epoch 3715\n",
            "Training acc and loss are 0.78488 and 0.610408\n",
            "Val acc and loss are 0.7759 and 0.61495966\n",
            "Processing Epoch 3716\n",
            "Training acc and loss are 0.7849 and 0.6103688\n",
            "Val acc and loss are 0.7759 and 0.61492187\n",
            "Processing Epoch 3717\n",
            "Training acc and loss are 0.7849 and 0.6103289\n",
            "Val acc and loss are 0.7759 and 0.6148841\n",
            "Processing Epoch 3718\n",
            "Training acc and loss are 0.78492 and 0.6102873\n",
            "Val acc and loss are 0.7759 and 0.6148432\n",
            "Processing Epoch 3719\n",
            "Training acc and loss are 0.7849 and 0.61024547\n",
            "Val acc and loss are 0.7759 and 0.6148027\n",
            "Processing Epoch 3720\n",
            "Training acc and loss are 0.7849 and 0.61020356\n",
            "Val acc and loss are 0.7759 and 0.61476207\n",
            "Processing Epoch 3721\n",
            "Training acc and loss are 0.78488 and 0.61016357\n",
            "Val acc and loss are 0.7759 and 0.6147236\n",
            "Processing Epoch 3722\n",
            "Training acc and loss are 0.78488 and 0.6101229\n",
            "Val acc and loss are 0.7758 and 0.61468494\n",
            "Processing Epoch 3723\n",
            "Training acc and loss are 0.78488 and 0.61008126\n",
            "Val acc and loss are 0.7759 and 0.61464447\n",
            "Processing Epoch 3724\n",
            "Training acc and loss are 0.78488 and 0.6100426\n",
            "Val acc and loss are 0.7759 and 0.6146072\n",
            "Processing Epoch 3725\n",
            "Training acc and loss are 0.78488 and 0.61000234\n",
            "Val acc and loss are 0.7761 and 0.6145682\n",
            "Processing Epoch 3726\n",
            "Training acc and loss are 0.78488 and 0.6099633\n",
            "Val acc and loss are 0.776 and 0.614531\n",
            "Processing Epoch 3727\n",
            "Training acc and loss are 0.78488 and 0.6099192\n",
            "Val acc and loss are 0.7761 and 0.6144883\n",
            "Processing Epoch 3728\n",
            "Training acc and loss are 0.78488 and 0.6098817\n",
            "Val acc and loss are 0.7761 and 0.6144525\n",
            "Processing Epoch 3729\n",
            "Training acc and loss are 0.78488 and 0.60984313\n",
            "Val acc and loss are 0.7761 and 0.6144151\n",
            "Processing Epoch 3730\n",
            "Training acc and loss are 0.78488 and 0.6098016\n",
            "Val acc and loss are 0.7761 and 0.6143753\n",
            "Processing Epoch 3731\n",
            "Training acc and loss are 0.7849 and 0.60976166\n",
            "Val acc and loss are 0.7762 and 0.6143365\n",
            "Processing Epoch 3732\n",
            "Training acc and loss are 0.7849 and 0.60972077\n",
            "Val acc and loss are 0.7764 and 0.61429757\n",
            "Processing Epoch 3733\n",
            "Training acc and loss are 0.78494 and 0.60967916\n",
            "Val acc and loss are 0.7764 and 0.61425674\n",
            "Processing Epoch 3734\n",
            "Training acc and loss are 0.78496 and 0.6096404\n",
            "Val acc and loss are 0.7764 and 0.6142193\n",
            "Processing Epoch 3735\n",
            "Training acc and loss are 0.78496 and 0.60960126\n",
            "Val acc and loss are 0.7764 and 0.6141819\n",
            "Processing Epoch 3736\n",
            "Training acc and loss are 0.78496 and 0.6095585\n",
            "Val acc and loss are 0.7765 and 0.6141399\n",
            "Processing Epoch 3737\n",
            "Training acc and loss are 0.78498 and 0.6095175\n",
            "Val acc and loss are 0.7765 and 0.61410016\n",
            "Processing Epoch 3738\n",
            "Training acc and loss are 0.78498 and 0.609478\n",
            "Val acc and loss are 0.7765 and 0.6140621\n",
            "Processing Epoch 3739\n",
            "Training acc and loss are 0.78498 and 0.6094394\n",
            "Val acc and loss are 0.7765 and 0.6140244\n",
            "Processing Epoch 3740\n",
            "Training acc and loss are 0.78498 and 0.6093977\n",
            "Val acc and loss are 0.7765 and 0.61398447\n",
            "Processing Epoch 3741\n",
            "Training acc and loss are 0.78502 and 0.60935843\n",
            "Val acc and loss are 0.7765 and 0.6139466\n",
            "Processing Epoch 3742\n",
            "Training acc and loss are 0.78502 and 0.609317\n",
            "Val acc and loss are 0.7765 and 0.6139066\n",
            "Processing Epoch 3743\n",
            "Training acc and loss are 0.78504 and 0.60927814\n",
            "Val acc and loss are 0.7765 and 0.61386955\n",
            "Processing Epoch 3744\n",
            "Training acc and loss are 0.78504 and 0.60923934\n",
            "Val acc and loss are 0.7765 and 0.6138329\n",
            "Processing Epoch 3745\n",
            "Training acc and loss are 0.78506 and 0.6091994\n",
            "Val acc and loss are 0.7765 and 0.61379445\n",
            "Processing Epoch 3746\n",
            "Training acc and loss are 0.78508 and 0.60916\n",
            "Val acc and loss are 0.7765 and 0.61375606\n",
            "Processing Epoch 3747\n",
            "Training acc and loss are 0.78508 and 0.6091194\n",
            "Val acc and loss are 0.7765 and 0.61371726\n",
            "Processing Epoch 3748\n",
            "Training acc and loss are 0.78516 and 0.6090805\n",
            "Val acc and loss are 0.7765 and 0.61367977\n",
            "Processing Epoch 3749\n",
            "Training acc and loss are 0.78516 and 0.6090419\n",
            "Val acc and loss are 0.7766 and 0.6136421\n",
            "Processing Epoch 3750\n",
            "Training acc and loss are 0.7852 and 0.60899943\n",
            "Val acc and loss are 0.7767 and 0.61360085\n",
            "Processing Epoch 3751\n",
            "Training acc and loss are 0.78522 and 0.6089599\n",
            "Val acc and loss are 0.7767 and 0.6135619\n",
            "Processing Epoch 3752\n",
            "Training acc and loss are 0.7852 and 0.60892314\n",
            "Val acc and loss are 0.7767 and 0.61352634\n",
            "Processing Epoch 3753\n",
            "Training acc and loss are 0.7852 and 0.6088826\n",
            "Val acc and loss are 0.7767 and 0.6134873\n",
            "Processing Epoch 3754\n",
            "Training acc and loss are 0.7852 and 0.6088437\n",
            "Val acc and loss are 0.7767 and 0.61344963\n",
            "Processing Epoch 3755\n",
            "Training acc and loss are 0.7852 and 0.60880554\n",
            "Val acc and loss are 0.7767 and 0.6134131\n",
            "Processing Epoch 3756\n",
            "Training acc and loss are 0.7852 and 0.6087668\n",
            "Val acc and loss are 0.7766 and 0.6133747\n",
            "Processing Epoch 3757\n",
            "Training acc and loss are 0.78524 and 0.608728\n",
            "Val acc and loss are 0.7766 and 0.6133369\n",
            "Processing Epoch 3758\n",
            "Training acc and loss are 0.78526 and 0.6086879\n",
            "Val acc and loss are 0.7766 and 0.61329806\n",
            "Processing Epoch 3759\n",
            "Training acc and loss are 0.78528 and 0.6086505\n",
            "Val acc and loss are 0.7766 and 0.61326253\n",
            "Processing Epoch 3760\n",
            "Training acc and loss are 0.78532 and 0.6086107\n",
            "Val acc and loss are 0.7766 and 0.6132239\n",
            "Processing Epoch 3761\n",
            "Training acc and loss are 0.78532 and 0.60857123\n",
            "Val acc and loss are 0.7766 and 0.61318594\n",
            "Processing Epoch 3762\n",
            "Training acc and loss are 0.78534 and 0.60853225\n",
            "Val acc and loss are 0.7766 and 0.61314833\n",
            "Processing Epoch 3763\n",
            "Training acc and loss are 0.78534 and 0.60849345\n",
            "Val acc and loss are 0.7766 and 0.613111\n",
            "Processing Epoch 3764\n",
            "Training acc and loss are 0.78532 and 0.6084553\n",
            "Val acc and loss are 0.7766 and 0.6130746\n",
            "Processing Epoch 3765\n",
            "Training acc and loss are 0.78532 and 0.60841626\n",
            "Val acc and loss are 0.7766 and 0.613037\n",
            "Processing Epoch 3766\n",
            "Training acc and loss are 0.78534 and 0.6083796\n",
            "Val acc and loss are 0.7767 and 0.61300206\n",
            "Processing Epoch 3767\n",
            "Training acc and loss are 0.78536 and 0.60834\n",
            "Val acc and loss are 0.7768 and 0.6129643\n",
            "Processing Epoch 3768\n",
            "Training acc and loss are 0.78538 and 0.6082986\n",
            "Val acc and loss are 0.7768 and 0.61292434\n",
            "Processing Epoch 3769\n",
            "Training acc and loss are 0.7854 and 0.60826194\n",
            "Val acc and loss are 0.7768 and 0.61288923\n",
            "Processing Epoch 3770\n",
            "Training acc and loss are 0.7854 and 0.6082249\n",
            "Val acc and loss are 0.7768 and 0.6128531\n",
            "Processing Epoch 3771\n",
            "Training acc and loss are 0.78542 and 0.6081876\n",
            "Val acc and loss are 0.7768 and 0.61281717\n",
            "Processing Epoch 3772\n",
            "Training acc and loss are 0.78546 and 0.6081477\n",
            "Val acc and loss are 0.7768 and 0.6127783\n",
            "Processing Epoch 3773\n",
            "Training acc and loss are 0.78544 and 0.60810757\n",
            "Val acc and loss are 0.7768 and 0.6127398\n",
            "Processing Epoch 3774\n",
            "Training acc and loss are 0.78546 and 0.6080705\n",
            "Val acc and loss are 0.7768 and 0.6127039\n",
            "Processing Epoch 3775\n",
            "Training acc and loss are 0.7855 and 0.6080298\n",
            "Val acc and loss are 0.7768 and 0.6126651\n",
            "Processing Epoch 3776\n",
            "Training acc and loss are 0.78554 and 0.6079908\n",
            "Val acc and loss are 0.7768 and 0.61262757\n",
            "Processing Epoch 3777\n",
            "Training acc and loss are 0.78556 and 0.60795635\n",
            "Val acc and loss are 0.7768 and 0.6125947\n",
            "Processing Epoch 3778\n",
            "Training acc and loss are 0.78556 and 0.60792273\n",
            "Val acc and loss are 0.7768 and 0.6125625\n",
            "Processing Epoch 3779\n",
            "Training acc and loss are 0.78554 and 0.6078848\n",
            "Val acc and loss are 0.7768 and 0.61252576\n",
            "Processing Epoch 3780\n",
            "Training acc and loss are 0.78556 and 0.6078481\n",
            "Val acc and loss are 0.7769 and 0.6124906\n",
            "Processing Epoch 3781\n",
            "Training acc and loss are 0.78558 and 0.60780823\n",
            "Val acc and loss are 0.777 and 0.61245126\n",
            "Processing Epoch 3782\n",
            "Training acc and loss are 0.78558 and 0.60776997\n",
            "Val acc and loss are 0.7771 and 0.61241496\n",
            "Processing Epoch 3783\n",
            "Training acc and loss are 0.7856 and 0.60772985\n",
            "Val acc and loss are 0.7771 and 0.612376\n",
            "Processing Epoch 3784\n",
            "Training acc and loss are 0.78562 and 0.6076877\n",
            "Val acc and loss are 0.7771 and 0.6123353\n",
            "Processing Epoch 3785\n",
            "Training acc and loss are 0.78564 and 0.60764617\n",
            "Val acc and loss are 0.7771 and 0.6122954\n",
            "Processing Epoch 3786\n",
            "Training acc and loss are 0.78564 and 0.60760933\n",
            "Val acc and loss are 0.7771 and 0.61225975\n",
            "Processing Epoch 3787\n",
            "Training acc and loss are 0.78566 and 0.60756713\n",
            "Val acc and loss are 0.7771 and 0.61221904\n",
            "Processing Epoch 3788\n",
            "Training acc and loss are 0.78566 and 0.60752755\n",
            "Val acc and loss are 0.7771 and 0.612181\n",
            "Processing Epoch 3789\n",
            "Training acc and loss are 0.7857 and 0.60748726\n",
            "Val acc and loss are 0.777 and 0.6121427\n",
            "Processing Epoch 3790\n",
            "Training acc and loss are 0.7857 and 0.6074481\n",
            "Val acc and loss are 0.777 and 0.61210525\n",
            "Processing Epoch 3791\n",
            "Training acc and loss are 0.78568 and 0.6074095\n",
            "Val acc and loss are 0.777 and 0.61206806\n",
            "Processing Epoch 3792\n",
            "Training acc and loss are 0.78566 and 0.6073705\n",
            "Val acc and loss are 0.777 and 0.6120306\n",
            "Processing Epoch 3793\n",
            "Training acc and loss are 0.78566 and 0.6073294\n",
            "Val acc and loss are 0.7771 and 0.6119913\n",
            "Processing Epoch 3794\n",
            "Training acc and loss are 0.78566 and 0.6072903\n",
            "Val acc and loss are 0.7771 and 0.6119543\n",
            "Processing Epoch 3795\n",
            "Training acc and loss are 0.78568 and 0.60725236\n",
            "Val acc and loss are 0.7771 and 0.6119171\n",
            "Processing Epoch 3796\n",
            "Training acc and loss are 0.78568 and 0.60721415\n",
            "Val acc and loss are 0.7771 and 0.61188006\n",
            "Processing Epoch 3797\n",
            "Training acc and loss are 0.78568 and 0.6071749\n",
            "Val acc and loss are 0.7771 and 0.6118423\n",
            "Processing Epoch 3798\n",
            "Training acc and loss are 0.78572 and 0.60713655\n",
            "Val acc and loss are 0.7771 and 0.6118053\n",
            "Processing Epoch 3799\n",
            "Training acc and loss are 0.78572 and 0.60709673\n",
            "Val acc and loss are 0.7772 and 0.6117671\n",
            "Processing Epoch 3800\n",
            "Training acc and loss are 0.7857 and 0.6070601\n",
            "Val acc and loss are 0.7772 and 0.6117322\n",
            "Processing Epoch 3801\n",
            "Training acc and loss are 0.7857 and 0.60702085\n",
            "Val acc and loss are 0.7772 and 0.6116942\n",
            "Processing Epoch 3802\n",
            "Training acc and loss are 0.7857 and 0.60698485\n",
            "Val acc and loss are 0.7772 and 0.6116599\n",
            "Processing Epoch 3803\n",
            "Training acc and loss are 0.7857 and 0.6069473\n",
            "Val acc and loss are 0.7772 and 0.6116235\n",
            "Processing Epoch 3804\n",
            "Training acc and loss are 0.7857 and 0.6069122\n",
            "Val acc and loss are 0.7772 and 0.6115898\n",
            "Processing Epoch 3805\n",
            "Training acc and loss are 0.78568 and 0.6068723\n",
            "Val acc and loss are 0.7772 and 0.61155134\n",
            "Processing Epoch 3806\n",
            "Training acc and loss are 0.78568 and 0.60683423\n",
            "Val acc and loss are 0.7772 and 0.61151534\n",
            "Processing Epoch 3807\n",
            "Training acc and loss are 0.78568 and 0.6067926\n",
            "Val acc and loss are 0.7772 and 0.61147517\n",
            "Processing Epoch 3808\n",
            "Training acc and loss are 0.78568 and 0.6067561\n",
            "Val acc and loss are 0.7773 and 0.6114395\n",
            "Processing Epoch 3809\n",
            "Training acc and loss are 0.78572 and 0.60671973\n",
            "Val acc and loss are 0.7773 and 0.6114045\n",
            "Processing Epoch 3810\n",
            "Training acc and loss are 0.78576 and 0.6066795\n",
            "Val acc and loss are 0.7773 and 0.61136556\n",
            "Processing Epoch 3811\n",
            "Training acc and loss are 0.78578 and 0.60664034\n",
            "Val acc and loss are 0.7773 and 0.61132765\n",
            "Processing Epoch 3812\n",
            "Training acc and loss are 0.78578 and 0.6066036\n",
            "Val acc and loss are 0.7773 and 0.61129206\n",
            "Processing Epoch 3813\n",
            "Training acc and loss are 0.7858 and 0.6065681\n",
            "Val acc and loss are 0.7773 and 0.61125803\n",
            "Processing Epoch 3814\n",
            "Training acc and loss are 0.7858 and 0.6065308\n",
            "Val acc and loss are 0.7773 and 0.611222\n",
            "Processing Epoch 3815\n",
            "Training acc and loss are 0.78582 and 0.6064909\n",
            "Val acc and loss are 0.7773 and 0.61118346\n",
            "Processing Epoch 3816\n",
            "Training acc and loss are 0.78582 and 0.6064517\n",
            "Val acc and loss are 0.7773 and 0.61114526\n",
            "Processing Epoch 3817\n",
            "Training acc and loss are 0.78584 and 0.60641086\n",
            "Val acc and loss are 0.7773 and 0.61110574\n",
            "Processing Epoch 3818\n",
            "Training acc and loss are 0.78584 and 0.60637164\n",
            "Val acc and loss are 0.7773 and 0.61106783\n",
            "Processing Epoch 3819\n",
            "Training acc and loss are 0.78584 and 0.6063299\n",
            "Val acc and loss are 0.7773 and 0.61102736\n",
            "Processing Epoch 3820\n",
            "Training acc and loss are 0.78586 and 0.6062918\n",
            "Val acc and loss are 0.7774 and 0.6109904\n",
            "Processing Epoch 3821\n",
            "Training acc and loss are 0.78586 and 0.6062497\n",
            "Val acc and loss are 0.7775 and 0.6109493\n",
            "Processing Epoch 3822\n",
            "Training acc and loss are 0.7859 and 0.6062104\n",
            "Val acc and loss are 0.7775 and 0.61091155\n",
            "Processing Epoch 3823\n",
            "Training acc and loss are 0.78592 and 0.6061733\n",
            "Val acc and loss are 0.7774 and 0.61087525\n",
            "Processing Epoch 3824\n",
            "Training acc and loss are 0.78592 and 0.6061362\n",
            "Val acc and loss are 0.7774 and 0.61083955\n",
            "Processing Epoch 3825\n",
            "Training acc and loss are 0.78596 and 0.6061014\n",
            "Val acc and loss are 0.7775 and 0.61080617\n",
            "Processing Epoch 3826\n",
            "Training acc and loss are 0.78602 and 0.60606194\n",
            "Val acc and loss are 0.7775 and 0.6107675\n",
            "Processing Epoch 3827\n",
            "Training acc and loss are 0.78602 and 0.6060244\n",
            "Val acc and loss are 0.7775 and 0.61073166\n",
            "Processing Epoch 3828\n",
            "Training acc and loss are 0.78602 and 0.60598534\n",
            "Val acc and loss are 0.7775 and 0.6106944\n",
            "Processing Epoch 3829\n",
            "Training acc and loss are 0.78606 and 0.6059463\n",
            "Val acc and loss are 0.7776 and 0.6106566\n",
            "Processing Epoch 3830\n",
            "Training acc and loss are 0.78606 and 0.60590816\n",
            "Val acc and loss are 0.7776 and 0.61061954\n",
            "Processing Epoch 3831\n",
            "Training acc and loss are 0.78604 and 0.60587084\n",
            "Val acc and loss are 0.7776 and 0.61058384\n",
            "Processing Epoch 3832\n",
            "Training acc and loss are 0.78604 and 0.6058323\n",
            "Val acc and loss are 0.7775 and 0.6105464\n",
            "Processing Epoch 3833\n",
            "Training acc and loss are 0.78602 and 0.605794\n",
            "Val acc and loss are 0.7775 and 0.6105094\n",
            "Processing Epoch 3834\n",
            "Training acc and loss are 0.78604 and 0.60575235\n",
            "Val acc and loss are 0.7775 and 0.6104686\n",
            "Processing Epoch 3835\n",
            "Training acc and loss are 0.78604 and 0.60571325\n",
            "Val acc and loss are 0.7775 and 0.61043054\n",
            "Processing Epoch 3836\n",
            "Training acc and loss are 0.78604 and 0.6056793\n",
            "Val acc and loss are 0.7776 and 0.6103979\n",
            "Processing Epoch 3837\n",
            "Training acc and loss are 0.78604 and 0.605642\n",
            "Val acc and loss are 0.7776 and 0.6103621\n",
            "Processing Epoch 3838\n",
            "Training acc and loss are 0.78604 and 0.6056032\n",
            "Val acc and loss are 0.7776 and 0.6103245\n",
            "Processing Epoch 3839\n",
            "Training acc and loss are 0.78606 and 0.6055678\n",
            "Val acc and loss are 0.7776 and 0.61029077\n",
            "Processing Epoch 3840\n",
            "Training acc and loss are 0.78608 and 0.60552454\n",
            "Val acc and loss are 0.7776 and 0.61024845\n",
            "Processing Epoch 3841\n",
            "Training acc and loss are 0.7861 and 0.6054879\n",
            "Val acc and loss are 0.7776 and 0.61021304\n",
            "Processing Epoch 3842\n",
            "Training acc and loss are 0.78614 and 0.6054501\n",
            "Val acc and loss are 0.7776 and 0.6101771\n",
            "Processing Epoch 3843\n",
            "Training acc and loss are 0.78618 and 0.6054133\n",
            "Val acc and loss are 0.7776 and 0.6101419\n",
            "Processing Epoch 3844\n",
            "Training acc and loss are 0.7862 and 0.6053754\n",
            "Val acc and loss are 0.7777 and 0.6101047\n",
            "Processing Epoch 3845\n",
            "Training acc and loss are 0.7862 and 0.60534185\n",
            "Val acc and loss are 0.7777 and 0.610073\n",
            "Processing Epoch 3846\n",
            "Training acc and loss are 0.7862 and 0.6053022\n",
            "Val acc and loss are 0.7777 and 0.6100343\n",
            "Processing Epoch 3847\n",
            "Training acc and loss are 0.78624 and 0.60526234\n",
            "Val acc and loss are 0.7777 and 0.60999554\n",
            "Processing Epoch 3848\n",
            "Training acc and loss are 0.78622 and 0.6052239\n",
            "Val acc and loss are 0.7777 and 0.60995865\n",
            "Processing Epoch 3849\n",
            "Training acc and loss are 0.78624 and 0.6051848\n",
            "Val acc and loss are 0.7777 and 0.6099209\n",
            "Processing Epoch 3850\n",
            "Training acc and loss are 0.78622 and 0.60514337\n",
            "Val acc and loss are 0.7777 and 0.60988116\n",
            "Processing Epoch 3851\n",
            "Training acc and loss are 0.78624 and 0.605105\n",
            "Val acc and loss are 0.7777 and 0.60984397\n",
            "Processing Epoch 3852\n",
            "Training acc and loss are 0.78622 and 0.60506296\n",
            "Val acc and loss are 0.7777 and 0.6098036\n",
            "Processing Epoch 3853\n",
            "Training acc and loss are 0.78622 and 0.6050253\n",
            "Val acc and loss are 0.7778 and 0.6097675\n",
            "Processing Epoch 3854\n",
            "Training acc and loss are 0.78622 and 0.604987\n",
            "Val acc and loss are 0.7778 and 0.60973066\n",
            "Processing Epoch 3855\n",
            "Training acc and loss are 0.78622 and 0.6049495\n",
            "Val acc and loss are 0.7777 and 0.6096947\n",
            "Processing Epoch 3856\n",
            "Training acc and loss are 0.78624 and 0.60491174\n",
            "Val acc and loss are 0.7778 and 0.6096584\n",
            "Processing Epoch 3857\n",
            "Training acc and loss are 0.78624 and 0.60487384\n",
            "Val acc and loss are 0.7777 and 0.60962105\n",
            "Processing Epoch 3858\n",
            "Training acc and loss are 0.78624 and 0.60483545\n",
            "Val acc and loss are 0.7777 and 0.6095832\n",
            "Processing Epoch 3859\n",
            "Training acc and loss are 0.78626 and 0.60479593\n",
            "Val acc and loss are 0.7777 and 0.60954523\n",
            "Processing Epoch 3860\n",
            "Training acc and loss are 0.78626 and 0.60475844\n",
            "Val acc and loss are 0.7778 and 0.6095089\n",
            "Processing Epoch 3861\n",
            "Training acc and loss are 0.78626 and 0.6047199\n",
            "Val acc and loss are 0.7779 and 0.6094711\n",
            "Processing Epoch 3862\n",
            "Training acc and loss are 0.78628 and 0.60467964\n",
            "Val acc and loss are 0.7779 and 0.6094321\n",
            "Processing Epoch 3863\n",
            "Training acc and loss are 0.78628 and 0.60464257\n",
            "Val acc and loss are 0.7778 and 0.60939664\n",
            "Processing Epoch 3864\n",
            "Training acc and loss are 0.78626 and 0.60460216\n",
            "Val acc and loss are 0.7779 and 0.6093577\n",
            "Processing Epoch 3865\n",
            "Training acc and loss are 0.78628 and 0.6045648\n",
            "Val acc and loss are 0.7779 and 0.60932165\n",
            "Processing Epoch 3866\n",
            "Training acc and loss are 0.7863 and 0.604526\n",
            "Val acc and loss are 0.7779 and 0.6092841\n",
            "Processing Epoch 3867\n",
            "Training acc and loss are 0.78632 and 0.60448956\n",
            "Val acc and loss are 0.7779 and 0.609249\n",
            "Processing Epoch 3868\n",
            "Training acc and loss are 0.78632 and 0.60445595\n",
            "Val acc and loss are 0.7779 and 0.6092165\n",
            "Processing Epoch 3869\n",
            "Training acc and loss are 0.78634 and 0.60441667\n",
            "Val acc and loss are 0.778 and 0.609179\n",
            "Processing Epoch 3870\n",
            "Training acc and loss are 0.78636 and 0.60437727\n",
            "Val acc and loss are 0.778 and 0.609141\n",
            "Processing Epoch 3871\n",
            "Training acc and loss are 0.78636 and 0.60434014\n",
            "Val acc and loss are 0.778 and 0.6091047\n",
            "Processing Epoch 3872\n",
            "Training acc and loss are 0.78638 and 0.6043026\n",
            "Val acc and loss are 0.778 and 0.6090683\n",
            "Processing Epoch 3873\n",
            "Training acc and loss are 0.78638 and 0.60426587\n",
            "Val acc and loss are 0.778 and 0.6090324\n",
            "Processing Epoch 3874\n",
            "Training acc and loss are 0.78638 and 0.60422635\n",
            "Val acc and loss are 0.778 and 0.60899407\n",
            "Processing Epoch 3875\n",
            "Training acc and loss are 0.78638 and 0.6041929\n",
            "Val acc and loss are 0.778 and 0.6089621\n",
            "Processing Epoch 3876\n",
            "Training acc and loss are 0.78638 and 0.60415417\n",
            "Val acc and loss are 0.7781 and 0.6089252\n",
            "Processing Epoch 3877\n",
            "Training acc and loss are 0.78638 and 0.6041172\n",
            "Val acc and loss are 0.7781 and 0.60889035\n",
            "Processing Epoch 3878\n",
            "Training acc and loss are 0.78642 and 0.60407573\n",
            "Val acc and loss are 0.7782 and 0.60885066\n",
            "Processing Epoch 3879\n",
            "Training acc and loss are 0.78644 and 0.6040376\n",
            "Val acc and loss are 0.7783 and 0.6088139\n",
            "Processing Epoch 3880\n",
            "Training acc and loss are 0.7865 and 0.60400057\n",
            "Val acc and loss are 0.7783 and 0.60877836\n",
            "Processing Epoch 3881\n",
            "Training acc and loss are 0.78654 and 0.603965\n",
            "Val acc and loss are 0.7783 and 0.6087441\n",
            "Processing Epoch 3882\n",
            "Training acc and loss are 0.78656 and 0.60392964\n",
            "Val acc and loss are 0.7782 and 0.60870993\n",
            "Processing Epoch 3883\n",
            "Training acc and loss are 0.7866 and 0.60389274\n",
            "Val acc and loss are 0.7782 and 0.6086745\n",
            "Processing Epoch 3884\n",
            "Training acc and loss are 0.7866 and 0.6038551\n",
            "Val acc and loss are 0.7782 and 0.60863864\n",
            "Processing Epoch 3885\n",
            "Training acc and loss are 0.78662 and 0.6038168\n",
            "Val acc and loss are 0.7782 and 0.6086021\n",
            "Processing Epoch 3886\n",
            "Training acc and loss are 0.78662 and 0.6037795\n",
            "Val acc and loss are 0.7782 and 0.6085666\n",
            "Processing Epoch 3887\n",
            "Training acc and loss are 0.78662 and 0.6037432\n",
            "Val acc and loss are 0.7782 and 0.6085315\n",
            "Processing Epoch 3888\n",
            "Training acc and loss are 0.78662 and 0.603709\n",
            "Val acc and loss are 0.7781 and 0.60849845\n",
            "Processing Epoch 3889\n",
            "Training acc and loss are 0.78664 and 0.6036738\n",
            "Val acc and loss are 0.7781 and 0.60846514\n",
            "Processing Epoch 3890\n",
            "Training acc and loss are 0.78664 and 0.60363936\n",
            "Val acc and loss are 0.7781 and 0.60843265\n",
            "Processing Epoch 3891\n",
            "Training acc and loss are 0.78664 and 0.60360426\n",
            "Val acc and loss are 0.7781 and 0.6083987\n",
            "Processing Epoch 3892\n",
            "Training acc and loss are 0.78668 and 0.60356545\n",
            "Val acc and loss are 0.7781 and 0.6083611\n",
            "Processing Epoch 3893\n",
            "Training acc and loss are 0.78668 and 0.6035284\n",
            "Val acc and loss are 0.7781 and 0.60832644\n",
            "Processing Epoch 3894\n",
            "Training acc and loss are 0.78668 and 0.6034897\n",
            "Val acc and loss are 0.7781 and 0.6082886\n",
            "Processing Epoch 3895\n",
            "Training acc and loss are 0.7867 and 0.60345745\n",
            "Val acc and loss are 0.7781 and 0.6082582\n",
            "Processing Epoch 3896\n",
            "Training acc and loss are 0.7867 and 0.60341865\n",
            "Val acc and loss are 0.7781 and 0.6082205\n",
            "Processing Epoch 3897\n",
            "Training acc and loss are 0.78674 and 0.6033794\n",
            "Val acc and loss are 0.7781 and 0.60818267\n",
            "Processing Epoch 3898\n",
            "Training acc and loss are 0.78674 and 0.6033427\n",
            "Val acc and loss are 0.7781 and 0.6081471\n",
            "Processing Epoch 3899\n",
            "Training acc and loss are 0.78674 and 0.60330534\n",
            "Val acc and loss are 0.7781 and 0.6081103\n",
            "Processing Epoch 3900\n",
            "Training acc and loss are 0.78674 and 0.6032721\n",
            "Val acc and loss are 0.7782 and 0.60807884\n",
            "Processing Epoch 3901\n",
            "Training acc and loss are 0.78674 and 0.60323477\n",
            "Val acc and loss are 0.7782 and 0.6080421\n",
            "Processing Epoch 3902\n",
            "Training acc and loss are 0.78676 and 0.60319734\n",
            "Val acc and loss are 0.7782 and 0.60800624\n",
            "Processing Epoch 3903\n",
            "Training acc and loss are 0.78678 and 0.60315984\n",
            "Val acc and loss are 0.7782 and 0.6079709\n",
            "Processing Epoch 3904\n",
            "Training acc and loss are 0.78678 and 0.60312164\n",
            "Val acc and loss are 0.7782 and 0.60793364\n",
            "Processing Epoch 3905\n",
            "Training acc and loss are 0.78678 and 0.6030812\n",
            "Val acc and loss are 0.7782 and 0.6078944\n",
            "Processing Epoch 3906\n",
            "Training acc and loss are 0.78682 and 0.6030403\n",
            "Val acc and loss are 0.7782 and 0.60785496\n",
            "Processing Epoch 3907\n",
            "Training acc and loss are 0.78686 and 0.6030045\n",
            "Val acc and loss are 0.7782 and 0.6078202\n",
            "Processing Epoch 3908\n",
            "Training acc and loss are 0.78688 and 0.6029697\n",
            "Val acc and loss are 0.7782 and 0.6077866\n",
            "Processing Epoch 3909\n",
            "Training acc and loss are 0.78688 and 0.6029319\n",
            "Val acc and loss are 0.7783 and 0.6077499\n",
            "Processing Epoch 3910\n",
            "Training acc and loss are 0.7869 and 0.60289335\n",
            "Val acc and loss are 0.7783 and 0.6077129\n",
            "Processing Epoch 3911\n",
            "Training acc and loss are 0.7869 and 0.60285485\n",
            "Val acc and loss are 0.7783 and 0.607675\n",
            "Processing Epoch 3912\n",
            "Training acc and loss are 0.78698 and 0.6028171\n",
            "Val acc and loss are 0.7783 and 0.60763866\n",
            "Processing Epoch 3913\n",
            "Training acc and loss are 0.78698 and 0.60277975\n",
            "Val acc and loss are 0.7784 and 0.60760266\n",
            "Processing Epoch 3914\n",
            "Training acc and loss are 0.78702 and 0.6027475\n",
            "Val acc and loss are 0.7784 and 0.6075719\n",
            "Processing Epoch 3915\n",
            "Training acc and loss are 0.78702 and 0.60270953\n",
            "Val acc and loss are 0.7784 and 0.60753417\n",
            "Processing Epoch 3916\n",
            "Training acc and loss are 0.78704 and 0.6026731\n",
            "Val acc and loss are 0.7784 and 0.607499\n",
            "Processing Epoch 3917\n",
            "Training acc and loss are 0.78704 and 0.6026391\n",
            "Val acc and loss are 0.7784 and 0.6074664\n",
            "Processing Epoch 3918\n",
            "Training acc and loss are 0.78704 and 0.6026007\n",
            "Val acc and loss are 0.7784 and 0.60742956\n",
            "Processing Epoch 3919\n",
            "Training acc and loss are 0.78706 and 0.6025627\n",
            "Val acc and loss are 0.7784 and 0.6073928\n",
            "Processing Epoch 3920\n",
            "Training acc and loss are 0.7871 and 0.6025287\n",
            "Val acc and loss are 0.7785 and 0.60736036\n",
            "Processing Epoch 3921\n",
            "Training acc and loss are 0.78712 and 0.60249287\n",
            "Val acc and loss are 0.7785 and 0.60732645\n",
            "Processing Epoch 3922\n",
            "Training acc and loss are 0.78712 and 0.60245633\n",
            "Val acc and loss are 0.7785 and 0.60729074\n",
            "Processing Epoch 3923\n",
            "Training acc and loss are 0.7871 and 0.6024197\n",
            "Val acc and loss are 0.7785 and 0.6072554\n",
            "Processing Epoch 3924\n",
            "Training acc and loss are 0.7871 and 0.6023812\n",
            "Val acc and loss are 0.7785 and 0.6072184\n",
            "Processing Epoch 3925\n",
            "Training acc and loss are 0.78712 and 0.60234404\n",
            "Val acc and loss are 0.7785 and 0.6071817\n",
            "Processing Epoch 3926\n",
            "Training acc and loss are 0.78712 and 0.6023095\n",
            "Val acc and loss are 0.7785 and 0.60714805\n",
            "Processing Epoch 3927\n",
            "Training acc and loss are 0.78714 and 0.60227185\n",
            "Val acc and loss are 0.7785 and 0.6071117\n",
            "Processing Epoch 3928\n",
            "Training acc and loss are 0.78714 and 0.6022356\n",
            "Val acc and loss are 0.7785 and 0.6070769\n",
            "Processing Epoch 3929\n",
            "Training acc and loss are 0.78716 and 0.6021994\n",
            "Val acc and loss are 0.7785 and 0.6070417\n",
            "Processing Epoch 3930\n",
            "Training acc and loss are 0.78722 and 0.60216427\n",
            "Val acc and loss are 0.7785 and 0.6070083\n",
            "Processing Epoch 3931\n",
            "Training acc and loss are 0.78722 and 0.6021278\n",
            "Val acc and loss are 0.7785 and 0.606973\n",
            "Processing Epoch 3932\n",
            "Training acc and loss are 0.78722 and 0.6020931\n",
            "Val acc and loss are 0.7785 and 0.60694003\n",
            "Processing Epoch 3933\n",
            "Training acc and loss are 0.78724 and 0.6020575\n",
            "Val acc and loss are 0.7785 and 0.60690564\n",
            "Processing Epoch 3934\n",
            "Training acc and loss are 0.78724 and 0.6020225\n",
            "Val acc and loss are 0.7785 and 0.60687155\n",
            "Processing Epoch 3935\n",
            "Training acc and loss are 0.78724 and 0.60198534\n",
            "Val acc and loss are 0.7785 and 0.60683507\n",
            "Processing Epoch 3936\n",
            "Training acc and loss are 0.78724 and 0.6019477\n",
            "Val acc and loss are 0.7785 and 0.60679907\n",
            "Processing Epoch 3937\n",
            "Training acc and loss are 0.78722 and 0.6019136\n",
            "Val acc and loss are 0.7785 and 0.60676616\n",
            "Processing Epoch 3938\n",
            "Training acc and loss are 0.78724 and 0.60188043\n",
            "Val acc and loss are 0.7786 and 0.60673404\n",
            "Processing Epoch 3939\n",
            "Training acc and loss are 0.78726 and 0.6018443\n",
            "Val acc and loss are 0.7786 and 0.6066992\n",
            "Processing Epoch 3940\n",
            "Training acc and loss are 0.78726 and 0.60181075\n",
            "Val acc and loss are 0.7786 and 0.6066676\n",
            "Processing Epoch 3941\n",
            "Training acc and loss are 0.78726 and 0.60177445\n",
            "Val acc and loss are 0.7786 and 0.6066326\n",
            "Processing Epoch 3942\n",
            "Training acc and loss are 0.78728 and 0.6017362\n",
            "Val acc and loss are 0.7786 and 0.6065956\n",
            "Processing Epoch 3943\n",
            "Training acc and loss are 0.7873 and 0.60170037\n",
            "Val acc and loss are 0.7786 and 0.6065615\n",
            "Processing Epoch 3944\n",
            "Training acc and loss are 0.7873 and 0.6016625\n",
            "Val acc and loss are 0.7786 and 0.606525\n",
            "Processing Epoch 3945\n",
            "Training acc and loss are 0.78732 and 0.6016246\n",
            "Val acc and loss are 0.7787 and 0.606488\n",
            "Processing Epoch 3946\n",
            "Training acc and loss are 0.78734 and 0.6015891\n",
            "Val acc and loss are 0.7787 and 0.6064538\n",
            "Processing Epoch 3947\n",
            "Training acc and loss are 0.78734 and 0.6015535\n",
            "Val acc and loss are 0.7787 and 0.60641927\n",
            "Processing Epoch 3948\n",
            "Training acc and loss are 0.78734 and 0.6015173\n",
            "Val acc and loss are 0.7787 and 0.606384\n",
            "Processing Epoch 3949\n",
            "Training acc and loss are 0.78734 and 0.6014805\n",
            "Val acc and loss are 0.7787 and 0.6063484\n",
            "Processing Epoch 3950\n",
            "Training acc and loss are 0.78734 and 0.6014469\n",
            "Val acc and loss are 0.7787 and 0.6063163\n",
            "Processing Epoch 3951\n",
            "Training acc and loss are 0.78738 and 0.60140896\n",
            "Val acc and loss are 0.7787 and 0.6062797\n",
            "Processing Epoch 3952\n",
            "Training acc and loss are 0.7874 and 0.60137236\n",
            "Val acc and loss are 0.7787 and 0.60624474\n",
            "Processing Epoch 3953\n",
            "Training acc and loss are 0.7874 and 0.6013372\n",
            "Val acc and loss are 0.7787 and 0.60621005\n",
            "Processing Epoch 3954\n",
            "Training acc and loss are 0.78738 and 0.60130006\n",
            "Val acc and loss are 0.7787 and 0.6061745\n",
            "Processing Epoch 3955\n",
            "Training acc and loss are 0.78738 and 0.60126454\n",
            "Val acc and loss are 0.7787 and 0.60614073\n",
            "Processing Epoch 3956\n",
            "Training acc and loss are 0.7874 and 0.6012298\n",
            "Val acc and loss are 0.7787 and 0.6061068\n",
            "Processing Epoch 3957\n",
            "Training acc and loss are 0.78744 and 0.6011951\n",
            "Val acc and loss are 0.7787 and 0.6060736\n",
            "Processing Epoch 3958\n",
            "Training acc and loss are 0.78744 and 0.6011532\n",
            "Val acc and loss are 0.7787 and 0.6060328\n",
            "Processing Epoch 3959\n",
            "Training acc and loss are 0.78746 and 0.6011161\n",
            "Val acc and loss are 0.7787 and 0.605997\n",
            "Processing Epoch 3960\n",
            "Training acc and loss are 0.78748 and 0.60107845\n",
            "Val acc and loss are 0.7787 and 0.60596037\n",
            "Processing Epoch 3961\n",
            "Training acc and loss are 0.7875 and 0.60104275\n",
            "Val acc and loss are 0.7787 and 0.6059252\n",
            "Processing Epoch 3962\n",
            "Training acc and loss are 0.7875 and 0.6010086\n",
            "Val acc and loss are 0.7787 and 0.6058924\n",
            "Processing Epoch 3963\n",
            "Training acc and loss are 0.7875 and 0.60097396\n",
            "Val acc and loss are 0.7787 and 0.6058593\n",
            "Processing Epoch 3964\n",
            "Training acc and loss are 0.7875 and 0.60093576\n",
            "Val acc and loss are 0.7787 and 0.60582274\n",
            "Processing Epoch 3965\n",
            "Training acc and loss are 0.7875 and 0.60090053\n",
            "Val acc and loss are 0.7787 and 0.60578835\n",
            "Processing Epoch 3966\n",
            "Training acc and loss are 0.7875 and 0.6008652\n",
            "Val acc and loss are 0.7787 and 0.6057543\n",
            "Processing Epoch 3967\n",
            "Training acc and loss are 0.78754 and 0.60083026\n",
            "Val acc and loss are 0.7787 and 0.6057202\n",
            "Processing Epoch 3968\n",
            "Training acc and loss are 0.78754 and 0.6007921\n",
            "Val acc and loss are 0.7787 and 0.6056837\n",
            "Processing Epoch 3969\n",
            "Training acc and loss are 0.78754 and 0.6007574\n",
            "Val acc and loss are 0.7787 and 0.6056498\n",
            "Processing Epoch 3970\n",
            "Training acc and loss are 0.78754 and 0.6007218\n",
            "Val acc and loss are 0.7787 and 0.60561556\n",
            "Processing Epoch 3971\n",
            "Training acc and loss are 0.78754 and 0.6006862\n",
            "Val acc and loss are 0.7787 and 0.605581\n",
            "Processing Epoch 3972\n",
            "Training acc and loss are 0.78752 and 0.60065013\n",
            "Val acc and loss are 0.7787 and 0.6055464\n",
            "Processing Epoch 3973\n",
            "Training acc and loss are 0.78752 and 0.60061556\n",
            "Val acc and loss are 0.7787 and 0.6055127\n",
            "Processing Epoch 3974\n",
            "Training acc and loss are 0.78754 and 0.6005799\n",
            "Val acc and loss are 0.7787 and 0.60547805\n",
            "Processing Epoch 3975\n",
            "Training acc and loss are 0.78754 and 0.6005415\n",
            "Val acc and loss are 0.7789 and 0.60544056\n",
            "Processing Epoch 3976\n",
            "Training acc and loss are 0.78754 and 0.6005059\n",
            "Val acc and loss are 0.7789 and 0.60540694\n",
            "Processing Epoch 3977\n",
            "Training acc and loss are 0.78754 and 0.6004698\n",
            "Val acc and loss are 0.7789 and 0.605372\n",
            "Processing Epoch 3978\n",
            "Training acc and loss are 0.78754 and 0.6004348\n",
            "Val acc and loss are 0.7789 and 0.6053384\n",
            "Processing Epoch 3979\n",
            "Training acc and loss are 0.78754 and 0.600401\n",
            "Val acc and loss are 0.7789 and 0.6053063\n",
            "Processing Epoch 3980\n",
            "Training acc and loss are 0.78756 and 0.60036737\n",
            "Val acc and loss are 0.7789 and 0.6052741\n",
            "Processing Epoch 3981\n",
            "Training acc and loss are 0.78754 and 0.6003318\n",
            "Val acc and loss are 0.779 and 0.60523957\n",
            "Processing Epoch 3982\n",
            "Training acc and loss are 0.78754 and 0.6002942\n",
            "Val acc and loss are 0.7789 and 0.6052035\n",
            "Processing Epoch 3983\n",
            "Training acc and loss are 0.78754 and 0.60025716\n",
            "Val acc and loss are 0.7789 and 0.60516727\n",
            "Processing Epoch 3984\n",
            "Training acc and loss are 0.78754 and 0.6002201\n",
            "Val acc and loss are 0.7789 and 0.60513145\n",
            "Processing Epoch 3985\n",
            "Training acc and loss are 0.78756 and 0.600183\n",
            "Val acc and loss are 0.7789 and 0.6050953\n",
            "Processing Epoch 3986\n",
            "Training acc and loss are 0.78758 and 0.600147\n",
            "Val acc and loss are 0.7789 and 0.60506064\n",
            "Processing Epoch 3987\n",
            "Training acc and loss are 0.78758 and 0.60010964\n",
            "Val acc and loss are 0.7789 and 0.6050238\n",
            "Processing Epoch 3988\n",
            "Training acc and loss are 0.7876 and 0.60007477\n",
            "Val acc and loss are 0.779 and 0.6049903\n",
            "Processing Epoch 3989\n",
            "Training acc and loss are 0.78758 and 0.6000405\n",
            "Val acc and loss are 0.779 and 0.6049572\n",
            "Processing Epoch 3990\n",
            "Training acc and loss are 0.78762 and 0.6000043\n",
            "Val acc and loss are 0.779 and 0.60492235\n",
            "Processing Epoch 3991\n",
            "Training acc and loss are 0.7876 and 0.5999685\n",
            "Val acc and loss are 0.779 and 0.6048879\n",
            "Processing Epoch 3992\n",
            "Training acc and loss are 0.7876 and 0.59993494\n",
            "Val acc and loss are 0.779 and 0.6048555\n",
            "Processing Epoch 3993\n",
            "Training acc and loss are 0.78764 and 0.5999016\n",
            "Val acc and loss are 0.779 and 0.6048228\n",
            "Processing Epoch 3994\n",
            "Training acc and loss are 0.78762 and 0.5998668\n",
            "Val acc and loss are 0.779 and 0.60478956\n",
            "Processing Epoch 3995\n",
            "Training acc and loss are 0.78764 and 0.5998314\n",
            "Val acc and loss are 0.779 and 0.60475546\n",
            "Processing Epoch 3996\n",
            "Training acc and loss are 0.78762 and 0.5997993\n",
            "Val acc and loss are 0.779 and 0.6047243\n",
            "Processing Epoch 3997\n",
            "Training acc and loss are 0.78764 and 0.59976417\n",
            "Val acc and loss are 0.779 and 0.6046911\n",
            "Processing Epoch 3998\n",
            "Training acc and loss are 0.78766 and 0.5997326\n",
            "Val acc and loss are 0.779 and 0.60466087\n",
            "Processing Epoch 3999\n",
            "Training acc and loss are 0.78766 and 0.5996975\n",
            "Val acc and loss are 0.779 and 0.6046271\n",
            "Processing Epoch 4000\n",
            "Training acc and loss are 0.78768 and 0.59966314\n",
            "Val acc and loss are 0.779 and 0.60459363\n",
            "Processing Epoch 4001\n",
            "Training acc and loss are 0.7877 and 0.59962654\n",
            "Val acc and loss are 0.779 and 0.6045582\n",
            "Processing Epoch 4002\n",
            "Training acc and loss are 0.7877 and 0.599595\n",
            "Val acc and loss are 0.779 and 0.60452795\n",
            "Processing Epoch 4003\n",
            "Training acc and loss are 0.7877 and 0.59956014\n",
            "Val acc and loss are 0.779 and 0.6044944\n",
            "Processing Epoch 4004\n",
            "Training acc and loss are 0.7877 and 0.5995209\n",
            "Val acc and loss are 0.779 and 0.6044556\n",
            "Processing Epoch 4005\n",
            "Training acc and loss are 0.7877 and 0.59948844\n",
            "Val acc and loss are 0.779 and 0.60442424\n",
            "Processing Epoch 4006\n",
            "Training acc and loss are 0.7877 and 0.59945214\n",
            "Val acc and loss are 0.779 and 0.6043891\n",
            "Processing Epoch 4007\n",
            "Training acc and loss are 0.78772 and 0.59941757\n",
            "Val acc and loss are 0.779 and 0.60435575\n",
            "Processing Epoch 4008\n",
            "Training acc and loss are 0.78772 and 0.59938\n",
            "Val acc and loss are 0.779 and 0.60431945\n",
            "Processing Epoch 4009\n",
            "Training acc and loss are 0.78772 and 0.599344\n",
            "Val acc and loss are 0.7791 and 0.60428494\n",
            "Processing Epoch 4010\n",
            "Training acc and loss are 0.78772 and 0.59930706\n",
            "Val acc and loss are 0.7791 and 0.6042492\n",
            "Processing Epoch 4011\n",
            "Training acc and loss are 0.78774 and 0.59927166\n",
            "Val acc and loss are 0.7791 and 0.604215\n",
            "Processing Epoch 4012\n",
            "Training acc and loss are 0.78774 and 0.59923446\n",
            "Val acc and loss are 0.7791 and 0.6041791\n",
            "Processing Epoch 4013\n",
            "Training acc and loss are 0.78772 and 0.5991971\n",
            "Val acc and loss are 0.7791 and 0.6041426\n",
            "Processing Epoch 4014\n",
            "Training acc and loss are 0.78776 and 0.599161\n",
            "Val acc and loss are 0.7791 and 0.60410786\n",
            "Processing Epoch 4015\n",
            "Training acc and loss are 0.78778 and 0.5991275\n",
            "Val acc and loss are 0.7792 and 0.60407627\n",
            "Processing Epoch 4016\n",
            "Training acc and loss are 0.7878 and 0.5990922\n",
            "Val acc and loss are 0.7792 and 0.6040416\n",
            "Processing Epoch 4017\n",
            "Training acc and loss are 0.7878 and 0.5990602\n",
            "Val acc and loss are 0.7792 and 0.60401124\n",
            "Processing Epoch 4018\n",
            "Training acc and loss are 0.78782 and 0.59902674\n",
            "Val acc and loss are 0.7792 and 0.6039792\n",
            "Processing Epoch 4019\n",
            "Training acc and loss are 0.78782 and 0.5989949\n",
            "Val acc and loss are 0.7792 and 0.6039488\n",
            "Processing Epoch 4020\n",
            "Training acc and loss are 0.7878 and 0.5989576\n",
            "Val acc and loss are 0.7791 and 0.60391223\n",
            "Processing Epoch 4021\n",
            "Training acc and loss are 0.78782 and 0.5989213\n",
            "Val acc and loss are 0.7791 and 0.6038766\n",
            "Processing Epoch 4022\n",
            "Training acc and loss are 0.78782 and 0.5988839\n",
            "Val acc and loss are 0.7791 and 0.60384053\n",
            "Processing Epoch 4023\n",
            "Training acc and loss are 0.78782 and 0.598848\n",
            "Val acc and loss are 0.7791 and 0.60380644\n",
            "Processing Epoch 4024\n",
            "Training acc and loss are 0.78784 and 0.59881216\n",
            "Val acc and loss are 0.7791 and 0.6037718\n",
            "Processing Epoch 4025\n",
            "Training acc and loss are 0.78784 and 0.5987758\n",
            "Val acc and loss are 0.7791 and 0.6037373\n",
            "Processing Epoch 4026\n",
            "Training acc and loss are 0.78786 and 0.59873974\n",
            "Val acc and loss are 0.7791 and 0.6037022\n",
            "Processing Epoch 4027\n",
            "Training acc and loss are 0.78788 and 0.5987057\n",
            "Val acc and loss are 0.7791 and 0.6036696\n",
            "Processing Epoch 4028\n",
            "Training acc and loss are 0.7879 and 0.5986705\n",
            "Val acc and loss are 0.7791 and 0.60363555\n",
            "Processing Epoch 4029\n",
            "Training acc and loss are 0.7879 and 0.59863585\n",
            "Val acc and loss are 0.7791 and 0.60360193\n",
            "Processing Epoch 4030\n",
            "Training acc and loss are 0.78794 and 0.59859854\n",
            "Val acc and loss are 0.7791 and 0.6035662\n",
            "Processing Epoch 4031\n",
            "Training acc and loss are 0.78796 and 0.59856224\n",
            "Val acc and loss are 0.7791 and 0.6035314\n",
            "Processing Epoch 4032\n",
            "Training acc and loss are 0.78798 and 0.59852916\n",
            "Val acc and loss are 0.7791 and 0.60349935\n",
            "Processing Epoch 4033\n",
            "Training acc and loss are 0.78798 and 0.5984957\n",
            "Val acc and loss are 0.7791 and 0.60346764\n",
            "Processing Epoch 4034\n",
            "Training acc and loss are 0.78802 and 0.59846014\n",
            "Val acc and loss are 0.7791 and 0.60343355\n",
            "Processing Epoch 4035\n",
            "Training acc and loss are 0.78804 and 0.5984251\n",
            "Val acc and loss are 0.7791 and 0.6034005\n",
            "Processing Epoch 4036\n",
            "Training acc and loss are 0.78804 and 0.5983904\n",
            "Val acc and loss are 0.7791 and 0.6033667\n",
            "Processing Epoch 4037\n",
            "Training acc and loss are 0.78806 and 0.59835327\n",
            "Val acc and loss are 0.779 and 0.6033316\n",
            "Processing Epoch 4038\n",
            "Training acc and loss are 0.78806 and 0.5983171\n",
            "Val acc and loss are 0.779 and 0.603297\n",
            "Processing Epoch 4039\n",
            "Training acc and loss are 0.78806 and 0.59828335\n",
            "Val acc and loss are 0.779 and 0.60326433\n",
            "Processing Epoch 4040\n",
            "Training acc and loss are 0.78806 and 0.59825015\n",
            "Val acc and loss are 0.779 and 0.6032318\n",
            "Processing Epoch 4041\n",
            "Training acc and loss are 0.78806 and 0.5982163\n",
            "Val acc and loss are 0.779 and 0.6031991\n",
            "Processing Epoch 4042\n",
            "Training acc and loss are 0.78806 and 0.59818244\n",
            "Val acc and loss are 0.779 and 0.6031675\n",
            "Processing Epoch 4043\n",
            "Training acc and loss are 0.78806 and 0.59814775\n",
            "Val acc and loss are 0.7791 and 0.6031336\n",
            "Processing Epoch 4044\n",
            "Training acc and loss are 0.78806 and 0.598111\n",
            "Val acc and loss are 0.7791 and 0.60309845\n",
            "Processing Epoch 4045\n",
            "Training acc and loss are 0.78808 and 0.59807694\n",
            "Val acc and loss are 0.7791 and 0.6030656\n",
            "Processing Epoch 4046\n",
            "Training acc and loss are 0.78808 and 0.59804153\n",
            "Val acc and loss are 0.7791 and 0.6030319\n",
            "Processing Epoch 4047\n",
            "Training acc and loss are 0.78808 and 0.59800345\n",
            "Val acc and loss are 0.7791 and 0.60299534\n",
            "Processing Epoch 4048\n",
            "Training acc and loss are 0.78808 and 0.5979675\n",
            "Val acc and loss are 0.7791 and 0.60296065\n",
            "Processing Epoch 4049\n",
            "Training acc and loss are 0.78808 and 0.59793186\n",
            "Val acc and loss are 0.7791 and 0.6029264\n",
            "Processing Epoch 4050\n",
            "Training acc and loss are 0.78808 and 0.59789705\n",
            "Val acc and loss are 0.7791 and 0.6028929\n",
            "Processing Epoch 4051\n",
            "Training acc and loss are 0.78808 and 0.5978611\n",
            "Val acc and loss are 0.7791 and 0.6028578\n",
            "Processing Epoch 4052\n",
            "Training acc and loss are 0.78808 and 0.59782714\n",
            "Val acc and loss are 0.7791 and 0.6028248\n",
            "Processing Epoch 4053\n",
            "Training acc and loss are 0.78808 and 0.59779143\n",
            "Val acc and loss are 0.7792 and 0.6027899\n",
            "Processing Epoch 4054\n",
            "Training acc and loss are 0.78808 and 0.5977567\n",
            "Val acc and loss are 0.7794 and 0.6027564\n",
            "Processing Epoch 4055\n",
            "Training acc and loss are 0.7881 and 0.5977228\n",
            "Val acc and loss are 0.7794 and 0.6027237\n",
            "Processing Epoch 4056\n",
            "Training acc and loss are 0.7881 and 0.5976848\n",
            "Val acc and loss are 0.7794 and 0.60268664\n",
            "Processing Epoch 4057\n",
            "Training acc and loss are 0.78812 and 0.59764767\n",
            "Val acc and loss are 0.7795 and 0.6026506\n",
            "Processing Epoch 4058\n",
            "Training acc and loss are 0.78814 and 0.59761375\n",
            "Val acc and loss are 0.7795 and 0.6026178\n",
            "Processing Epoch 4059\n",
            "Training acc and loss are 0.78814 and 0.5975775\n",
            "Val acc and loss are 0.7795 and 0.6025825\n",
            "Processing Epoch 4060\n",
            "Training acc and loss are 0.78814 and 0.5975466\n",
            "Val acc and loss are 0.7795 and 0.602552\n",
            "Processing Epoch 4061\n",
            "Training acc and loss are 0.78814 and 0.59751284\n",
            "Val acc and loss are 0.7795 and 0.6025192\n",
            "Processing Epoch 4062\n",
            "Training acc and loss are 0.78816 and 0.59747523\n",
            "Val acc and loss are 0.7795 and 0.60248244\n",
            "Processing Epoch 4063\n",
            "Training acc and loss are 0.7882 and 0.59744096\n",
            "Val acc and loss are 0.7795 and 0.60244983\n",
            "Processing Epoch 4064\n",
            "Training acc and loss are 0.78822 and 0.59740824\n",
            "Val acc and loss are 0.7795 and 0.60241854\n",
            "Processing Epoch 4065\n",
            "Training acc and loss are 0.78824 and 0.5973736\n",
            "Val acc and loss are 0.7795 and 0.6023853\n",
            "Processing Epoch 4066\n",
            "Training acc and loss are 0.78824 and 0.5973377\n",
            "Val acc and loss are 0.7795 and 0.6023504\n",
            "Processing Epoch 4067\n",
            "Training acc and loss are 0.78824 and 0.5973029\n",
            "Val acc and loss are 0.7795 and 0.6023163\n",
            "Processing Epoch 4068\n",
            "Training acc and loss are 0.78824 and 0.5972681\n",
            "Val acc and loss are 0.7795 and 0.6022828\n",
            "Processing Epoch 4069\n",
            "Training acc and loss are 0.78824 and 0.5972348\n",
            "Val acc and loss are 0.7795 and 0.60225075\n",
            "Processing Epoch 4070\n",
            "Training acc and loss are 0.78824 and 0.59719884\n",
            "Val acc and loss are 0.7795 and 0.60221595\n",
            "Processing Epoch 4071\n",
            "Training acc and loss are 0.78822 and 0.5971635\n",
            "Val acc and loss are 0.7796 and 0.6021821\n",
            "Processing Epoch 4072\n",
            "Training acc and loss are 0.78822 and 0.597131\n",
            "Val acc and loss are 0.7796 and 0.6021511\n",
            "Processing Epoch 4073\n",
            "Training acc and loss are 0.78824 and 0.5970961\n",
            "Val acc and loss are 0.7797 and 0.60211664\n",
            "Processing Epoch 4074\n",
            "Training acc and loss are 0.78822 and 0.59705806\n",
            "Val acc and loss are 0.7797 and 0.6020801\n",
            "Processing Epoch 4075\n",
            "Training acc and loss are 0.78824 and 0.59702355\n",
            "Val acc and loss are 0.7797 and 0.6020468\n",
            "Processing Epoch 4076\n",
            "Training acc and loss are 0.78824 and 0.59698874\n",
            "Val acc and loss are 0.7797 and 0.6020129\n",
            "Processing Epoch 4077\n",
            "Training acc and loss are 0.78824 and 0.5969557\n",
            "Val acc and loss are 0.7797 and 0.60198176\n",
            "Processing Epoch 4078\n",
            "Training acc and loss are 0.78824 and 0.5969211\n",
            "Val acc and loss are 0.7797 and 0.6019483\n",
            "Processing Epoch 4079\n",
            "Training acc and loss are 0.78826 and 0.59688634\n",
            "Val acc and loss are 0.7797 and 0.60191494\n",
            "Processing Epoch 4080\n",
            "Training acc and loss are 0.78828 and 0.59685296\n",
            "Val acc and loss are 0.7797 and 0.6018831\n",
            "Processing Epoch 4081\n",
            "Training acc and loss are 0.78828 and 0.5968189\n",
            "Val acc and loss are 0.7797 and 0.60185087\n",
            "Processing Epoch 4082\n",
            "Training acc and loss are 0.78828 and 0.59678435\n",
            "Val acc and loss are 0.7797 and 0.6018173\n",
            "Processing Epoch 4083\n",
            "Training acc and loss are 0.78828 and 0.5967532\n",
            "Val acc and loss are 0.7797 and 0.6017877\n",
            "Processing Epoch 4084\n",
            "Training acc and loss are 0.78828 and 0.5967181\n",
            "Val acc and loss are 0.7797 and 0.60175407\n",
            "Processing Epoch 4085\n",
            "Training acc and loss are 0.78828 and 0.59668565\n",
            "Val acc and loss are 0.7797 and 0.60172284\n",
            "Processing Epoch 4086\n",
            "Training acc and loss are 0.7883 and 0.596652\n",
            "Val acc and loss are 0.7798 and 0.60169166\n",
            "Processing Epoch 4087\n",
            "Training acc and loss are 0.7883 and 0.5966162\n",
            "Val acc and loss are 0.7798 and 0.60165733\n",
            "Processing Epoch 4088\n",
            "Training acc and loss are 0.7883 and 0.596584\n",
            "Val acc and loss are 0.7798 and 0.6016269\n",
            "Processing Epoch 4089\n",
            "Training acc and loss are 0.7883 and 0.59655017\n",
            "Val acc and loss are 0.7798 and 0.60159516\n",
            "Processing Epoch 4090\n",
            "Training acc and loss are 0.7883 and 0.5965146\n",
            "Val acc and loss are 0.7798 and 0.6015607\n",
            "Processing Epoch 4091\n",
            "Training acc and loss are 0.78832 and 0.5964806\n",
            "Val acc and loss are 0.7798 and 0.601527\n",
            "Processing Epoch 4092\n",
            "Training acc and loss are 0.78832 and 0.5964441\n",
            "Val acc and loss are 0.7798 and 0.60149187\n",
            "Processing Epoch 4093\n",
            "Training acc and loss are 0.78834 and 0.59640646\n",
            "Val acc and loss are 0.7798 and 0.6014553\n",
            "Processing Epoch 4094\n",
            "Training acc and loss are 0.78834 and 0.5963735\n",
            "Val acc and loss are 0.7798 and 0.6014232\n",
            "Processing Epoch 4095\n",
            "Training acc and loss are 0.78836 and 0.59634084\n",
            "Val acc and loss are 0.7798 and 0.6013921\n",
            "Processing Epoch 4096\n",
            "Training acc and loss are 0.78836 and 0.59630555\n",
            "Val acc and loss are 0.7799 and 0.60135794\n",
            "Processing Epoch 4097\n",
            "Training acc and loss are 0.78836 and 0.59626967\n",
            "Val acc and loss are 0.7799 and 0.60132354\n",
            "Processing Epoch 4098\n",
            "Training acc and loss are 0.78838 and 0.5962326\n",
            "Val acc and loss are 0.7799 and 0.6012873\n",
            "Processing Epoch 4099\n",
            "Training acc and loss are 0.78842 and 0.5962009\n",
            "Val acc and loss are 0.7799 and 0.6012569\n",
            "Processing Epoch 4100\n",
            "Training acc and loss are 0.78842 and 0.59616715\n",
            "Val acc and loss are 0.7799 and 0.6012244\n",
            "Processing Epoch 4101\n",
            "Training acc and loss are 0.78844 and 0.5961318\n",
            "Val acc and loss are 0.7799 and 0.6011901\n",
            "Processing Epoch 4102\n",
            "Training acc and loss are 0.78844 and 0.5961046\n",
            "Val acc and loss are 0.78 and 0.601164\n",
            "Processing Epoch 4103\n",
            "Training acc and loss are 0.78844 and 0.5960718\n",
            "Val acc and loss are 0.78 and 0.60113263\n",
            "Processing Epoch 4104\n",
            "Training acc and loss are 0.78844 and 0.59603727\n",
            "Val acc and loss are 0.78 and 0.6010998\n",
            "Processing Epoch 4105\n",
            "Training acc and loss are 0.78844 and 0.5960041\n",
            "Val acc and loss are 0.78 and 0.60106856\n",
            "Processing Epoch 4106\n",
            "Training acc and loss are 0.78842 and 0.5959717\n",
            "Val acc and loss are 0.78 and 0.6010376\n",
            "Processing Epoch 4107\n",
            "Training acc and loss are 0.78844 and 0.5959388\n",
            "Val acc and loss are 0.78 and 0.60100603\n",
            "Processing Epoch 4108\n",
            "Training acc and loss are 0.78844 and 0.5959038\n",
            "Val acc and loss are 0.78 and 0.6009723\n",
            "Processing Epoch 4109\n",
            "Training acc and loss are 0.78844 and 0.595871\n",
            "Val acc and loss are 0.78 and 0.600941\n",
            "Processing Epoch 4110\n",
            "Training acc and loss are 0.78844 and 0.59584165\n",
            "Val acc and loss are 0.78 and 0.6009133\n",
            "Processing Epoch 4111\n",
            "Training acc and loss are 0.78844 and 0.5958093\n",
            "Val acc and loss are 0.78 and 0.6008824\n",
            "Processing Epoch 4112\n",
            "Training acc and loss are 0.78844 and 0.59577507\n",
            "Val acc and loss are 0.78 and 0.60085\n",
            "Processing Epoch 4113\n",
            "Training acc and loss are 0.78844 and 0.59574157\n",
            "Val acc and loss are 0.78 and 0.60081786\n",
            "Processing Epoch 4114\n",
            "Training acc and loss are 0.78848 and 0.5957077\n",
            "Val acc and loss are 0.78 and 0.6007846\n",
            "Processing Epoch 4115\n",
            "Training acc and loss are 0.78848 and 0.5956725\n",
            "Val acc and loss are 0.7801 and 0.60075027\n",
            "Processing Epoch 4116\n",
            "Training acc and loss are 0.78848 and 0.59563804\n",
            "Val acc and loss are 0.7801 and 0.6007168\n",
            "Processing Epoch 4117\n",
            "Training acc and loss are 0.78848 and 0.59560466\n",
            "Val acc and loss are 0.78 and 0.6006841\n",
            "Processing Epoch 4118\n",
            "Training acc and loss are 0.78846 and 0.5955707\n",
            "Val acc and loss are 0.7801 and 0.60065144\n",
            "Processing Epoch 4119\n",
            "Training acc and loss are 0.78852 and 0.59553677\n",
            "Val acc and loss are 0.7801 and 0.60061896\n",
            "Processing Epoch 4120\n",
            "Training acc and loss are 0.78854 and 0.59550685\n",
            "Val acc and loss are 0.7802 and 0.60058963\n",
            "Processing Epoch 4121\n",
            "Training acc and loss are 0.78854 and 0.5954743\n",
            "Val acc and loss are 0.7801 and 0.6005586\n",
            "Processing Epoch 4122\n",
            "Training acc and loss are 0.78854 and 0.5954394\n",
            "Val acc and loss are 0.7802 and 0.6005245\n",
            "Processing Epoch 4123\n",
            "Training acc and loss are 0.78858 and 0.59540564\n",
            "Val acc and loss are 0.7802 and 0.6004914\n",
            "Processing Epoch 4124\n",
            "Training acc and loss are 0.78862 and 0.5953718\n",
            "Val acc and loss are 0.7802 and 0.6004592\n",
            "Processing Epoch 4125\n",
            "Training acc and loss are 0.78866 and 0.5953372\n",
            "Val acc and loss are 0.7802 and 0.6004259\n",
            "Processing Epoch 4126\n",
            "Training acc and loss are 0.78874 and 0.5953033\n",
            "Val acc and loss are 0.7802 and 0.60039353\n",
            "Processing Epoch 4127\n",
            "Training acc and loss are 0.78874 and 0.5952681\n",
            "Val acc and loss are 0.7802 and 0.6003595\n",
            "Processing Epoch 4128\n",
            "Training acc and loss are 0.78874 and 0.5952337\n",
            "Val acc and loss are 0.7802 and 0.6003253\n",
            "Processing Epoch 4129\n",
            "Training acc and loss are 0.78874 and 0.5952003\n",
            "Val acc and loss are 0.7802 and 0.60029334\n",
            "Processing Epoch 4130\n",
            "Training acc and loss are 0.78872 and 0.5951629\n",
            "Val acc and loss are 0.7802 and 0.6002569\n",
            "Processing Epoch 4131\n",
            "Training acc and loss are 0.78874 and 0.59512883\n",
            "Val acc and loss are 0.7802 and 0.60022444\n",
            "Processing Epoch 4132\n",
            "Training acc and loss are 0.78874 and 0.5950924\n",
            "Val acc and loss are 0.7802 and 0.6001888\n",
            "Processing Epoch 4133\n",
            "Training acc and loss are 0.78876 and 0.5950607\n",
            "Val acc and loss are 0.7802 and 0.6001581\n",
            "Processing Epoch 4134\n",
            "Training acc and loss are 0.78878 and 0.59502584\n",
            "Val acc and loss are 0.7802 and 0.60012347\n",
            "Processing Epoch 4135\n",
            "Training acc and loss are 0.78884 and 0.594991\n",
            "Val acc and loss are 0.7802 and 0.6000897\n",
            "Processing Epoch 4136\n",
            "Training acc and loss are 0.78884 and 0.59495884\n",
            "Val acc and loss are 0.7802 and 0.6000583\n",
            "Processing Epoch 4137\n",
            "Training acc and loss are 0.78886 and 0.5949227\n",
            "Val acc and loss are 0.7803 and 0.60002327\n",
            "Processing Epoch 4138\n",
            "Training acc and loss are 0.78886 and 0.5948891\n",
            "Val acc and loss are 0.7803 and 0.5999912\n",
            "Processing Epoch 4139\n",
            "Training acc and loss are 0.78888 and 0.59485626\n",
            "Val acc and loss are 0.7803 and 0.5999597\n",
            "Processing Epoch 4140\n",
            "Training acc and loss are 0.7889 and 0.5948204\n",
            "Val acc and loss are 0.7803 and 0.5999246\n",
            "Processing Epoch 4141\n",
            "Training acc and loss are 0.78886 and 0.59478784\n",
            "Val acc and loss are 0.7803 and 0.5998932\n",
            "Processing Epoch 4142\n",
            "Training acc and loss are 0.78888 and 0.5947536\n",
            "Val acc and loss are 0.7804 and 0.5998598\n",
            "Processing Epoch 4143\n",
            "Training acc and loss are 0.78888 and 0.5947239\n",
            "Val acc and loss are 0.7804 and 0.59983104\n",
            "Processing Epoch 4144\n",
            "Training acc and loss are 0.78888 and 0.59468985\n",
            "Val acc and loss are 0.7804 and 0.5997977\n",
            "Processing Epoch 4145\n",
            "Training acc and loss are 0.78888 and 0.594655\n",
            "Val acc and loss are 0.7804 and 0.5997639\n",
            "Processing Epoch 4146\n",
            "Training acc and loss are 0.78888 and 0.5946232\n",
            "Val acc and loss are 0.7804 and 0.5997329\n",
            "Processing Epoch 4147\n",
            "Training acc and loss are 0.7889 and 0.5945891\n",
            "Val acc and loss are 0.7804 and 0.5997003\n",
            "Processing Epoch 4148\n",
            "Training acc and loss are 0.78892 and 0.5945558\n",
            "Val acc and loss are 0.7804 and 0.5996674\n",
            "Processing Epoch 4149\n",
            "Training acc and loss are 0.78894 and 0.5945206\n",
            "Val acc and loss are 0.7804 and 0.5996338\n",
            "Processing Epoch 4150\n",
            "Training acc and loss are 0.78894 and 0.5944883\n",
            "Val acc and loss are 0.7804 and 0.5996025\n",
            "Processing Epoch 4151\n",
            "Training acc and loss are 0.78896 and 0.5944547\n",
            "Val acc and loss are 0.7804 and 0.59957165\n",
            "Processing Epoch 4152\n",
            "Training acc and loss are 0.78898 and 0.5944209\n",
            "Val acc and loss are 0.7804 and 0.59953904\n",
            "Processing Epoch 4153\n",
            "Training acc and loss are 0.78898 and 0.5943854\n",
            "Val acc and loss are 0.7804 and 0.5995048\n",
            "Processing Epoch 4154\n",
            "Training acc and loss are 0.789 and 0.5943547\n",
            "Val acc and loss are 0.7804 and 0.599475\n",
            "Processing Epoch 4155\n",
            "Training acc and loss are 0.789 and 0.5943206\n",
            "Val acc and loss are 0.7804 and 0.5994425\n",
            "Processing Epoch 4156\n",
            "Training acc and loss are 0.78904 and 0.5942873\n",
            "Val acc and loss are 0.7804 and 0.59941065\n",
            "Processing Epoch 4157\n",
            "Training acc and loss are 0.78904 and 0.5942519\n",
            "Val acc and loss are 0.7804 and 0.59937614\n",
            "Processing Epoch 4158\n",
            "Training acc and loss are 0.78904 and 0.5942181\n",
            "Val acc and loss are 0.7804 and 0.59934336\n",
            "Processing Epoch 4159\n",
            "Training acc and loss are 0.78906 and 0.5941848\n",
            "Val acc and loss are 0.7805 and 0.59931135\n",
            "Processing Epoch 4160\n",
            "Training acc and loss are 0.7891 and 0.5941528\n",
            "Val acc and loss are 0.7805 and 0.5992806\n",
            "Processing Epoch 4161\n",
            "Training acc and loss are 0.7891 and 0.5941213\n",
            "Val acc and loss are 0.7805 and 0.59925056\n",
            "Processing Epoch 4162\n",
            "Training acc and loss are 0.78912 and 0.5940883\n",
            "Val acc and loss are 0.7805 and 0.5992187\n",
            "Processing Epoch 4163\n",
            "Training acc and loss are 0.7891 and 0.59405464\n",
            "Val acc and loss are 0.7805 and 0.5991863\n",
            "Processing Epoch 4164\n",
            "Training acc and loss are 0.78912 and 0.5940182\n",
            "Val acc and loss are 0.7805 and 0.5991515\n",
            "Processing Epoch 4165\n",
            "Training acc and loss are 0.7891 and 0.59398514\n",
            "Val acc and loss are 0.7805 and 0.5991197\n",
            "Processing Epoch 4166\n",
            "Training acc and loss are 0.78914 and 0.59395367\n",
            "Val acc and loss are 0.7805 and 0.5990887\n",
            "Processing Epoch 4167\n",
            "Training acc and loss are 0.78914 and 0.59391975\n",
            "Val acc and loss are 0.7805 and 0.5990554\n",
            "Processing Epoch 4168\n",
            "Training acc and loss are 0.78912 and 0.5938866\n",
            "Val acc and loss are 0.7805 and 0.5990237\n",
            "Processing Epoch 4169\n",
            "Training acc and loss are 0.7891 and 0.5938543\n",
            "Val acc and loss are 0.7805 and 0.5989927\n",
            "Processing Epoch 4170\n",
            "Training acc and loss are 0.78912 and 0.5938212\n",
            "Val acc and loss are 0.7805 and 0.5989611\n",
            "Processing Epoch 4171\n",
            "Training acc and loss are 0.78916 and 0.59378326\n",
            "Val acc and loss are 0.7805 and 0.5989238\n",
            "Processing Epoch 4172\n",
            "Training acc and loss are 0.78918 and 0.59375256\n",
            "Val acc and loss are 0.7806 and 0.5988935\n",
            "Processing Epoch 4173\n",
            "Training acc and loss are 0.78916 and 0.5937189\n",
            "Val acc and loss are 0.7806 and 0.59886116\n",
            "Processing Epoch 4174\n",
            "Training acc and loss are 0.78918 and 0.59368795\n",
            "Val acc and loss are 0.7806 and 0.5988313\n",
            "Processing Epoch 4175\n",
            "Training acc and loss are 0.78918 and 0.59365076\n",
            "Val acc and loss are 0.7807 and 0.598795\n",
            "Processing Epoch 4176\n",
            "Training acc and loss are 0.78918 and 0.5936154\n",
            "Val acc and loss are 0.7807 and 0.5987615\n",
            "Processing Epoch 4177\n",
            "Training acc and loss are 0.7892 and 0.59358174\n",
            "Val acc and loss are 0.7807 and 0.59872967\n",
            "Processing Epoch 4178\n",
            "Training acc and loss are 0.7892 and 0.5935504\n",
            "Val acc and loss are 0.7807 and 0.59869915\n",
            "Processing Epoch 4179\n",
            "Training acc and loss are 0.78924 and 0.5935145\n",
            "Val acc and loss are 0.7808 and 0.59866446\n",
            "Processing Epoch 4180\n",
            "Training acc and loss are 0.78924 and 0.59348005\n",
            "Val acc and loss are 0.7808 and 0.5986305\n",
            "Processing Epoch 4181\n",
            "Training acc and loss are 0.78926 and 0.59344536\n",
            "Val acc and loss are 0.7808 and 0.5985956\n",
            "Processing Epoch 4182\n",
            "Training acc and loss are 0.78926 and 0.5934118\n",
            "Val acc and loss are 0.7809 and 0.5985631\n",
            "Processing Epoch 4183\n",
            "Training acc and loss are 0.7893 and 0.59338015\n",
            "Val acc and loss are 0.7809 and 0.598532\n",
            "Processing Epoch 4184\n",
            "Training acc and loss are 0.7893 and 0.5933493\n",
            "Val acc and loss are 0.7809 and 0.59850234\n",
            "Processing Epoch 4185\n",
            "Training acc and loss are 0.78932 and 0.5933163\n",
            "Val acc and loss are 0.7809 and 0.598471\n",
            "Processing Epoch 4186\n",
            "Training acc and loss are 0.78928 and 0.59328234\n",
            "Val acc and loss are 0.781 and 0.5984385\n",
            "Processing Epoch 4187\n",
            "Training acc and loss are 0.7893 and 0.59324783\n",
            "Val acc and loss are 0.781 and 0.59840506\n",
            "Processing Epoch 4188\n",
            "Training acc and loss are 0.78934 and 0.5932136\n",
            "Val acc and loss are 0.781 and 0.5983718\n",
            "Processing Epoch 4189\n",
            "Training acc and loss are 0.78934 and 0.5931795\n",
            "Val acc and loss are 0.781 and 0.5983386\n",
            "Processing Epoch 4190\n",
            "Training acc and loss are 0.78936 and 0.59314424\n",
            "Val acc and loss are 0.781 and 0.5983045\n",
            "Processing Epoch 4191\n",
            "Training acc and loss are 0.78938 and 0.5931107\n",
            "Val acc and loss are 0.781 and 0.5982724\n",
            "Processing Epoch 4192\n",
            "Training acc and loss are 0.78936 and 0.593077\n",
            "Val acc and loss are 0.7811 and 0.59824014\n",
            "Processing Epoch 4193\n",
            "Training acc and loss are 0.78938 and 0.59304523\n",
            "Val acc and loss are 0.7812 and 0.5982096\n",
            "Processing Epoch 4194\n",
            "Training acc and loss are 0.78936 and 0.59301466\n",
            "Val acc and loss are 0.7813 and 0.5981796\n",
            "Processing Epoch 4195\n",
            "Training acc and loss are 0.78936 and 0.5929809\n",
            "Val acc and loss are 0.7813 and 0.59814656\n",
            "Processing Epoch 4196\n",
            "Training acc and loss are 0.78936 and 0.5929467\n",
            "Val acc and loss are 0.7813 and 0.5981133\n",
            "Processing Epoch 4197\n",
            "Training acc and loss are 0.78936 and 0.5929112\n",
            "Val acc and loss are 0.7813 and 0.5980785\n",
            "Processing Epoch 4198\n",
            "Training acc and loss are 0.78938 and 0.59287846\n",
            "Val acc and loss are 0.7813 and 0.5980462\n",
            "Processing Epoch 4199\n",
            "Training acc and loss are 0.7894 and 0.5928432\n",
            "Val acc and loss are 0.7813 and 0.5980122\n",
            "Processing Epoch 4200\n",
            "Training acc and loss are 0.78942 and 0.5928112\n",
            "Val acc and loss are 0.7814 and 0.5979813\n",
            "Processing Epoch 4201\n",
            "Training acc and loss are 0.78946 and 0.5927794\n",
            "Val acc and loss are 0.7812 and 0.59795046\n",
            "Processing Epoch 4202\n",
            "Training acc and loss are 0.78948 and 0.59274846\n",
            "Val acc and loss are 0.7813 and 0.5979203\n",
            "Processing Epoch 4203\n",
            "Training acc and loss are 0.78948 and 0.5927166\n",
            "Val acc and loss are 0.7813 and 0.5978897\n",
            "Processing Epoch 4204\n",
            "Training acc and loss are 0.78946 and 0.59268147\n",
            "Val acc and loss are 0.7813 and 0.5978551\n",
            "Processing Epoch 4205\n",
            "Training acc and loss are 0.78946 and 0.5926525\n",
            "Val acc and loss are 0.7813 and 0.5978283\n",
            "Processing Epoch 4206\n",
            "Training acc and loss are 0.78946 and 0.5926215\n",
            "Val acc and loss are 0.7813 and 0.5977988\n",
            "Processing Epoch 4207\n",
            "Training acc and loss are 0.78946 and 0.59258866\n",
            "Val acc and loss are 0.7814 and 0.5977678\n",
            "Processing Epoch 4208\n",
            "Training acc and loss are 0.78946 and 0.592555\n",
            "Val acc and loss are 0.7815 and 0.5977354\n",
            "Processing Epoch 4209\n",
            "Training acc and loss are 0.7895 and 0.59252185\n",
            "Val acc and loss are 0.7815 and 0.5977031\n",
            "Processing Epoch 4210\n",
            "Training acc and loss are 0.78948 and 0.59248984\n",
            "Val acc and loss are 0.7815 and 0.59767205\n",
            "Processing Epoch 4211\n",
            "Training acc and loss are 0.78948 and 0.5924547\n",
            "Val acc and loss are 0.7815 and 0.597638\n",
            "Processing Epoch 4212\n",
            "Training acc and loss are 0.7895 and 0.592422\n",
            "Val acc and loss are 0.7815 and 0.59760576\n",
            "Processing Epoch 4213\n",
            "Training acc and loss are 0.7895 and 0.5923921\n",
            "Val acc and loss are 0.7815 and 0.59757674\n",
            "Processing Epoch 4214\n",
            "Training acc and loss are 0.78954 and 0.5923624\n",
            "Val acc and loss are 0.7815 and 0.59754837\n",
            "Processing Epoch 4215\n",
            "Training acc and loss are 0.78954 and 0.5923286\n",
            "Val acc and loss are 0.7815 and 0.5975152\n",
            "Processing Epoch 4216\n",
            "Training acc and loss are 0.78954 and 0.59229463\n",
            "Val acc and loss are 0.7815 and 0.5974828\n",
            "Processing Epoch 4217\n",
            "Training acc and loss are 0.78954 and 0.5922632\n",
            "Val acc and loss are 0.7815 and 0.59745175\n",
            "Processing Epoch 4218\n",
            "Training acc and loss are 0.78952 and 0.5922325\n",
            "Val acc and loss are 0.7815 and 0.59742236\n",
            "Processing Epoch 4219\n",
            "Training acc and loss are 0.78952 and 0.59219915\n",
            "Val acc and loss are 0.7815 and 0.5973898\n",
            "Processing Epoch 4220\n",
            "Training acc and loss are 0.78954 and 0.59216243\n",
            "Val acc and loss are 0.7815 and 0.5973541\n",
            "Processing Epoch 4221\n",
            "Training acc and loss are 0.78954 and 0.59212834\n",
            "Val acc and loss are 0.7815 and 0.59732133\n",
            "Processing Epoch 4222\n",
            "Training acc and loss are 0.78954 and 0.5920964\n",
            "Val acc and loss are 0.7815 and 0.597291\n",
            "Processing Epoch 4223\n",
            "Training acc and loss are 0.78956 and 0.59206444\n",
            "Val acc and loss are 0.7815 and 0.59725976\n",
            "Processing Epoch 4224\n",
            "Training acc and loss are 0.78962 and 0.5920346\n",
            "Val acc and loss are 0.7815 and 0.597231\n",
            "Processing Epoch 4225\n",
            "Training acc and loss are 0.78962 and 0.59200096\n",
            "Val acc and loss are 0.7815 and 0.59719807\n",
            "Processing Epoch 4226\n",
            "Training acc and loss are 0.78958 and 0.59196866\n",
            "Val acc and loss are 0.7815 and 0.5971668\n",
            "Processing Epoch 4227\n",
            "Training acc and loss are 0.78958 and 0.59193337\n",
            "Val acc and loss are 0.7815 and 0.5971326\n",
            "Processing Epoch 4228\n",
            "Training acc and loss are 0.78962 and 0.59190184\n",
            "Val acc and loss are 0.7815 and 0.5971017\n",
            "Processing Epoch 4229\n",
            "Training acc and loss are 0.78962 and 0.59186655\n",
            "Val acc and loss are 0.7815 and 0.5970668\n",
            "Processing Epoch 4230\n",
            "Training acc and loss are 0.78962 and 0.59183466\n",
            "Val acc and loss are 0.7815 and 0.59703547\n",
            "Processing Epoch 4231\n",
            "Training acc and loss are 0.78962 and 0.59180087\n",
            "Val acc and loss are 0.7815 and 0.59700274\n",
            "Processing Epoch 4232\n",
            "Training acc and loss are 0.78964 and 0.5917674\n",
            "Val acc and loss are 0.7814 and 0.5969706\n",
            "Processing Epoch 4233\n",
            "Training acc and loss are 0.7896 and 0.59173423\n",
            "Val acc and loss are 0.7814 and 0.59693867\n",
            "Processing Epoch 4234\n",
            "Training acc and loss are 0.78962 and 0.5917029\n",
            "Val acc and loss are 0.7814 and 0.59690887\n",
            "Processing Epoch 4235\n",
            "Training acc and loss are 0.78962 and 0.5916719\n",
            "Val acc and loss are 0.7814 and 0.5968796\n",
            "Processing Epoch 4236\n",
            "Training acc and loss are 0.78962 and 0.59164053\n",
            "Val acc and loss are 0.7814 and 0.5968496\n",
            "Processing Epoch 4237\n",
            "Training acc and loss are 0.78964 and 0.5916082\n",
            "Val acc and loss are 0.7814 and 0.5968179\n",
            "Processing Epoch 4238\n",
            "Training acc and loss are 0.78964 and 0.5915773\n",
            "Val acc and loss are 0.7814 and 0.5967887\n",
            "Processing Epoch 4239\n",
            "Training acc and loss are 0.78964 and 0.59154713\n",
            "Val acc and loss are 0.7814 and 0.59675974\n",
            "Processing Epoch 4240\n",
            "Training acc and loss are 0.78966 and 0.59151626\n",
            "Val acc and loss are 0.7814 and 0.5967299\n",
            "Processing Epoch 4241\n",
            "Training acc and loss are 0.78968 and 0.59148437\n",
            "Val acc and loss are 0.7814 and 0.596699\n",
            "Processing Epoch 4242\n",
            "Training acc and loss are 0.78972 and 0.5914528\n",
            "Val acc and loss are 0.7813 and 0.5966687\n",
            "Processing Epoch 4243\n",
            "Training acc and loss are 0.78976 and 0.5914222\n",
            "Val acc and loss are 0.7813 and 0.5966388\n",
            "Processing Epoch 4244\n",
            "Training acc and loss are 0.78978 and 0.5913888\n",
            "Val acc and loss are 0.7813 and 0.5966067\n",
            "Processing Epoch 4245\n",
            "Training acc and loss are 0.7898 and 0.5913574\n",
            "Val acc and loss are 0.7813 and 0.59657687\n",
            "Processing Epoch 4246\n",
            "Training acc and loss are 0.78978 and 0.59132487\n",
            "Val acc and loss are 0.7813 and 0.59654546\n",
            "Processing Epoch 4247\n",
            "Training acc and loss are 0.78976 and 0.5912927\n",
            "Val acc and loss are 0.7813 and 0.59651434\n",
            "Processing Epoch 4248\n",
            "Training acc and loss are 0.78974 and 0.59125847\n",
            "Val acc and loss are 0.7813 and 0.5964813\n",
            "Processing Epoch 4249\n",
            "Training acc and loss are 0.78974 and 0.5912255\n",
            "Val acc and loss are 0.7813 and 0.59644884\n",
            "Processing Epoch 4250\n",
            "Training acc and loss are 0.78974 and 0.5911941\n",
            "Val acc and loss are 0.7813 and 0.596419\n",
            "Processing Epoch 4251\n",
            "Training acc and loss are 0.78982 and 0.5911632\n",
            "Val acc and loss are 0.7814 and 0.59638965\n",
            "Processing Epoch 4252\n",
            "Training acc and loss are 0.78982 and 0.59113175\n",
            "Val acc and loss are 0.7814 and 0.59635955\n",
            "Processing Epoch 4253\n",
            "Training acc and loss are 0.78982 and 0.5911007\n",
            "Val acc and loss are 0.7814 and 0.5963288\n",
            "Processing Epoch 4254\n",
            "Training acc and loss are 0.7898 and 0.59106827\n",
            "Val acc and loss are 0.7814 and 0.5962975\n",
            "Processing Epoch 4255\n",
            "Training acc and loss are 0.78978 and 0.5910331\n",
            "Val acc and loss are 0.7814 and 0.5962638\n",
            "Processing Epoch 4256\n",
            "Training acc and loss are 0.78978 and 0.59099966\n",
            "Val acc and loss are 0.7813 and 0.59623146\n",
            "Processing Epoch 4257\n",
            "Training acc and loss are 0.7898 and 0.59096545\n",
            "Val acc and loss are 0.7813 and 0.5961979\n",
            "Processing Epoch 4258\n",
            "Training acc and loss are 0.78978 and 0.5909344\n",
            "Val acc and loss are 0.7813 and 0.5961681\n",
            "Processing Epoch 4259\n",
            "Training acc and loss are 0.78978 and 0.5908991\n",
            "Val acc and loss are 0.7813 and 0.59613454\n",
            "Processing Epoch 4260\n",
            "Training acc and loss are 0.78984 and 0.59086585\n",
            "Val acc and loss are 0.7815 and 0.59610254\n",
            "Processing Epoch 4261\n",
            "Training acc and loss are 0.78978 and 0.5908341\n",
            "Val acc and loss are 0.7815 and 0.5960721\n",
            "Processing Epoch 4262\n",
            "Training acc and loss are 0.7898 and 0.5908023\n",
            "Val acc and loss are 0.7816 and 0.596042\n",
            "Processing Epoch 4263\n",
            "Training acc and loss are 0.78978 and 0.59076774\n",
            "Val acc and loss are 0.7817 and 0.59600866\n",
            "Processing Epoch 4264\n",
            "Training acc and loss are 0.78982 and 0.5907308\n",
            "Val acc and loss are 0.7816 and 0.59597206\n",
            "Processing Epoch 4265\n",
            "Training acc and loss are 0.78982 and 0.59069693\n",
            "Val acc and loss are 0.7817 and 0.59593934\n",
            "Processing Epoch 4266\n",
            "Training acc and loss are 0.78986 and 0.5906647\n",
            "Val acc and loss are 0.7817 and 0.59590876\n",
            "Processing Epoch 4267\n",
            "Training acc and loss are 0.78982 and 0.59063166\n",
            "Val acc and loss are 0.7817 and 0.595877\n",
            "Processing Epoch 4268\n",
            "Training acc and loss are 0.78982 and 0.590597\n",
            "Val acc and loss are 0.7817 and 0.59584343\n",
            "Processing Epoch 4269\n",
            "Training acc and loss are 0.78986 and 0.5905657\n",
            "Val acc and loss are 0.7817 and 0.59581316\n",
            "Processing Epoch 4270\n",
            "Training acc and loss are 0.78984 and 0.5905314\n",
            "Val acc and loss are 0.7817 and 0.5957803\n",
            "Processing Epoch 4271\n",
            "Training acc and loss are 0.78984 and 0.59050435\n",
            "Val acc and loss are 0.7817 and 0.5957543\n",
            "Processing Epoch 4272\n",
            "Training acc and loss are 0.78984 and 0.5904706\n",
            "Val acc and loss are 0.7817 and 0.5957215\n",
            "Processing Epoch 4273\n",
            "Training acc and loss are 0.78986 and 0.59044135\n",
            "Val acc and loss are 0.7817 and 0.5956935\n",
            "Processing Epoch 4274\n",
            "Training acc and loss are 0.78986 and 0.5904098\n",
            "Val acc and loss are 0.7817 and 0.5956632\n",
            "Processing Epoch 4275\n",
            "Training acc and loss are 0.78986 and 0.5903771\n",
            "Val acc and loss are 0.7818 and 0.5956317\n",
            "Processing Epoch 4276\n",
            "Training acc and loss are 0.78986 and 0.5903462\n",
            "Val acc and loss are 0.7818 and 0.59560263\n",
            "Processing Epoch 4277\n",
            "Training acc and loss are 0.78988 and 0.59031487\n",
            "Val acc and loss are 0.7818 and 0.5955725\n",
            "Processing Epoch 4278\n",
            "Training acc and loss are 0.7899 and 0.5902848\n",
            "Val acc and loss are 0.7818 and 0.5955436\n",
            "Processing Epoch 4279\n",
            "Training acc and loss are 0.78992 and 0.590255\n",
            "Val acc and loss are 0.7818 and 0.5955152\n",
            "Processing Epoch 4280\n",
            "Training acc and loss are 0.78996 and 0.59022516\n",
            "Val acc and loss are 0.7819 and 0.59548634\n",
            "Processing Epoch 4281\n",
            "Training acc and loss are 0.78996 and 0.5901962\n",
            "Val acc and loss are 0.7818 and 0.5954595\n",
            "Processing Epoch 4282\n",
            "Training acc and loss are 0.78998 and 0.5901675\n",
            "Val acc and loss are 0.7818 and 0.59543234\n",
            "Processing Epoch 4283\n",
            "Training acc and loss are 0.78998 and 0.5901342\n",
            "Val acc and loss are 0.7818 and 0.5954003\n",
            "Processing Epoch 4284\n",
            "Training acc and loss are 0.79002 and 0.59010345\n",
            "Val acc and loss are 0.7818 and 0.5953708\n",
            "Processing Epoch 4285\n",
            "Training acc and loss are 0.79004 and 0.59007376\n",
            "Val acc and loss are 0.7819 and 0.5953425\n",
            "Processing Epoch 4286\n",
            "Training acc and loss are 0.79004 and 0.59004223\n",
            "Val acc and loss are 0.7819 and 0.5953125\n",
            "Processing Epoch 4287\n",
            "Training acc and loss are 0.79004 and 0.59001124\n",
            "Val acc and loss are 0.7819 and 0.5952825\n",
            "Processing Epoch 4288\n",
            "Training acc and loss are 0.79004 and 0.58997947\n",
            "Val acc and loss are 0.7819 and 0.59525144\n",
            "Processing Epoch 4289\n",
            "Training acc and loss are 0.79004 and 0.58994716\n",
            "Val acc and loss are 0.7821 and 0.5952199\n",
            "Processing Epoch 4290\n",
            "Training acc and loss are 0.79006 and 0.58991456\n",
            "Val acc and loss are 0.7821 and 0.5951881\n",
            "Processing Epoch 4291\n",
            "Training acc and loss are 0.79006 and 0.5898822\n",
            "Val acc and loss are 0.7822 and 0.5951569\n",
            "Processing Epoch 4292\n",
            "Training acc and loss are 0.79008 and 0.58985007\n",
            "Val acc and loss are 0.782 and 0.5951255\n",
            "Processing Epoch 4293\n",
            "Training acc and loss are 0.79006 and 0.58981764\n",
            "Val acc and loss are 0.782 and 0.5950945\n",
            "Processing Epoch 4294\n",
            "Training acc and loss are 0.79008 and 0.5897846\n",
            "Val acc and loss are 0.7821 and 0.5950626\n",
            "Processing Epoch 4295\n",
            "Training acc and loss are 0.79008 and 0.5897529\n",
            "Val acc and loss are 0.7821 and 0.5950322\n",
            "Processing Epoch 4296\n",
            "Training acc and loss are 0.79014 and 0.5897188\n",
            "Val acc and loss are 0.7822 and 0.5949992\n",
            "Processing Epoch 4297\n",
            "Training acc and loss are 0.79016 and 0.5896857\n",
            "Val acc and loss are 0.7823 and 0.594967\n",
            "Processing Epoch 4298\n",
            "Training acc and loss are 0.79016 and 0.5896527\n",
            "Val acc and loss are 0.7823 and 0.59493524\n",
            "Processing Epoch 4299\n",
            "Training acc and loss are 0.79018 and 0.58962345\n",
            "Val acc and loss are 0.7823 and 0.5949067\n",
            "Processing Epoch 4300\n",
            "Training acc and loss are 0.79018 and 0.589593\n",
            "Val acc and loss are 0.7823 and 0.5948768\n",
            "Processing Epoch 4301\n",
            "Training acc and loss are 0.79018 and 0.5895619\n",
            "Val acc and loss are 0.7823 and 0.5948469\n",
            "Processing Epoch 4302\n",
            "Training acc and loss are 0.79018 and 0.5895302\n",
            "Val acc and loss are 0.7823 and 0.594817\n",
            "Processing Epoch 4303\n",
            "Training acc and loss are 0.79018 and 0.58949775\n",
            "Val acc and loss are 0.7823 and 0.5947859\n",
            "Processing Epoch 4304\n",
            "Training acc and loss are 0.7902 and 0.5894671\n",
            "Val acc and loss are 0.7823 and 0.5947567\n",
            "Processing Epoch 4305\n",
            "Training acc and loss are 0.79024 and 0.58943117\n",
            "Val acc and loss are 0.7823 and 0.5947216\n",
            "Processing Epoch 4306\n",
            "Training acc and loss are 0.79026 and 0.5893971\n",
            "Val acc and loss are 0.7824 and 0.5946888\n",
            "Processing Epoch 4307\n",
            "Training acc and loss are 0.79026 and 0.5893666\n",
            "Val acc and loss are 0.7824 and 0.59466\n",
            "Processing Epoch 4308\n",
            "Training acc and loss are 0.79026 and 0.5893344\n",
            "Val acc and loss are 0.7824 and 0.5946293\n",
            "Processing Epoch 4309\n",
            "Training acc and loss are 0.79026 and 0.58930373\n",
            "Val acc and loss are 0.7824 and 0.5946\n",
            "Processing Epoch 4310\n",
            "Training acc and loss are 0.79024 and 0.5892722\n",
            "Val acc and loss are 0.7824 and 0.5945696\n",
            "Processing Epoch 4311\n",
            "Training acc and loss are 0.7903 and 0.5892404\n",
            "Val acc and loss are 0.7824 and 0.5945395\n",
            "Processing Epoch 4312\n",
            "Training acc and loss are 0.7903 and 0.5892068\n",
            "Val acc and loss are 0.7824 and 0.59450674\n",
            "Processing Epoch 4313\n",
            "Training acc and loss are 0.7903 and 0.5891744\n",
            "Val acc and loss are 0.7824 and 0.5944751\n",
            "Processing Epoch 4314\n",
            "Training acc and loss are 0.79032 and 0.5891438\n",
            "Val acc and loss are 0.7824 and 0.5944459\n",
            "Processing Epoch 4315\n",
            "Training acc and loss are 0.79032 and 0.58911353\n",
            "Val acc and loss are 0.7824 and 0.5944164\n",
            "Processing Epoch 4316\n",
            "Training acc and loss are 0.79032 and 0.58907944\n",
            "Val acc and loss are 0.7824 and 0.5943828\n",
            "Processing Epoch 4317\n",
            "Training acc and loss are 0.79036 and 0.58904725\n",
            "Val acc and loss are 0.7824 and 0.5943516\n",
            "Processing Epoch 4318\n",
            "Training acc and loss are 0.79038 and 0.58901596\n",
            "Val acc and loss are 0.7824 and 0.5943215\n",
            "Processing Epoch 4319\n",
            "Training acc and loss are 0.79036 and 0.58898455\n",
            "Val acc and loss are 0.7823 and 0.5942911\n",
            "Processing Epoch 4320\n",
            "Training acc and loss are 0.79036 and 0.58895206\n",
            "Val acc and loss are 0.7823 and 0.5942594\n",
            "Processing Epoch 4321\n",
            "Training acc and loss are 0.79038 and 0.5889235\n",
            "Val acc and loss are 0.7823 and 0.594232\n",
            "Processing Epoch 4322\n",
            "Training acc and loss are 0.7904 and 0.588893\n",
            "Val acc and loss are 0.7823 and 0.59420216\n",
            "Processing Epoch 4323\n",
            "Training acc and loss are 0.7904 and 0.5888626\n",
            "Val acc and loss are 0.7824 and 0.5941733\n",
            "Processing Epoch 4324\n",
            "Training acc and loss are 0.79038 and 0.5888282\n",
            "Val acc and loss are 0.7824 and 0.59413993\n",
            "Processing Epoch 4325\n",
            "Training acc and loss are 0.79036 and 0.58879906\n",
            "Val acc and loss are 0.7824 and 0.59411234\n",
            "Processing Epoch 4326\n",
            "Training acc and loss are 0.79034 and 0.58876586\n",
            "Val acc and loss are 0.7824 and 0.59408045\n",
            "Processing Epoch 4327\n",
            "Training acc and loss are 0.79036 and 0.58873534\n",
            "Val acc and loss are 0.7824 and 0.5940515\n",
            "Processing Epoch 4328\n",
            "Training acc and loss are 0.79038 and 0.58870643\n",
            "Val acc and loss are 0.7824 and 0.59402364\n",
            "Processing Epoch 4329\n",
            "Training acc and loss are 0.79038 and 0.5886762\n",
            "Val acc and loss are 0.7824 and 0.59399444\n",
            "Processing Epoch 4330\n",
            "Training acc and loss are 0.79042 and 0.5886427\n",
            "Val acc and loss are 0.7824 and 0.59396183\n",
            "Processing Epoch 4331\n",
            "Training acc and loss are 0.79042 and 0.5886088\n",
            "Val acc and loss are 0.7824 and 0.5939287\n",
            "Processing Epoch 4332\n",
            "Training acc and loss are 0.79044 and 0.58857936\n",
            "Val acc and loss are 0.7824 and 0.5939005\n",
            "Processing Epoch 4333\n",
            "Training acc and loss are 0.79046 and 0.58854896\n",
            "Val acc and loss are 0.7824 and 0.5938717\n",
            "Processing Epoch 4334\n",
            "Training acc and loss are 0.79048 and 0.5885184\n",
            "Val acc and loss are 0.7824 and 0.593843\n",
            "Processing Epoch 4335\n",
            "Training acc and loss are 0.79044 and 0.58848494\n",
            "Val acc and loss are 0.7824 and 0.59381026\n",
            "Processing Epoch 4336\n",
            "Training acc and loss are 0.79046 and 0.5884533\n",
            "Val acc and loss are 0.7824 and 0.5937789\n",
            "Processing Epoch 4337\n",
            "Training acc and loss are 0.79046 and 0.58842206\n",
            "Val acc and loss are 0.7823 and 0.5937494\n",
            "Processing Epoch 4338\n",
            "Training acc and loss are 0.79048 and 0.58838934\n",
            "Val acc and loss are 0.7823 and 0.59371835\n",
            "Processing Epoch 4339\n",
            "Training acc and loss are 0.79048 and 0.5883576\n",
            "Val acc and loss are 0.7823 and 0.5936874\n",
            "Processing Epoch 4340\n",
            "Training acc and loss are 0.79048 and 0.5883273\n",
            "Val acc and loss are 0.7823 and 0.5936585\n",
            "Processing Epoch 4341\n",
            "Training acc and loss are 0.79046 and 0.58829665\n",
            "Val acc and loss are 0.7823 and 0.5936293\n",
            "Processing Epoch 4342\n",
            "Training acc and loss are 0.79048 and 0.58826643\n",
            "Val acc and loss are 0.7823 and 0.5935999\n",
            "Processing Epoch 4343\n",
            "Training acc and loss are 0.7905 and 0.58823603\n",
            "Val acc and loss are 0.7823 and 0.5935704\n",
            "Processing Epoch 4344\n",
            "Training acc and loss are 0.79052 and 0.5882075\n",
            "Val acc and loss are 0.7823 and 0.59354305\n",
            "Processing Epoch 4345\n",
            "Training acc and loss are 0.79056 and 0.5881724\n",
            "Val acc and loss are 0.7824 and 0.5935094\n",
            "Processing Epoch 4346\n",
            "Training acc and loss are 0.79056 and 0.5881397\n",
            "Val acc and loss are 0.7824 and 0.5934775\n",
            "Processing Epoch 4347\n",
            "Training acc and loss are 0.79058 and 0.58811027\n",
            "Val acc and loss are 0.7824 and 0.59344935\n",
            "Processing Epoch 4348\n",
            "Training acc and loss are 0.79058 and 0.58807844\n",
            "Val acc and loss are 0.7824 and 0.59341896\n",
            "Processing Epoch 4349\n",
            "Training acc and loss are 0.79062 and 0.5880475\n",
            "Val acc and loss are 0.7824 and 0.59338945\n",
            "Processing Epoch 4350\n",
            "Training acc and loss are 0.79062 and 0.58801806\n",
            "Val acc and loss are 0.7824 and 0.59336114\n",
            "Processing Epoch 4351\n",
            "Training acc and loss are 0.79062 and 0.58798736\n",
            "Val acc and loss are 0.7824 and 0.59333134\n",
            "Processing Epoch 4352\n",
            "Training acc and loss are 0.79062 and 0.5879552\n",
            "Val acc and loss are 0.7824 and 0.5933002\n",
            "Processing Epoch 4353\n",
            "Training acc and loss are 0.79064 and 0.5879231\n",
            "Val acc and loss are 0.7824 and 0.5932687\n",
            "Processing Epoch 4354\n",
            "Training acc and loss are 0.79062 and 0.58789265\n",
            "Val acc and loss are 0.7824 and 0.59323955\n",
            "Processing Epoch 4355\n",
            "Training acc and loss are 0.79062 and 0.5878618\n",
            "Val acc and loss are 0.7825 and 0.5932099\n",
            "Processing Epoch 4356\n",
            "Training acc and loss are 0.79064 and 0.5878306\n",
            "Val acc and loss are 0.7825 and 0.59318036\n",
            "Processing Epoch 4357\n",
            "Training acc and loss are 0.79066 and 0.58780336\n",
            "Val acc and loss are 0.7825 and 0.59315467\n",
            "Processing Epoch 4358\n",
            "Training acc and loss are 0.79068 and 0.58777344\n",
            "Val acc and loss are 0.7825 and 0.5931263\n",
            "Processing Epoch 4359\n",
            "Training acc and loss are 0.79068 and 0.5877436\n",
            "Val acc and loss are 0.7826 and 0.5930971\n",
            "Processing Epoch 4360\n",
            "Training acc and loss are 0.79066 and 0.5877119\n",
            "Val acc and loss are 0.7826 and 0.5930659\n",
            "Processing Epoch 4361\n",
            "Training acc and loss are 0.79068 and 0.5876818\n",
            "Val acc and loss are 0.7826 and 0.59303695\n",
            "Processing Epoch 4362\n",
            "Training acc and loss are 0.79068 and 0.58764815\n",
            "Val acc and loss are 0.7826 and 0.5930053\n",
            "Processing Epoch 4363\n",
            "Training acc and loss are 0.7907 and 0.58761483\n",
            "Val acc and loss are 0.7826 and 0.5929734\n",
            "Processing Epoch 4364\n",
            "Training acc and loss are 0.79068 and 0.5875824\n",
            "Val acc and loss are 0.7826 and 0.5929412\n",
            "Processing Epoch 4365\n",
            "Training acc and loss are 0.7907 and 0.5875514\n",
            "Val acc and loss are 0.7826 and 0.5929118\n",
            "Processing Epoch 4366\n",
            "Training acc and loss are 0.7907 and 0.58752334\n",
            "Val acc and loss are 0.7826 and 0.59288484\n",
            "Processing Epoch 4367\n",
            "Training acc and loss are 0.79068 and 0.58749455\n",
            "Val acc and loss are 0.7826 and 0.5928577\n",
            "Processing Epoch 4368\n",
            "Training acc and loss are 0.7907 and 0.5874615\n",
            "Val acc and loss are 0.7827 and 0.5928258\n",
            "Processing Epoch 4369\n",
            "Training acc and loss are 0.7907 and 0.58743006\n",
            "Val acc and loss are 0.7827 and 0.5927957\n",
            "Processing Epoch 4370\n",
            "Training acc and loss are 0.79068 and 0.58739907\n",
            "Val acc and loss are 0.7827 and 0.59276575\n",
            "Processing Epoch 4371\n",
            "Training acc and loss are 0.79068 and 0.5873675\n",
            "Val acc and loss are 0.7827 and 0.592735\n",
            "Processing Epoch 4372\n",
            "Training acc and loss are 0.7907 and 0.58733726\n",
            "Val acc and loss are 0.7828 and 0.5927061\n",
            "Processing Epoch 4373\n",
            "Training acc and loss are 0.7907 and 0.58730495\n",
            "Val acc and loss are 0.7829 and 0.59267455\n",
            "Processing Epoch 4374\n",
            "Training acc and loss are 0.7907 and 0.5872741\n",
            "Val acc and loss are 0.7829 and 0.5926446\n",
            "Processing Epoch 4375\n",
            "Training acc and loss are 0.7907 and 0.58724225\n",
            "Val acc and loss are 0.7829 and 0.5926137\n",
            "Processing Epoch 4376\n",
            "Training acc and loss are 0.7907 and 0.5872097\n",
            "Val acc and loss are 0.7829 and 0.59258187\n",
            "Processing Epoch 4377\n",
            "Training acc and loss are 0.79068 and 0.58717966\n",
            "Val acc and loss are 0.7829 and 0.5925527\n",
            "Processing Epoch 4378\n",
            "Training acc and loss are 0.79068 and 0.5871458\n",
            "Val acc and loss are 0.7829 and 0.59251964\n",
            "Processing Epoch 4379\n",
            "Training acc and loss are 0.79068 and 0.5871143\n",
            "Val acc and loss are 0.7829 and 0.5924892\n",
            "Processing Epoch 4380\n",
            "Training acc and loss are 0.79068 and 0.587083\n",
            "Val acc and loss are 0.7829 and 0.5924592\n",
            "Processing Epoch 4381\n",
            "Training acc and loss are 0.7907 and 0.587055\n",
            "Val acc and loss are 0.7829 and 0.592432\n",
            "Processing Epoch 4382\n",
            "Training acc and loss are 0.7907 and 0.5870242\n",
            "Val acc and loss are 0.7829 and 0.5924022\n",
            "Processing Epoch 4383\n",
            "Training acc and loss are 0.7907 and 0.586993\n",
            "Val acc and loss are 0.7829 and 0.59237206\n",
            "Processing Epoch 4384\n",
            "Training acc and loss are 0.7907 and 0.58696085\n",
            "Val acc and loss are 0.783 and 0.59234035\n",
            "Processing Epoch 4385\n",
            "Training acc and loss are 0.7907 and 0.58693147\n",
            "Val acc and loss are 0.783 and 0.5923129\n",
            "Processing Epoch 4386\n",
            "Training acc and loss are 0.7907 and 0.5869008\n",
            "Val acc and loss are 0.783 and 0.5922839\n",
            "Processing Epoch 4387\n",
            "Training acc and loss are 0.7907 and 0.5868703\n",
            "Val acc and loss are 0.7831 and 0.5922541\n",
            "Processing Epoch 4388\n",
            "Training acc and loss are 0.7907 and 0.58683604\n",
            "Val acc and loss are 0.7831 and 0.59221977\n",
            "Processing Epoch 4389\n",
            "Training acc and loss are 0.79072 and 0.5868037\n",
            "Val acc and loss are 0.7831 and 0.5921886\n",
            "Processing Epoch 4390\n",
            "Training acc and loss are 0.79072 and 0.58677125\n",
            "Val acc and loss are 0.7831 and 0.59215695\n",
            "Processing Epoch 4391\n",
            "Training acc and loss are 0.79074 and 0.58673936\n",
            "Val acc and loss are 0.7831 and 0.59212685\n",
            "Processing Epoch 4392\n",
            "Training acc and loss are 0.79076 and 0.58670974\n",
            "Val acc and loss are 0.7831 and 0.5920979\n",
            "Processing Epoch 4393\n",
            "Training acc and loss are 0.79076 and 0.58668166\n",
            "Val acc and loss are 0.7831 and 0.59207076\n",
            "Processing Epoch 4394\n",
            "Training acc and loss are 0.79078 and 0.58665454\n",
            "Val acc and loss are 0.7831 and 0.5920452\n",
            "Processing Epoch 4395\n",
            "Training acc and loss are 0.79078 and 0.58662426\n",
            "Val acc and loss are 0.7833 and 0.59201604\n",
            "Processing Epoch 4396\n",
            "Training acc and loss are 0.7908 and 0.58659154\n",
            "Val acc and loss are 0.7832 and 0.5919839\n",
            "Processing Epoch 4397\n",
            "Training acc and loss are 0.79082 and 0.5865591\n",
            "Val acc and loss are 0.7833 and 0.59195286\n",
            "Processing Epoch 4398\n",
            "Training acc and loss are 0.7908 and 0.58652765\n",
            "Val acc and loss are 0.7833 and 0.59192246\n",
            "Processing Epoch 4399\n",
            "Training acc and loss are 0.7908 and 0.5864976\n",
            "Val acc and loss are 0.7833 and 0.5918937\n",
            "Processing Epoch 4400\n",
            "Training acc and loss are 0.79082 and 0.5864638\n",
            "Val acc and loss are 0.7834 and 0.5918607\n",
            "Processing Epoch 4401\n",
            "Training acc and loss are 0.7908 and 0.5864349\n",
            "Val acc and loss are 0.7834 and 0.5918331\n",
            "Processing Epoch 4402\n",
            "Training acc and loss are 0.79082 and 0.58640563\n",
            "Val acc and loss are 0.7835 and 0.59180504\n",
            "Processing Epoch 4403\n",
            "Training acc and loss are 0.79082 and 0.5863778\n",
            "Val acc and loss are 0.7835 and 0.5917788\n",
            "Processing Epoch 4404\n",
            "Training acc and loss are 0.79084 and 0.58634657\n",
            "Val acc and loss are 0.7835 and 0.5917484\n",
            "Processing Epoch 4405\n",
            "Training acc and loss are 0.79086 and 0.5863171\n",
            "Val acc and loss are 0.7835 and 0.59172064\n",
            "Processing Epoch 4406\n",
            "Training acc and loss are 0.79088 and 0.5862877\n",
            "Val acc and loss are 0.7835 and 0.5916923\n",
            "Processing Epoch 4407\n",
            "Training acc and loss are 0.79086 and 0.58626056\n",
            "Val acc and loss are 0.7836 and 0.5916671\n",
            "Processing Epoch 4408\n",
            "Training acc and loss are 0.79086 and 0.58623326\n",
            "Val acc and loss are 0.7836 and 0.59164035\n",
            "Processing Epoch 4409\n",
            "Training acc and loss are 0.79086 and 0.58620423\n",
            "Val acc and loss are 0.7836 and 0.59161264\n",
            "Processing Epoch 4410\n",
            "Training acc and loss are 0.79086 and 0.58617336\n",
            "Val acc and loss are 0.7836 and 0.59158254\n",
            "Processing Epoch 4411\n",
            "Training acc and loss are 0.79086 and 0.58614284\n",
            "Val acc and loss are 0.7835 and 0.5915526\n",
            "Processing Epoch 4412\n",
            "Training acc and loss are 0.79086 and 0.58611226\n",
            "Val acc and loss are 0.7836 and 0.59152234\n",
            "Processing Epoch 4413\n",
            "Training acc and loss are 0.79086 and 0.5860789\n",
            "Val acc and loss are 0.7836 and 0.59148926\n",
            "Processing Epoch 4414\n",
            "Training acc and loss are 0.79088 and 0.58604735\n",
            "Val acc and loss are 0.7836 and 0.5914585\n",
            "Processing Epoch 4415\n",
            "Training acc and loss are 0.79086 and 0.58601755\n",
            "Val acc and loss are 0.7836 and 0.5914299\n",
            "Processing Epoch 4416\n",
            "Training acc and loss are 0.79086 and 0.58598727\n",
            "Val acc and loss are 0.7836 and 0.5914009\n",
            "Processing Epoch 4417\n",
            "Training acc and loss are 0.79088 and 0.58595836\n",
            "Val acc and loss are 0.7836 and 0.59137315\n",
            "Processing Epoch 4418\n",
            "Training acc and loss are 0.79088 and 0.5859271\n",
            "Val acc and loss are 0.7836 and 0.59134316\n",
            "Processing Epoch 4419\n",
            "Training acc and loss are 0.7909 and 0.5858978\n",
            "Val acc and loss are 0.7836 and 0.5913144\n",
            "Processing Epoch 4420\n",
            "Training acc and loss are 0.7909 and 0.5858651\n",
            "Val acc and loss are 0.7837 and 0.59128296\n",
            "Processing Epoch 4421\n",
            "Training acc and loss are 0.79092 and 0.58583546\n",
            "Val acc and loss are 0.7837 and 0.591254\n",
            "Processing Epoch 4422\n",
            "Training acc and loss are 0.79092 and 0.5858069\n",
            "Val acc and loss are 0.7837 and 0.5912265\n",
            "Processing Epoch 4423\n",
            "Training acc and loss are 0.79092 and 0.58577734\n",
            "Val acc and loss are 0.7837 and 0.59119815\n",
            "Processing Epoch 4424\n",
            "Training acc and loss are 0.79092 and 0.58574986\n",
            "Val acc and loss are 0.7837 and 0.5911721\n",
            "Processing Epoch 4425\n",
            "Training acc and loss are 0.7909 and 0.58572185\n",
            "Val acc and loss are 0.7838 and 0.59114397\n",
            "Processing Epoch 4426\n",
            "Training acc and loss are 0.7909 and 0.58568954\n",
            "Val acc and loss are 0.7838 and 0.5911127\n",
            "Processing Epoch 4427\n",
            "Training acc and loss are 0.7909 and 0.58565766\n",
            "Val acc and loss are 0.7838 and 0.59108156\n",
            "Processing Epoch 4428\n",
            "Training acc and loss are 0.7909 and 0.58563054\n",
            "Val acc and loss are 0.7839 and 0.5910554\n",
            "Processing Epoch 4429\n",
            "Training acc and loss are 0.7909 and 0.58560133\n",
            "Val acc and loss are 0.7839 and 0.5910268\n",
            "Processing Epoch 4430\n",
            "Training acc and loss are 0.7909 and 0.5855711\n",
            "Val acc and loss are 0.7839 and 0.5909978\n",
            "Processing Epoch 4431\n",
            "Training acc and loss are 0.7909 and 0.58554417\n",
            "Val acc and loss are 0.7839 and 0.5909717\n",
            "Processing Epoch 4432\n",
            "Training acc and loss are 0.79092 and 0.5855144\n",
            "Val acc and loss are 0.7839 and 0.590943\n",
            "Processing Epoch 4433\n",
            "Training acc and loss are 0.79094 and 0.58548397\n",
            "Val acc and loss are 0.7839 and 0.5909131\n",
            "Processing Epoch 4434\n",
            "Training acc and loss are 0.79094 and 0.5854538\n",
            "Val acc and loss are 0.7839 and 0.5908836\n",
            "Processing Epoch 4435\n",
            "Training acc and loss are 0.79096 and 0.5854234\n",
            "Val acc and loss are 0.7839 and 0.59085476\n",
            "Processing Epoch 4436\n",
            "Training acc and loss are 0.79096 and 0.5853897\n",
            "Val acc and loss are 0.7839 and 0.59082234\n",
            "Processing Epoch 4437\n",
            "Training acc and loss are 0.79096 and 0.58535814\n",
            "Val acc and loss are 0.7839 and 0.59079236\n",
            "Processing Epoch 4438\n",
            "Training acc and loss are 0.79096 and 0.5853281\n",
            "Val acc and loss are 0.7839 and 0.5907632\n",
            "Processing Epoch 4439\n",
            "Training acc and loss are 0.79102 and 0.58529896\n",
            "Val acc and loss are 0.7839 and 0.5907348\n",
            "Processing Epoch 4440\n",
            "Training acc and loss are 0.79102 and 0.5852689\n",
            "Val acc and loss are 0.7839 and 0.5907057\n",
            "Processing Epoch 4441\n",
            "Training acc and loss are 0.79102 and 0.5852406\n",
            "Val acc and loss are 0.7839 and 0.5906784\n",
            "Processing Epoch 4442\n",
            "Training acc and loss are 0.791 and 0.58520806\n",
            "Val acc and loss are 0.7839 and 0.5906465\n",
            "Processing Epoch 4443\n",
            "Training acc and loss are 0.79102 and 0.58517766\n",
            "Val acc and loss are 0.7839 and 0.5906178\n",
            "Processing Epoch 4444\n",
            "Training acc and loss are 0.79106 and 0.5851443\n",
            "Val acc and loss are 0.7839 and 0.5905858\n",
            "Processing Epoch 4445\n",
            "Training acc and loss are 0.79106 and 0.58511436\n",
            "Val acc and loss are 0.7839 and 0.59055656\n",
            "Processing Epoch 4446\n",
            "Training acc and loss are 0.79106 and 0.5850841\n",
            "Val acc and loss are 0.784 and 0.5905272\n",
            "Processing Epoch 4447\n",
            "Training acc and loss are 0.79108 and 0.5850542\n",
            "Val acc and loss are 0.7839 and 0.5904988\n",
            "Processing Epoch 4448\n",
            "Training acc and loss are 0.79112 and 0.58502513\n",
            "Val acc and loss are 0.7841 and 0.5904715\n",
            "Processing Epoch 4449\n",
            "Training acc and loss are 0.7911 and 0.58499444\n",
            "Val acc and loss are 0.7841 and 0.5904418\n",
            "Processing Epoch 4450\n",
            "Training acc and loss are 0.79112 and 0.58496374\n",
            "Val acc and loss are 0.7842 and 0.590412\n",
            "Processing Epoch 4451\n",
            "Training acc and loss are 0.79114 and 0.5849327\n",
            "Val acc and loss are 0.7842 and 0.5903827\n",
            "Processing Epoch 4452\n",
            "Training acc and loss are 0.79114 and 0.5849006\n",
            "Val acc and loss are 0.7842 and 0.59035164\n",
            "Processing Epoch 4453\n",
            "Training acc and loss are 0.79116 and 0.58486885\n",
            "Val acc and loss are 0.7842 and 0.5903214\n",
            "Processing Epoch 4454\n",
            "Training acc and loss are 0.79118 and 0.58483845\n",
            "Val acc and loss are 0.7842 and 0.5902918\n",
            "Processing Epoch 4455\n",
            "Training acc and loss are 0.79118 and 0.5848101\n",
            "Val acc and loss are 0.7842 and 0.5902642\n",
            "Processing Epoch 4456\n",
            "Training acc and loss are 0.79118 and 0.584781\n",
            "Val acc and loss are 0.7842 and 0.590236\n",
            "Processing Epoch 4457\n",
            "Training acc and loss are 0.79122 and 0.58475304\n",
            "Val acc and loss are 0.7842 and 0.59020823\n",
            "Processing Epoch 4458\n",
            "Training acc and loss are 0.79124 and 0.5847226\n",
            "Val acc and loss are 0.7842 and 0.59017843\n",
            "Processing Epoch 4459\n",
            "Training acc and loss are 0.79124 and 0.5846945\n",
            "Val acc and loss are 0.7842 and 0.590152\n",
            "Processing Epoch 4460\n",
            "Training acc and loss are 0.79126 and 0.5846646\n",
            "Val acc and loss are 0.7841 and 0.5901242\n",
            "Processing Epoch 4461\n",
            "Training acc and loss are 0.79128 and 0.58463573\n",
            "Val acc and loss are 0.7841 and 0.5900966\n",
            "Processing Epoch 4462\n",
            "Training acc and loss are 0.79132 and 0.584604\n",
            "Val acc and loss are 0.7841 and 0.59006584\n",
            "Processing Epoch 4463\n",
            "Training acc and loss are 0.7913 and 0.58457446\n",
            "Val acc and loss are 0.7841 and 0.590037\n",
            "Processing Epoch 4464\n",
            "Training acc and loss are 0.79134 and 0.5845446\n",
            "Val acc and loss are 0.7841 and 0.5900085\n",
            "Processing Epoch 4465\n",
            "Training acc and loss are 0.79132 and 0.5845157\n",
            "Val acc and loss are 0.7841 and 0.58998024\n",
            "Processing Epoch 4466\n",
            "Training acc and loss are 0.79132 and 0.584486\n",
            "Val acc and loss are 0.7841 and 0.5899522\n",
            "Processing Epoch 4467\n",
            "Training acc and loss are 0.79134 and 0.58445686\n",
            "Val acc and loss are 0.7841 and 0.58992434\n",
            "Processing Epoch 4468\n",
            "Training acc and loss are 0.79136 and 0.58442414\n",
            "Val acc and loss are 0.7841 and 0.5898922\n",
            "Processing Epoch 4469\n",
            "Training acc and loss are 0.79136 and 0.5843955\n",
            "Val acc and loss are 0.7841 and 0.5898641\n",
            "Processing Epoch 4470\n",
            "Training acc and loss are 0.7914 and 0.58436614\n",
            "Val acc and loss are 0.7841 and 0.5898355\n",
            "Processing Epoch 4471\n",
            "Training acc and loss are 0.7914 and 0.58433694\n",
            "Val acc and loss are 0.7841 and 0.58980757\n",
            "Processing Epoch 4472\n",
            "Training acc and loss are 0.79136 and 0.58430874\n",
            "Val acc and loss are 0.7842 and 0.5897808\n",
            "Processing Epoch 4473\n",
            "Training acc and loss are 0.79138 and 0.5842784\n",
            "Val acc and loss are 0.7842 and 0.58975077\n",
            "Processing Epoch 4474\n",
            "Training acc and loss are 0.7914 and 0.5842462\n",
            "Val acc and loss are 0.7842 and 0.58971965\n",
            "Processing Epoch 4475\n",
            "Training acc and loss are 0.79146 and 0.58421755\n",
            "Val acc and loss are 0.7843 and 0.5896918\n",
            "Processing Epoch 4476\n",
            "Training acc and loss are 0.79144 and 0.5841895\n",
            "Val acc and loss are 0.7843 and 0.5896646\n",
            "Processing Epoch 4477\n",
            "Training acc and loss are 0.79148 and 0.5841576\n",
            "Val acc and loss are 0.7843 and 0.58963376\n",
            "Processing Epoch 4478\n",
            "Training acc and loss are 0.79148 and 0.58412796\n",
            "Val acc and loss are 0.7843 and 0.58960515\n",
            "Processing Epoch 4479\n",
            "Training acc and loss are 0.79148 and 0.5841002\n",
            "Val acc and loss are 0.7844 and 0.5895792\n",
            "Processing Epoch 4480\n",
            "Training acc and loss are 0.79148 and 0.58407\n",
            "Val acc and loss are 0.7845 and 0.5895503\n",
            "Processing Epoch 4481\n",
            "Training acc and loss are 0.79148 and 0.5840371\n",
            "Val acc and loss are 0.7845 and 0.5895186\n",
            "Processing Epoch 4482\n",
            "Training acc and loss are 0.7915 and 0.5840077\n",
            "Val acc and loss are 0.7845 and 0.58948994\n",
            "Processing Epoch 4483\n",
            "Training acc and loss are 0.7915 and 0.5839769\n",
            "Val acc and loss are 0.7845 and 0.5894609\n",
            "Processing Epoch 4484\n",
            "Training acc and loss are 0.7915 and 0.58394474\n",
            "Val acc and loss are 0.7845 and 0.58943045\n",
            "Processing Epoch 4485\n",
            "Training acc and loss are 0.79152 and 0.5839162\n",
            "Val acc and loss are 0.7845 and 0.589403\n",
            "Processing Epoch 4486\n",
            "Training acc and loss are 0.79152 and 0.5838875\n",
            "Val acc and loss are 0.7845 and 0.5893753\n",
            "Processing Epoch 4487\n",
            "Training acc and loss are 0.79154 and 0.5838575\n",
            "Val acc and loss are 0.7845 and 0.58934647\n",
            "Processing Epoch 4488\n",
            "Training acc and loss are 0.79154 and 0.5838302\n",
            "Val acc and loss are 0.7845 and 0.5893202\n",
            "Processing Epoch 4489\n",
            "Training acc and loss are 0.79154 and 0.5837974\n",
            "Val acc and loss are 0.7845 and 0.5892881\n",
            "Processing Epoch 4490\n",
            "Training acc and loss are 0.79154 and 0.58377045\n",
            "Val acc and loss are 0.7847 and 0.5892626\n",
            "Processing Epoch 4491\n",
            "Training acc and loss are 0.79154 and 0.5837442\n",
            "Val acc and loss are 0.7847 and 0.5892375\n",
            "Processing Epoch 4492\n",
            "Training acc and loss are 0.79156 and 0.58371425\n",
            "Val acc and loss are 0.7848 and 0.5892081\n",
            "Processing Epoch 4493\n",
            "Training acc and loss are 0.79158 and 0.5836851\n",
            "Val acc and loss are 0.7848 and 0.5891807\n",
            "Processing Epoch 4494\n",
            "Training acc and loss are 0.79158 and 0.58365524\n",
            "Val acc and loss are 0.7849 and 0.58915246\n",
            "Processing Epoch 4495\n",
            "Training acc and loss are 0.79156 and 0.5836256\n",
            "Val acc and loss are 0.785 and 0.5891236\n",
            "Processing Epoch 4496\n",
            "Training acc and loss are 0.79158 and 0.5835961\n",
            "Val acc and loss are 0.7849 and 0.5890959\n",
            "Processing Epoch 4497\n",
            "Training acc and loss are 0.79162 and 0.58356434\n",
            "Val acc and loss are 0.785 and 0.5890652\n",
            "Processing Epoch 4498\n",
            "Training acc and loss are 0.79164 and 0.5835378\n",
            "Val acc and loss are 0.785 and 0.5890396\n",
            "Processing Epoch 4499\n",
            "Training acc and loss are 0.79166 and 0.5835068\n",
            "Val acc and loss are 0.785 and 0.58901024\n",
            "Processing Epoch 4500\n",
            "Training acc and loss are 0.79168 and 0.58347476\n",
            "Val acc and loss are 0.785 and 0.58897954\n",
            "Processing Epoch 4501\n",
            "Training acc and loss are 0.79172 and 0.5834405\n",
            "Val acc and loss are 0.785 and 0.5889461\n",
            "Processing Epoch 4502\n",
            "Training acc and loss are 0.79174 and 0.5834103\n",
            "Val acc and loss are 0.785 and 0.58891684\n",
            "Processing Epoch 4503\n",
            "Training acc and loss are 0.79174 and 0.5833821\n",
            "Val acc and loss are 0.785 and 0.58888996\n",
            "Processing Epoch 4504\n",
            "Training acc and loss are 0.79174 and 0.583353\n",
            "Val acc and loss are 0.785 and 0.5888612\n",
            "Processing Epoch 4505\n",
            "Training acc and loss are 0.79174 and 0.5833193\n",
            "Val acc and loss are 0.785 and 0.58882844\n",
            "Processing Epoch 4506\n",
            "Training acc and loss are 0.79174 and 0.58328915\n",
            "Val acc and loss are 0.785 and 0.58879906\n",
            "Processing Epoch 4507\n",
            "Training acc and loss are 0.79174 and 0.58325744\n",
            "Val acc and loss are 0.785 and 0.5887682\n",
            "Processing Epoch 4508\n",
            "Training acc and loss are 0.79176 and 0.5832289\n",
            "Val acc and loss are 0.785 and 0.5887405\n",
            "Processing Epoch 4509\n",
            "Training acc and loss are 0.7918 and 0.58319795\n",
            "Val acc and loss are 0.785 and 0.58871025\n",
            "Processing Epoch 4510\n",
            "Training acc and loss are 0.7918 and 0.58316815\n",
            "Val acc and loss are 0.785 and 0.58868164\n",
            "Processing Epoch 4511\n",
            "Training acc and loss are 0.7918 and 0.5831421\n",
            "Val acc and loss are 0.785 and 0.5886568\n",
            "Processing Epoch 4512\n",
            "Training acc and loss are 0.7918 and 0.58310914\n",
            "Val acc and loss are 0.7851 and 0.58862454\n",
            "Processing Epoch 4513\n",
            "Training acc and loss are 0.79182 and 0.5830808\n",
            "Val acc and loss are 0.7852 and 0.5885978\n",
            "Processing Epoch 4514\n",
            "Training acc and loss are 0.79182 and 0.58305097\n",
            "Val acc and loss are 0.7852 and 0.5885693\n",
            "Processing Epoch 4515\n",
            "Training acc and loss are 0.79182 and 0.5830213\n",
            "Val acc and loss are 0.7854 and 0.58854043\n",
            "Processing Epoch 4516\n",
            "Training acc and loss are 0.79182 and 0.5829907\n",
            "Val acc and loss are 0.7854 and 0.58851045\n",
            "Processing Epoch 4517\n",
            "Training acc and loss are 0.7918 and 0.5829622\n",
            "Val acc and loss are 0.7854 and 0.5884833\n",
            "Processing Epoch 4518\n",
            "Training acc and loss are 0.79184 and 0.582932\n",
            "Val acc and loss are 0.7854 and 0.5884544\n",
            "Processing Epoch 4519\n",
            "Training acc and loss are 0.79184 and 0.58290553\n",
            "Val acc and loss are 0.7854 and 0.5884289\n",
            "Processing Epoch 4520\n",
            "Training acc and loss are 0.79184 and 0.58287585\n",
            "Val acc and loss are 0.7854 and 0.5884008\n",
            "Processing Epoch 4521\n",
            "Training acc and loss are 0.79184 and 0.5828451\n",
            "Val acc and loss are 0.7854 and 0.588371\n",
            "Processing Epoch 4522\n",
            "Training acc and loss are 0.79184 and 0.5828149\n",
            "Val acc and loss are 0.7854 and 0.588342\n",
            "Processing Epoch 4523\n",
            "Training acc and loss are 0.79184 and 0.5827857\n",
            "Val acc and loss are 0.7854 and 0.5883142\n",
            "Processing Epoch 4524\n",
            "Training acc and loss are 0.79186 and 0.5827597\n",
            "Val acc and loss are 0.7854 and 0.58828956\n",
            "Processing Epoch 4525\n",
            "Training acc and loss are 0.79186 and 0.5827293\n",
            "Val acc and loss are 0.7854 and 0.58825976\n",
            "Processing Epoch 4526\n",
            "Training acc and loss are 0.79186 and 0.58269507\n",
            "Val acc and loss are 0.7857 and 0.588226\n",
            "Processing Epoch 4527\n",
            "Training acc and loss are 0.79188 and 0.5826677\n",
            "Val acc and loss are 0.7856 and 0.5882004\n",
            "Processing Epoch 4528\n",
            "Training acc and loss are 0.79188 and 0.5826392\n",
            "Val acc and loss are 0.7856 and 0.5881727\n",
            "Processing Epoch 4529\n",
            "Training acc and loss are 0.7919 and 0.5826089\n",
            "Val acc and loss are 0.7855 and 0.5881431\n",
            "Processing Epoch 4530\n",
            "Training acc and loss are 0.7919 and 0.5825806\n",
            "Val acc and loss are 0.7855 and 0.5881158\n",
            "Processing Epoch 4531\n",
            "Training acc and loss are 0.79192 and 0.5825522\n",
            "Val acc and loss are 0.7855 and 0.5880881\n",
            "Processing Epoch 4532\n",
            "Training acc and loss are 0.79194 and 0.58252424\n",
            "Val acc and loss are 0.7855 and 0.58806115\n",
            "Processing Epoch 4533\n",
            "Training acc and loss are 0.79194 and 0.58249426\n",
            "Val acc and loss are 0.7855 and 0.5880324\n",
            "Processing Epoch 4534\n",
            "Training acc and loss are 0.79196 and 0.5824635\n",
            "Val acc and loss are 0.7855 and 0.58800274\n",
            "Processing Epoch 4535\n",
            "Training acc and loss are 0.79196 and 0.58243555\n",
            "Val acc and loss are 0.7855 and 0.58797526\n",
            "Processing Epoch 4536\n",
            "Training acc and loss are 0.79196 and 0.5824057\n",
            "Val acc and loss are 0.7855 and 0.5879464\n",
            "Processing Epoch 4537\n",
            "Training acc and loss are 0.79196 and 0.58237356\n",
            "Val acc and loss are 0.7855 and 0.5879156\n",
            "Processing Epoch 4538\n",
            "Training acc and loss are 0.79196 and 0.5823458\n",
            "Val acc and loss are 0.7855 and 0.58788955\n",
            "Processing Epoch 4539\n",
            "Training acc and loss are 0.79196 and 0.5823157\n",
            "Val acc and loss are 0.7855 and 0.58786047\n",
            "Processing Epoch 4540\n",
            "Training acc and loss are 0.79196 and 0.5822836\n",
            "Val acc and loss are 0.7855 and 0.5878296\n",
            "Processing Epoch 4541\n",
            "Training acc and loss are 0.79194 and 0.58225423\n",
            "Val acc and loss are 0.7855 and 0.58780175\n",
            "Processing Epoch 4542\n",
            "Training acc and loss are 0.79198 and 0.5822261\n",
            "Val acc and loss are 0.7855 and 0.58777463\n",
            "Processing Epoch 4543\n",
            "Training acc and loss are 0.792 and 0.5821989\n",
            "Val acc and loss are 0.7855 and 0.5877481\n",
            "Processing Epoch 4544\n",
            "Training acc and loss are 0.79202 and 0.5821672\n",
            "Val acc and loss are 0.7855 and 0.5877173\n",
            "Processing Epoch 4545\n",
            "Training acc and loss are 0.79202 and 0.58213866\n",
            "Val acc and loss are 0.7856 and 0.5876899\n",
            "Processing Epoch 4546\n",
            "Training acc and loss are 0.79202 and 0.5821093\n",
            "Val acc and loss are 0.7856 and 0.5876612\n",
            "Processing Epoch 4547\n",
            "Training acc and loss are 0.79204 and 0.58207905\n",
            "Val acc and loss are 0.7856 and 0.58763206\n",
            "Processing Epoch 4548\n",
            "Training acc and loss are 0.79204 and 0.5820517\n",
            "Val acc and loss are 0.7856 and 0.58760595\n",
            "Processing Epoch 4549\n",
            "Training acc and loss are 0.79204 and 0.5820229\n",
            "Val acc and loss are 0.7856 and 0.58757794\n",
            "Processing Epoch 4550\n",
            "Training acc and loss are 0.79204 and 0.5819946\n",
            "Val acc and loss are 0.7856 and 0.58755076\n",
            "Processing Epoch 4551\n",
            "Training acc and loss are 0.79204 and 0.5819644\n",
            "Val acc and loss are 0.7856 and 0.5875214\n",
            "Processing Epoch 4552\n",
            "Training acc and loss are 0.79204 and 0.5819346\n",
            "Val acc and loss are 0.7856 and 0.58749235\n",
            "Processing Epoch 4553\n",
            "Training acc and loss are 0.79204 and 0.5819075\n",
            "Val acc and loss are 0.7856 and 0.5874669\n",
            "Processing Epoch 4554\n",
            "Training acc and loss are 0.79206 and 0.5818781\n",
            "Val acc and loss are 0.7856 and 0.58743876\n",
            "Processing Epoch 4555\n",
            "Training acc and loss are 0.79208 and 0.58184665\n",
            "Val acc and loss are 0.7856 and 0.58740866\n",
            "Processing Epoch 4556\n",
            "Training acc and loss are 0.79206 and 0.5818162\n",
            "Val acc and loss are 0.7856 and 0.5873789\n",
            "Processing Epoch 4557\n",
            "Training acc and loss are 0.79204 and 0.5817902\n",
            "Val acc and loss are 0.7856 and 0.58735454\n",
            "Processing Epoch 4558\n",
            "Training acc and loss are 0.79204 and 0.58176124\n",
            "Val acc and loss are 0.7856 and 0.58732665\n",
            "Processing Epoch 4559\n",
            "Training acc and loss are 0.79204 and 0.5817341\n",
            "Val acc and loss are 0.7856 and 0.5873006\n",
            "Processing Epoch 4560\n",
            "Training acc and loss are 0.79204 and 0.58170354\n",
            "Val acc and loss are 0.7856 and 0.587271\n",
            "Processing Epoch 4561\n",
            "Training acc and loss are 0.79204 and 0.5816738\n",
            "Val acc and loss are 0.7857 and 0.5872422\n",
            "Processing Epoch 4562\n",
            "Training acc and loss are 0.79206 and 0.5816456\n",
            "Val acc and loss are 0.7857 and 0.5872146\n",
            "Processing Epoch 4563\n",
            "Training acc and loss are 0.79206 and 0.58161515\n",
            "Val acc and loss are 0.7857 and 0.58718526\n",
            "Processing Epoch 4564\n",
            "Training acc and loss are 0.7921 and 0.5815864\n",
            "Val acc and loss are 0.7857 and 0.5871573\n",
            "Processing Epoch 4565\n",
            "Training acc and loss are 0.79212 and 0.581559\n",
            "Val acc and loss are 0.7857 and 0.5871307\n",
            "Processing Epoch 4566\n",
            "Training acc and loss are 0.79216 and 0.5815292\n",
            "Val acc and loss are 0.7857 and 0.5871016\n",
            "Processing Epoch 4567\n",
            "Training acc and loss are 0.7922 and 0.58150053\n",
            "Val acc and loss are 0.7857 and 0.58707434\n",
            "Processing Epoch 4568\n",
            "Training acc and loss are 0.7922 and 0.581471\n",
            "Val acc and loss are 0.7857 and 0.58704585\n",
            "Processing Epoch 4569\n",
            "Training acc and loss are 0.7922 and 0.58143944\n",
            "Val acc and loss are 0.7857 and 0.5870141\n",
            "Processing Epoch 4570\n",
            "Training acc and loss are 0.7922 and 0.5814105\n",
            "Val acc and loss are 0.7858 and 0.58698595\n",
            "Processing Epoch 4571\n",
            "Training acc and loss are 0.79222 and 0.5813821\n",
            "Val acc and loss are 0.7859 and 0.5869586\n",
            "Processing Epoch 4572\n",
            "Training acc and loss are 0.79222 and 0.5813534\n",
            "Val acc and loss are 0.7859 and 0.5869304\n",
            "Processing Epoch 4573\n",
            "Training acc and loss are 0.79222 and 0.58132505\n",
            "Val acc and loss are 0.7859 and 0.5869037\n",
            "Processing Epoch 4574\n",
            "Training acc and loss are 0.79222 and 0.58129555\n",
            "Val acc and loss are 0.7859 and 0.5868752\n",
            "Processing Epoch 4575\n",
            "Training acc and loss are 0.79222 and 0.58126354\n",
            "Val acc and loss are 0.7859 and 0.5868438\n",
            "Processing Epoch 4576\n",
            "Training acc and loss are 0.79224 and 0.58123577\n",
            "Val acc and loss are 0.7859 and 0.586818\n",
            "Processing Epoch 4577\n",
            "Training acc and loss are 0.79224 and 0.581207\n",
            "Val acc and loss are 0.7859 and 0.58679014\n",
            "Processing Epoch 4578\n",
            "Training acc and loss are 0.79226 and 0.58118\n",
            "Val acc and loss are 0.7859 and 0.58676463\n",
            "Processing Epoch 4579\n",
            "Training acc and loss are 0.79224 and 0.5811474\n",
            "Val acc and loss are 0.7859 and 0.5867327\n",
            "Processing Epoch 4580\n",
            "Training acc and loss are 0.79224 and 0.58111805\n",
            "Val acc and loss are 0.7859 and 0.58670443\n",
            "Processing Epoch 4581\n",
            "Training acc and loss are 0.79224 and 0.5810894\n",
            "Val acc and loss are 0.7859 and 0.5866767\n",
            "Processing Epoch 4582\n",
            "Training acc and loss are 0.79226 and 0.58105993\n",
            "Val acc and loss are 0.7859 and 0.58664864\n",
            "Processing Epoch 4583\n",
            "Training acc and loss are 0.79226 and 0.5810325\n",
            "Val acc and loss are 0.7861 and 0.5866223\n",
            "Processing Epoch 4584\n",
            "Training acc and loss are 0.79226 and 0.5810031\n",
            "Val acc and loss are 0.7861 and 0.58659345\n",
            "Processing Epoch 4585\n",
            "Training acc and loss are 0.79228 and 0.5809765\n",
            "Val acc and loss are 0.7861 and 0.5865675\n",
            "Processing Epoch 4586\n",
            "Training acc and loss are 0.79232 and 0.58094996\n",
            "Val acc and loss are 0.7861 and 0.5865418\n",
            "Processing Epoch 4587\n",
            "Training acc and loss are 0.79232 and 0.5809191\n",
            "Val acc and loss are 0.7861 and 0.58651227\n",
            "Processing Epoch 4588\n",
            "Training acc and loss are 0.79232 and 0.580891\n",
            "Val acc and loss are 0.7861 and 0.58648545\n",
            "Processing Epoch 4589\n",
            "Training acc and loss are 0.79234 and 0.5808638\n",
            "Val acc and loss are 0.7861 and 0.5864584\n",
            "Processing Epoch 4590\n",
            "Training acc and loss are 0.79234 and 0.5808347\n",
            "Val acc and loss are 0.7861 and 0.5864305\n",
            "Processing Epoch 4591\n",
            "Training acc and loss are 0.7924 and 0.58080643\n",
            "Val acc and loss are 0.7862 and 0.5864032\n",
            "Processing Epoch 4592\n",
            "Training acc and loss are 0.7924 and 0.5807771\n",
            "Val acc and loss are 0.7862 and 0.5863751\n",
            "Processing Epoch 4593\n",
            "Training acc and loss are 0.79246 and 0.5807525\n",
            "Val acc and loss are 0.7862 and 0.58635193\n",
            "Processing Epoch 4594\n",
            "Training acc and loss are 0.79246 and 0.5807191\n",
            "Val acc and loss are 0.7862 and 0.58631957\n",
            "Processing Epoch 4595\n",
            "Training acc and loss are 0.7925 and 0.58068913\n",
            "Val acc and loss are 0.7862 and 0.5862912\n",
            "Processing Epoch 4596\n",
            "Training acc and loss are 0.79252 and 0.58065814\n",
            "Val acc and loss are 0.7863 and 0.58626175\n",
            "Processing Epoch 4597\n",
            "Training acc and loss are 0.79254 and 0.5806282\n",
            "Val acc and loss are 0.7863 and 0.5862331\n",
            "Processing Epoch 4598\n",
            "Training acc and loss are 0.79254 and 0.5806008\n",
            "Val acc and loss are 0.7863 and 0.5862065\n",
            "Processing Epoch 4599\n",
            "Training acc and loss are 0.79254 and 0.5805696\n",
            "Val acc and loss are 0.7863 and 0.5861767\n",
            "Processing Epoch 4600\n",
            "Training acc and loss are 0.79256 and 0.58053863\n",
            "Val acc and loss are 0.7863 and 0.5861473\n",
            "Processing Epoch 4601\n",
            "Training acc and loss are 0.79258 and 0.5805124\n",
            "Val acc and loss are 0.7863 and 0.5861219\n",
            "Processing Epoch 4602\n",
            "Training acc and loss are 0.79264 and 0.5804836\n",
            "Val acc and loss are 0.7863 and 0.58609474\n",
            "Processing Epoch 4603\n",
            "Training acc and loss are 0.79264 and 0.58045626\n",
            "Val acc and loss are 0.7863 and 0.58606833\n",
            "Processing Epoch 4604\n",
            "Training acc and loss are 0.79264 and 0.5804259\n",
            "Val acc and loss are 0.7863 and 0.58603907\n",
            "Processing Epoch 4605\n",
            "Training acc and loss are 0.79264 and 0.5803975\n",
            "Val acc and loss are 0.7863 and 0.5860112\n",
            "Processing Epoch 4606\n",
            "Training acc and loss are 0.79266 and 0.58036876\n",
            "Val acc and loss are 0.7863 and 0.5859836\n",
            "Processing Epoch 4607\n",
            "Training acc and loss are 0.79262 and 0.58033884\n",
            "Val acc and loss are 0.7864 and 0.5859542\n",
            "Processing Epoch 4608\n",
            "Training acc and loss are 0.79262 and 0.5803111\n",
            "Val acc and loss are 0.7864 and 0.58592767\n",
            "Processing Epoch 4609\n",
            "Training acc and loss are 0.79266 and 0.58028334\n",
            "Val acc and loss are 0.7864 and 0.58590037\n",
            "Processing Epoch 4610\n",
            "Training acc and loss are 0.79264 and 0.5802541\n",
            "Val acc and loss are 0.7864 and 0.5858722\n",
            "Processing Epoch 4611\n",
            "Training acc and loss are 0.79264 and 0.580227\n",
            "Val acc and loss are 0.7864 and 0.58584607\n",
            "Processing Epoch 4612\n",
            "Training acc and loss are 0.79264 and 0.5801976\n",
            "Val acc and loss are 0.7864 and 0.5858177\n",
            "Processing Epoch 4613\n",
            "Training acc and loss are 0.79266 and 0.5801676\n",
            "Val acc and loss are 0.7865 and 0.58578944\n",
            "Processing Epoch 4614\n",
            "Training acc and loss are 0.79266 and 0.58014077\n",
            "Val acc and loss are 0.7865 and 0.58576375\n",
            "Processing Epoch 4615\n",
            "Training acc and loss are 0.7927 and 0.5801128\n",
            "Val acc and loss are 0.7865 and 0.5857363\n",
            "Processing Epoch 4616\n",
            "Training acc and loss are 0.79272 and 0.5800808\n",
            "Val acc and loss are 0.7865 and 0.5857056\n",
            "Processing Epoch 4617\n",
            "Training acc and loss are 0.79272 and 0.58005154\n",
            "Val acc and loss are 0.7865 and 0.5856778\n",
            "Processing Epoch 4618\n",
            "Training acc and loss are 0.79274 and 0.5800245\n",
            "Val acc and loss are 0.7865 and 0.5856522\n",
            "Processing Epoch 4619\n",
            "Training acc and loss are 0.79272 and 0.5799984\n",
            "Val acc and loss are 0.7865 and 0.585627\n",
            "Processing Epoch 4620\n",
            "Training acc and loss are 0.79274 and 0.5799667\n",
            "Val acc and loss are 0.7865 and 0.585596\n",
            "Processing Epoch 4621\n",
            "Training acc and loss are 0.79276 and 0.57994086\n",
            "Val acc and loss are 0.7865 and 0.5855713\n",
            "Processing Epoch 4622\n",
            "Training acc and loss are 0.79276 and 0.5799126\n",
            "Val acc and loss are 0.7865 and 0.58554363\n",
            "Processing Epoch 4623\n",
            "Training acc and loss are 0.79276 and 0.579883\n",
            "Val acc and loss are 0.7865 and 0.58551526\n",
            "Processing Epoch 4624\n",
            "Training acc and loss are 0.79276 and 0.57985455\n",
            "Val acc and loss are 0.7865 and 0.5854877\n",
            "Processing Epoch 4625\n",
            "Training acc and loss are 0.79276 and 0.5798258\n",
            "Val acc and loss are 0.7865 and 0.58546036\n",
            "Processing Epoch 4626\n",
            "Training acc and loss are 0.79278 and 0.57979923\n",
            "Val acc and loss are 0.7865 and 0.5854353\n",
            "Processing Epoch 4627\n",
            "Training acc and loss are 0.79282 and 0.57977206\n",
            "Val acc and loss are 0.7866 and 0.5854086\n",
            "Processing Epoch 4628\n",
            "Training acc and loss are 0.79286 and 0.5797443\n",
            "Val acc and loss are 0.7866 and 0.585382\n",
            "Processing Epoch 4629\n",
            "Training acc and loss are 0.79288 and 0.5797162\n",
            "Val acc and loss are 0.7866 and 0.58535457\n",
            "Processing Epoch 4630\n",
            "Training acc and loss are 0.79288 and 0.5796852\n",
            "Val acc and loss are 0.7867 and 0.5853246\n",
            "Processing Epoch 4631\n",
            "Training acc and loss are 0.79288 and 0.57965755\n",
            "Val acc and loss are 0.7866 and 0.5852976\n",
            "Processing Epoch 4632\n",
            "Training acc and loss are 0.79286 and 0.57962954\n",
            "Val acc and loss are 0.7866 and 0.5852705\n",
            "Processing Epoch 4633\n",
            "Training acc and loss are 0.79288 and 0.579602\n",
            "Val acc and loss are 0.7866 and 0.5852436\n",
            "Processing Epoch 4634\n",
            "Training acc and loss are 0.7929 and 0.57957226\n",
            "Val acc and loss are 0.7867 and 0.58521515\n",
            "Processing Epoch 4635\n",
            "Training acc and loss are 0.7929 and 0.579544\n",
            "Val acc and loss are 0.7867 and 0.5851878\n",
            "Processing Epoch 4636\n",
            "Training acc and loss are 0.7929 and 0.5795169\n",
            "Val acc and loss are 0.7867 and 0.58516145\n",
            "Processing Epoch 4637\n",
            "Training acc and loss are 0.7929 and 0.5794873\n",
            "Val acc and loss are 0.7867 and 0.5851329\n",
            "Processing Epoch 4638\n",
            "Training acc and loss are 0.79292 and 0.57945853\n",
            "Val acc and loss are 0.7867 and 0.58510554\n",
            "Processing Epoch 4639\n",
            "Training acc and loss are 0.79292 and 0.5794299\n",
            "Val acc and loss are 0.7867 and 0.5850778\n",
            "Processing Epoch 4640\n",
            "Training acc and loss are 0.79294 and 0.57940185\n",
            "Val acc and loss are 0.7866 and 0.5850507\n",
            "Processing Epoch 4641\n",
            "Training acc and loss are 0.79294 and 0.57937086\n",
            "Val acc and loss are 0.7866 and 0.5850212\n",
            "Processing Epoch 4642\n",
            "Training acc and loss are 0.79296 and 0.57934314\n",
            "Val acc and loss are 0.7866 and 0.5849952\n",
            "Processing Epoch 4643\n",
            "Training acc and loss are 0.79296 and 0.57931554\n",
            "Val acc and loss are 0.7866 and 0.5849687\n",
            "Processing Epoch 4644\n",
            "Training acc and loss are 0.79296 and 0.5792867\n",
            "Val acc and loss are 0.7866 and 0.5849406\n",
            "Processing Epoch 4645\n",
            "Training acc and loss are 0.79298 and 0.57926154\n",
            "Val acc and loss are 0.7866 and 0.5849163\n",
            "Processing Epoch 4646\n",
            "Training acc and loss are 0.79298 and 0.57923436\n",
            "Val acc and loss are 0.7866 and 0.58489007\n",
            "Processing Epoch 4647\n",
            "Training acc and loss are 0.793 and 0.57920486\n",
            "Val acc and loss are 0.7866 and 0.5848618\n",
            "Processing Epoch 4648\n",
            "Training acc and loss are 0.79302 and 0.57917583\n",
            "Val acc and loss are 0.7866 and 0.5848335\n",
            "Processing Epoch 4649\n",
            "Training acc and loss are 0.79306 and 0.57914627\n",
            "Val acc and loss are 0.7866 and 0.58480537\n",
            "Processing Epoch 4650\n",
            "Training acc and loss are 0.79306 and 0.5791177\n",
            "Val acc and loss are 0.7866 and 0.58477795\n",
            "Processing Epoch 4651\n",
            "Training acc and loss are 0.7931 and 0.5790897\n",
            "Val acc and loss are 0.7866 and 0.58475125\n",
            "Processing Epoch 4652\n",
            "Training acc and loss are 0.79312 and 0.5790612\n",
            "Val acc and loss are 0.7866 and 0.5847236\n",
            "Processing Epoch 4653\n",
            "Training acc and loss are 0.7931 and 0.579035\n",
            "Val acc and loss are 0.7866 and 0.584698\n",
            "Processing Epoch 4654\n",
            "Training acc and loss are 0.79312 and 0.579006\n",
            "Val acc and loss are 0.7866 and 0.5846708\n",
            "Processing Epoch 4655\n",
            "Training acc and loss are 0.79312 and 0.57897794\n",
            "Val acc and loss are 0.7866 and 0.5846434\n",
            "Processing Epoch 4656\n",
            "Training acc and loss are 0.79314 and 0.5789493\n",
            "Val acc and loss are 0.7867 and 0.5846162\n",
            "Processing Epoch 4657\n",
            "Training acc and loss are 0.79316 and 0.5789183\n",
            "Val acc and loss are 0.7867 and 0.58458555\n",
            "Processing Epoch 4658\n",
            "Training acc and loss are 0.79318 and 0.5788925\n",
            "Val acc and loss are 0.7869 and 0.5845615\n",
            "Processing Epoch 4659\n",
            "Training acc and loss are 0.79318 and 0.57886297\n",
            "Val acc and loss are 0.7869 and 0.584533\n",
            "Processing Epoch 4660\n",
            "Training acc and loss are 0.79322 and 0.5788358\n",
            "Val acc and loss are 0.787 and 0.58450705\n",
            "Processing Epoch 4661\n",
            "Training acc and loss are 0.79322 and 0.5788085\n",
            "Val acc and loss are 0.7871 and 0.58448017\n",
            "Processing Epoch 4662\n",
            "Training acc and loss are 0.79324 and 0.5787824\n",
            "Val acc and loss are 0.7871 and 0.5844555\n",
            "Processing Epoch 4663\n",
            "Training acc and loss are 0.79324 and 0.5787572\n",
            "Val acc and loss are 0.7871 and 0.5844317\n",
            "Processing Epoch 4664\n",
            "Training acc and loss are 0.79324 and 0.5787267\n",
            "Val acc and loss are 0.7871 and 0.58440137\n",
            "Processing Epoch 4665\n",
            "Training acc and loss are 0.79324 and 0.5786978\n",
            "Val acc and loss are 0.787 and 0.5843736\n",
            "Processing Epoch 4666\n",
            "Training acc and loss are 0.79322 and 0.5786689\n",
            "Val acc and loss are 0.787 and 0.58434564\n",
            "Processing Epoch 4667\n",
            "Training acc and loss are 0.79322 and 0.5786401\n",
            "Val acc and loss are 0.7869 and 0.5843175\n",
            "Processing Epoch 4668\n",
            "Training acc and loss are 0.79328 and 0.578611\n",
            "Val acc and loss are 0.787 and 0.58428925\n",
            "Processing Epoch 4669\n",
            "Training acc and loss are 0.7933 and 0.5785817\n",
            "Val acc and loss are 0.787 and 0.58426136\n",
            "Processing Epoch 4670\n",
            "Training acc and loss are 0.7933 and 0.57855326\n",
            "Val acc and loss are 0.7869 and 0.5842338\n",
            "Processing Epoch 4671\n",
            "Training acc and loss are 0.7933 and 0.5785275\n",
            "Val acc and loss are 0.7869 and 0.58420956\n",
            "Processing Epoch 4672\n",
            "Training acc and loss are 0.7933 and 0.5785011\n",
            "Val acc and loss are 0.787 and 0.58418435\n",
            "Processing Epoch 4673\n",
            "Training acc and loss are 0.79332 and 0.57847524\n",
            "Val acc and loss are 0.7869 and 0.5841595\n",
            "Processing Epoch 4674\n",
            "Training acc and loss are 0.79332 and 0.5784474\n",
            "Val acc and loss are 0.787 and 0.5841328\n",
            "Processing Epoch 4675\n",
            "Training acc and loss are 0.79334 and 0.57841986\n",
            "Val acc and loss are 0.7871 and 0.58410615\n",
            "Processing Epoch 4676\n",
            "Training acc and loss are 0.79336 and 0.57839006\n",
            "Val acc and loss are 0.7871 and 0.58407784\n",
            "Processing Epoch 4677\n",
            "Training acc and loss are 0.79336 and 0.57836545\n",
            "Val acc and loss are 0.7871 and 0.5840544\n",
            "Processing Epoch 4678\n",
            "Training acc and loss are 0.79338 and 0.5783378\n",
            "Val acc and loss are 0.7871 and 0.5840277\n",
            "Processing Epoch 4679\n",
            "Training acc and loss are 0.7934 and 0.5783123\n",
            "Val acc and loss are 0.7871 and 0.5840029\n",
            "Processing Epoch 4680\n",
            "Training acc and loss are 0.7934 and 0.5782868\n",
            "Val acc and loss are 0.7871 and 0.5839786\n",
            "Processing Epoch 4681\n",
            "Training acc and loss are 0.79342 and 0.5782572\n",
            "Val acc and loss are 0.7871 and 0.5839501\n",
            "Processing Epoch 4682\n",
            "Training acc and loss are 0.79344 and 0.57823163\n",
            "Val acc and loss are 0.7871 and 0.58392555\n",
            "Processing Epoch 4683\n",
            "Training acc and loss are 0.79346 and 0.5782061\n",
            "Val acc and loss are 0.7871 and 0.5839009\n",
            "Processing Epoch 4684\n",
            "Training acc and loss are 0.79346 and 0.57817835\n",
            "Val acc and loss are 0.7872 and 0.583875\n",
            "Processing Epoch 4685\n",
            "Training acc and loss are 0.79346 and 0.57814896\n",
            "Val acc and loss are 0.7872 and 0.5838462\n",
            "Processing Epoch 4686\n",
            "Training acc and loss are 0.79344 and 0.57811713\n",
            "Val acc and loss are 0.7872 and 0.5838149\n",
            "Processing Epoch 4687\n",
            "Training acc and loss are 0.79344 and 0.5780886\n",
            "Val acc and loss are 0.7873 and 0.5837876\n",
            "Processing Epoch 4688\n",
            "Training acc and loss are 0.79344 and 0.5780626\n",
            "Val acc and loss are 0.7873 and 0.5837627\n",
            "Processing Epoch 4689\n",
            "Training acc and loss are 0.79344 and 0.57803124\n",
            "Val acc and loss are 0.7873 and 0.5837322\n",
            "Processing Epoch 4690\n",
            "Training acc and loss are 0.79344 and 0.5780035\n",
            "Val acc and loss are 0.7873 and 0.5837054\n",
            "Processing Epoch 4691\n",
            "Training acc and loss are 0.79346 and 0.5779757\n",
            "Val acc and loss are 0.7874 and 0.5836796\n",
            "Processing Epoch 4692\n",
            "Training acc and loss are 0.79346 and 0.57794976\n",
            "Val acc and loss are 0.7874 and 0.58365464\n",
            "Processing Epoch 4693\n",
            "Training acc and loss are 0.79348 and 0.5779209\n",
            "Val acc and loss are 0.7874 and 0.58362675\n",
            "Processing Epoch 4694\n",
            "Training acc and loss are 0.79348 and 0.5778935\n",
            "Val acc and loss are 0.7874 and 0.5835997\n",
            "Processing Epoch 4695\n",
            "Training acc and loss are 0.79348 and 0.5778674\n",
            "Val acc and loss are 0.7874 and 0.5835746\n",
            "Processing Epoch 4696\n",
            "Training acc and loss are 0.79348 and 0.57783985\n",
            "Val acc and loss are 0.7874 and 0.5835485\n",
            "Processing Epoch 4697\n",
            "Training acc and loss are 0.79348 and 0.57781434\n",
            "Val acc and loss are 0.7874 and 0.58352345\n",
            "Processing Epoch 4698\n",
            "Training acc and loss are 0.79352 and 0.57778436\n",
            "Val acc and loss are 0.7874 and 0.58349454\n",
            "Processing Epoch 4699\n",
            "Training acc and loss are 0.79354 and 0.5777553\n",
            "Val acc and loss are 0.7873 and 0.58346635\n",
            "Processing Epoch 4700\n",
            "Training acc and loss are 0.79354 and 0.57772607\n",
            "Val acc and loss are 0.7874 and 0.583438\n",
            "Processing Epoch 4701\n",
            "Training acc and loss are 0.79356 and 0.5776974\n",
            "Val acc and loss are 0.7874 and 0.5834101\n",
            "Processing Epoch 4702\n",
            "Training acc and loss are 0.79358 and 0.57766974\n",
            "Val acc and loss are 0.7874 and 0.58338416\n",
            "Processing Epoch 4703\n",
            "Training acc and loss are 0.79362 and 0.5776408\n",
            "Val acc and loss are 0.7874 and 0.5833567\n",
            "Processing Epoch 4704\n",
            "Training acc and loss are 0.79364 and 0.577608\n",
            "Val acc and loss are 0.7874 and 0.58332473\n",
            "Processing Epoch 4705\n",
            "Training acc and loss are 0.79364 and 0.577581\n",
            "Val acc and loss are 0.7874 and 0.5832991\n",
            "Processing Epoch 4706\n",
            "Training acc and loss are 0.79366 and 0.577553\n",
            "Val acc and loss are 0.7874 and 0.5832731\n",
            "Processing Epoch 4707\n",
            "Training acc and loss are 0.79364 and 0.5775229\n",
            "Val acc and loss are 0.7874 and 0.5832437\n",
            "Processing Epoch 4708\n",
            "Training acc and loss are 0.79366 and 0.5774956\n",
            "Val acc and loss are 0.7875 and 0.58321756\n",
            "Processing Epoch 4709\n",
            "Training acc and loss are 0.79362 and 0.5774684\n",
            "Val acc and loss are 0.7875 and 0.5831915\n",
            "Processing Epoch 4710\n",
            "Training acc and loss are 0.79368 and 0.5774406\n",
            "Val acc and loss are 0.7875 and 0.58316416\n",
            "Processing Epoch 4711\n",
            "Training acc and loss are 0.7937 and 0.5774097\n",
            "Val acc and loss are 0.7876 and 0.5831345\n",
            "Processing Epoch 4712\n",
            "Training acc and loss are 0.79366 and 0.5773841\n",
            "Val acc and loss are 0.7876 and 0.58311\n",
            "Processing Epoch 4713\n",
            "Training acc and loss are 0.79368 and 0.5773582\n",
            "Val acc and loss are 0.7876 and 0.583086\n",
            "Processing Epoch 4714\n",
            "Training acc and loss are 0.79368 and 0.57733077\n",
            "Val acc and loss are 0.7876 and 0.5830593\n",
            "Processing Epoch 4715\n",
            "Training acc and loss are 0.79366 and 0.57730436\n",
            "Val acc and loss are 0.7876 and 0.5830336\n",
            "Processing Epoch 4716\n",
            "Training acc and loss are 0.7937 and 0.57727414\n",
            "Val acc and loss are 0.7876 and 0.5830046\n",
            "Processing Epoch 4717\n",
            "Training acc and loss are 0.79372 and 0.5772453\n",
            "Val acc and loss are 0.7876 and 0.5829774\n",
            "Processing Epoch 4718\n",
            "Training acc and loss are 0.79372 and 0.57722044\n",
            "Val acc and loss are 0.7876 and 0.5829532\n",
            "Processing Epoch 4719\n",
            "Training acc and loss are 0.79372 and 0.57719266\n",
            "Val acc and loss are 0.7876 and 0.5829263\n",
            "Processing Epoch 4720\n",
            "Training acc and loss are 0.79374 and 0.57716227\n",
            "Val acc and loss are 0.7877 and 0.5828966\n",
            "Processing Epoch 4721\n",
            "Training acc and loss are 0.79374 and 0.5771341\n",
            "Val acc and loss are 0.7877 and 0.5828698\n",
            "Processing Epoch 4722\n",
            "Training acc and loss are 0.79376 and 0.5771075\n",
            "Val acc and loss are 0.7877 and 0.58284384\n",
            "Processing Epoch 4723\n",
            "Training acc and loss are 0.79374 and 0.5770801\n",
            "Val acc and loss are 0.7876 and 0.58281684\n",
            "Processing Epoch 4724\n",
            "Training acc and loss are 0.79376 and 0.57705367\n",
            "Val acc and loss are 0.7876 and 0.5827911\n",
            "Processing Epoch 4725\n",
            "Training acc and loss are 0.79372 and 0.57702667\n",
            "Val acc and loss are 0.7876 and 0.5827642\n",
            "Processing Epoch 4726\n",
            "Training acc and loss are 0.79372 and 0.57700056\n",
            "Val acc and loss are 0.7876 and 0.58273906\n",
            "Processing Epoch 4727\n",
            "Training acc and loss are 0.7937 and 0.57697403\n",
            "Val acc and loss are 0.7876 and 0.5827133\n",
            "Processing Epoch 4728\n",
            "Training acc and loss are 0.79372 and 0.5769491\n",
            "Val acc and loss are 0.7876 and 0.5826898\n",
            "Processing Epoch 4729\n",
            "Training acc and loss are 0.79372 and 0.5769212\n",
            "Val acc and loss are 0.7876 and 0.5826631\n",
            "Processing Epoch 4730\n",
            "Training acc and loss are 0.79372 and 0.57689244\n",
            "Val acc and loss are 0.7876 and 0.5826357\n",
            "Processing Epoch 4731\n",
            "Training acc and loss are 0.79372 and 0.57686657\n",
            "Val acc and loss are 0.7876 and 0.5826104\n",
            "Processing Epoch 4732\n",
            "Training acc and loss are 0.7937 and 0.5768395\n",
            "Val acc and loss are 0.7876 and 0.58258456\n",
            "Processing Epoch 4733\n",
            "Training acc and loss are 0.79372 and 0.5768115\n",
            "Val acc and loss are 0.7876 and 0.58255774\n",
            "Processing Epoch 4734\n",
            "Training acc and loss are 0.79374 and 0.57678276\n",
            "Val acc and loss are 0.7877 and 0.5825305\n",
            "Processing Epoch 4735\n",
            "Training acc and loss are 0.79376 and 0.57675576\n",
            "Val acc and loss are 0.7877 and 0.5825043\n",
            "Processing Epoch 4736\n",
            "Training acc and loss are 0.79378 and 0.5767278\n",
            "Val acc and loss are 0.7877 and 0.5824773\n",
            "Processing Epoch 4737\n",
            "Training acc and loss are 0.79378 and 0.5767029\n",
            "Val acc and loss are 0.7877 and 0.5824534\n",
            "Processing Epoch 4738\n",
            "Training acc and loss are 0.79378 and 0.5766738\n",
            "Val acc and loss are 0.7877 and 0.5824249\n",
            "Processing Epoch 4739\n",
            "Training acc and loss are 0.79376 and 0.5766469\n",
            "Val acc and loss are 0.7877 and 0.58239913\n",
            "Processing Epoch 4740\n",
            "Training acc and loss are 0.79376 and 0.57661724\n",
            "Val acc and loss are 0.7877 and 0.5823706\n",
            "Processing Epoch 4741\n",
            "Training acc and loss are 0.79378 and 0.5765903\n",
            "Val acc and loss are 0.7877 and 0.5823441\n",
            "Processing Epoch 4742\n",
            "Training acc and loss are 0.7938 and 0.5765638\n",
            "Val acc and loss are 0.7877 and 0.58231837\n",
            "Processing Epoch 4743\n",
            "Training acc and loss are 0.7938 and 0.57653654\n",
            "Val acc and loss are 0.7877 and 0.5822915\n",
            "Processing Epoch 4744\n",
            "Training acc and loss are 0.7938 and 0.57651114\n",
            "Val acc and loss are 0.7877 and 0.5822674\n",
            "Processing Epoch 4745\n",
            "Training acc and loss are 0.79382 and 0.57648486\n",
            "Val acc and loss are 0.7878 and 0.5822415\n",
            "Processing Epoch 4746\n",
            "Training acc and loss are 0.79382 and 0.5764569\n",
            "Val acc and loss are 0.7878 and 0.5822145\n",
            "Processing Epoch 4747\n",
            "Training acc and loss are 0.7938 and 0.5764317\n",
            "Val acc and loss are 0.7878 and 0.5821899\n",
            "Processing Epoch 4748\n",
            "Training acc and loss are 0.79384 and 0.57640517\n",
            "Val acc and loss are 0.7878 and 0.5821651\n",
            "Processing Epoch 4749\n",
            "Training acc and loss are 0.79394 and 0.5763788\n",
            "Val acc and loss are 0.7878 and 0.5821399\n",
            "Processing Epoch 4750\n",
            "Training acc and loss are 0.79394 and 0.5763482\n",
            "Val acc and loss are 0.7879 and 0.58211046\n",
            "Processing Epoch 4751\n",
            "Training acc and loss are 0.79394 and 0.5763206\n",
            "Val acc and loss are 0.7879 and 0.5820834\n",
            "Processing Epoch 4752\n",
            "Training acc and loss are 0.79394 and 0.57629365\n",
            "Val acc and loss are 0.7879 and 0.5820572\n",
            "Processing Epoch 4753\n",
            "Training acc and loss are 0.79396 and 0.57626635\n",
            "Val acc and loss are 0.7879 and 0.58203095\n",
            "Processing Epoch 4754\n",
            "Training acc and loss are 0.79396 and 0.57623756\n",
            "Val acc and loss are 0.7879 and 0.5820036\n",
            "Processing Epoch 4755\n",
            "Training acc and loss are 0.79398 and 0.5762117\n",
            "Val acc and loss are 0.7879 and 0.5819789\n",
            "Processing Epoch 4756\n",
            "Training acc and loss are 0.79398 and 0.57618475\n",
            "Val acc and loss are 0.7879 and 0.58195305\n",
            "Processing Epoch 4757\n",
            "Training acc and loss are 0.79398 and 0.57615846\n",
            "Val acc and loss are 0.7879 and 0.5819281\n",
            "Processing Epoch 4758\n",
            "Training acc and loss are 0.794 and 0.57613164\n",
            "Val acc and loss are 0.7879 and 0.5819017\n",
            "Processing Epoch 4759\n",
            "Training acc and loss are 0.79404 and 0.5761046\n",
            "Val acc and loss are 0.7879 and 0.5818747\n",
            "Processing Epoch 4760\n",
            "Training acc and loss are 0.79406 and 0.5760798\n",
            "Val acc and loss are 0.7879 and 0.58185124\n",
            "Processing Epoch 4761\n",
            "Training acc and loss are 0.79406 and 0.576054\n",
            "Val acc and loss are 0.7879 and 0.58182514\n",
            "Processing Epoch 4762\n",
            "Training acc and loss are 0.79406 and 0.5760253\n",
            "Val acc and loss are 0.7879 and 0.5817963\n",
            "Processing Epoch 4763\n",
            "Training acc and loss are 0.79406 and 0.5759985\n",
            "Val acc and loss are 0.7879 and 0.581771\n",
            "Processing Epoch 4764\n",
            "Training acc and loss are 0.79406 and 0.57597303\n",
            "Val acc and loss are 0.7879 and 0.58174676\n",
            "Processing Epoch 4765\n",
            "Training acc and loss are 0.79404 and 0.5759496\n",
            "Val acc and loss are 0.7879 and 0.58172464\n",
            "Processing Epoch 4766\n",
            "Training acc and loss are 0.79408 and 0.5759203\n",
            "Val acc and loss are 0.7879 and 0.5816961\n",
            "Processing Epoch 4767\n",
            "Training acc and loss are 0.79404 and 0.5758962\n",
            "Val acc and loss are 0.7879 and 0.58167315\n",
            "Processing Epoch 4768\n",
            "Training acc and loss are 0.79404 and 0.57587117\n",
            "Val acc and loss are 0.7879 and 0.5816493\n",
            "Processing Epoch 4769\n",
            "Training acc and loss are 0.79408 and 0.57584345\n",
            "Val acc and loss are 0.7879 and 0.581623\n",
            "Processing Epoch 4770\n",
            "Training acc and loss are 0.7941 and 0.57581365\n",
            "Val acc and loss are 0.7879 and 0.58159405\n",
            "Processing Epoch 4771\n",
            "Training acc and loss are 0.79406 and 0.5757869\n",
            "Val acc and loss are 0.7879 and 0.5815689\n",
            "Processing Epoch 4772\n",
            "Training acc and loss are 0.7941 and 0.5757595\n",
            "Val acc and loss are 0.7879 and 0.58154255\n",
            "Processing Epoch 4773\n",
            "Training acc and loss are 0.7941 and 0.5757336\n",
            "Val acc and loss are 0.7879 and 0.5815179\n",
            "Processing Epoch 4774\n",
            "Training acc and loss are 0.79414 and 0.57570505\n",
            "Val acc and loss are 0.788 and 0.58149034\n",
            "Processing Epoch 4775\n",
            "Training acc and loss are 0.79414 and 0.57567626\n",
            "Val acc and loss are 0.788 and 0.5814624\n",
            "Processing Epoch 4776\n",
            "Training acc and loss are 0.79414 and 0.57565045\n",
            "Val acc and loss are 0.788 and 0.5814368\n",
            "Processing Epoch 4777\n",
            "Training acc and loss are 0.79418 and 0.5756245\n",
            "Val acc and loss are 0.7881 and 0.5814117\n",
            "Processing Epoch 4778\n",
            "Training acc and loss are 0.79418 and 0.5755984\n",
            "Val acc and loss are 0.7881 and 0.5813865\n",
            "Processing Epoch 4779\n",
            "Training acc and loss are 0.79418 and 0.57557195\n",
            "Val acc and loss are 0.7881 and 0.58136106\n",
            "Processing Epoch 4780\n",
            "Training acc and loss are 0.79418 and 0.57554466\n",
            "Val acc and loss are 0.7881 and 0.5813343\n",
            "Processing Epoch 4781\n",
            "Training acc and loss are 0.79418 and 0.57551706\n",
            "Val acc and loss are 0.7881 and 0.5813071\n",
            "Processing Epoch 4782\n",
            "Training acc and loss are 0.79422 and 0.57549024\n",
            "Val acc and loss are 0.7881 and 0.5812809\n",
            "Processing Epoch 4783\n",
            "Training acc and loss are 0.79422 and 0.5754604\n",
            "Val acc and loss are 0.7882 and 0.58125204\n",
            "Processing Epoch 4784\n",
            "Training acc and loss are 0.79422 and 0.5754332\n",
            "Val acc and loss are 0.7882 and 0.5812258\n",
            "Processing Epoch 4785\n",
            "Training acc and loss are 0.79426 and 0.57541096\n",
            "Val acc and loss are 0.7881 and 0.5812047\n",
            "Processing Epoch 4786\n",
            "Training acc and loss are 0.79426 and 0.5753827\n",
            "Val acc and loss are 0.7881 and 0.5811774\n",
            "Processing Epoch 4787\n",
            "Training acc and loss are 0.79426 and 0.57535714\n",
            "Val acc and loss are 0.7881 and 0.58115214\n",
            "Processing Epoch 4788\n",
            "Training acc and loss are 0.79428 and 0.57533175\n",
            "Val acc and loss are 0.7881 and 0.58112776\n",
            "Processing Epoch 4789\n",
            "Training acc and loss are 0.7943 and 0.5753038\n",
            "Val acc and loss are 0.7881 and 0.581101\n",
            "Processing Epoch 4790\n",
            "Training acc and loss are 0.7943 and 0.57527643\n",
            "Val acc and loss are 0.7881 and 0.581074\n",
            "Processing Epoch 4791\n",
            "Training acc and loss are 0.79432 and 0.5752501\n",
            "Val acc and loss are 0.7881 and 0.5810489\n",
            "Processing Epoch 4792\n",
            "Training acc and loss are 0.79432 and 0.5752238\n",
            "Val acc and loss are 0.7881 and 0.58102334\n",
            "Processing Epoch 4793\n",
            "Training acc and loss are 0.79432 and 0.57519656\n",
            "Val acc and loss are 0.7881 and 0.58099675\n",
            "Processing Epoch 4794\n",
            "Training acc and loss are 0.79436 and 0.57516974\n",
            "Val acc and loss are 0.7881 and 0.58097076\n",
            "Processing Epoch 4795\n",
            "Training acc and loss are 0.79436 and 0.5751413\n",
            "Val acc and loss are 0.7881 and 0.5809431\n",
            "Processing Epoch 4796\n",
            "Training acc and loss are 0.79436 and 0.5751126\n",
            "Val acc and loss are 0.7881 and 0.58091575\n",
            "Processing Epoch 4797\n",
            "Training acc and loss are 0.79436 and 0.5750851\n",
            "Val acc and loss are 0.7881 and 0.58088887\n",
            "Processing Epoch 4798\n",
            "Training acc and loss are 0.79436 and 0.57505983\n",
            "Val acc and loss are 0.7881 and 0.58086413\n",
            "Processing Epoch 4799\n",
            "Training acc and loss are 0.79436 and 0.57503164\n",
            "Val acc and loss are 0.7881 and 0.5808374\n",
            "Processing Epoch 4800\n",
            "Training acc and loss are 0.7944 and 0.57500255\n",
            "Val acc and loss are 0.7881 and 0.5808093\n",
            "Processing Epoch 4801\n",
            "Training acc and loss are 0.7944 and 0.574977\n",
            "Val acc and loss are 0.7881 and 0.58078474\n",
            "Processing Epoch 4802\n",
            "Training acc and loss are 0.7944 and 0.57494944\n",
            "Val acc and loss are 0.7881 and 0.5807579\n",
            "Processing Epoch 4803\n",
            "Training acc and loss are 0.79444 and 0.57492304\n",
            "Val acc and loss are 0.7881 and 0.5807323\n",
            "Processing Epoch 4804\n",
            "Training acc and loss are 0.79444 and 0.57489413\n",
            "Val acc and loss are 0.7881 and 0.58070433\n",
            "Processing Epoch 4805\n",
            "Training acc and loss are 0.79444 and 0.5748709\n",
            "Val acc and loss are 0.7881 and 0.58068216\n",
            "Processing Epoch 4806\n",
            "Training acc and loss are 0.79444 and 0.5748461\n",
            "Val acc and loss are 0.7881 and 0.5806584\n",
            "Processing Epoch 4807\n",
            "Training acc and loss are 0.79444 and 0.57482046\n",
            "Val acc and loss are 0.7881 and 0.5806332\n",
            "Processing Epoch 4808\n",
            "Training acc and loss are 0.79444 and 0.57479274\n",
            "Val acc and loss are 0.7881 and 0.58060646\n",
            "Processing Epoch 4809\n",
            "Training acc and loss are 0.79444 and 0.57476807\n",
            "Val acc and loss are 0.7881 and 0.58058304\n",
            "Processing Epoch 4810\n",
            "Training acc and loss are 0.79446 and 0.5747396\n",
            "Val acc and loss are 0.7881 and 0.58055484\n",
            "Processing Epoch 4811\n",
            "Training acc and loss are 0.79446 and 0.57470983\n",
            "Val acc and loss are 0.7881 and 0.5805259\n",
            "Processing Epoch 4812\n",
            "Training acc and loss are 0.79444 and 0.574685\n",
            "Val acc and loss are 0.7881 and 0.580502\n",
            "Processing Epoch 4813\n",
            "Training acc and loss are 0.79442 and 0.5746604\n",
            "Val acc and loss are 0.7882 and 0.5804777\n",
            "Processing Epoch 4814\n",
            "Training acc and loss are 0.79444 and 0.5746349\n",
            "Val acc and loss are 0.7881 and 0.5804537\n",
            "Processing Epoch 4815\n",
            "Training acc and loss are 0.79448 and 0.574608\n",
            "Val acc and loss are 0.7882 and 0.5804274\n",
            "Processing Epoch 4816\n",
            "Training acc and loss are 0.79452 and 0.5745798\n",
            "Val acc and loss are 0.7882 and 0.580401\n",
            "Processing Epoch 4817\n",
            "Training acc and loss are 0.79454 and 0.57455695\n",
            "Val acc and loss are 0.7882 and 0.5803793\n",
            "Processing Epoch 4818\n",
            "Training acc and loss are 0.79454 and 0.57452786\n",
            "Val acc and loss are 0.7883 and 0.58035094\n",
            "Processing Epoch 4819\n",
            "Training acc and loss are 0.79454 and 0.57449883\n",
            "Val acc and loss are 0.7883 and 0.58032227\n",
            "Processing Epoch 4820\n",
            "Training acc and loss are 0.79454 and 0.5744741\n",
            "Val acc and loss are 0.7883 and 0.58029854\n",
            "Processing Epoch 4821\n",
            "Training acc and loss are 0.79456 and 0.57444805\n",
            "Val acc and loss are 0.7883 and 0.5802732\n",
            "Processing Epoch 4822\n",
            "Training acc and loss are 0.79456 and 0.5744251\n",
            "Val acc and loss are 0.7882 and 0.5802514\n",
            "Processing Epoch 4823\n",
            "Training acc and loss are 0.79456 and 0.57439584\n",
            "Val acc and loss are 0.7883 and 0.5802235\n",
            "Processing Epoch 4824\n",
            "Training acc and loss are 0.79456 and 0.5743695\n",
            "Val acc and loss are 0.7883 and 0.58019805\n",
            "Processing Epoch 4825\n",
            "Training acc and loss are 0.79456 and 0.57434195\n",
            "Val acc and loss are 0.7882 and 0.58017164\n",
            "Processing Epoch 4826\n",
            "Training acc and loss are 0.79454 and 0.5743167\n",
            "Val acc and loss are 0.7882 and 0.5801476\n",
            "Processing Epoch 4827\n",
            "Training acc and loss are 0.79458 and 0.57429224\n",
            "Val acc and loss are 0.7882 and 0.5801249\n",
            "Processing Epoch 4828\n",
            "Training acc and loss are 0.79458 and 0.5742632\n",
            "Val acc and loss are 0.7882 and 0.58009636\n",
            "Processing Epoch 4829\n",
            "Training acc and loss are 0.79458 and 0.574233\n",
            "Val acc and loss are 0.7883 and 0.5800666\n",
            "Processing Epoch 4830\n",
            "Training acc and loss are 0.7946 and 0.5742071\n",
            "Val acc and loss are 0.7882 and 0.58004147\n",
            "Processing Epoch 4831\n",
            "Training acc and loss are 0.79464 and 0.5741813\n",
            "Val acc and loss are 0.7882 and 0.58001643\n",
            "Processing Epoch 4832\n",
            "Training acc and loss are 0.79462 and 0.5741587\n",
            "Val acc and loss are 0.7882 and 0.57999444\n",
            "Processing Epoch 4833\n",
            "Training acc and loss are 0.79464 and 0.5741333\n",
            "Val acc and loss are 0.7882 and 0.57996994\n",
            "Processing Epoch 4834\n",
            "Training acc and loss are 0.79466 and 0.5741067\n",
            "Val acc and loss are 0.7882 and 0.5799442\n",
            "Processing Epoch 4835\n",
            "Training acc and loss are 0.7947 and 0.5740805\n",
            "Val acc and loss are 0.7883 and 0.57991815\n",
            "Processing Epoch 4836\n",
            "Training acc and loss are 0.79474 and 0.5740519\n",
            "Val acc and loss are 0.7883 and 0.57989055\n",
            "Processing Epoch 4837\n",
            "Training acc and loss are 0.7947 and 0.5740228\n",
            "Val acc and loss are 0.7883 and 0.57986194\n",
            "Processing Epoch 4838\n",
            "Training acc and loss are 0.79472 and 0.57399863\n",
            "Val acc and loss are 0.7883 and 0.5798395\n",
            "Processing Epoch 4839\n",
            "Training acc and loss are 0.7947 and 0.57397\n",
            "Val acc and loss are 0.7883 and 0.5798117\n",
            "Processing Epoch 4840\n",
            "Training acc and loss are 0.7947 and 0.5739467\n",
            "Val acc and loss are 0.7883 and 0.57979023\n",
            "Processing Epoch 4841\n",
            "Training acc and loss are 0.79472 and 0.5739178\n",
            "Val acc and loss are 0.7883 and 0.579762\n",
            "Processing Epoch 4842\n",
            "Training acc and loss are 0.79474 and 0.5738908\n",
            "Val acc and loss are 0.7883 and 0.5797357\n",
            "Processing Epoch 4843\n",
            "Training acc and loss are 0.79476 and 0.5738663\n",
            "Val acc and loss are 0.7883 and 0.57971156\n",
            "Processing Epoch 4844\n",
            "Training acc and loss are 0.79482 and 0.5738408\n",
            "Val acc and loss are 0.7884 and 0.5796873\n",
            "Processing Epoch 4845\n",
            "Training acc and loss are 0.79482 and 0.5738169\n",
            "Val acc and loss are 0.7884 and 0.5796648\n",
            "Processing Epoch 4846\n",
            "Training acc and loss are 0.79482 and 0.57378954\n",
            "Val acc and loss are 0.7884 and 0.57963836\n",
            "Processing Epoch 4847\n",
            "Training acc and loss are 0.79482 and 0.5737645\n",
            "Val acc and loss are 0.7885 and 0.57961386\n",
            "Processing Epoch 4848\n",
            "Training acc and loss are 0.79482 and 0.57373834\n",
            "Val acc and loss are 0.7886 and 0.57958907\n",
            "Processing Epoch 4849\n",
            "Training acc and loss are 0.79484 and 0.57371074\n",
            "Val acc and loss are 0.7886 and 0.5795628\n",
            "Processing Epoch 4850\n",
            "Training acc and loss are 0.79484 and 0.5736862\n",
            "Val acc and loss are 0.7886 and 0.5795395\n",
            "Processing Epoch 4851\n",
            "Training acc and loss are 0.79484 and 0.5736581\n",
            "Val acc and loss are 0.7887 and 0.57951164\n",
            "Processing Epoch 4852\n",
            "Training acc and loss are 0.79484 and 0.5736297\n",
            "Val acc and loss are 0.7887 and 0.5794841\n",
            "Processing Epoch 4853\n",
            "Training acc and loss are 0.79484 and 0.57360184\n",
            "Val acc and loss are 0.7887 and 0.57945645\n",
            "Processing Epoch 4854\n",
            "Training acc and loss are 0.79486 and 0.57357675\n",
            "Val acc and loss are 0.7887 and 0.57943225\n",
            "Processing Epoch 4855\n",
            "Training acc and loss are 0.79486 and 0.5735486\n",
            "Val acc and loss are 0.7887 and 0.5794048\n",
            "Processing Epoch 4856\n",
            "Training acc and loss are 0.79486 and 0.5735198\n",
            "Val acc and loss are 0.7887 and 0.5793769\n",
            "Processing Epoch 4857\n",
            "Training acc and loss are 0.79488 and 0.57349557\n",
            "Val acc and loss are 0.7888 and 0.5793535\n",
            "Processing Epoch 4858\n",
            "Training acc and loss are 0.79488 and 0.5734667\n",
            "Val acc and loss are 0.7887 and 0.5793251\n",
            "Processing Epoch 4859\n",
            "Training acc and loss are 0.79488 and 0.5734395\n",
            "Val acc and loss are 0.7887 and 0.57929903\n",
            "Processing Epoch 4860\n",
            "Training acc and loss are 0.79488 and 0.5734137\n",
            "Val acc and loss are 0.7887 and 0.5792747\n",
            "Processing Epoch 4861\n",
            "Training acc and loss are 0.79492 and 0.5733869\n",
            "Val acc and loss are 0.7887 and 0.57924855\n",
            "Processing Epoch 4862\n",
            "Training acc and loss are 0.79492 and 0.573359\n",
            "Val acc and loss are 0.7888 and 0.5792213\n",
            "Processing Epoch 4863\n",
            "Training acc and loss are 0.79492 and 0.57333267\n",
            "Val acc and loss are 0.7888 and 0.5791965\n",
            "Processing Epoch 4864\n",
            "Training acc and loss are 0.79494 and 0.5733066\n",
            "Val acc and loss are 0.7888 and 0.57917154\n",
            "Processing Epoch 4865\n",
            "Training acc and loss are 0.79494 and 0.57327884\n",
            "Val acc and loss are 0.7888 and 0.57914513\n",
            "Processing Epoch 4866\n",
            "Training acc and loss are 0.7949 and 0.5732502\n",
            "Val acc and loss are 0.7888 and 0.57911736\n",
            "Processing Epoch 4867\n",
            "Training acc and loss are 0.79492 and 0.573223\n",
            "Val acc and loss are 0.7888 and 0.57909113\n",
            "Processing Epoch 4868\n",
            "Training acc and loss are 0.79488 and 0.57319814\n",
            "Val acc and loss are 0.7888 and 0.57906735\n",
            "Processing Epoch 4869\n",
            "Training acc and loss are 0.79488 and 0.57317466\n",
            "Val acc and loss are 0.7888 and 0.5790448\n",
            "Processing Epoch 4870\n",
            "Training acc and loss are 0.79492 and 0.57314795\n",
            "Val acc and loss are 0.7889 and 0.5790191\n",
            "Processing Epoch 4871\n",
            "Training acc and loss are 0.79492 and 0.57312423\n",
            "Val acc and loss are 0.7889 and 0.57899606\n",
            "Processing Epoch 4872\n",
            "Training acc and loss are 0.79494 and 0.5730979\n",
            "Val acc and loss are 0.7889 and 0.57897043\n",
            "Processing Epoch 4873\n",
            "Training acc and loss are 0.79494 and 0.57307196\n",
            "Val acc and loss are 0.7889 and 0.5789459\n",
            "Processing Epoch 4874\n",
            "Training acc and loss are 0.79494 and 0.5730449\n",
            "Val acc and loss are 0.7889 and 0.5789206\n",
            "Processing Epoch 4875\n",
            "Training acc and loss are 0.79494 and 0.5730169\n",
            "Val acc and loss are 0.7889 and 0.57889336\n",
            "Processing Epoch 4876\n",
            "Training acc and loss are 0.79494 and 0.57298744\n",
            "Val acc and loss are 0.7889 and 0.5788644\n",
            "Processing Epoch 4877\n",
            "Training acc and loss are 0.79496 and 0.57296056\n",
            "Val acc and loss are 0.7889 and 0.5788381\n",
            "Processing Epoch 4878\n",
            "Training acc and loss are 0.79496 and 0.57293344\n",
            "Val acc and loss are 0.7889 and 0.57881254\n",
            "Processing Epoch 4879\n",
            "Training acc and loss are 0.79496 and 0.5729091\n",
            "Val acc and loss are 0.7889 and 0.57878876\n",
            "Processing Epoch 4880\n",
            "Training acc and loss are 0.79496 and 0.5728845\n",
            "Val acc and loss are 0.7889 and 0.5787651\n",
            "Processing Epoch 4881\n",
            "Training acc and loss are 0.79496 and 0.5728576\n",
            "Val acc and loss are 0.7889 and 0.57873917\n",
            "Processing Epoch 4882\n",
            "Training acc and loss are 0.79498 and 0.57282966\n",
            "Val acc and loss are 0.7889 and 0.5787113\n",
            "Processing Epoch 4883\n",
            "Training acc and loss are 0.79498 and 0.5728012\n",
            "Val acc and loss are 0.7889 and 0.5786837\n",
            "Processing Epoch 4884\n",
            "Training acc and loss are 0.795 and 0.57277596\n",
            "Val acc and loss are 0.7889 and 0.57865983\n",
            "Processing Epoch 4885\n",
            "Training acc and loss are 0.79504 and 0.57274866\n",
            "Val acc and loss are 0.7889 and 0.5786336\n",
            "Processing Epoch 4886\n",
            "Training acc and loss are 0.7951 and 0.5727234\n",
            "Val acc and loss are 0.7889 and 0.5786091\n",
            "Processing Epoch 4887\n",
            "Training acc and loss are 0.79508 and 0.57269675\n",
            "Val acc and loss are 0.7889 and 0.5785838\n",
            "Processing Epoch 4888\n",
            "Training acc and loss are 0.79508 and 0.5726716\n",
            "Val acc and loss are 0.7889 and 0.5785605\n",
            "Processing Epoch 4889\n",
            "Training acc and loss are 0.79508 and 0.57264286\n",
            "Val acc and loss are 0.7889 and 0.5785325\n",
            "Processing Epoch 4890\n",
            "Training acc and loss are 0.79508 and 0.5726172\n",
            "Val acc and loss are 0.7889 and 0.5785077\n",
            "Processing Epoch 4891\n",
            "Training acc and loss are 0.7951 and 0.5725915\n",
            "Val acc and loss are 0.7889 and 0.578482\n",
            "Processing Epoch 4892\n",
            "Training acc and loss are 0.7951 and 0.5725648\n",
            "Val acc and loss are 0.7889 and 0.5784563\n",
            "Processing Epoch 4893\n",
            "Training acc and loss are 0.7951 and 0.57253915\n",
            "Val acc and loss are 0.7889 and 0.57843125\n",
            "Processing Epoch 4894\n",
            "Training acc and loss are 0.7951 and 0.57251126\n",
            "Val acc and loss are 0.7889 and 0.5784043\n",
            "Processing Epoch 4895\n",
            "Training acc and loss are 0.79514 and 0.5724864\n",
            "Val acc and loss are 0.789 and 0.5783801\n",
            "Processing Epoch 4896\n",
            "Training acc and loss are 0.79512 and 0.57245845\n",
            "Val acc and loss are 0.789 and 0.5783531\n",
            "Processing Epoch 4897\n",
            "Training acc and loss are 0.79514 and 0.5724316\n",
            "Val acc and loss are 0.789 and 0.5783274\n",
            "Processing Epoch 4898\n",
            "Training acc and loss are 0.79512 and 0.57240516\n",
            "Val acc and loss are 0.789 and 0.57830137\n",
            "Processing Epoch 4899\n",
            "Training acc and loss are 0.79514 and 0.5723785\n",
            "Val acc and loss are 0.789 and 0.5782763\n",
            "Processing Epoch 4900\n",
            "Training acc and loss are 0.79514 and 0.5723525\n",
            "Val acc and loss are 0.789 and 0.578251\n",
            "Processing Epoch 4901\n",
            "Training acc and loss are 0.79514 and 0.5723239\n",
            "Val acc and loss are 0.789 and 0.5782236\n",
            "Processing Epoch 4902\n",
            "Training acc and loss are 0.79514 and 0.57229525\n",
            "Val acc and loss are 0.789 and 0.5781955\n",
            "Processing Epoch 4903\n",
            "Training acc and loss are 0.79514 and 0.57226694\n",
            "Val acc and loss are 0.789 and 0.5781683\n",
            "Processing Epoch 4904\n",
            "Training acc and loss are 0.79514 and 0.5722415\n",
            "Val acc and loss are 0.789 and 0.578144\n",
            "Processing Epoch 4905\n",
            "Training acc and loss are 0.79514 and 0.5722155\n",
            "Val acc and loss are 0.789 and 0.57811874\n",
            "Processing Epoch 4906\n",
            "Training acc and loss are 0.79514 and 0.57218826\n",
            "Val acc and loss are 0.789 and 0.57809275\n",
            "Processing Epoch 4907\n",
            "Training acc and loss are 0.79514 and 0.5721631\n",
            "Val acc and loss are 0.789 and 0.57806826\n",
            "Processing Epoch 4908\n",
            "Training acc and loss are 0.79512 and 0.57213664\n",
            "Val acc and loss are 0.789 and 0.57804275\n",
            "Processing Epoch 4909\n",
            "Training acc and loss are 0.79512 and 0.5721097\n",
            "Val acc and loss are 0.789 and 0.5780169\n",
            "Processing Epoch 4910\n",
            "Training acc and loss are 0.79512 and 0.5720831\n",
            "Val acc and loss are 0.789 and 0.57799125\n",
            "Processing Epoch 4911\n",
            "Training acc and loss are 0.79512 and 0.5720577\n",
            "Val acc and loss are 0.789 and 0.57796663\n",
            "Processing Epoch 4912\n",
            "Training acc and loss are 0.79512 and 0.572032\n",
            "Val acc and loss are 0.789 and 0.5779414\n",
            "Processing Epoch 4913\n",
            "Training acc and loss are 0.7951 and 0.5720059\n",
            "Val acc and loss are 0.789 and 0.5779164\n",
            "Processing Epoch 4914\n",
            "Training acc and loss are 0.7951 and 0.571981\n",
            "Val acc and loss are 0.789 and 0.5778928\n",
            "Processing Epoch 4915\n",
            "Training acc and loss are 0.79512 and 0.571956\n",
            "Val acc and loss are 0.789 and 0.5778688\n",
            "Processing Epoch 4916\n",
            "Training acc and loss are 0.79514 and 0.5719303\n",
            "Val acc and loss are 0.789 and 0.57784396\n",
            "Processing Epoch 4917\n",
            "Training acc and loss are 0.79512 and 0.5719046\n",
            "Val acc and loss are 0.789 and 0.57781845\n",
            "Processing Epoch 4918\n",
            "Training acc and loss are 0.79514 and 0.5718802\n",
            "Val acc and loss are 0.789 and 0.57779455\n",
            "Processing Epoch 4919\n",
            "Training acc and loss are 0.79512 and 0.57185346\n",
            "Val acc and loss are 0.789 and 0.5777678\n",
            "Processing Epoch 4920\n",
            "Training acc and loss are 0.79516 and 0.5718273\n",
            "Val acc and loss are 0.789 and 0.5777427\n",
            "Processing Epoch 4921\n",
            "Training acc and loss are 0.79516 and 0.5718028\n",
            "Val acc and loss are 0.789 and 0.57771933\n",
            "Processing Epoch 4922\n",
            "Training acc and loss are 0.79514 and 0.5717812\n",
            "Val acc and loss are 0.789 and 0.5776995\n",
            "Processing Epoch 4923\n",
            "Training acc and loss are 0.79514 and 0.5717563\n",
            "Val acc and loss are 0.789 and 0.5776758\n",
            "Processing Epoch 4924\n",
            "Training acc and loss are 0.79514 and 0.571733\n",
            "Val acc and loss are 0.789 and 0.57765394\n",
            "Processing Epoch 4925\n",
            "Training acc and loss are 0.79514 and 0.5717076\n",
            "Val acc and loss are 0.789 and 0.5776301\n",
            "Processing Epoch 4926\n",
            "Training acc and loss are 0.79514 and 0.5716832\n",
            "Val acc and loss are 0.789 and 0.5776069\n",
            "Processing Epoch 4927\n",
            "Training acc and loss are 0.79514 and 0.5716571\n",
            "Val acc and loss are 0.789 and 0.57758206\n",
            "Processing Epoch 4928\n",
            "Training acc and loss are 0.79516 and 0.5716339\n",
            "Val acc and loss are 0.7891 and 0.5775601\n",
            "Processing Epoch 4929\n",
            "Training acc and loss are 0.79516 and 0.5716071\n",
            "Val acc and loss are 0.7891 and 0.577534\n",
            "Processing Epoch 4930\n",
            "Training acc and loss are 0.79518 and 0.57158154\n",
            "Val acc and loss are 0.7891 and 0.57750887\n",
            "Processing Epoch 4931\n",
            "Training acc and loss are 0.79518 and 0.5715564\n",
            "Val acc and loss are 0.7891 and 0.5774848\n",
            "Processing Epoch 4932\n",
            "Training acc and loss are 0.79524 and 0.57153076\n",
            "Val acc and loss are 0.7891 and 0.5774602\n",
            "Processing Epoch 4933\n",
            "Training acc and loss are 0.79526 and 0.5715063\n",
            "Val acc and loss are 0.7892 and 0.577437\n",
            "Processing Epoch 4934\n",
            "Training acc and loss are 0.79524 and 0.5714797\n",
            "Val acc and loss are 0.7892 and 0.57741106\n",
            "Processing Epoch 4935\n",
            "Training acc and loss are 0.79528 and 0.5714516\n",
            "Val acc and loss are 0.7892 and 0.57738423\n",
            "Processing Epoch 4936\n",
            "Training acc and loss are 0.79528 and 0.57142764\n",
            "Val acc and loss are 0.7892 and 0.5773612\n",
            "Processing Epoch 4937\n",
            "Training acc and loss are 0.79528 and 0.5714043\n",
            "Val acc and loss are 0.7892 and 0.57733905\n",
            "Processing Epoch 4938\n",
            "Training acc and loss are 0.79528 and 0.5713784\n",
            "Val acc and loss are 0.7892 and 0.57731414\n",
            "Processing Epoch 4939\n",
            "Training acc and loss are 0.79526 and 0.5713519\n",
            "Val acc and loss are 0.7892 and 0.57728845\n",
            "Processing Epoch 4940\n",
            "Training acc and loss are 0.79528 and 0.57132417\n",
            "Val acc and loss are 0.7892 and 0.5772619\n",
            "Processing Epoch 4941\n",
            "Training acc and loss are 0.79526 and 0.5712971\n",
            "Val acc and loss are 0.7892 and 0.57723606\n",
            "Processing Epoch 4942\n",
            "Training acc and loss are 0.79526 and 0.5712733\n",
            "Val acc and loss are 0.7892 and 0.5772131\n",
            "Processing Epoch 4943\n",
            "Training acc and loss are 0.79528 and 0.57124764\n",
            "Val acc and loss are 0.7892 and 0.5771876\n",
            "Processing Epoch 4944\n",
            "Training acc and loss are 0.79528 and 0.5712195\n",
            "Val acc and loss are 0.7892 and 0.5771605\n",
            "Processing Epoch 4945\n",
            "Training acc and loss are 0.79528 and 0.5711956\n",
            "Val acc and loss are 0.7892 and 0.5771376\n",
            "Processing Epoch 4946\n",
            "Training acc and loss are 0.79528 and 0.5711692\n",
            "Val acc and loss are 0.7892 and 0.5771118\n",
            "Processing Epoch 4947\n",
            "Training acc and loss are 0.7953 and 0.57114154\n",
            "Val acc and loss are 0.7893 and 0.5770847\n",
            "Processing Epoch 4948\n",
            "Training acc and loss are 0.7953 and 0.57111746\n",
            "Val acc and loss are 0.7893 and 0.57706165\n",
            "Processing Epoch 4949\n",
            "Training acc and loss are 0.79532 and 0.5710905\n",
            "Val acc and loss are 0.7893 and 0.57703495\n",
            "Processing Epoch 4950\n",
            "Training acc and loss are 0.79534 and 0.5710627\n",
            "Val acc and loss are 0.7893 and 0.5770082\n",
            "Processing Epoch 4951\n",
            "Training acc and loss are 0.79534 and 0.5710352\n",
            "Val acc and loss are 0.7893 and 0.57698166\n",
            "Processing Epoch 4952\n",
            "Training acc and loss are 0.79532 and 0.57101256\n",
            "Val acc and loss are 0.7893 and 0.5769605\n",
            "Processing Epoch 4953\n",
            "Training acc and loss are 0.79532 and 0.57098734\n",
            "Val acc and loss are 0.7894 and 0.5769364\n",
            "Processing Epoch 4954\n",
            "Training acc and loss are 0.79534 and 0.5709648\n",
            "Val acc and loss are 0.7894 and 0.57691425\n",
            "Processing Epoch 4955\n",
            "Training acc and loss are 0.79534 and 0.5709394\n",
            "Val acc and loss are 0.7894 and 0.5768894\n",
            "Processing Epoch 4956\n",
            "Training acc and loss are 0.79534 and 0.57091284\n",
            "Val acc and loss are 0.7894 and 0.5768636\n",
            "Processing Epoch 4957\n",
            "Training acc and loss are 0.79534 and 0.5708853\n",
            "Val acc and loss are 0.7893 and 0.5768361\n",
            "Processing Epoch 4958\n",
            "Training acc and loss are 0.79534 and 0.5708579\n",
            "Val acc and loss are 0.7893 and 0.5768094\n",
            "Processing Epoch 4959\n",
            "Training acc and loss are 0.79536 and 0.5708339\n",
            "Val acc and loss are 0.7893 and 0.5767864\n",
            "Processing Epoch 4960\n",
            "Training acc and loss are 0.79536 and 0.5708071\n",
            "Val acc and loss are 0.7893 and 0.5767602\n",
            "Processing Epoch 4961\n",
            "Training acc and loss are 0.79538 and 0.57078135\n",
            "Val acc and loss are 0.7893 and 0.5767354\n",
            "Processing Epoch 4962\n",
            "Training acc and loss are 0.79538 and 0.5707561\n",
            "Val acc and loss are 0.7893 and 0.5767111\n",
            "Processing Epoch 4963\n",
            "Training acc and loss are 0.7954 and 0.57073283\n",
            "Val acc and loss are 0.7894 and 0.57668936\n",
            "Processing Epoch 4964\n",
            "Training acc and loss are 0.7954 and 0.5707061\n",
            "Val acc and loss are 0.7894 and 0.57666355\n",
            "Processing Epoch 4965\n",
            "Training acc and loss are 0.79542 and 0.5706808\n",
            "Val acc and loss are 0.7894 and 0.57663876\n",
            "Processing Epoch 4966\n",
            "Training acc and loss are 0.79542 and 0.5706551\n",
            "Val acc and loss are 0.7894 and 0.57661366\n",
            "Processing Epoch 4967\n",
            "Training acc and loss are 0.7954 and 0.57063174\n",
            "Val acc and loss are 0.7894 and 0.57659125\n",
            "Processing Epoch 4968\n",
            "Training acc and loss are 0.7954 and 0.570607\n",
            "Val acc and loss are 0.7894 and 0.5765676\n",
            "Processing Epoch 4969\n",
            "Training acc and loss are 0.79542 and 0.57058185\n",
            "Val acc and loss are 0.7895 and 0.57654315\n",
            "Processing Epoch 4970\n",
            "Training acc and loss are 0.7954 and 0.57055485\n",
            "Val acc and loss are 0.7895 and 0.5765165\n",
            "Processing Epoch 4971\n",
            "Training acc and loss are 0.7954 and 0.57052916\n",
            "Val acc and loss are 0.7895 and 0.5764919\n",
            "Processing Epoch 4972\n",
            "Training acc and loss are 0.79544 and 0.5705024\n",
            "Val acc and loss are 0.7894 and 0.57646656\n",
            "Processing Epoch 4973\n",
            "Training acc and loss are 0.79544 and 0.5704769\n",
            "Val acc and loss are 0.7894 and 0.57644176\n",
            "Processing Epoch 4974\n",
            "Training acc and loss are 0.79544 and 0.57044923\n",
            "Val acc and loss are 0.7894 and 0.5764145\n",
            "Processing Epoch 4975\n",
            "Training acc and loss are 0.79546 and 0.5704231\n",
            "Val acc and loss are 0.7894 and 0.5763897\n",
            "Processing Epoch 4976\n",
            "Training acc and loss are 0.79544 and 0.5703963\n",
            "Val acc and loss are 0.7894 and 0.5763637\n",
            "Processing Epoch 4977\n",
            "Training acc and loss are 0.79548 and 0.5703734\n",
            "Val acc and loss are 0.7894 and 0.57634187\n",
            "Processing Epoch 4978\n",
            "Training acc and loss are 0.7955 and 0.57034975\n",
            "Val acc and loss are 0.7894 and 0.5763189\n",
            "Processing Epoch 4979\n",
            "Training acc and loss are 0.79552 and 0.5703268\n",
            "Val acc and loss are 0.7894 and 0.5762973\n",
            "Processing Epoch 4980\n",
            "Training acc and loss are 0.79554 and 0.5703024\n",
            "Val acc and loss are 0.7894 and 0.5762742\n",
            "Processing Epoch 4981\n",
            "Training acc and loss are 0.79554 and 0.57027704\n",
            "Val acc and loss are 0.7895 and 0.5762496\n",
            "Processing Epoch 4982\n",
            "Training acc and loss are 0.79556 and 0.5702549\n",
            "Val acc and loss are 0.7895 and 0.57622814\n",
            "Processing Epoch 4983\n",
            "Training acc and loss are 0.79552 and 0.57022893\n",
            "Val acc and loss are 0.7895 and 0.5762032\n",
            "Processing Epoch 4984\n",
            "Training acc and loss are 0.79554 and 0.5702008\n",
            "Val acc and loss are 0.7895 and 0.57617635\n",
            "Processing Epoch 4985\n",
            "Training acc and loss are 0.79554 and 0.57017624\n",
            "Val acc and loss are 0.7895 and 0.57615304\n",
            "Processing Epoch 4986\n",
            "Training acc and loss are 0.79556 and 0.5701492\n",
            "Val acc and loss are 0.7895 and 0.57612664\n",
            "Processing Epoch 4987\n",
            "Training acc and loss are 0.79556 and 0.57012385\n",
            "Val acc and loss are 0.7895 and 0.5761021\n",
            "Processing Epoch 4988\n",
            "Training acc and loss are 0.79558 and 0.57009906\n",
            "Val acc and loss are 0.7895 and 0.576078\n",
            "Processing Epoch 4989\n",
            "Training acc and loss are 0.7956 and 0.57007474\n",
            "Val acc and loss are 0.7895 and 0.57605547\n",
            "Processing Epoch 4990\n",
            "Training acc and loss are 0.79558 and 0.5700497\n",
            "Val acc and loss are 0.7895 and 0.5760305\n",
            "Processing Epoch 4991\n",
            "Training acc and loss are 0.7956 and 0.5700229\n",
            "Val acc and loss are 0.7895 and 0.5760048\n",
            "Processing Epoch 4992\n",
            "Training acc and loss are 0.7956 and 0.56999564\n",
            "Val acc and loss are 0.7895 and 0.57597864\n",
            "Processing Epoch 4993\n",
            "Training acc and loss are 0.7956 and 0.5699678\n",
            "Val acc and loss are 0.7895 and 0.5759516\n",
            "Processing Epoch 4994\n",
            "Training acc and loss are 0.79558 and 0.56994414\n",
            "Val acc and loss are 0.7895 and 0.57592905\n",
            "Processing Epoch 4995\n",
            "Training acc and loss are 0.79558 and 0.56991804\n",
            "Val acc and loss are 0.7895 and 0.57590383\n",
            "Processing Epoch 4996\n",
            "Training acc and loss are 0.79558 and 0.5698947\n",
            "Val acc and loss are 0.7895 and 0.57588136\n",
            "Processing Epoch 4997\n",
            "Training acc and loss are 0.79558 and 0.56986904\n",
            "Val acc and loss are 0.7895 and 0.5758563\n",
            "Processing Epoch 4998\n",
            "Training acc and loss are 0.79556 and 0.5698433\n",
            "Val acc and loss are 0.7895 and 0.57583123\n",
            "Processing Epoch 4999\n",
            "Training acc and loss are 0.79556 and 0.56982076\n",
            "Val acc and loss are 0.7895 and 0.5758096\n",
            "Processing Epoch 5000\n",
            "Training acc and loss are 0.79556 and 0.5697943\n",
            "Val acc and loss are 0.7895 and 0.5757839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyZ6mo4YUSrg",
        "outputId": "6721bd7b-053d-4f38-b95e-22c59cf63bb7"
      },
      "source": [
        "print(f\"Highest validation accuracy obtained is {np.max(val_acc_arr)} at epoch {np.argmax(val_acc_arr)+1} with a corresponding training accuracy of {train_acc_arr[np.argmax(val_acc_arr)]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest validation accuracy obtained is 0.7895 at epoch 4969 with a corresponding training accuracy of 0.79542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u1b22gcowwz"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "zQo9s1SFow3J",
        "outputId": "ee86ec84-f15e-466c-e2cb-c1a8b6f67913"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.7956 achieved at epoch: 4988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k2aPxmpow93",
        "outputId": "cc9334e5-9df9-4cea-ef25-cba7e787ec71"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3561,   44,  120,  468,   30,   44,  583,    6,  117,    4],\n",
              "       [  40, 4537,   59,  248,   28,    5,   85,    0,    4,    6],\n",
              "       [ 116,   20, 2941,   69,  883,   74,  780,    0,  103,    6],\n",
              "       [ 324,  196,   88, 3828,  211,   32,  243,    5,   46,    6],\n",
              "       [  20,   42,  800,  313, 2959,   37,  690,    1,   83,    5],\n",
              "       [  19,    5,   18,   16,    7, 3644,   36,  806,  121,  332],\n",
              "       [ 883,   35,  841,  264,  876,   78, 1842,    5,  197,    9],\n",
              "       [   1,    5,    0,    3,    3,  589,    5, 3940,   58,  441],\n",
              "       [  64,    7,   69,   57,   63,  193,  138,   84, 4307,   50],\n",
              "       [   6,    2,    7,    9,    4,  288,    4,  359,   51, 4249]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "DCfZWFHYoxEW",
        "outputId": "d75a396f-c687-44af-8ebe-06eedcac2c2a"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.7895 achieved at epoch: 4968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7yMqOQ2oxKW",
        "outputId": "bdee5767-e870-4fc1-9eea-a2bef2325123"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[843,   3,  11,  84,   2,   6,  53,   0,  19,   2],\n",
              "       [  1, 921,  11,  45,   0,   0,   8,   0,   2,   0],\n",
              "       [ 22,   2, 669,   4, 189,   4,  94,   0,  24,   0],\n",
              "       [ 53,   7,   1, 887,  25,   0,  42,   0,   5,   1],\n",
              "       [  1,   3, 102,  55, 773,   1, 106,   0,   9,   0],\n",
              "       [  0,   0,   0,   0,   0, 768,   2, 170,   7,  49],\n",
              "       [216,   3, 138,  47, 158,   4, 366,   1,  37,   0],\n",
              "       [  0,   0,   0,   0,   0,  61,   0, 816,   1,  77],\n",
              "       [ 12,   0,   3,   7,   1,  17,  18,   8, 896,   6],\n",
              "       [  0,   0,   0,   0,   0,  20,   1,  44,   0, 956]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnVHO5d4oxQO",
        "outputId": "034dbb00-2dda-4959-ba33-28f30a5e12e4"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5980206\n",
            "0.786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZr2oo23oxWG",
        "outputId": "fe95d310-e57b-4d18-9068-ff07c2b223a3"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[805,   3,  14,  82,   4,   8,  61,   0,  23,   0],\n",
              "       [  0, 931,  14,  43,   5,   0,   5,   0,   2,   0],\n",
              "       [ 22,   1, 643,   9, 195,   5, 103,   0,  22,   0],\n",
              "       [ 47,  11,   4, 852,  22,   1,  56,   0,   7,   0],\n",
              "       [  0,   4, 106,  47, 743,   1,  87,   0,  12,   0],\n",
              "       [  2,   0,   0,   1,   0, 785,   2, 151,   4,  55],\n",
              "       [222,   2, 139,  56, 153,   8, 380,   1,  39,   0],\n",
              "       [  0,   0,   0,   0,   0,  48,   0, 875,   1,  76],\n",
              "       [ 12,   1,   6,   5,   3,  18,  14,  12, 926,   3],\n",
              "       [  1,   0,   0,   0,   0,  25,   0,  53,   1, 920]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4TYvvtMr1qg"
      },
      "source": [
        "# **Test 7** Using Grid Search to find the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUAK5gIJsdZm"
      },
      "source": [
        "learning_rate_values = [0.001, 0.055, 0.01]\n",
        "hidden_layers = [32, 128, 300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qt8CQOer_E4"
      },
      "source": [
        "nData, dim = x_train.shape \n",
        "num_labels = 10\n",
        "layers = 3\n",
        "batch_size = nData\n",
        "reg_coeff = 2e-06\n",
        "drop_prob = 0.4\n",
        "min_loss = float('inf')\n",
        "for hidden1 in hidden_layers:\n",
        "  for hidden2 in hidden_layers:\n",
        "    for learn_rate in learning_rate_values:\n",
        "      print('Hidden 1 = '+str(hidden1))\n",
        "      print('Hidden 2 = '+str(hidden2))\n",
        "      print('Learning rate = '+str(learn_rate))\n",
        "      nodes_per_layer = [dim, hidden1, hidden2, num_labels]\n",
        "      # Reset everytime we build a new model.\n",
        "      tf.reset_default_graph()\n",
        "      sess = tf.Session()\n",
        "      MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"tanh\", optimizer_name = 'Adam', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "      # Initialize all variables in the constructed graph (resulting from model construction)\n",
        "      init = tf.initialize_all_variables()\n",
        "      sess.run(init)\n",
        "      train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)\n",
        "      if min_loss > min(val_loss_arr):\n",
        "        min_loss = min(val_loss_arr)\n",
        "        best_hidden_1 = hidden1\n",
        "        best_hidden_2 = hidden2\n",
        "        best_learning_rate = learn_rate\n",
        "        best_train_loss_arr = train_loss_arr\n",
        "        best_val_loss_arr = val_loss_arr\n",
        "        best_train_acc_arr = train_acc_arr\n",
        "        best_val_acc_arr = val_acc_arr\n",
        "        print('Best hidden node 1: ', best_hidden_1)\n",
        "        print('Best hidden node 2: ', best_hidden_2)\n",
        "        print('Best learning rate: ', best_learning_rate)\n",
        "\n",
        "print('Best hidden nodes:', best_hidden_1, ',', best_hidden_2, '\\n', 'Best learning rate:', best_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99hd1dibJZ1"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 300, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"tanh\", optimizer_name = 'Adam', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxAJx8VffFQW",
        "outputId": "0ee46936-1d9d-498a-b851-10bda6c20eb4"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.51278 and 1.4499332\n",
            "Val acc and loss are 0.506 and 1.4571884\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.63194 and 1.1051925\n",
            "Val acc and loss are 0.6283 and 1.1048174\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.66522 and 0.9506824\n",
            "Val acc and loss are 0.6655 and 0.94691604\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.69378 and 0.8478272\n",
            "Val acc and loss are 0.6961 and 0.8430835\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.72014 and 0.77307737\n",
            "Val acc and loss are 0.7219 and 0.76884717\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.74122 and 0.7212122\n",
            "Val acc and loss are 0.7406 and 0.71822315\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.75636 and 0.68511313\n",
            "Val acc and loss are 0.7531 and 0.6836449\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.76518 and 0.65824115\n",
            "Val acc and loss are 0.7622 and 0.65840954\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.77268 and 0.63702446\n",
            "Val acc and loss are 0.7682 and 0.6388878\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.77922 and 0.6197375\n",
            "Val acc and loss are 0.7748 and 0.6231624\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.7839 and 0.60505104\n",
            "Val acc and loss are 0.7804 and 0.60975003\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.78916 and 0.59156185\n",
            "Val acc and loss are 0.7856 and 0.5970828\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.79494 and 0.57879466\n",
            "Val acc and loss are 0.7902 and 0.5847159\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.7998 and 0.5668037\n",
            "Val acc and loss are 0.7946 and 0.5727967\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.80398 and 0.5556767\n",
            "Val acc and loss are 0.7986 and 0.56158936\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.80758 and 0.5452597\n",
            "Val acc and loss are 0.8028 and 0.55104864\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.81146 and 0.53530073\n",
            "Val acc and loss are 0.807 and 0.5410898\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.81536 and 0.52599186\n",
            "Val acc and loss are 0.811 and 0.5319558\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.81752 and 0.5175222\n",
            "Val acc and loss are 0.813 and 0.5238247\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.82052 and 0.50996757\n",
            "Val acc and loss are 0.8166 and 0.51672554\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.82306 and 0.5031514\n",
            "Val acc and loss are 0.8187 and 0.5103964\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.82512 and 0.49669522\n",
            "Val acc and loss are 0.82 and 0.5043919\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.82736 and 0.49027124\n",
            "Val acc and loss are 0.8211 and 0.49842784\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.829 and 0.48395628\n",
            "Val acc and loss are 0.8223 and 0.49255118\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.83144 and 0.4778091\n",
            "Val acc and loss are 0.8236 and 0.4868093\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.8337 and 0.4720132\n",
            "Val acc and loss are 0.825 and 0.48141623\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.83538 and 0.46674475\n",
            "Val acc and loss are 0.8271 and 0.4766229\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.83708 and 0.46194786\n",
            "Val acc and loss are 0.828 and 0.4723744\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.83868 and 0.45763844\n",
            "Val acc and loss are 0.8297 and 0.46868193\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.8394 and 0.4537601\n",
            "Val acc and loss are 0.8316 and 0.46550533\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.84114 and 0.45013595\n",
            "Val acc and loss are 0.8331 and 0.4625917\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.84242 and 0.44678414\n",
            "Val acc and loss are 0.8337 and 0.45993602\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.8438 and 0.44360983\n",
            "Val acc and loss are 0.8343 and 0.45750988\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.84508 and 0.44052678\n",
            "Val acc and loss are 0.8344 and 0.45519608\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.84646 and 0.43756196\n",
            "Val acc and loss are 0.8352 and 0.45300528\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.84734 and 0.4346364\n",
            "Val acc and loss are 0.8356 and 0.45085028\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.848 and 0.43168786\n",
            "Val acc and loss are 0.8368 and 0.44862553\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.84914 and 0.42866874\n",
            "Val acc and loss are 0.838 and 0.44628656\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.85028 and 0.42566296\n",
            "Val acc and loss are 0.8389 and 0.44389322\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.85114 and 0.42261085\n",
            "Val acc and loss are 0.8407 and 0.44138107\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.85232 and 0.41965863\n",
            "Val acc and loss are 0.8421 and 0.43886465\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.85304 and 0.41670907\n",
            "Val acc and loss are 0.8432 and 0.43631557\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.85382 and 0.41380703\n",
            "Val acc and loss are 0.8434 and 0.43381065\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.85494 and 0.41085866\n",
            "Val acc and loss are 0.844 and 0.4312742\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.85616 and 0.40796492\n",
            "Val acc and loss are 0.844 and 0.42883807\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.85694 and 0.40515286\n",
            "Val acc and loss are 0.8445 and 0.42654893\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.85846 and 0.40256283\n",
            "Val acc and loss are 0.8443 and 0.4244992\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.8591 and 0.4001153\n",
            "Val acc and loss are 0.8452 and 0.42259535\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.85976 and 0.397759\n",
            "Val acc and loss are 0.8464 and 0.4207734\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.86042 and 0.3954535\n",
            "Val acc and loss are 0.8468 and 0.4189838\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.86076 and 0.39335757\n",
            "Val acc and loss are 0.8475 and 0.41737065\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.86106 and 0.3914449\n",
            "Val acc and loss are 0.848 and 0.41593862\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.86164 and 0.38975668\n",
            "Val acc and loss are 0.8487 and 0.41474628\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.86242 and 0.38806227\n",
            "Val acc and loss are 0.8495 and 0.41360244\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.86342 and 0.3863608\n",
            "Val acc and loss are 0.8492 and 0.4124824\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.8638 and 0.3844902\n",
            "Val acc and loss are 0.8502 and 0.41114572\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.86444 and 0.38246125\n",
            "Val acc and loss are 0.8515 and 0.4095606\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.86532 and 0.3804713\n",
            "Val acc and loss are 0.8522 and 0.4079596\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.8661 and 0.3787223\n",
            "Val acc and loss are 0.8531 and 0.40658602\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.86658 and 0.3771689\n",
            "Val acc and loss are 0.8538 and 0.40542\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.8671 and 0.37575406\n",
            "Val acc and loss are 0.8538 and 0.4044077\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.86756 and 0.37434137\n",
            "Val acc and loss are 0.8543 and 0.40342152\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.86802 and 0.372854\n",
            "Val acc and loss are 0.8547 and 0.40238506\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.8688 and 0.37137532\n",
            "Val acc and loss are 0.855 and 0.40132555\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.86942 and 0.36985788\n",
            "Val acc and loss are 0.8555 and 0.4001822\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.86994 and 0.3682768\n",
            "Val acc and loss are 0.8562 and 0.39896652\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.87056 and 0.36679086\n",
            "Val acc and loss are 0.8567 and 0.39784247\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.87086 and 0.36535478\n",
            "Val acc and loss are 0.8575 and 0.3968392\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.8711 and 0.36392063\n",
            "Val acc and loss are 0.8576 and 0.39583194\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.87156 and 0.36243337\n",
            "Val acc and loss are 0.858 and 0.39472198\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.87186 and 0.36088163\n",
            "Val acc and loss are 0.8593 and 0.3935279\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.87272 and 0.35939798\n",
            "Val acc and loss are 0.8592 and 0.39237544\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.87314 and 0.35802063\n",
            "Val acc and loss are 0.8599 and 0.391341\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.87384 and 0.35668945\n",
            "Val acc and loss are 0.86 and 0.3904091\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.87438 and 0.35538864\n",
            "Val acc and loss are 0.8609 and 0.38946095\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.87468 and 0.35402867\n",
            "Val acc and loss are 0.8615 and 0.38840458\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.87508 and 0.35281196\n",
            "Val acc and loss are 0.8619 and 0.38752687\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.87562 and 0.3517046\n",
            "Val acc and loss are 0.8626 and 0.38674712\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.87594 and 0.35066262\n",
            "Val acc and loss are 0.863 and 0.38596195\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.87612 and 0.34969872\n",
            "Val acc and loss are 0.8632 and 0.38533905\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.87646 and 0.34880504\n",
            "Val acc and loss are 0.8645 and 0.38477626\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.87664 and 0.34773335\n",
            "Val acc and loss are 0.8645 and 0.3839351\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.87706 and 0.3465569\n",
            "Val acc and loss are 0.8643 and 0.3829175\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.87738 and 0.34541085\n",
            "Val acc and loss are 0.8645 and 0.38193533\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.87824 and 0.3441904\n",
            "Val acc and loss are 0.8652 and 0.38092\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.8784 and 0.34306613\n",
            "Val acc and loss are 0.865 and 0.38011476\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.8788 and 0.34204194\n",
            "Val acc and loss are 0.8654 and 0.37949476\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.87908 and 0.34110156\n",
            "Val acc and loss are 0.8669 and 0.37897044\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.87922 and 0.3402141\n",
            "Val acc and loss are 0.867 and 0.37844217\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.8798 and 0.3391651\n",
            "Val acc and loss are 0.868 and 0.3776673\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.88012 and 0.33810696\n",
            "Val acc and loss are 0.8679 and 0.376769\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.88032 and 0.3371549\n",
            "Val acc and loss are 0.8678 and 0.376044\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.88048 and 0.3362657\n",
            "Val acc and loss are 0.8681 and 0.37556252\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.8811 and 0.3354975\n",
            "Val acc and loss are 0.8682 and 0.37528107\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.88116 and 0.33477077\n",
            "Val acc and loss are 0.8685 and 0.37499845\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.88148 and 0.33382794\n",
            "Val acc and loss are 0.8685 and 0.37441087\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.8821 and 0.33261096\n",
            "Val acc and loss are 0.8684 and 0.37343112\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.88276 and 0.33148298\n",
            "Val acc and loss are 0.8681 and 0.37249058\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.8831 and 0.330575\n",
            "Val acc and loss are 0.8681 and 0.37187496\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.88328 and 0.32977718\n",
            "Val acc and loss are 0.8682 and 0.3715405\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.88354 and 0.32898492\n",
            "Val acc and loss are 0.8685 and 0.3711986\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.88344 and 0.32821062\n",
            "Val acc and loss are 0.8689 and 0.37083304\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.88396 and 0.32719222\n",
            "Val acc and loss are 0.8692 and 0.3700749\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.88424 and 0.32627243\n",
            "Val acc and loss are 0.8693 and 0.36938348\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.88478 and 0.32543752\n",
            "Val acc and loss are 0.8698 and 0.36885318\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.88496 and 0.3247006\n",
            "Val acc and loss are 0.8693 and 0.36858022\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.88484 and 0.32393768\n",
            "Val acc and loss are 0.8701 and 0.36826906\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.88494 and 0.32306308\n",
            "Val acc and loss are 0.8699 and 0.36772007\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.88546 and 0.3221374\n",
            "Val acc and loss are 0.8702 and 0.36703882\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.88578 and 0.32130405\n",
            "Val acc and loss are 0.8703 and 0.36647615\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.88594 and 0.32054266\n",
            "Val acc and loss are 0.8707 and 0.3660143\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.8859 and 0.31974983\n",
            "Val acc and loss are 0.8706 and 0.36560294\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.88642 and 0.31879437\n",
            "Val acc and loss are 0.8705 and 0.3649631\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.887 and 0.3177504\n",
            "Val acc and loss are 0.8706 and 0.36415392\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.88726 and 0.31687406\n",
            "Val acc and loss are 0.8702 and 0.3635444\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.88752 and 0.31617564\n",
            "Val acc and loss are 0.8702 and 0.36314926\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.88788 and 0.3155658\n",
            "Val acc and loss are 0.8713 and 0.36288393\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.88806 and 0.31490836\n",
            "Val acc and loss are 0.8715 and 0.36249542\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.8883 and 0.31414273\n",
            "Val acc and loss are 0.8713 and 0.36195683\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.88838 and 0.31343704\n",
            "Val acc and loss are 0.872 and 0.3615449\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.8887 and 0.31271312\n",
            "Val acc and loss are 0.8717 and 0.36118588\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.88884 and 0.31211567\n",
            "Val acc and loss are 0.872 and 0.3610009\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.889 and 0.3115593\n",
            "Val acc and loss are 0.872 and 0.36087805\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.8892 and 0.31089723\n",
            "Val acc and loss are 0.8718 and 0.36056316\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.88958 and 0.31025368\n",
            "Val acc and loss are 0.8712 and 0.36019877\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.88974 and 0.30951488\n",
            "Val acc and loss are 0.8718 and 0.3597788\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.8897 and 0.30855614\n",
            "Val acc and loss are 0.8722 and 0.3591222\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.89012 and 0.30771175\n",
            "Val acc and loss are 0.8732 and 0.35860038\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.89052 and 0.30697995\n",
            "Val acc and loss are 0.8738 and 0.35814205\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.891 and 0.30608\n",
            "Val acc and loss are 0.874 and 0.3574581\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.8912 and 0.3053494\n",
            "Val acc and loss are 0.8732 and 0.35699192\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.89138 and 0.3047701\n",
            "Val acc and loss are 0.8735 and 0.35676527\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.89146 and 0.30405104\n",
            "Val acc and loss are 0.8736 and 0.3563734\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.89156 and 0.30322474\n",
            "Val acc and loss are 0.8741 and 0.35589838\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.8917 and 0.3025246\n",
            "Val acc and loss are 0.8744 and 0.3555785\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.89222 and 0.3017432\n",
            "Val acc and loss are 0.8738 and 0.35515153\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.8925 and 0.3011111\n",
            "Val acc and loss are 0.8733 and 0.3548669\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.89254 and 0.30064398\n",
            "Val acc and loss are 0.8732 and 0.35471216\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.89274 and 0.30018708\n",
            "Val acc and loss are 0.8734 and 0.35456043\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.89288 and 0.29952183\n",
            "Val acc and loss are 0.8735 and 0.3542443\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.8928 and 0.29880714\n",
            "Val acc and loss are 0.8731 and 0.35384533\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.89328 and 0.2980505\n",
            "Val acc and loss are 0.8732 and 0.3533572\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.89356 and 0.29727617\n",
            "Val acc and loss are 0.8742 and 0.3528457\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.89406 and 0.296653\n",
            "Val acc and loss are 0.8742 and 0.35250887\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.89384 and 0.29620755\n",
            "Val acc and loss are 0.8736 and 0.3524807\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.8942 and 0.29554632\n",
            "Val acc and loss are 0.8739 and 0.35227945\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.89448 and 0.29494494\n",
            "Val acc and loss are 0.874 and 0.35207534\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.8948 and 0.29444036\n",
            "Val acc and loss are 0.8743 and 0.3518761\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.8949 and 0.29364097\n",
            "Val acc and loss are 0.8752 and 0.35124236\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.89544 and 0.2929995\n",
            "Val acc and loss are 0.8748 and 0.3508006\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.89546 and 0.29263067\n",
            "Val acc and loss are 0.8747 and 0.35070893\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.89574 and 0.2921862\n",
            "Val acc and loss are 0.8752 and 0.35069743\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.89588 and 0.29155183\n",
            "Val acc and loss are 0.8756 and 0.3504521\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.89584 and 0.29082245\n",
            "Val acc and loss are 0.8743 and 0.34997755\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.8964 and 0.28988057\n",
            "Val acc and loss are 0.8752 and 0.3491236\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.89682 and 0.2891628\n",
            "Val acc and loss are 0.8752 and 0.34861544\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.89688 and 0.28875116\n",
            "Val acc and loss are 0.8754 and 0.3486675\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.89704 and 0.28843558\n",
            "Val acc and loss are 0.8765 and 0.34888\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.8972 and 0.28780958\n",
            "Val acc and loss are 0.8769 and 0.34855375\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.89768 and 0.28699192\n",
            "Val acc and loss are 0.8763 and 0.34793955\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.89826 and 0.28608745\n",
            "Val acc and loss are 0.8755 and 0.34717542\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.89832 and 0.28536695\n",
            "Val acc and loss are 0.8755 and 0.34680796\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.89796 and 0.28504124\n",
            "Val acc and loss are 0.8765 and 0.34720537\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.89818 and 0.28494188\n",
            "Val acc and loss are 0.8763 and 0.34770197\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.89854 and 0.28418294\n",
            "Val acc and loss are 0.8766 and 0.34718284\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.89876 and 0.2832744\n",
            "Val acc and loss are 0.8766 and 0.34623453\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.89914 and 0.28271845\n",
            "Val acc and loss are 0.8765 and 0.3458236\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.89946 and 0.28237545\n",
            "Val acc and loss are 0.8767 and 0.34586918\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.89932 and 0.28228098\n",
            "Val acc and loss are 0.8771 and 0.34629694\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.89934 and 0.28171322\n",
            "Val acc and loss are 0.8773 and 0.34609386\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.89956 and 0.28068554\n",
            "Val acc and loss are 0.8765 and 0.34531343\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.89988 and 0.27987275\n",
            "Val acc and loss are 0.8773 and 0.34477076\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.9004 and 0.27916005\n",
            "Val acc and loss are 0.8775 and 0.34434155\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.9006 and 0.27866518\n",
            "Val acc and loss are 0.8774 and 0.34424394\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.90088 and 0.27834114\n",
            "Val acc and loss are 0.8772 and 0.34430358\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.90104 and 0.27776295\n",
            "Val acc and loss are 0.8775 and 0.34395707\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.90138 and 0.27703324\n",
            "Val acc and loss are 0.8777 and 0.34339228\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.90128 and 0.2763911\n",
            "Val acc and loss are 0.8777 and 0.34294447\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.90152 and 0.27588826\n",
            "Val acc and loss are 0.878 and 0.34271058\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.90158 and 0.27545816\n",
            "Val acc and loss are 0.8783 and 0.34261888\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.90188 and 0.275128\n",
            "Val acc and loss are 0.8777 and 0.3426724\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.90204 and 0.274713\n",
            "Val acc and loss are 0.8775 and 0.34256053\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.9021 and 0.27409717\n",
            "Val acc and loss are 0.8779 and 0.34223795\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.90244 and 0.2735579\n",
            "Val acc and loss are 0.8781 and 0.34202662\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.90254 and 0.27304652\n",
            "Val acc and loss are 0.8792 and 0.34177807\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.90286 and 0.27251345\n",
            "Val acc and loss are 0.8794 and 0.3414633\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.90316 and 0.27191266\n",
            "Val acc and loss are 0.8788 and 0.34121886\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.90356 and 0.27137905\n",
            "Val acc and loss are 0.8782 and 0.34107333\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.9035 and 0.27099293\n",
            "Val acc and loss are 0.8789 and 0.3411548\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.90364 and 0.2702773\n",
            "Val acc and loss are 0.8786 and 0.34071118\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.90396 and 0.2695421\n",
            "Val acc and loss are 0.8787 and 0.34024474\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.9039 and 0.26903394\n",
            "Val acc and loss are 0.8791 and 0.34003842\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.904 and 0.26878253\n",
            "Val acc and loss are 0.8797 and 0.3402376\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.9043 and 0.268383\n",
            "Val acc and loss are 0.8795 and 0.340243\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.9045 and 0.26791614\n",
            "Val acc and loss are 0.88 and 0.34008\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.90466 and 0.26741433\n",
            "Val acc and loss are 0.8803 and 0.33993477\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.9047 and 0.26681718\n",
            "Val acc and loss are 0.8797 and 0.33981073\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.90518 and 0.26625907\n",
            "Val acc and loss are 0.8793 and 0.33954903\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.90532 and 0.26566675\n",
            "Val acc and loss are 0.8799 and 0.33918846\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.905 and 0.26514637\n",
            "Val acc and loss are 0.8797 and 0.3388728\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.90528 and 0.26477605\n",
            "Val acc and loss are 0.8798 and 0.3387859\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.90522 and 0.2644219\n",
            "Val acc and loss are 0.8799 and 0.33873922\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.90538 and 0.26409402\n",
            "Val acc and loss are 0.88 and 0.33877164\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.90566 and 0.26357427\n",
            "Val acc and loss are 0.8795 and 0.3384706\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.90606 and 0.2631215\n",
            "Val acc and loss are 0.8799 and 0.33840898\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.90628 and 0.2626852\n",
            "Val acc and loss are 0.8801 and 0.33839598\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.90654 and 0.26201016\n",
            "Val acc and loss are 0.8801 and 0.338041\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.90668 and 0.261403\n",
            "Val acc and loss are 0.8795 and 0.33770102\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.90694 and 0.26096725\n",
            "Val acc and loss are 0.88 and 0.3375511\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.90706 and 0.260379\n",
            "Val acc and loss are 0.8799 and 0.33706063\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.90726 and 0.25987265\n",
            "Val acc and loss are 0.8796 and 0.33667427\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.90716 and 0.25956872\n",
            "Val acc and loss are 0.88 and 0.3367203\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.90682 and 0.25922406\n",
            "Val acc and loss are 0.8801 and 0.33677894\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.90688 and 0.25866023\n",
            "Val acc and loss are 0.8798 and 0.33652276\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.9073 and 0.2579881\n",
            "Val acc and loss are 0.8795 and 0.33603936\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.9075 and 0.2573285\n",
            "Val acc and loss are 0.8795 and 0.3355583\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.90776 and 0.25665897\n",
            "Val acc and loss are 0.8801 and 0.33521357\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.90824 and 0.2562751\n",
            "Val acc and loss are 0.8803 and 0.33530763\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.9082 and 0.25610903\n",
            "Val acc and loss are 0.8806 and 0.33566815\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.90852 and 0.2555672\n",
            "Val acc and loss are 0.8805 and 0.33537534\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.90862 and 0.25496808\n",
            "Val acc and loss are 0.8804 and 0.33488825\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.90868 and 0.25457466\n",
            "Val acc and loss are 0.8811 and 0.33467618\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.90874 and 0.2543406\n",
            "Val acc and loss are 0.8802 and 0.33489662\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.90868 and 0.2540888\n",
            "Val acc and loss are 0.8806 and 0.33499688\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.90886 and 0.25387776\n",
            "Val acc and loss are 0.8804 and 0.33518636\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.90902 and 0.25324908\n",
            "Val acc and loss are 0.8809 and 0.33474603\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.90908 and 0.25259218\n",
            "Val acc and loss are 0.8809 and 0.33421922\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.90948 and 0.25200608\n",
            "Val acc and loss are 0.881 and 0.33391666\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.90972 and 0.25157142\n",
            "Val acc and loss are 0.8813 and 0.3339325\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.90992 and 0.25110734\n",
            "Val acc and loss are 0.8806 and 0.3337147\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.91012 and 0.25065106\n",
            "Val acc and loss are 0.8809 and 0.33360267\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.91024 and 0.2500577\n",
            "Val acc and loss are 0.8809 and 0.33320317\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.91026 and 0.24950579\n",
            "Val acc and loss are 0.8805 and 0.3327453\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.9103 and 0.24918462\n",
            "Val acc and loss are 0.8806 and 0.3328013\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.91068 and 0.24899665\n",
            "Val acc and loss are 0.8813 and 0.33302745\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.91084 and 0.24841516\n",
            "Val acc and loss are 0.881 and 0.33274204\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.91098 and 0.24777898\n",
            "Val acc and loss are 0.8808 and 0.33230975\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.91148 and 0.24743904\n",
            "Val acc and loss are 0.8811 and 0.3323381\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.91164 and 0.24719849\n",
            "Val acc and loss are 0.8809 and 0.33255902\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.91166 and 0.24677075\n",
            "Val acc and loss are 0.881 and 0.33251557\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.91164 and 0.24619696\n",
            "Val acc and loss are 0.8811 and 0.33225298\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.91184 and 0.24558128\n",
            "Val acc and loss are 0.8809 and 0.33186358\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.91194 and 0.24520105\n",
            "Val acc and loss are 0.8806 and 0.33184028\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.91196 and 0.24467197\n",
            "Val acc and loss are 0.8807 and 0.33151558\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.91218 and 0.24441531\n",
            "Val acc and loss are 0.8813 and 0.33162743\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.9123 and 0.24424188\n",
            "Val acc and loss are 0.8815 and 0.33188042\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.91256 and 0.24371357\n",
            "Val acc and loss are 0.8814 and 0.3316706\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.91256 and 0.24308895\n",
            "Val acc and loss are 0.8814 and 0.3312917\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.91264 and 0.2427329\n",
            "Val acc and loss are 0.881 and 0.33135092\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.91278 and 0.2425816\n",
            "Val acc and loss are 0.882 and 0.33169523\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.91302 and 0.24243622\n",
            "Val acc and loss are 0.8825 and 0.33197877\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.913 and 0.24199258\n",
            "Val acc and loss are 0.8825 and 0.3317195\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.9132 and 0.24130133\n",
            "Val acc and loss are 0.8828 and 0.33103615\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.91362 and 0.24061131\n",
            "Val acc and loss are 0.8827 and 0.33048925\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.9137 and 0.24032715\n",
            "Val acc and loss are 0.8828 and 0.33053464\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.91374 and 0.24008884\n",
            "Val acc and loss are 0.8828 and 0.33074307\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.91418 and 0.23943833\n",
            "Val acc and loss are 0.8817 and 0.33040905\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.91426 and 0.23900454\n",
            "Val acc and loss are 0.8822 and 0.33021334\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.91416 and 0.23848899\n",
            "Val acc and loss are 0.8826 and 0.32995763\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.91422 and 0.23815152\n",
            "Val acc and loss are 0.8821 and 0.33000562\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.9145 and 0.23798287\n",
            "Val acc and loss are 0.8827 and 0.3302314\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.91476 and 0.23725373\n",
            "Val acc and loss are 0.8831 and 0.32965335\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.91488 and 0.23685768\n",
            "Val acc and loss are 0.882 and 0.3293368\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.915 and 0.2364609\n",
            "Val acc and loss are 0.8823 and 0.3291203\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.91504 and 0.23607683\n",
            "Val acc and loss are 0.8825 and 0.32906508\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.91536 and 0.23598802\n",
            "Val acc and loss are 0.8835 and 0.32954797\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.91542 and 0.2354561\n",
            "Val acc and loss are 0.8837 and 0.3292683\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.91566 and 0.23473555\n",
            "Val acc and loss are 0.8828 and 0.32866475\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.91588 and 0.23411632\n",
            "Val acc and loss are 0.8833 and 0.32808515\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.91618 and 0.23381959\n",
            "Val acc and loss are 0.8827 and 0.32819813\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.91612 and 0.23382664\n",
            "Val acc and loss are 0.8834 and 0.32880554\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.91612 and 0.23338647\n",
            "Val acc and loss are 0.8844 and 0.32864225\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.91658 and 0.23275048\n",
            "Val acc and loss are 0.8838 and 0.32816637\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.91614 and 0.23235457\n",
            "Val acc and loss are 0.8841 and 0.32806304\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.91646 and 0.23206118\n",
            "Val acc and loss are 0.8839 and 0.32821065\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.91672 and 0.23183866\n",
            "Val acc and loss are 0.8836 and 0.32848996\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.91708 and 0.23134154\n",
            "Val acc and loss are 0.8832 and 0.32826862\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.91726 and 0.23071727\n",
            "Val acc and loss are 0.8829 and 0.32779413\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.91742 and 0.23006696\n",
            "Val acc and loss are 0.8831 and 0.32727045\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.9177 and 0.22978121\n",
            "Val acc and loss are 0.8833 and 0.32749978\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.91758 and 0.22976649\n",
            "Val acc and loss are 0.8829 and 0.32821372\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.918 and 0.22944441\n",
            "Val acc and loss are 0.883 and 0.32836223\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.91804 and 0.22897549\n",
            "Val acc and loss are 0.8829 and 0.3281464\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.91824 and 0.22831453\n",
            "Val acc and loss are 0.8828 and 0.3274613\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.91818 and 0.2278407\n",
            "Val acc and loss are 0.8837 and 0.32698008\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.91858 and 0.22755861\n",
            "Val acc and loss are 0.8839 and 0.32698223\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.91854 and 0.22737297\n",
            "Val acc and loss are 0.884 and 0.32705247\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.91842 and 0.227066\n",
            "Val acc and loss are 0.8836 and 0.3270509\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.9185 and 0.22640462\n",
            "Val acc and loss are 0.8837 and 0.32650298\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.91906 and 0.22594075\n",
            "Val acc and loss are 0.8839 and 0.32635352\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.91876 and 0.22578578\n",
            "Val acc and loss are 0.8841 and 0.32664317\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.91906 and 0.22559516\n",
            "Val acc and loss are 0.8839 and 0.32700166\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.91896 and 0.2252047\n",
            "Val acc and loss are 0.8836 and 0.3270085\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.9191 and 0.22472605\n",
            "Val acc and loss are 0.8834 and 0.3268438\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.9192 and 0.2242856\n",
            "Val acc and loss are 0.8834 and 0.32679448\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.9193 and 0.22363792\n",
            "Val acc and loss are 0.8834 and 0.32630953\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.91972 and 0.22315113\n",
            "Val acc and loss are 0.8828 and 0.32617182\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.92012 and 0.22273977\n",
            "Val acc and loss are 0.884 and 0.32625028\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.92006 and 0.2226704\n",
            "Val acc and loss are 0.8847 and 0.3267208\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.91988 and 0.22235969\n",
            "Val acc and loss are 0.8845 and 0.3265438\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.92002 and 0.2217966\n",
            "Val acc and loss are 0.8851 and 0.3258322\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.92022 and 0.22131687\n",
            "Val acc and loss are 0.885 and 0.32515162\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.9203 and 0.22097045\n",
            "Val acc and loss are 0.8847 and 0.3251237\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.92072 and 0.22060412\n",
            "Val acc and loss are 0.8845 and 0.32517105\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.92102 and 0.22033918\n",
            "Val acc and loss are 0.8845 and 0.3254858\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.92082 and 0.21985789\n",
            "Val acc and loss are 0.8847 and 0.32537526\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.921 and 0.21963723\n",
            "Val acc and loss are 0.884 and 0.32557064\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.92138 and 0.21940154\n",
            "Val acc and loss are 0.8844 and 0.32562923\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.92118 and 0.21900477\n",
            "Val acc and loss are 0.8847 and 0.32539362\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.92152 and 0.21857387\n",
            "Val acc and loss are 0.8847 and 0.32492515\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.92184 and 0.21825255\n",
            "Val acc and loss are 0.8847 and 0.32496306\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.9217 and 0.21822903\n",
            "Val acc and loss are 0.8843 and 0.32541978\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.92186 and 0.21763809\n",
            "Val acc and loss are 0.884 and 0.32513556\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9216 and 0.21706691\n",
            "Val acc and loss are 0.8832 and 0.32469448\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.92234 and 0.21647723\n",
            "Val acc and loss are 0.8832 and 0.3244315\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.92234 and 0.21653132\n",
            "Val acc and loss are 0.8848 and 0.3252023\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.92242 and 0.21647713\n",
            "Val acc and loss are 0.885 and 0.32559475\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.92244 and 0.2158571\n",
            "Val acc and loss are 0.8845 and 0.32518882\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.9222 and 0.21549101\n",
            "Val acc and loss are 0.8842 and 0.3249527\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.92282 and 0.21508734\n",
            "Val acc and loss are 0.8846 and 0.3248583\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.92266 and 0.2151025\n",
            "Val acc and loss are 0.8853 and 0.32517004\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.92242 and 0.21493441\n",
            "Val acc and loss are 0.8857 and 0.32509226\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.92286 and 0.21394977\n",
            "Val acc and loss are 0.8859 and 0.32405448\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.92276 and 0.2136221\n",
            "Val acc and loss are 0.8853 and 0.32407704\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.92366 and 0.21308126\n",
            "Val acc and loss are 0.8855 and 0.32407013\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.92362 and 0.21279898\n",
            "Val acc and loss are 0.8851 and 0.3244785\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.92366 and 0.21234564\n",
            "Val acc and loss are 0.8857 and 0.32446647\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.92384 and 0.21172242\n",
            "Val acc and loss are 0.8854 and 0.32401958\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.92384 and 0.21141031\n",
            "Val acc and loss are 0.8857 and 0.32382622\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.92416 and 0.2112384\n",
            "Val acc and loss are 0.8861 and 0.32402673\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.9243 and 0.21131769\n",
            "Val acc and loss are 0.887 and 0.3244385\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.9242 and 0.21097277\n",
            "Val acc and loss are 0.8864 and 0.32433003\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.92396 and 0.21028787\n",
            "Val acc and loss are 0.8868 and 0.32394725\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.924 and 0.20995125\n",
            "Val acc and loss are 0.8859 and 0.32429898\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.92436 and 0.20952532\n",
            "Val acc and loss are 0.8858 and 0.32448477\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.92452 and 0.20918357\n",
            "Val acc and loss are 0.886 and 0.3244724\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.92466 and 0.2086075\n",
            "Val acc and loss are 0.8862 and 0.32384354\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.92486 and 0.20821266\n",
            "Val acc and loss are 0.8869 and 0.32356945\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.92498 and 0.2078571\n",
            "Val acc and loss are 0.8865 and 0.3232102\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.92492 and 0.20750144\n",
            "Val acc and loss are 0.8872 and 0.32296613\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.9252 and 0.20737144\n",
            "Val acc and loss are 0.8873 and 0.32328996\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.9251 and 0.2070807\n",
            "Val acc and loss are 0.887 and 0.32339296\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.92544 and 0.20654622\n",
            "Val acc and loss are 0.8862 and 0.3232908\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.92522 and 0.2064084\n",
            "Val acc and loss are 0.886 and 0.32388213\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.9253 and 0.20605074\n",
            "Val acc and loss are 0.8866 and 0.32380185\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.9254 and 0.20572\n",
            "Val acc and loss are 0.8871 and 0.32367927\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.9258 and 0.20548257\n",
            "Val acc and loss are 0.8865 and 0.32370403\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.92584 and 0.20486253\n",
            "Val acc and loss are 0.886 and 0.3230424\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.92576 and 0.20437832\n",
            "Val acc and loss are 0.8865 and 0.32276976\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.92606 and 0.204198\n",
            "Val acc and loss are 0.8867 and 0.32308587\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.926 and 0.20425159\n",
            "Val acc and loss are 0.8866 and 0.32383302\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.92628 and 0.20366833\n",
            "Val acc and loss are 0.8863 and 0.3234618\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.92654 and 0.2032157\n",
            "Val acc and loss are 0.8859 and 0.32291806\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.92656 and 0.20311072\n",
            "Val acc and loss are 0.8866 and 0.32305673\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.92656 and 0.20312995\n",
            "Val acc and loss are 0.8874 and 0.32359472\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.92648 and 0.20279989\n",
            "Val acc and loss are 0.8876 and 0.32333392\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.927 and 0.20230414\n",
            "Val acc and loss are 0.8866 and 0.32292822\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.92694 and 0.20162946\n",
            "Val acc and loss are 0.886 and 0.32246527\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.9273 and 0.2013223\n",
            "Val acc and loss are 0.886 and 0.3227119\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.92748 and 0.20088533\n",
            "Val acc and loss are 0.8861 and 0.32255098\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.92778 and 0.20045668\n",
            "Val acc and loss are 0.8866 and 0.32232496\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.92784 and 0.20007092\n",
            "Val acc and loss are 0.8873 and 0.32217246\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.92754 and 0.1997101\n",
            "Val acc and loss are 0.8871 and 0.32214028\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.9279 and 0.19926575\n",
            "Val acc and loss are 0.8872 and 0.32191238\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.92786 and 0.1989318\n",
            "Val acc and loss are 0.8874 and 0.3217997\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.92798 and 0.19857316\n",
            "Val acc and loss are 0.8878 and 0.3215946\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.92826 and 0.19841349\n",
            "Val acc and loss are 0.8873 and 0.3217616\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.9285 and 0.19848354\n",
            "Val acc and loss are 0.8872 and 0.3224434\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.92842 and 0.1982584\n",
            "Val acc and loss are 0.8874 and 0.32259935\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.9283 and 0.19781274\n",
            "Val acc and loss are 0.8879 and 0.3223966\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.9285 and 0.19725649\n",
            "Val acc and loss are 0.8881 and 0.32201675\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.9286 and 0.1969289\n",
            "Val acc and loss are 0.8885 and 0.3220668\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.92872 and 0.19662164\n",
            "Val acc and loss are 0.8879 and 0.32221898\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.92896 and 0.19615664\n",
            "Val acc and loss are 0.8882 and 0.32198736\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.92922 and 0.19593403\n",
            "Val acc and loss are 0.8877 and 0.3222748\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.92938 and 0.1955807\n",
            "Val acc and loss are 0.8873 and 0.32237953\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.92958 and 0.19513707\n",
            "Val acc and loss are 0.8876 and 0.32231057\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.92984 and 0.19476748\n",
            "Val acc and loss are 0.8877 and 0.32226706\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.9297 and 0.19440861\n",
            "Val acc and loss are 0.887 and 0.32232982\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.92986 and 0.19421633\n",
            "Val acc and loss are 0.8875 and 0.32257158\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.92998 and 0.19393109\n",
            "Val acc and loss are 0.888 and 0.32246882\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.93014 and 0.19354631\n",
            "Val acc and loss are 0.8876 and 0.3221707\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.93034 and 0.19316073\n",
            "Val acc and loss are 0.8881 and 0.3218358\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.93038 and 0.1928892\n",
            "Val acc and loss are 0.8875 and 0.32177323\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.93052 and 0.19283356\n",
            "Val acc and loss are 0.8877 and 0.32217574\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.93052 and 0.1926525\n",
            "Val acc and loss are 0.8882 and 0.32257277\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.93078 and 0.19201577\n",
            "Val acc and loss are 0.8888 and 0.3220353\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.93086 and 0.19158313\n",
            "Val acc and loss are 0.8889 and 0.32152975\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.93096 and 0.19124906\n",
            "Val acc and loss are 0.8877 and 0.32133755\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.93108 and 0.19131483\n",
            "Val acc and loss are 0.8879 and 0.32200518\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.93122 and 0.190992\n",
            "Val acc and loss are 0.8886 and 0.32211512\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.9311 and 0.19045211\n",
            "Val acc and loss are 0.8888 and 0.3217647\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.9315 and 0.19002175\n",
            "Val acc and loss are 0.8887 and 0.3215825\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.93158 and 0.18994775\n",
            "Val acc and loss are 0.8882 and 0.32244867\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.93124 and 0.19008152\n",
            "Val acc and loss are 0.8877 and 0.32331875\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.93164 and 0.18918014\n",
            "Val acc and loss are 0.8876 and 0.32218686\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.93174 and 0.18868417\n",
            "Val acc and loss are 0.8881 and 0.32165557\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.93164 and 0.1887633\n",
            "Val acc and loss are 0.8885 and 0.32212195\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.93212 and 0.18845035\n",
            "Val acc and loss are 0.888 and 0.32229108\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.93214 and 0.18813431\n",
            "Val acc and loss are 0.8885 and 0.3221243\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.93256 and 0.18744935\n",
            "Val acc and loss are 0.8879 and 0.32134113\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.93274 and 0.18704174\n",
            "Val acc and loss are 0.8882 and 0.3209429\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.93268 and 0.18687148\n",
            "Val acc and loss are 0.8891 and 0.32109672\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.93278 and 0.18673648\n",
            "Val acc and loss are 0.8893 and 0.32152867\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.93248 and 0.1864382\n",
            "Val acc and loss are 0.8891 and 0.32133517\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.9327 and 0.1859413\n",
            "Val acc and loss are 0.8887 and 0.32106134\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.93262 and 0.18587707\n",
            "Val acc and loss are 0.8885 and 0.32134327\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.933 and 0.18549307\n",
            "Val acc and loss are 0.8879 and 0.32123014\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.9337 and 0.18518297\n",
            "Val acc and loss are 0.8884 and 0.3211094\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.93358 and 0.18517996\n",
            "Val acc and loss are 0.8888 and 0.32142207\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.93326 and 0.18476148\n",
            "Val acc and loss are 0.8887 and 0.32120177\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.93342 and 0.1844789\n",
            "Val acc and loss are 0.889 and 0.32120064\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.93356 and 0.18375567\n",
            "Val acc and loss are 0.8889 and 0.32082134\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.93388 and 0.18308592\n",
            "Val acc and loss are 0.8891 and 0.32041574\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.9341 and 0.1829264\n",
            "Val acc and loss are 0.8886 and 0.32096353\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.9344 and 0.18281555\n",
            "Val acc and loss are 0.8891 and 0.3216353\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.93446 and 0.18255594\n",
            "Val acc and loss are 0.8897 and 0.32159567\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.9346 and 0.1818601\n",
            "Val acc and loss are 0.8896 and 0.32066074\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.93464 and 0.18166126\n",
            "Val acc and loss are 0.8888 and 0.32057816\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.93452 and 0.18188998\n",
            "Val acc and loss are 0.8888 and 0.32118887\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.93472 and 0.18160324\n",
            "Val acc and loss are 0.8891 and 0.32131156\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.9347 and 0.18132348\n",
            "Val acc and loss are 0.8903 and 0.32123902\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.93466 and 0.18096314\n",
            "Val acc and loss are 0.8902 and 0.320986\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.9351 and 0.18040523\n",
            "Val acc and loss are 0.8891 and 0.3207672\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.93536 and 0.18036687\n",
            "Val acc and loss are 0.889 and 0.32138243\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.9355 and 0.18040887\n",
            "Val acc and loss are 0.8899 and 0.32187077\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.93552 and 0.17967734\n",
            "Val acc and loss are 0.8897 and 0.3209201\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.9356 and 0.17897184\n",
            "Val acc and loss are 0.8901 and 0.32000333\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.93606 and 0.17875794\n",
            "Val acc and loss are 0.8895 and 0.32050622\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.93542 and 0.17928724\n",
            "Val acc and loss are 0.8897 and 0.32206535\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.9359 and 0.17848335\n",
            "Val acc and loss are 0.8898 and 0.3213809\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.9361 and 0.17775275\n",
            "Val acc and loss are 0.8908 and 0.32046756\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.93614 and 0.17742808\n",
            "Val acc and loss are 0.8905 and 0.32045195\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.93616 and 0.17712882\n",
            "Val acc and loss are 0.8901 and 0.32104993\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.9363 and 0.17753543\n",
            "Val acc and loss are 0.8899 and 0.32242084\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.9366 and 0.17683959\n",
            "Val acc and loss are 0.8892 and 0.321825\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.93684 and 0.17605346\n",
            "Val acc and loss are 0.8894 and 0.32071626\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.93724 and 0.17579204\n",
            "Val acc and loss are 0.8899 and 0.32086986\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.93724 and 0.1760733\n",
            "Val acc and loss are 0.89 and 0.32205427\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.93708 and 0.17622873\n",
            "Val acc and loss are 0.8895 and 0.32276195\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.93746 and 0.1753056\n",
            "Val acc and loss are 0.8897 and 0.32152072\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.93752 and 0.17484242\n",
            "Val acc and loss are 0.8899 and 0.3207188\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.93754 and 0.17446958\n",
            "Val acc and loss are 0.8896 and 0.3211626\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.93786 and 0.17472203\n",
            "Val acc and loss are 0.8896 and 0.32239276\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.93792 and 0.17446794\n",
            "Val acc and loss are 0.8898 and 0.3223602\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.93782 and 0.17363746\n",
            "Val acc and loss are 0.89 and 0.3210748\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.938 and 0.17355663\n",
            "Val acc and loss are 0.8903 and 0.3209386\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.93762 and 0.17366496\n",
            "Val acc and loss are 0.8907 and 0.32199362\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.93812 and 0.1741593\n",
            "Val acc and loss are 0.8908 and 0.32338187\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.93792 and 0.1732538\n",
            "Val acc and loss are 0.8905 and 0.32256132\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.93842 and 0.17238937\n",
            "Val acc and loss are 0.8898 and 0.32123014\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.93856 and 0.17196603\n",
            "Val acc and loss are 0.8901 and 0.32132232\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.93838 and 0.17224227\n",
            "Val acc and loss are 0.89 and 0.3223818\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.93856 and 0.17214389\n",
            "Val acc and loss are 0.8902 and 0.3223685\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.9388 and 0.17136608\n",
            "Val acc and loss are 0.8898 and 0.32127103\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.93946 and 0.1710813\n",
            "Val acc and loss are 0.8899 and 0.32098818\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.93926 and 0.17111585\n",
            "Val acc and loss are 0.8894 and 0.32164\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.93926 and 0.17083512\n",
            "Val acc and loss are 0.8902 and 0.3215682\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.93954 and 0.17017676\n",
            "Val acc and loss are 0.8903 and 0.32073325\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.93978 and 0.17003894\n",
            "Val acc and loss are 0.8896 and 0.3209514\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.93966 and 0.16969503\n",
            "Val acc and loss are 0.8899 and 0.32102403\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.93994 and 0.16919196\n",
            "Val acc and loss are 0.8901 and 0.32067716\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.94042 and 0.16888253\n",
            "Val acc and loss are 0.8897 and 0.320655\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.9405 and 0.16870834\n",
            "Val acc and loss are 0.8896 and 0.32108974\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.94036 and 0.16857928\n",
            "Val acc and loss are 0.8895 and 0.3213644\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.94034 and 0.16848965\n",
            "Val acc and loss are 0.8903 and 0.32159135\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.94048 and 0.16819197\n",
            "Val acc and loss are 0.8894 and 0.321487\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.94066 and 0.16783562\n",
            "Val acc and loss are 0.8899 and 0.32123262\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.94044 and 0.16762127\n",
            "Val acc and loss are 0.8898 and 0.32153285\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.94084 and 0.16708465\n",
            "Val acc and loss are 0.8894 and 0.32121372\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.94108 and 0.16659462\n",
            "Val acc and loss are 0.8895 and 0.32091376\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.94128 and 0.16622275\n",
            "Val acc and loss are 0.8895 and 0.32073388\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.94136 and 0.16602843\n",
            "Val acc and loss are 0.8902 and 0.32071108\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.9413 and 0.16581368\n",
            "Val acc and loss are 0.8898 and 0.32042113\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.94136 and 0.16574347\n",
            "Val acc and loss are 0.89 and 0.32036284\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.9414 and 0.16575271\n",
            "Val acc and loss are 0.8906 and 0.32051533\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.94112 and 0.16583075\n",
            "Val acc and loss are 0.8901 and 0.32129633\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.94102 and 0.16563293\n",
            "Val acc and loss are 0.8899 and 0.32178533\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.94194 and 0.16483898\n",
            "Val acc and loss are 0.89 and 0.32129043\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.94198 and 0.16437143\n",
            "Val acc and loss are 0.8899 and 0.32076487\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.94228 and 0.1642681\n",
            "Val acc and loss are 0.8891 and 0.32122782\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.9418 and 0.16438997\n",
            "Val acc and loss are 0.8902 and 0.3219489\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.94222 and 0.16379113\n",
            "Val acc and loss are 0.8901 and 0.32122594\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.94226 and 0.16309047\n",
            "Val acc and loss are 0.8903 and 0.32036978\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.94258 and 0.16296367\n",
            "Val acc and loss are 0.8907 and 0.32071587\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.94234 and 0.16294685\n",
            "Val acc and loss are 0.8908 and 0.3214107\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.94254 and 0.16286165\n",
            "Val acc and loss are 0.8906 and 0.3216634\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.94262 and 0.1624781\n",
            "Val acc and loss are 0.8911 and 0.32132438\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.94288 and 0.16224897\n",
            "Val acc and loss are 0.8901 and 0.32120225\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.94298 and 0.16199376\n",
            "Val acc and loss are 0.8899 and 0.32152593\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.94334 and 0.16169572\n",
            "Val acc and loss are 0.8895 and 0.32183856\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.94358 and 0.16154675\n",
            "Val acc and loss are 0.8902 and 0.3222692\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.94364 and 0.16131616\n",
            "Val acc and loss are 0.8905 and 0.32231876\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.94346 and 0.16118978\n",
            "Val acc and loss are 0.8905 and 0.32262897\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.94344 and 0.1607122\n",
            "Val acc and loss are 0.8907 and 0.32219425\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.9437 and 0.160515\n",
            "Val acc and loss are 0.8903 and 0.32213283\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.944 and 0.16024493\n",
            "Val acc and loss are 0.8904 and 0.32214776\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.94398 and 0.15999557\n",
            "Val acc and loss are 0.8901 and 0.32213625\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.94428 and 0.15947922\n",
            "Val acc and loss are 0.8904 and 0.3217419\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.94398 and 0.15946873\n",
            "Val acc and loss are 0.8899 and 0.32221684\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.94414 and 0.15946731\n",
            "Val acc and loss are 0.8897 and 0.32274452\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.94408 and 0.1591649\n",
            "Val acc and loss are 0.8902 and 0.322525\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.94414 and 0.1590489\n",
            "Val acc and loss are 0.8902 and 0.32246327\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.94406 and 0.15873367\n",
            "Val acc and loss are 0.8893 and 0.3222582\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.94406 and 0.15860952\n",
            "Val acc and loss are 0.8897 and 0.3224686\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.94478 and 0.1577022\n",
            "Val acc and loss are 0.8895 and 0.32156515\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.94486 and 0.15732467\n",
            "Val acc and loss are 0.8911 and 0.32130924\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.945 and 0.15699545\n",
            "Val acc and loss are 0.8914 and 0.32134786\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.94484 and 0.1566963\n",
            "Val acc and loss are 0.8909 and 0.32151976\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.94484 and 0.15677227\n",
            "Val acc and loss are 0.8903 and 0.3220471\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.94512 and 0.15665795\n",
            "Val acc and loss are 0.8904 and 0.3222336\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.9452 and 0.1564857\n",
            "Val acc and loss are 0.8907 and 0.32235444\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.94532 and 0.15608898\n",
            "Val acc and loss are 0.8907 and 0.32217026\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.94554 and 0.15607943\n",
            "Val acc and loss are 0.8903 and 0.32249263\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.94544 and 0.15586706\n",
            "Val acc and loss are 0.8898 and 0.32250622\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.9456 and 0.155809\n",
            "Val acc and loss are 0.8905 and 0.32260138\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.9456 and 0.15531988\n",
            "Val acc and loss are 0.8907 and 0.32211736\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.9456 and 0.15517484\n",
            "Val acc and loss are 0.8905 and 0.32248715\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.94598 and 0.15477578\n",
            "Val acc and loss are 0.8906 and 0.32200634\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.94666 and 0.15417336\n",
            "Val acc and loss are 0.8898 and 0.32112095\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.9466 and 0.15415096\n",
            "Val acc and loss are 0.8899 and 0.32182634\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.9464 and 0.15410471\n",
            "Val acc and loss are 0.8906 and 0.322346\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.9464 and 0.15331313\n",
            "Val acc and loss are 0.8907 and 0.3214626\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.94646 and 0.15298504\n",
            "Val acc and loss are 0.8907 and 0.3209901\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.94674 and 0.15300746\n",
            "Val acc and loss are 0.8906 and 0.32165533\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.9468 and 0.15287162\n",
            "Val acc and loss are 0.8905 and 0.322174\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.94682 and 0.15254256\n",
            "Val acc and loss are 0.8906 and 0.32221282\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.94682 and 0.15229668\n",
            "Val acc and loss are 0.8902 and 0.3217571\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.94664 and 0.1518716\n",
            "Val acc and loss are 0.8906 and 0.32124993\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.9469 and 0.15193537\n",
            "Val acc and loss are 0.8901 and 0.3218845\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.94726 and 0.15181834\n",
            "Val acc and loss are 0.8901 and 0.3224553\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.94736 and 0.15111457\n",
            "Val acc and loss are 0.89 and 0.32169667\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.94768 and 0.150921\n",
            "Val acc and loss are 0.8898 and 0.32192144\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.94776 and 0.15103392\n",
            "Val acc and loss are 0.8899 and 0.32289535\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.94764 and 0.15091705\n",
            "Val acc and loss are 0.891 and 0.32293776\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.94774 and 0.15031204\n",
            "Val acc and loss are 0.8911 and 0.32164326\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.9479 and 0.15006515\n",
            "Val acc and loss are 0.8915 and 0.32137325\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.9478 and 0.15021597\n",
            "Val acc and loss are 0.8909 and 0.32279876\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.94768 and 0.15032417\n",
            "Val acc and loss are 0.8904 and 0.32402763\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.94794 and 0.14935188\n",
            "Val acc and loss are 0.89 and 0.3229921\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.94828 and 0.1487691\n",
            "Val acc and loss are 0.8902 and 0.32201186\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.94796 and 0.14887525\n",
            "Val acc and loss are 0.8908 and 0.3227953\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.94732 and 0.14932045\n",
            "Val acc and loss are 0.8906 and 0.32372138\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.94812 and 0.14881778\n",
            "Val acc and loss are 0.8909 and 0.3229274\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.94828 and 0.1481575\n",
            "Val acc and loss are 0.8908 and 0.321835\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.94856 and 0.14756837\n",
            "Val acc and loss are 0.8902 and 0.32178828\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.9488 and 0.14743955\n",
            "Val acc and loss are 0.8905 and 0.32274184\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.94884 and 0.14754744\n",
            "Val acc and loss are 0.8909 and 0.32337353\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.94866 and 0.1471874\n",
            "Val acc and loss are 0.8915 and 0.3230617\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.94862 and 0.1467393\n",
            "Val acc and loss are 0.8918 and 0.32209706\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.94882 and 0.14655276\n",
            "Val acc and loss are 0.8918 and 0.3221164\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.94924 and 0.1462391\n",
            "Val acc and loss are 0.8917 and 0.32241288\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.94944 and 0.14600947\n",
            "Val acc and loss are 0.8909 and 0.3226298\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.94972 and 0.1458163\n",
            "Val acc and loss are 0.8907 and 0.32269943\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.94962 and 0.14572449\n",
            "Val acc and loss are 0.8909 and 0.32277992\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.9498 and 0.14568144\n",
            "Val acc and loss are 0.8914 and 0.32278076\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.94958 and 0.14573982\n",
            "Val acc and loss are 0.8915 and 0.32294747\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.94998 and 0.14511721\n",
            "Val acc and loss are 0.8913 and 0.322378\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.95008 and 0.14471364\n",
            "Val acc and loss are 0.8912 and 0.32249334\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.94986 and 0.14491655\n",
            "Val acc and loss are 0.8905 and 0.3236872\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.95008 and 0.14472666\n",
            "Val acc and loss are 0.8905 and 0.32385322\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.9502 and 0.14419203\n",
            "Val acc and loss are 0.8905 and 0.32287118\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.95068 and 0.14378218\n",
            "Val acc and loss are 0.8916 and 0.32218212\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.95026 and 0.14376804\n",
            "Val acc and loss are 0.8919 and 0.32257053\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.95062 and 0.14354217\n",
            "Val acc and loss are 0.8911 and 0.3229364\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.95118 and 0.14302316\n",
            "Val acc and loss are 0.8908 and 0.32253408\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.95128 and 0.14258409\n",
            "Val acc and loss are 0.8916 and 0.32224032\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.9508 and 0.1424903\n",
            "Val acc and loss are 0.8915 and 0.32236528\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.95102 and 0.14221406\n",
            "Val acc and loss are 0.8912 and 0.32230824\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.95108 and 0.14223422\n",
            "Val acc and loss are 0.892 and 0.322573\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.9509 and 0.14193954\n",
            "Val acc and loss are 0.8923 and 0.32260343\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.95064 and 0.1416098\n",
            "Val acc and loss are 0.892 and 0.32231718\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.95094 and 0.14139795\n",
            "Val acc and loss are 0.8911 and 0.32250187\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.95134 and 0.14076205\n",
            "Val acc and loss are 0.892 and 0.32181954\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.95176 and 0.14048377\n",
            "Val acc and loss are 0.8919 and 0.32171854\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.9517 and 0.14037521\n",
            "Val acc and loss are 0.8922 and 0.32196784\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.95124 and 0.14017788\n",
            "Val acc and loss are 0.892 and 0.32230633\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.95116 and 0.14026028\n",
            "Val acc and loss are 0.8913 and 0.32314178\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.95208 and 0.13980635\n",
            "Val acc and loss are 0.8919 and 0.3232242\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.95206 and 0.1399029\n",
            "Val acc and loss are 0.8918 and 0.32371035\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.952 and 0.13958946\n",
            "Val acc and loss are 0.8911 and 0.32397377\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.9515 and 0.13986409\n",
            "Val acc and loss are 0.8904 and 0.32480365\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.95208 and 0.13915515\n",
            "Val acc and loss are 0.891 and 0.324195\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.95236 and 0.13929874\n",
            "Val acc and loss are 0.8917 and 0.32468823\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.95272 and 0.13880472\n",
            "Val acc and loss are 0.8914 and 0.32432857\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.95248 and 0.1387197\n",
            "Val acc and loss are 0.8909 and 0.32417032\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.95264 and 0.13810274\n",
            "Val acc and loss are 0.8914 and 0.32321292\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.95302 and 0.13794717\n",
            "Val acc and loss are 0.8913 and 0.32335246\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.95314 and 0.13761939\n",
            "Val acc and loss are 0.8919 and 0.32365102\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.95264 and 0.13741577\n",
            "Val acc and loss are 0.8913 and 0.32376748\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.95328 and 0.1370625\n",
            "Val acc and loss are 0.891 and 0.32363868\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.95314 and 0.13715391\n",
            "Val acc and loss are 0.892 and 0.32410413\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.95266 and 0.13732913\n",
            "Val acc and loss are 0.8914 and 0.3249819\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.95282 and 0.13710792\n",
            "Val acc and loss are 0.8907 and 0.32525143\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.953 and 0.13638817\n",
            "Val acc and loss are 0.8911 and 0.32404584\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.95356 and 0.13590352\n",
            "Val acc and loss are 0.8914 and 0.32351184\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.9535 and 0.13581449\n",
            "Val acc and loss are 0.8913 and 0.3240394\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.95324 and 0.13579229\n",
            "Val acc and loss are 0.8916 and 0.32417962\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.95346 and 0.13510147\n",
            "Val acc and loss are 0.8916 and 0.3230945\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.95376 and 0.1350992\n",
            "Val acc and loss are 0.8916 and 0.32310325\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.95392 and 0.13482592\n",
            "Val acc and loss are 0.8919 and 0.323613\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.9535 and 0.13498609\n",
            "Val acc and loss are 0.8925 and 0.32431996\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.95372 and 0.13452885\n",
            "Val acc and loss are 0.8922 and 0.32388282\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.95408 and 0.13430865\n",
            "Val acc and loss are 0.8913 and 0.3233657\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.95442 and 0.13403283\n",
            "Val acc and loss are 0.8916 and 0.323358\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.95438 and 0.13403203\n",
            "Val acc and loss are 0.8918 and 0.32398194\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.9543 and 0.13380928\n",
            "Val acc and loss are 0.8918 and 0.32373375\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.9541 and 0.13345121\n",
            "Val acc and loss are 0.8915 and 0.32318994\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.95406 and 0.13346024\n",
            "Val acc and loss are 0.8914 and 0.3237004\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.95426 and 0.13354601\n",
            "Val acc and loss are 0.8922 and 0.32495123\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.95444 and 0.13323636\n",
            "Val acc and loss are 0.8927 and 0.32509896\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.95472 and 0.1326572\n",
            "Val acc and loss are 0.8923 and 0.32456014\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.9547 and 0.13231938\n",
            "Val acc and loss are 0.8924 and 0.32422358\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.95464 and 0.13223936\n",
            "Val acc and loss are 0.8918 and 0.32498208\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.9547 and 0.13236406\n",
            "Val acc and loss are 0.8921 and 0.32607403\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.95462 and 0.13194403\n",
            "Val acc and loss are 0.8914 and 0.3255356\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.95518 and 0.1315282\n",
            "Val acc and loss are 0.8912 and 0.32459596\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.95516 and 0.13146201\n",
            "Val acc and loss are 0.8916 and 0.3246477\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.9552 and 0.13167228\n",
            "Val acc and loss are 0.8916 and 0.32547915\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.95508 and 0.13159166\n",
            "Val acc and loss are 0.892 and 0.3256384\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.95526 and 0.130843\n",
            "Val acc and loss are 0.8921 and 0.32464972\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.95554 and 0.13049233\n",
            "Val acc and loss are 0.8914 and 0.32455167\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.95562 and 0.130268\n",
            "Val acc and loss are 0.8911 and 0.32532066\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.95572 and 0.13018522\n",
            "Val acc and loss are 0.8913 and 0.32593238\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.95618 and 0.12944432\n",
            "Val acc and loss are 0.8916 and 0.32473308\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.956 and 0.12919885\n",
            "Val acc and loss are 0.8914 and 0.324475\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.9558 and 0.12943746\n",
            "Val acc and loss are 0.8918 and 0.325418\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.9556 and 0.12986994\n",
            "Val acc and loss are 0.8916 and 0.3264752\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.95564 and 0.12948722\n",
            "Val acc and loss are 0.8911 and 0.32606077\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.95612 and 0.12904108\n",
            "Val acc and loss are 0.8912 and 0.32533428\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.95596 and 0.12888183\n",
            "Val acc and loss are 0.8919 and 0.32539788\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.95588 and 0.1290641\n",
            "Val acc and loss are 0.8916 and 0.32653734\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.95602 and 0.12918936\n",
            "Val acc and loss are 0.8919 and 0.3274764\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.95658 and 0.12839937\n",
            "Val acc and loss are 0.8916 and 0.32642442\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.95674 and 0.12789308\n",
            "Val acc and loss are 0.8915 and 0.32585377\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.9565 and 0.12771828\n",
            "Val acc and loss are 0.8918 and 0.32649562\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.95684 and 0.12752502\n",
            "Val acc and loss are 0.8915 and 0.3273675\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.9572 and 0.12687731\n",
            "Val acc and loss are 0.8914 and 0.32633325\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.9574 and 0.12635663\n",
            "Val acc and loss are 0.8918 and 0.3255632\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.95698 and 0.12655012\n",
            "Val acc and loss are 0.8922 and 0.32597545\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.957 and 0.12667611\n",
            "Val acc and loss are 0.8922 and 0.3267156\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.95698 and 0.12666145\n",
            "Val acc and loss are 0.8922 and 0.3270858\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.95744 and 0.12598677\n",
            "Val acc and loss are 0.8919 and 0.32655132\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.95744 and 0.1255879\n",
            "Val acc and loss are 0.8914 and 0.32615292\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.95762 and 0.12561414\n",
            "Val acc and loss are 0.8917 and 0.3269024\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.9578 and 0.1259293\n",
            "Val acc and loss are 0.8917 and 0.32808092\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.95776 and 0.12543374\n",
            "Val acc and loss are 0.8921 and 0.32712594\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.95792 and 0.12500577\n",
            "Val acc and loss are 0.8918 and 0.3262398\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.95774 and 0.12477525\n",
            "Val acc and loss are 0.8913 and 0.3263198\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.95756 and 0.12482711\n",
            "Val acc and loss are 0.8915 and 0.32690468\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.958 and 0.12458414\n",
            "Val acc and loss are 0.8917 and 0.32717687\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.95824 and 0.12437234\n",
            "Val acc and loss are 0.8918 and 0.3271124\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.95838 and 0.124171935\n",
            "Val acc and loss are 0.8915 and 0.32734364\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.95778 and 0.124362156\n",
            "Val acc and loss are 0.8908 and 0.3282437\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.9579 and 0.12384752\n",
            "Val acc and loss are 0.8922 and 0.32766157\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.95812 and 0.12377457\n",
            "Val acc and loss are 0.892 and 0.32748407\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.95788 and 0.12365761\n",
            "Val acc and loss are 0.8915 and 0.3274661\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.95814 and 0.12371374\n",
            "Val acc and loss are 0.8909 and 0.32757506\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.95818 and 0.123247296\n",
            "Val acc and loss are 0.8912 and 0.32731852\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.9588 and 0.12309019\n",
            "Val acc and loss are 0.8923 and 0.32714352\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.95866 and 0.122435726\n",
            "Val acc and loss are 0.8923 and 0.32687667\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.95838 and 0.12267258\n",
            "Val acc and loss are 0.8917 and 0.3277935\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.95864 and 0.12243972\n",
            "Val acc and loss are 0.8914 and 0.3281657\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.95906 and 0.122075744\n",
            "Val acc and loss are 0.8926 and 0.32762787\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.9589 and 0.12170279\n",
            "Val acc and loss are 0.8922 and 0.327039\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.95896 and 0.12157976\n",
            "Val acc and loss are 0.8924 and 0.32725504\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.9589 and 0.121541224\n",
            "Val acc and loss are 0.8916 and 0.32765839\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.95952 and 0.120834485\n",
            "Val acc and loss are 0.8926 and 0.32662338\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.9595 and 0.12058467\n",
            "Val acc and loss are 0.8932 and 0.32618773\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.95988 and 0.12015779\n",
            "Val acc and loss are 0.8931 and 0.32607108\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.95986 and 0.120015\n",
            "Val acc and loss are 0.8925 and 0.32613125\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.95994 and 0.12019374\n",
            "Val acc and loss are 0.8915 and 0.32704106\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.95968 and 0.11985192\n",
            "Val acc and loss are 0.892 and 0.32684073\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.95976 and 0.119880974\n",
            "Val acc and loss are 0.892 and 0.3270605\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.95988 and 0.12004373\n",
            "Val acc and loss are 0.8925 and 0.32770446\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.95984 and 0.1198217\n",
            "Val acc and loss are 0.8917 and 0.32755938\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.95994 and 0.11981689\n",
            "Val acc and loss are 0.891 and 0.32797292\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.96002 and 0.11936752\n",
            "Val acc and loss are 0.8915 and 0.32755253\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.96012 and 0.119085714\n",
            "Val acc and loss are 0.8922 and 0.3272476\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.96026 and 0.11901853\n",
            "Val acc and loss are 0.8927 and 0.3273268\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.96022 and 0.11891439\n",
            "Val acc and loss are 0.8922 and 0.32746705\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.96042 and 0.11896289\n",
            "Val acc and loss are 0.8915 and 0.32791182\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.96028 and 0.118477\n",
            "Val acc and loss are 0.8921 and 0.3274992\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.96018 and 0.11812093\n",
            "Val acc and loss are 0.8921 and 0.32780245\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.96088 and 0.118015915\n",
            "Val acc and loss are 0.8912 and 0.32873136\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.96084 and 0.117717445\n",
            "Val acc and loss are 0.8917 and 0.3291378\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.96078 and 0.11749082\n",
            "Val acc and loss are 0.8916 and 0.3290425\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.9609 and 0.11753656\n",
            "Val acc and loss are 0.8919 and 0.32901427\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.96096 and 0.11734593\n",
            "Val acc and loss are 0.8929 and 0.32829252\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.96054 and 0.11725632\n",
            "Val acc and loss are 0.8923 and 0.32829478\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.96092 and 0.1168917\n",
            "Val acc and loss are 0.8924 and 0.32806915\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.96086 and 0.11647645\n",
            "Val acc and loss are 0.892 and 0.3277729\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.96162 and 0.11616081\n",
            "Val acc and loss are 0.8921 and 0.3287427\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.9613 and 0.11632374\n",
            "Val acc and loss are 0.8919 and 0.32943988\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.96136 and 0.11607594\n",
            "Val acc and loss are 0.8922 and 0.32912758\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.96118 and 0.11585652\n",
            "Val acc and loss are 0.8933 and 0.32869795\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.96084 and 0.11597312\n",
            "Val acc and loss are 0.8934 and 0.32929736\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.96126 and 0.11565144\n",
            "Val acc and loss are 0.8924 and 0.32967553\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.96126 and 0.115880616\n",
            "Val acc and loss are 0.8921 and 0.33035094\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.96134 and 0.11526684\n",
            "Val acc and loss are 0.8914 and 0.32947016\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.96146 and 0.11523456\n",
            "Val acc and loss are 0.8923 and 0.32886353\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.9612 and 0.11479849\n",
            "Val acc and loss are 0.8927 and 0.32815054\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.96146 and 0.11468973\n",
            "Val acc and loss are 0.8922 and 0.3283014\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.96186 and 0.11418078\n",
            "Val acc and loss are 0.8903 and 0.32805848\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.9624 and 0.11393067\n",
            "Val acc and loss are 0.8904 and 0.32798466\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.96262 and 0.113591306\n",
            "Val acc and loss are 0.8912 and 0.3278484\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.96222 and 0.113504894\n",
            "Val acc and loss are 0.8923 and 0.32760504\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.96194 and 0.113654\n",
            "Val acc and loss are 0.8929 and 0.32752517\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.96196 and 0.11384365\n",
            "Val acc and loss are 0.8921 and 0.3284791\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.96226 and 0.113493405\n",
            "Val acc and loss are 0.8914 and 0.32882088\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.96262 and 0.11283643\n",
            "Val acc and loss are 0.8911 and 0.32850003\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.96268 and 0.11247201\n",
            "Val acc and loss are 0.8914 and 0.3287126\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.9627 and 0.11252965\n",
            "Val acc and loss are 0.8917 and 0.32954723\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.96282 and 0.11259508\n",
            "Val acc and loss are 0.8924 and 0.32994086\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.96254 and 0.112255886\n",
            "Val acc and loss are 0.8914 and 0.3291814\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.96264 and 0.11211146\n",
            "Val acc and loss are 0.892 and 0.32910195\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.96308 and 0.11209619\n",
            "Val acc and loss are 0.8907 and 0.33026147\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.96278 and 0.11242799\n",
            "Val acc and loss are 0.8914 and 0.3313418\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.963 and 0.11120832\n",
            "Val acc and loss are 0.8918 and 0.32918656\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.96288 and 0.11125577\n",
            "Val acc and loss are 0.8924 and 0.32894498\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.96304 and 0.111183055\n",
            "Val acc and loss are 0.8915 and 0.33003974\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.9626 and 0.111788414\n",
            "Val acc and loss are 0.8927 and 0.33180177\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.96328 and 0.110956624\n",
            "Val acc and loss are 0.8923 and 0.33075684\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.96346 and 0.11059775\n",
            "Val acc and loss are 0.8918 and 0.32983002\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.96342 and 0.11035149\n",
            "Val acc and loss are 0.8916 and 0.32974946\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.96324 and 0.11046891\n",
            "Val acc and loss are 0.8922 and 0.33056176\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.96342 and 0.11036341\n",
            "Val acc and loss are 0.8922 and 0.33087358\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.96344 and 0.10962914\n",
            "Val acc and loss are 0.8922 and 0.3296535\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.96344 and 0.10939618\n",
            "Val acc and loss are 0.8924 and 0.32963645\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.96332 and 0.10972518\n",
            "Val acc and loss are 0.8922 and 0.3310169\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.96382 and 0.1096523\n",
            "Val acc and loss are 0.8933 and 0.3313564\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.96356 and 0.10964145\n",
            "Val acc and loss are 0.8933 and 0.33164445\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.96392 and 0.10908088\n",
            "Val acc and loss are 0.8919 and 0.33111018\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.96392 and 0.10873184\n",
            "Val acc and loss are 0.8925 and 0.33060065\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.96428 and 0.108430564\n",
            "Val acc and loss are 0.8927 and 0.33062646\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.96414 and 0.10869135\n",
            "Val acc and loss are 0.8928 and 0.3315403\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.96404 and 0.10840734\n",
            "Val acc and loss are 0.892 and 0.33120015\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.96406 and 0.10815019\n",
            "Val acc and loss are 0.8926 and 0.33059704\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.96454 and 0.10785412\n",
            "Val acc and loss are 0.8927 and 0.3304116\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.96442 and 0.107741445\n",
            "Val acc and loss are 0.893 and 0.33058286\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.96436 and 0.107769236\n",
            "Val acc and loss are 0.8926 and 0.331049\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.96462 and 0.10751335\n",
            "Val acc and loss are 0.892 and 0.33109102\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.96474 and 0.107195735\n",
            "Val acc and loss are 0.8925 and 0.33059537\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.96476 and 0.10702354\n",
            "Val acc and loss are 0.8924 and 0.33068627\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.9646 and 0.107219934\n",
            "Val acc and loss are 0.8925 and 0.33129418\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.9645 and 0.10694832\n",
            "Val acc and loss are 0.8919 and 0.33082813\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.96446 and 0.10692045\n",
            "Val acc and loss are 0.892 and 0.33034185\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.9646 and 0.10667449\n",
            "Val acc and loss are 0.8925 and 0.33007032\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.96452 and 0.10664617\n",
            "Val acc and loss are 0.8926 and 0.33108774\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.96454 and 0.106524445\n",
            "Val acc and loss are 0.893 and 0.33171535\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.96486 and 0.106211096\n",
            "Val acc and loss are 0.8929 and 0.3315544\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.96432 and 0.10630015\n",
            "Val acc and loss are 0.8922 and 0.33149993\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.96482 and 0.10599807\n",
            "Val acc and loss are 0.8929 and 0.3317755\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.9648 and 0.106096834\n",
            "Val acc and loss are 0.8929 and 0.33284956\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.96476 and 0.10575824\n",
            "Val acc and loss are 0.8914 and 0.33316395\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.9648 and 0.105299406\n",
            "Val acc and loss are 0.8918 and 0.33208603\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.96504 and 0.10513582\n",
            "Val acc and loss are 0.892 and 0.33196494\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.96506 and 0.10483273\n",
            "Val acc and loss are 0.8914 and 0.33239213\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.96552 and 0.10482782\n",
            "Val acc and loss are 0.8922 and 0.33307603\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.96532 and 0.10456222\n",
            "Val acc and loss are 0.8922 and 0.3325271\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.96526 and 0.104266234\n",
            "Val acc and loss are 0.8918 and 0.3317291\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.96532 and 0.10418947\n",
            "Val acc and loss are 0.8922 and 0.3318401\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.9657 and 0.1039254\n",
            "Val acc and loss are 0.8928 and 0.33217084\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.9657 and 0.10391383\n",
            "Val acc and loss are 0.8935 and 0.33253214\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.96558 and 0.10340129\n",
            "Val acc and loss are 0.8928 and 0.33194268\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.96556 and 0.1034137\n",
            "Val acc and loss are 0.8923 and 0.3318899\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.96582 and 0.10306927\n",
            "Val acc and loss are 0.8934 and 0.33142692\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.9663 and 0.10272158\n",
            "Val acc and loss are 0.8926 and 0.33164\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.96596 and 0.10305135\n",
            "Val acc and loss are 0.8928 and 0.3331787\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.96596 and 0.10286765\n",
            "Val acc and loss are 0.892 and 0.3330834\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.96582 and 0.102936104\n",
            "Val acc and loss are 0.8917 and 0.3330758\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.96604 and 0.10272311\n",
            "Val acc and loss are 0.8924 and 0.33311555\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.96586 and 0.103242405\n",
            "Val acc and loss are 0.8929 and 0.33412254\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.96566 and 0.10294254\n",
            "Val acc and loss are 0.8916 and 0.33403084\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.9661 and 0.10210134\n",
            "Val acc and loss are 0.8921 and 0.3330541\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.9665 and 0.1013733\n",
            "Val acc and loss are 0.8915 and 0.33244702\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.9668 and 0.10123568\n",
            "Val acc and loss are 0.8929 and 0.3330658\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.96666 and 0.10121524\n",
            "Val acc and loss are 0.893 and 0.33367085\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.96642 and 0.1012642\n",
            "Val acc and loss are 0.8922 and 0.33395568\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.96624 and 0.10091752\n",
            "Val acc and loss are 0.8919 and 0.33317155\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.96624 and 0.100842185\n",
            "Val acc and loss are 0.8925 and 0.33354592\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.96656 and 0.10094499\n",
            "Val acc and loss are 0.8908 and 0.33499783\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.96666 and 0.10090625\n",
            "Val acc and loss are 0.8918 and 0.33590606\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.96678 and 0.10032445\n",
            "Val acc and loss are 0.8918 and 0.3349439\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.96706 and 0.099981286\n",
            "Val acc and loss are 0.8917 and 0.3342665\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.9668 and 0.100043885\n",
            "Val acc and loss are 0.8912 and 0.33437\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.9662 and 0.10023435\n",
            "Val acc and loss are 0.8916 and 0.334624\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.96654 and 0.09992411\n",
            "Val acc and loss are 0.8922 and 0.33422738\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.967 and 0.09968918\n",
            "Val acc and loss are 0.8914 and 0.33431622\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.96682 and 0.09956\n",
            "Val acc and loss are 0.8917 and 0.33458355\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.96732 and 0.09896922\n",
            "Val acc and loss are 0.8919 and 0.33381915\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.96714 and 0.09885586\n",
            "Val acc and loss are 0.8923 and 0.3341369\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.96722 and 0.09931683\n",
            "Val acc and loss are 0.8925 and 0.33591533\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.96726 and 0.09925725\n",
            "Val acc and loss are 0.8923 and 0.3360048\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.96762 and 0.09842342\n",
            "Val acc and loss are 0.8918 and 0.33448365\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.96762 and 0.09799851\n",
            "Val acc and loss are 0.8923 and 0.33464995\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.96738 and 0.09842347\n",
            "Val acc and loss are 0.8915 and 0.336108\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.96786 and 0.09792874\n",
            "Val acc and loss are 0.8926 and 0.33540487\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.9674 and 0.09744972\n",
            "Val acc and loss are 0.8929 and 0.33428988\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.96754 and 0.09779215\n",
            "Val acc and loss are 0.8918 and 0.33479515\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.9677 and 0.098074764\n",
            "Val acc and loss are 0.8914 and 0.33605456\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.9676 and 0.09835187\n",
            "Val acc and loss are 0.8927 and 0.33684674\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.96784 and 0.09710414\n",
            "Val acc and loss are 0.8927 and 0.33527794\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.96764 and 0.097042695\n",
            "Val acc and loss are 0.8929 and 0.3350684\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.96798 and 0.09670307\n",
            "Val acc and loss are 0.8924 and 0.33537757\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.96792 and 0.09735174\n",
            "Val acc and loss are 0.8917 and 0.33660948\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.96772 and 0.09684512\n",
            "Val acc and loss are 0.892 and 0.33579585\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.96774 and 0.09674418\n",
            "Val acc and loss are 0.8917 and 0.33587348\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.96818 and 0.09640019\n",
            "Val acc and loss are 0.8917 and 0.33560637\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.96852 and 0.096204355\n",
            "Val acc and loss are 0.8923 and 0.33581176\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.96838 and 0.096050754\n",
            "Val acc and loss are 0.8922 and 0.33612588\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.96856 and 0.09568797\n",
            "Val acc and loss are 0.892 and 0.3358597\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.96842 and 0.09609851\n",
            "Val acc and loss are 0.8929 and 0.33679947\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.9681 and 0.095853284\n",
            "Val acc and loss are 0.8933 and 0.33639762\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.96816 and 0.09601013\n",
            "Val acc and loss are 0.8935 and 0.33612144\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.96856 and 0.09514481\n",
            "Val acc and loss are 0.8927 and 0.3356937\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.96854 and 0.09546495\n",
            "Val acc and loss are 0.8926 and 0.3369186\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.96852 and 0.094984286\n",
            "Val acc and loss are 0.8935 and 0.3364304\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.96846 and 0.09525809\n",
            "Val acc and loss are 0.8933 and 0.33548975\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.96864 and 0.09450701\n",
            "Val acc and loss are 0.8929 and 0.33468688\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.96832 and 0.09541816\n",
            "Val acc and loss are 0.893 and 0.33690432\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.96874 and 0.095177814\n",
            "Val acc and loss are 0.8926 and 0.33802533\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.96862 and 0.09516497\n",
            "Val acc and loss are 0.8928 and 0.33769894\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.9692 and 0.09385817\n",
            "Val acc and loss are 0.8923 and 0.336027\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.96862 and 0.09462538\n",
            "Val acc and loss are 0.8935 and 0.33750054\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.96874 and 0.094384305\n",
            "Val acc and loss are 0.8931 and 0.33765978\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.96852 and 0.09447162\n",
            "Val acc and loss are 0.8929 and 0.3374221\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.96888 and 0.09382553\n",
            "Val acc and loss are 0.8922 and 0.33705336\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.96894 and 0.09366411\n",
            "Val acc and loss are 0.8924 and 0.33743578\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.96918 and 0.09331176\n",
            "Val acc and loss are 0.892 and 0.33756256\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.9695 and 0.09313896\n",
            "Val acc and loss are 0.8928 and 0.33778316\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.9694 and 0.09309453\n",
            "Val acc and loss are 0.8931 and 0.33751333\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.96942 and 0.09266511\n",
            "Val acc and loss are 0.8925 and 0.336422\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.96898 and 0.093197405\n",
            "Val acc and loss are 0.8923 and 0.33767325\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.96904 and 0.0933735\n",
            "Val acc and loss are 0.8916 and 0.33900478\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.96896 and 0.09319582\n",
            "Val acc and loss are 0.8922 and 0.33888832\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.96928 and 0.092298776\n",
            "Val acc and loss are 0.8921 and 0.33760846\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.96962 and 0.092436835\n",
            "Val acc and loss are 0.8919 and 0.33766314\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.96978 and 0.09193686\n",
            "Val acc and loss are 0.8914 and 0.33714548\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.9696 and 0.091884255\n",
            "Val acc and loss are 0.8911 and 0.33814716\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.9695 and 0.09227334\n",
            "Val acc and loss are 0.892 and 0.33938268\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.9696 and 0.091758065\n",
            "Val acc and loss are 0.8931 and 0.33849576\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.9695 and 0.09160895\n",
            "Val acc and loss are 0.8917 and 0.33829883\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.96974 and 0.09125207\n",
            "Val acc and loss are 0.8921 and 0.3383012\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.96952 and 0.09137274\n",
            "Val acc and loss are 0.8928 and 0.33910087\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.96974 and 0.09099329\n",
            "Val acc and loss are 0.8923 and 0.338863\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.96974 and 0.09126363\n",
            "Val acc and loss are 0.8915 and 0.33928558\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.97012 and 0.091007225\n",
            "Val acc and loss are 0.8917 and 0.3387807\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.97008 and 0.09051479\n",
            "Val acc and loss are 0.8923 and 0.3382463\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.96992 and 0.09057146\n",
            "Val acc and loss are 0.8933 and 0.3388228\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.97014 and 0.0902758\n",
            "Val acc and loss are 0.8935 and 0.33886462\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.97042 and 0.089835525\n",
            "Val acc and loss are 0.8921 and 0.33819333\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.9708 and 0.08932555\n",
            "Val acc and loss are 0.8915 and 0.33775452\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.9706 and 0.08962501\n",
            "Val acc and loss are 0.8934 and 0.33906734\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.97032 and 0.09000288\n",
            "Val acc and loss are 0.893 and 0.34075156\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.97044 and 0.09014678\n",
            "Val acc and loss are 0.8923 and 0.3411145\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.97072 and 0.089324854\n",
            "Val acc and loss are 0.8934 and 0.33920026\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.9705 and 0.08903055\n",
            "Val acc and loss are 0.8931 and 0.33869871\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.9706 and 0.08894303\n",
            "Val acc and loss are 0.8939 and 0.3392486\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.97022 and 0.08927799\n",
            "Val acc and loss are 0.8929 and 0.34058276\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.9705 and 0.088988006\n",
            "Val acc and loss are 0.8918 and 0.34031847\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.97054 and 0.08872248\n",
            "Val acc and loss are 0.8929 and 0.33977047\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.97076 and 0.088612266\n",
            "Val acc and loss are 0.8937 and 0.33946368\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.97106 and 0.08870527\n",
            "Val acc and loss are 0.8933 and 0.3400708\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.97124 and 0.08843196\n",
            "Val acc and loss are 0.8928 and 0.34033465\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.97142 and 0.08809068\n",
            "Val acc and loss are 0.8932 and 0.34014377\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.97178 and 0.08757229\n",
            "Val acc and loss are 0.8933 and 0.33924663\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.9716 and 0.087044545\n",
            "Val acc and loss are 0.893 and 0.33819696\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.97184 and 0.08679896\n",
            "Val acc and loss are 0.8935 and 0.3379628\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.97178 and 0.0869157\n",
            "Val acc and loss are 0.8934 and 0.33843878\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.97192 and 0.08677379\n",
            "Val acc and loss are 0.8932 and 0.33804384\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.97144 and 0.08704908\n",
            "Val acc and loss are 0.8925 and 0.33887118\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.97146 and 0.087229505\n",
            "Val acc and loss are 0.8922 and 0.33931732\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.9714 and 0.08695303\n",
            "Val acc and loss are 0.8922 and 0.33933538\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.97156 and 0.086799\n",
            "Val acc and loss are 0.8921 and 0.3394267\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.97182 and 0.08638113\n",
            "Val acc and loss are 0.892 and 0.33957788\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.97172 and 0.08628275\n",
            "Val acc and loss are 0.8931 and 0.34015092\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.97162 and 0.086268924\n",
            "Val acc and loss are 0.8937 and 0.34008244\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.97184 and 0.08619131\n",
            "Val acc and loss are 0.8934 and 0.3397142\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.9717 and 0.085999064\n",
            "Val acc and loss are 0.8932 and 0.3393554\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.97188 and 0.0859951\n",
            "Val acc and loss are 0.8929 and 0.34000397\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.97168 and 0.08576967\n",
            "Val acc and loss are 0.8925 and 0.34075814\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.97218 and 0.085207306\n",
            "Val acc and loss are 0.893 and 0.33982217\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.97244 and 0.08498969\n",
            "Val acc and loss are 0.8923 and 0.33864892\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.97236 and 0.08505622\n",
            "Val acc and loss are 0.8929 and 0.33947465\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.97224 and 0.0852997\n",
            "Val acc and loss are 0.8922 and 0.34069076\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.9726 and 0.08500243\n",
            "Val acc and loss are 0.8932 and 0.34056386\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.9726 and 0.084546834\n",
            "Val acc and loss are 0.8922 and 0.3395778\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.9727 and 0.084326535\n",
            "Val acc and loss are 0.8922 and 0.33978534\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.97244 and 0.08470676\n",
            "Val acc and loss are 0.8925 and 0.3412831\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.97258 and 0.084289774\n",
            "Val acc and loss are 0.8932 and 0.34117404\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.97268 and 0.08422201\n",
            "Val acc and loss are 0.8926 and 0.3410288\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.97286 and 0.08399531\n",
            "Val acc and loss are 0.892 and 0.34128526\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.97284 and 0.084318005\n",
            "Val acc and loss are 0.8909 and 0.3421262\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.9727 and 0.08396929\n",
            "Val acc and loss are 0.891 and 0.3417103\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.97254 and 0.084057756\n",
            "Val acc and loss are 0.8916 and 0.34160015\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.97296 and 0.08400705\n",
            "Val acc and loss are 0.8925 and 0.34201983\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.97256 and 0.08427205\n",
            "Val acc and loss are 0.8926 and 0.34328482\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.9731 and 0.083173156\n",
            "Val acc and loss are 0.8922 and 0.34186548\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.97316 and 0.082916796\n",
            "Val acc and loss are 0.8924 and 0.34101468\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.97332 and 0.08267039\n",
            "Val acc and loss are 0.8928 and 0.34156793\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.97286 and 0.08378291\n",
            "Val acc and loss are 0.8924 and 0.34391084\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.97312 and 0.08318579\n",
            "Val acc and loss are 0.8928 and 0.342885\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.97316 and 0.08282783\n",
            "Val acc and loss are 0.8936 and 0.34181884\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.9731 and 0.08230371\n",
            "Val acc and loss are 0.8931 and 0.34192201\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.97324 and 0.082440235\n",
            "Val acc and loss are 0.8919 and 0.3429638\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.97358 and 0.08213852\n",
            "Val acc and loss are 0.8924 and 0.3426545\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.97384 and 0.08214512\n",
            "Val acc and loss are 0.8929 and 0.34259287\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.97382 and 0.08181204\n",
            "Val acc and loss are 0.8934 and 0.3422022\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.97398 and 0.081785925\n",
            "Val acc and loss are 0.8935 and 0.3422728\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.97406 and 0.08158795\n",
            "Val acc and loss are 0.8937 and 0.3426826\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.97378 and 0.08173471\n",
            "Val acc and loss are 0.8942 and 0.34330663\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.9738 and 0.081685655\n",
            "Val acc and loss are 0.8944 and 0.34321946\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.97354 and 0.081610516\n",
            "Val acc and loss are 0.8932 and 0.3430296\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.97376 and 0.08129792\n",
            "Val acc and loss are 0.8924 and 0.34280524\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.9737 and 0.08085084\n",
            "Val acc and loss are 0.893 and 0.34325194\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.9736 and 0.08098571\n",
            "Val acc and loss are 0.893 and 0.34409967\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.9737 and 0.080986775\n",
            "Val acc and loss are 0.8938 and 0.34415156\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.97366 and 0.081109196\n",
            "Val acc and loss are 0.8938 and 0.34361413\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.97366 and 0.08067912\n",
            "Val acc and loss are 0.8941 and 0.34300655\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.97408 and 0.08060294\n",
            "Val acc and loss are 0.8932 and 0.34379774\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.97406 and 0.08056549\n",
            "Val acc and loss are 0.893 and 0.34496307\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.97382 and 0.08016148\n",
            "Val acc and loss are 0.8931 and 0.3450191\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.97422 and 0.07980657\n",
            "Val acc and loss are 0.8926 and 0.3445788\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.97442 and 0.07960059\n",
            "Val acc and loss are 0.893 and 0.34430593\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.97444 and 0.07954488\n",
            "Val acc and loss are 0.8926 and 0.34465832\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.97404 and 0.079590686\n",
            "Val acc and loss are 0.8936 and 0.34505278\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.97398 and 0.079638876\n",
            "Val acc and loss are 0.8932 and 0.34472245\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.97414 and 0.07932823\n",
            "Val acc and loss are 0.8939 and 0.3442639\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.97442 and 0.07927103\n",
            "Val acc and loss are 0.8925 and 0.34484932\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.97484 and 0.0787981\n",
            "Val acc and loss are 0.8928 and 0.34506127\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.97488 and 0.07858793\n",
            "Val acc and loss are 0.8929 and 0.34504983\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.97484 and 0.07844817\n",
            "Val acc and loss are 0.8931 and 0.34476742\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.97458 and 0.0789733\n",
            "Val acc and loss are 0.8927 and 0.3454846\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.97448 and 0.07889192\n",
            "Val acc and loss are 0.8934 and 0.34519333\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.97488 and 0.07844187\n",
            "Val acc and loss are 0.8927 and 0.3438563\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.9749 and 0.07818526\n",
            "Val acc and loss are 0.8928 and 0.34378374\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.97514 and 0.078270465\n",
            "Val acc and loss are 0.8932 and 0.34483096\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.97532 and 0.07802372\n",
            "Val acc and loss are 0.8935 and 0.3448744\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.97532 and 0.07794379\n",
            "Val acc and loss are 0.8934 and 0.34481674\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.9754 and 0.07783529\n",
            "Val acc and loss are 0.8934 and 0.3446582\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.97486 and 0.07804444\n",
            "Val acc and loss are 0.8926 and 0.34555736\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.97462 and 0.07782828\n",
            "Val acc and loss are 0.8929 and 0.34562635\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.97472 and 0.07761957\n",
            "Val acc and loss are 0.8926 and 0.3462873\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.97506 and 0.07764626\n",
            "Val acc and loss are 0.8915 and 0.34692115\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.97498 and 0.077354394\n",
            "Val acc and loss are 0.8924 and 0.3465824\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.97514 and 0.07733449\n",
            "Val acc and loss are 0.8924 and 0.34655142\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.97502 and 0.07777479\n",
            "Val acc and loss are 0.8932 and 0.34765103\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.97496 and 0.07785074\n",
            "Val acc and loss are 0.8934 and 0.3481095\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.97506 and 0.07721305\n",
            "Val acc and loss are 0.8934 and 0.34678936\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.97508 and 0.0771093\n",
            "Val acc and loss are 0.8929 and 0.3461102\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.97526 and 0.07674552\n",
            "Val acc and loss are 0.893 and 0.34539276\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.9751 and 0.076685496\n",
            "Val acc and loss are 0.8929 and 0.3464378\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.97524 and 0.07697998\n",
            "Val acc and loss are 0.8927 and 0.34757295\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.9751 and 0.076743156\n",
            "Val acc and loss are 0.8932 and 0.34747714\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.97542 and 0.07611649\n",
            "Val acc and loss are 0.8935 and 0.34618905\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.97574 and 0.075891994\n",
            "Val acc and loss are 0.8927 and 0.3459349\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.97572 and 0.07600266\n",
            "Val acc and loss are 0.8925 and 0.3465717\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.97548 and 0.075893946\n",
            "Val acc and loss are 0.8926 and 0.34681225\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.97562 and 0.07554156\n",
            "Val acc and loss are 0.8928 and 0.34598568\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.9759 and 0.075504825\n",
            "Val acc and loss are 0.8932 and 0.34597054\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.9759 and 0.07549433\n",
            "Val acc and loss are 0.8927 and 0.34684008\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.97574 and 0.075904384\n",
            "Val acc and loss are 0.8923 and 0.34885082\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.97608 and 0.07533911\n",
            "Val acc and loss are 0.8925 and 0.34804264\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.97586 and 0.07490224\n",
            "Val acc and loss are 0.8918 and 0.34789607\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.97584 and 0.07493221\n",
            "Val acc and loss are 0.8921 and 0.34844053\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.97576 and 0.07515071\n",
            "Val acc and loss are 0.8925 and 0.34889254\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.97628 and 0.07483876\n",
            "Val acc and loss are 0.8928 and 0.34842628\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.97666 and 0.07457245\n",
            "Val acc and loss are 0.8929 and 0.34753835\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.9766 and 0.07417601\n",
            "Val acc and loss are 0.8929 and 0.34739032\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.97636 and 0.074180394\n",
            "Val acc and loss are 0.893 and 0.34839055\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.97642 and 0.07379314\n",
            "Val acc and loss are 0.8934 and 0.34814537\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.97656 and 0.07362861\n",
            "Val acc and loss are 0.8938 and 0.34741294\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.9765 and 0.07368223\n",
            "Val acc and loss are 0.8944 and 0.34707636\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.97634 and 0.073808946\n",
            "Val acc and loss are 0.8938 and 0.34759703\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.97632 and 0.07357188\n",
            "Val acc and loss are 0.8941 and 0.34778568\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.97628 and 0.07330709\n",
            "Val acc and loss are 0.8934 and 0.34795246\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.97654 and 0.07342523\n",
            "Val acc and loss are 0.8928 and 0.3483195\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.97684 and 0.073178075\n",
            "Val acc and loss are 0.8938 and 0.34733996\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.97662 and 0.07343005\n",
            "Val acc and loss are 0.8942 and 0.34751266\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.97666 and 0.07307144\n",
            "Val acc and loss are 0.8941 and 0.34725988\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.97678 and 0.07293788\n",
            "Val acc and loss are 0.893 and 0.3479837\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.97694 and 0.07262712\n",
            "Val acc and loss are 0.8929 and 0.34858707\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.97684 and 0.072600886\n",
            "Val acc and loss are 0.8924 and 0.3488007\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.97718 and 0.072568364\n",
            "Val acc and loss are 0.8926 and 0.3491294\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.97694 and 0.073009804\n",
            "Val acc and loss are 0.892 and 0.3502291\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.97702 and 0.072526984\n",
            "Val acc and loss are 0.8923 and 0.35018355\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.97708 and 0.071934804\n",
            "Val acc and loss are 0.893 and 0.34893286\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.97708 and 0.07178156\n",
            "Val acc and loss are 0.8936 and 0.34899294\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.97708 and 0.071754545\n",
            "Val acc and loss are 0.8943 and 0.34923768\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.97722 and 0.07187808\n",
            "Val acc and loss are 0.8934 and 0.3496246\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.97724 and 0.07184253\n",
            "Val acc and loss are 0.8934 and 0.34994254\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.97722 and 0.07157511\n",
            "Val acc and loss are 0.8935 and 0.34991702\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.97722 and 0.071701445\n",
            "Val acc and loss are 0.8926 and 0.35119486\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.97674 and 0.07231302\n",
            "Val acc and loss are 0.891 and 0.3524424\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.9771 and 0.071630135\n",
            "Val acc and loss are 0.8924 and 0.3499351\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.97732 and 0.07163441\n",
            "Val acc and loss are 0.8929 and 0.3485398\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.97738 and 0.0713272\n",
            "Val acc and loss are 0.8938 and 0.3492297\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.97694 and 0.07227049\n",
            "Val acc and loss are 0.8928 and 0.35225615\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.97742 and 0.07112928\n",
            "Val acc and loss are 0.8931 and 0.35072124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOCMe-ddr_R-"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "oNA76Z6047OM",
        "outputId": "71943258-eff1-4ccb-fd8a-6bcc08342f68"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.97742 achieved at epoch: 999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vndyeU_47Xa",
        "outputId": "f2a7fca3-2701-4f98-88b9-53b8052c5836"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4564,    3,   45,   66,    6,    0,  283,    1,    9,    0],\n",
              "       [   6, 4975,    1,   26,    0,    0,    2,    0,    1,    1],\n",
              "       [  31,    2, 4474,   27,  294,    1,  161,    1,    1,    0],\n",
              "       [  48,   24,   29, 4737,   88,    0,   47,    0,    6,    0],\n",
              "       [   5,    5,  227,   96, 4456,    1,  158,    0,    2,    0],\n",
              "       [   0,    1,    0,    1,    0, 4939,    0,   45,    6,   12],\n",
              "       [ 297,    5,  176,   69,  162,    3, 4308,    0,    9,    1],\n",
              "       [   0,    0,    0,    0,    0,   21,    0, 4971,    2,   51],\n",
              "       [   5,    2,    3,    7,    5,    2,   12,    5, 4991,    0],\n",
              "       [   0,    1,    0,    1,    0,    8,    0,   68,    1, 4900]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "XebQp_8a47gx",
        "outputId": "e78aba43-8171-4cb5-cf68-daf954ed3b87"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8944 achieved at epoch: 912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJLR0Uzk47rE",
        "outputId": "90b14f83-a91f-44bf-ab93-8df057dded67"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[852,   0,  16,  28,   3,   2, 116,   0,   6,   0],\n",
              "       [  2, 965,   1,  13,   2,   0,   3,   0,   2,   0],\n",
              "       [ 15,   1, 844,   4,  82,   0,  57,   0,   5,   0],\n",
              "       [ 19,   3,   5, 930,  37,   0,  26,   0,   1,   0],\n",
              "       [  1,   3,  71,  28, 885,   0,  57,   0,   5,   0],\n",
              "       [  1,   0,   0,   0,   0, 946,   0,  30,   7,  12],\n",
              "       [113,   1,  72,  18,  70,   0, 683,   0,  12,   1],\n",
              "       [  0,   0,   1,   0,   0,  14,   0, 914,   0,  26],\n",
              "       [  4,   0,   4,   3,   6,   1,   7,   4, 936,   3],\n",
              "       [  0,   0,   0,   0,   0,   5,   0,  40,   0, 976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF3oOQpz47wd",
        "outputId": "df0b5cf3-4d79-41fc-97e1-4ed3eb2e00ea"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37190554\n",
            "0.8872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kekmI6z4763",
        "outputId": "c11904fe-84f8-481a-9b3e-80eb78d2f7ae"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[829,   3,  16,  28,   3,   1, 111,   0,   9,   0],\n",
              "       [  1, 971,   2,  19,   4,   0,   1,   0,   2,   0],\n",
              "       [ 19,   1, 809,  11,  84,   0,  73,   1,   2,   0],\n",
              "       [ 17,   8,  10, 903,  28,   1,  28,   0,   4,   1],\n",
              "       [  0,   0,  81,  27, 837,   1,  52,   0,   2,   0],\n",
              "       [  1,   0,   0,   1,   0, 945,   0,  30,   1,  22],\n",
              "       [112,   0,  86,  28,  62,   0, 700,   0,  12,   0],\n",
              "       [  0,   0,   0,   0,   0,  16,   0, 960,   0,  24],\n",
              "       [  6,   0,   2,   7,   6,   3,  11,   6, 959,   0],\n",
              "       [  0,   0,   0,   0,   0,   8,   1,  32,   0, 959]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlo-JxT2mn3b"
      },
      "source": [
        "# **Test 8** *(Revised from test 7: Changing the epoch size from 1000 to 600)* **Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Vd8USimkru",
        "outputId": "a06c476d-cffc-41b8-b04f-c6bc61a59a75"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 3; nodes_per_layer = [dim, 300, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"tanh\", optimizer_name = 'Adam', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-4114d2b8d98b>:30: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-10-4114d2b8d98b>:31: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uoANU_jmkru",
        "outputId": "194e5679-9cca-465d-8233-7d8d800ed177"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 600, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.5024 and 1.4861325\n",
            "Val acc and loss are 0.4987 and 1.4813821\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.64298 and 1.0843589\n",
            "Val acc and loss are 0.6441 and 1.0799321\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.67812 and 0.9408923\n",
            "Val acc and loss are 0.6764 and 0.93567693\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.69688 and 0.85994595\n",
            "Val acc and loss are 0.6983 and 0.8551362\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.7131 and 0.7982899\n",
            "Val acc and loss are 0.713 and 0.7948824\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.72978 and 0.7483644\n",
            "Val acc and loss are 0.7275 and 0.7466989\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.74634 and 0.7086944\n",
            "Val acc and loss are 0.7428 and 0.70846623\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.75844 and 0.67822903\n",
            "Val acc and loss are 0.7551 and 0.67897403\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.7654 and 0.65492946\n",
            "Val acc and loss are 0.7644 and 0.65625125\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.7717 and 0.6360424\n",
            "Val acc and loss are 0.7685 and 0.637741\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.77766 and 0.61942136\n",
            "Val acc and loss are 0.7743 and 0.6213391\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.78342 and 0.60430485\n",
            "Val acc and loss are 0.7794 and 0.6064025\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.78904 and 0.59068054\n",
            "Val acc and loss are 0.7856 and 0.59294266\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.79376 and 0.57869905\n",
            "Val acc and loss are 0.7911 and 0.58115137\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.79832 and 0.56800026\n",
            "Val acc and loss are 0.7959 and 0.570716\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.80162 and 0.55797243\n",
            "Val acc and loss are 0.7982 and 0.56105536\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.80384 and 0.54797775\n",
            "Val acc and loss are 0.8022 and 0.5515821\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.80734 and 0.5378412\n",
            "Val acc and loss are 0.8057 and 0.54210997\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.81172 and 0.527923\n",
            "Val acc and loss are 0.8096 and 0.53290695\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.81486 and 0.5186767\n",
            "Val acc and loss are 0.8105 and 0.5244361\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.8179 and 0.5102392\n",
            "Val acc and loss are 0.8135 and 0.5168041\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.82088 and 0.50258285\n",
            "Val acc and loss are 0.8152 and 0.5099643\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.82334 and 0.49553993\n",
            "Val acc and loss are 0.8174 and 0.50372773\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.82562 and 0.489053\n",
            "Val acc and loss are 0.82 and 0.49799594\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.82828 and 0.4831244\n",
            "Val acc and loss are 0.82 and 0.492734\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.83032 and 0.4776704\n",
            "Val acc and loss are 0.8214 and 0.4878646\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.8321 and 0.47252294\n",
            "Val acc and loss are 0.8237 and 0.48317245\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.83438 and 0.46769935\n",
            "Val acc and loss are 0.8253 and 0.47874248\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.83642 and 0.4633066\n",
            "Val acc and loss are 0.8272 and 0.47473288\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.83836 and 0.45916536\n",
            "Val acc and loss are 0.8283 and 0.47098094\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.83994 and 0.45517102\n",
            "Val acc and loss are 0.8296 and 0.4673876\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.84144 and 0.4512853\n",
            "Val acc and loss are 0.8307 and 0.46393505\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.84286 and 0.44746464\n",
            "Val acc and loss are 0.8314 and 0.4606247\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.84384 and 0.44377828\n",
            "Val acc and loss are 0.8334 and 0.45752388\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.84468 and 0.44028\n",
            "Val acc and loss are 0.8347 and 0.4546454\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.84508 and 0.43707934\n",
            "Val acc and loss are 0.8358 and 0.4521127\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.84662 and 0.4341485\n",
            "Val acc and loss are 0.8362 and 0.44982252\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.84726 and 0.43144622\n",
            "Val acc and loss are 0.8361 and 0.44767323\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.84804 and 0.4287537\n",
            "Val acc and loss are 0.8369 and 0.4454817\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.84898 and 0.42591023\n",
            "Val acc and loss are 0.8377 and 0.44308233\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.8499 and 0.42302793\n",
            "Val acc and loss are 0.8389 and 0.44062182\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.85126 and 0.42008653\n",
            "Val acc and loss are 0.8394 and 0.43810517\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.85238 and 0.41710824\n",
            "Val acc and loss are 0.84 and 0.43559894\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.85374 and 0.4141435\n",
            "Val acc and loss are 0.8416 and 0.4330876\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.85456 and 0.41124508\n",
            "Val acc and loss are 0.8432 and 0.43060264\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.85568 and 0.40841296\n",
            "Val acc and loss are 0.8441 and 0.42820933\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.85614 and 0.40569237\n",
            "Val acc and loss are 0.8451 and 0.4259544\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.8571 and 0.40314507\n",
            "Val acc and loss are 0.8453 and 0.42393222\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.85788 and 0.40067428\n",
            "Val acc and loss are 0.8471 and 0.422\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.85832 and 0.39843774\n",
            "Val acc and loss are 0.8475 and 0.42030215\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.85918 and 0.39631352\n",
            "Val acc and loss are 0.849 and 0.41866776\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.85998 and 0.3943111\n",
            "Val acc and loss are 0.8494 and 0.41712442\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.86062 and 0.39241025\n",
            "Val acc and loss are 0.8497 and 0.41566768\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.861 and 0.3905683\n",
            "Val acc and loss are 0.8498 and 0.41428515\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.86186 and 0.388716\n",
            "Val acc and loss are 0.851 and 0.41285512\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.86258 and 0.3869114\n",
            "Val acc and loss are 0.8517 and 0.41143844\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.8632 and 0.38509712\n",
            "Val acc and loss are 0.8526 and 0.41000128\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.86424 and 0.38332483\n",
            "Val acc and loss are 0.8533 and 0.40863842\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.86484 and 0.38170397\n",
            "Val acc and loss are 0.8539 and 0.40745613\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.86536 and 0.380054\n",
            "Val acc and loss are 0.8545 and 0.40616894\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.8661 and 0.37843892\n",
            "Val acc and loss are 0.8541 and 0.40491354\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.86656 and 0.37688822\n",
            "Val acc and loss are 0.8549 and 0.40380198\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.86686 and 0.37531507\n",
            "Val acc and loss are 0.8557 and 0.40273243\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.86714 and 0.37376058\n",
            "Val acc and loss are 0.8565 and 0.40164104\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.86778 and 0.37216297\n",
            "Val acc and loss are 0.8571 and 0.40039206\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.86832 and 0.3705473\n",
            "Val acc and loss are 0.8571 and 0.3990399\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.86912 and 0.36902273\n",
            "Val acc and loss are 0.8578 and 0.39780375\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.8697 and 0.3675631\n",
            "Val acc and loss are 0.8578 and 0.39661264\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.86994 and 0.36622384\n",
            "Val acc and loss are 0.8579 and 0.39558086\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.87092 and 0.3648758\n",
            "Val acc and loss are 0.8579 and 0.39457497\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.87134 and 0.36342996\n",
            "Val acc and loss are 0.859 and 0.39346704\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.87184 and 0.36199644\n",
            "Val acc and loss are 0.8594 and 0.39239526\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.8723 and 0.36064935\n",
            "Val acc and loss are 0.8595 and 0.39145148\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.87272 and 0.35933828\n",
            "Val acc and loss are 0.8602 and 0.39056003\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.87312 and 0.35803884\n",
            "Val acc and loss are 0.8604 and 0.3897317\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.8736 and 0.35683802\n",
            "Val acc and loss are 0.8605 and 0.38898483\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.87394 and 0.35560313\n",
            "Val acc and loss are 0.8611 and 0.38816604\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.87438 and 0.3542915\n",
            "Val acc and loss are 0.8614 and 0.38719922\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.87482 and 0.35299367\n",
            "Val acc and loss are 0.8612 and 0.3862462\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.87528 and 0.35177928\n",
            "Val acc and loss are 0.8617 and 0.3853381\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.87602 and 0.35055396\n",
            "Val acc and loss are 0.8624 and 0.38440043\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.8764 and 0.34934422\n",
            "Val acc and loss are 0.8627 and 0.38350078\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.87704 and 0.34817994\n",
            "Val acc and loss are 0.8633 and 0.38266957\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.87746 and 0.34708863\n",
            "Val acc and loss are 0.8634 and 0.3818791\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.87772 and 0.34605345\n",
            "Val acc and loss are 0.8637 and 0.3811056\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.87796 and 0.34503108\n",
            "Val acc and loss are 0.8638 and 0.3803818\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.87806 and 0.3440068\n",
            "Val acc and loss are 0.8641 and 0.37972173\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.87858 and 0.34291422\n",
            "Val acc and loss are 0.8644 and 0.37899166\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.87898 and 0.3418441\n",
            "Val acc and loss are 0.8644 and 0.37826267\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.87914 and 0.34073725\n",
            "Val acc and loss are 0.8644 and 0.37747496\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.87948 and 0.3398082\n",
            "Val acc and loss are 0.8647 and 0.37694848\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.8799 and 0.33896944\n",
            "Val acc and loss are 0.8645 and 0.3764943\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.88008 and 0.33813363\n",
            "Val acc and loss are 0.8647 and 0.3760054\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.88062 and 0.33709008\n",
            "Val acc and loss are 0.8647 and 0.37530574\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.88132 and 0.3359604\n",
            "Val acc and loss are 0.8654 and 0.37445256\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.88162 and 0.33498907\n",
            "Val acc and loss are 0.8656 and 0.37378237\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.88202 and 0.33407703\n",
            "Val acc and loss are 0.866 and 0.37318715\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.88234 and 0.33322087\n",
            "Val acc and loss are 0.8659 and 0.37269625\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.8825 and 0.33242285\n",
            "Val acc and loss are 0.8658 and 0.37231928\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.8828 and 0.33154038\n",
            "Val acc and loss are 0.8662 and 0.371791\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.88308 and 0.33051515\n",
            "Val acc and loss are 0.8668 and 0.37103927\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.88334 and 0.3294484\n",
            "Val acc and loss are 0.8671 and 0.37029356\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.8837 and 0.32844028\n",
            "Val acc and loss are 0.8675 and 0.36959982\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.88402 and 0.32756084\n",
            "Val acc and loss are 0.8674 and 0.36911637\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.88406 and 0.32681876\n",
            "Val acc and loss are 0.8676 and 0.36878243\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.88412 and 0.32606184\n",
            "Val acc and loss are 0.8673 and 0.36831442\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.88438 and 0.3251189\n",
            "Val acc and loss are 0.868 and 0.36757365\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.88428 and 0.32414562\n",
            "Val acc and loss are 0.8674 and 0.36675927\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.88474 and 0.32323354\n",
            "Val acc and loss are 0.8674 and 0.36611286\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.88522 and 0.32245094\n",
            "Val acc and loss are 0.8679 and 0.36564666\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.88572 and 0.32184\n",
            "Val acc and loss are 0.868 and 0.36545843\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.88622 and 0.32116306\n",
            "Val acc and loss are 0.8682 and 0.3651537\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.88656 and 0.32032648\n",
            "Val acc and loss are 0.8682 and 0.36459208\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.8864 and 0.3193594\n",
            "Val acc and loss are 0.8683 and 0.36381015\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.88654 and 0.31855953\n",
            "Val acc and loss are 0.8682 and 0.36333624\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.8866 and 0.31778738\n",
            "Val acc and loss are 0.8681 and 0.36295563\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.88724 and 0.31696036\n",
            "Val acc and loss are 0.8689 and 0.36258614\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.88772 and 0.31611583\n",
            "Val acc and loss are 0.8694 and 0.36208186\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.88786 and 0.31520543\n",
            "Val acc and loss are 0.8694 and 0.36140263\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.88828 and 0.31433782\n",
            "Val acc and loss are 0.8702 and 0.36076343\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.8884 and 0.3135735\n",
            "Val acc and loss are 0.8703 and 0.3602678\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.88834 and 0.3129964\n",
            "Val acc and loss are 0.8704 and 0.36005294\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.8885 and 0.3123826\n",
            "Val acc and loss are 0.8709 and 0.35983723\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.8888 and 0.3117906\n",
            "Val acc and loss are 0.8709 and 0.3596586\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.8891 and 0.31098542\n",
            "Val acc and loss are 0.8707 and 0.35916293\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.88926 and 0.31019843\n",
            "Val acc and loss are 0.871 and 0.35860622\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.88942 and 0.3093742\n",
            "Val acc and loss are 0.8704 and 0.35794288\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.88998 and 0.3086144\n",
            "Val acc and loss are 0.8712 and 0.35747832\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.89018 and 0.30801544\n",
            "Val acc and loss are 0.8714 and 0.35726902\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.89048 and 0.30739802\n",
            "Val acc and loss are 0.8714 and 0.3571033\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.89106 and 0.30665782\n",
            "Val acc and loss are 0.8712 and 0.3567561\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.89142 and 0.30569345\n",
            "Val acc and loss are 0.8722 and 0.35601977\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.89156 and 0.30491138\n",
            "Val acc and loss are 0.8723 and 0.3555105\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.89186 and 0.3042247\n",
            "Val acc and loss are 0.8726 and 0.3551531\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.89186 and 0.3036956\n",
            "Val acc and loss are 0.8725 and 0.3551342\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.89212 and 0.30327278\n",
            "Val acc and loss are 0.8729 and 0.35517958\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.89236 and 0.3024203\n",
            "Val acc and loss are 0.8732 and 0.35460177\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.89284 and 0.30144095\n",
            "Val acc and loss are 0.8731 and 0.35377508\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.89344 and 0.30068392\n",
            "Val acc and loss are 0.8733 and 0.3533102\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.89368 and 0.30016923\n",
            "Val acc and loss are 0.8729 and 0.3532629\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.8938 and 0.2998989\n",
            "Val acc and loss are 0.8737 and 0.35337475\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.89398 and 0.29924512\n",
            "Val acc and loss are 0.8732 and 0.35294273\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.89432 and 0.29824382\n",
            "Val acc and loss are 0.8741 and 0.35204443\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.89456 and 0.2975241\n",
            "Val acc and loss are 0.8742 and 0.35152343\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.89454 and 0.29695272\n",
            "Val acc and loss are 0.8738 and 0.35122448\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.89482 and 0.29643103\n",
            "Val acc and loss are 0.874 and 0.35109383\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.89494 and 0.29595014\n",
            "Val acc and loss are 0.8743 and 0.35103577\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.89504 and 0.29540014\n",
            "Val acc and loss are 0.8739 and 0.35100448\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.89498 and 0.29459178\n",
            "Val acc and loss are 0.8744 and 0.350637\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.89538 and 0.29374945\n",
            "Val acc and loss are 0.8752 and 0.3500082\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.89592 and 0.29306778\n",
            "Val acc and loss are 0.8756 and 0.34959295\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.89598 and 0.29259518\n",
            "Val acc and loss are 0.8749 and 0.34944052\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.89578 and 0.2923612\n",
            "Val acc and loss are 0.8745 and 0.34947485\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.89596 and 0.29184157\n",
            "Val acc and loss are 0.8749 and 0.34918553\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.89618 and 0.2910173\n",
            "Val acc and loss are 0.8747 and 0.34860688\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.89662 and 0.2903965\n",
            "Val acc and loss are 0.8753 and 0.34837458\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.89708 and 0.2898369\n",
            "Val acc and loss are 0.8751 and 0.3482357\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.89706 and 0.28903535\n",
            "Val acc and loss are 0.8751 and 0.34780967\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.89708 and 0.28859654\n",
            "Val acc and loss are 0.8749 and 0.3476643\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.89716 and 0.28796434\n",
            "Val acc and loss are 0.8746 and 0.3473\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.8974 and 0.28718552\n",
            "Val acc and loss are 0.8748 and 0.34683686\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.89802 and 0.2865054\n",
            "Val acc and loss are 0.8755 and 0.34640908\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.8983 and 0.28585386\n",
            "Val acc and loss are 0.8755 and 0.34607187\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.89822 and 0.28519347\n",
            "Val acc and loss are 0.8754 and 0.34584883\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.89822 and 0.28475147\n",
            "Val acc and loss are 0.8753 and 0.34599185\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.89824 and 0.28437588\n",
            "Val acc and loss are 0.8754 and 0.34625\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.89866 and 0.2838264\n",
            "Val acc and loss are 0.8756 and 0.34620747\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.89888 and 0.28288293\n",
            "Val acc and loss are 0.8755 and 0.3453975\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.89916 and 0.28225395\n",
            "Val acc and loss are 0.8757 and 0.34484106\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.89906 and 0.2816108\n",
            "Val acc and loss are 0.8758 and 0.34446108\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.89952 and 0.28114435\n",
            "Val acc and loss are 0.8756 and 0.3444599\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.89964 and 0.28087905\n",
            "Val acc and loss are 0.8763 and 0.3446294\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.90004 and 0.28018218\n",
            "Val acc and loss are 0.8763 and 0.34409273\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.89994 and 0.2793131\n",
            "Val acc and loss are 0.8765 and 0.3432757\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.89996 and 0.27874663\n",
            "Val acc and loss are 0.877 and 0.3428307\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.90024 and 0.27853155\n",
            "Val acc and loss are 0.8766 and 0.34301758\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.90058 and 0.2782266\n",
            "Val acc and loss are 0.8763 and 0.3431526\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.90072 and 0.2775982\n",
            "Val acc and loss are 0.8765 and 0.34292886\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.90118 and 0.27682516\n",
            "Val acc and loss are 0.8767 and 0.342524\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.90124 and 0.27606612\n",
            "Val acc and loss are 0.8772 and 0.34210113\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.90126 and 0.27548793\n",
            "Val acc and loss are 0.8781 and 0.34199366\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.90182 and 0.27498877\n",
            "Val acc and loss are 0.8777 and 0.34193292\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.90198 and 0.27454838\n",
            "Val acc and loss are 0.8776 and 0.34176418\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.90186 and 0.274079\n",
            "Val acc and loss are 0.8777 and 0.34152168\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.90204 and 0.27349716\n",
            "Val acc and loss are 0.878 and 0.34135112\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.90236 and 0.27303126\n",
            "Val acc and loss are 0.8781 and 0.3413298\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.90222 and 0.2725027\n",
            "Val acc and loss are 0.878 and 0.3411306\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.90258 and 0.2718711\n",
            "Val acc and loss are 0.8777 and 0.34070474\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.90286 and 0.27139083\n",
            "Val acc and loss are 0.8776 and 0.3405454\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.90314 and 0.2710111\n",
            "Val acc and loss are 0.8781 and 0.34051865\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.9032 and 0.27050465\n",
            "Val acc and loss are 0.8782 and 0.34031615\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.90344 and 0.2698455\n",
            "Val acc and loss are 0.8783 and 0.339818\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.90364 and 0.2691298\n",
            "Val acc and loss are 0.8791 and 0.33921885\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.90354 and 0.26852095\n",
            "Val acc and loss are 0.8793 and 0.33882448\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.90382 and 0.2680978\n",
            "Val acc and loss are 0.8788 and 0.33884388\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.90426 and 0.26775005\n",
            "Val acc and loss are 0.8789 and 0.33903414\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.90422 and 0.2674581\n",
            "Val acc and loss are 0.879 and 0.3392633\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.90418 and 0.2668211\n",
            "Val acc and loss are 0.8791 and 0.33892587\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.90444 and 0.26603004\n",
            "Val acc and loss are 0.8785 and 0.33823746\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.9043 and 0.26553577\n",
            "Val acc and loss are 0.878 and 0.33810943\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.90476 and 0.26515406\n",
            "Val acc and loss are 0.878 and 0.33825827\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.90516 and 0.26489615\n",
            "Val acc and loss are 0.8785 and 0.33848602\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.90566 and 0.26434052\n",
            "Val acc and loss are 0.8787 and 0.33823445\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.90574 and 0.26364723\n",
            "Val acc and loss are 0.8791 and 0.33765212\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.90556 and 0.26320437\n",
            "Val acc and loss are 0.8785 and 0.33739233\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.90578 and 0.2628451\n",
            "Val acc and loss are 0.8789 and 0.33749598\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.90588 and 0.26282823\n",
            "Val acc and loss are 0.8785 and 0.33792862\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.906 and 0.2621055\n",
            "Val acc and loss are 0.8788 and 0.3373182\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.9063 and 0.26134574\n",
            "Val acc and loss are 0.8791 and 0.33654794\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.9064 and 0.26089087\n",
            "Val acc and loss are 0.8791 and 0.33648992\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.90676 and 0.26046765\n",
            "Val acc and loss are 0.8796 and 0.33663544\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.9069 and 0.2602342\n",
            "Val acc and loss are 0.88 and 0.3369368\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.90706 and 0.25941128\n",
            "Val acc and loss are 0.8799 and 0.33636096\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.90728 and 0.25871995\n",
            "Val acc and loss are 0.8797 and 0.33578944\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.90778 and 0.2582812\n",
            "Val acc and loss are 0.8797 and 0.33578095\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.9079 and 0.25796986\n",
            "Val acc and loss are 0.8789 and 0.33598506\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.90786 and 0.25772384\n",
            "Val acc and loss are 0.8788 and 0.33619443\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.9078 and 0.25705844\n",
            "Val acc and loss are 0.8792 and 0.33571124\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.90808 and 0.25618967\n",
            "Val acc and loss are 0.8797 and 0.3347509\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.9084 and 0.25569594\n",
            "Val acc and loss are 0.8796 and 0.33437383\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.90826 and 0.2554172\n",
            "Val acc and loss are 0.8797 and 0.3345535\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.90866 and 0.25517425\n",
            "Val acc and loss are 0.8795 and 0.33469194\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.909 and 0.2547021\n",
            "Val acc and loss are 0.8798 and 0.33448568\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.9092 and 0.25422868\n",
            "Val acc and loss are 0.8798 and 0.3342594\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.90936 and 0.25378153\n",
            "Val acc and loss are 0.8808 and 0.33405986\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.90962 and 0.25322685\n",
            "Val acc and loss are 0.8799 and 0.33382937\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.90978 and 0.25278595\n",
            "Val acc and loss are 0.8809 and 0.33386156\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.90994 and 0.25201648\n",
            "Val acc and loss are 0.8806 and 0.33340204\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.91048 and 0.25141975\n",
            "Val acc and loss are 0.8798 and 0.3331711\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.9106 and 0.25096953\n",
            "Val acc and loss are 0.88 and 0.33302066\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.91048 and 0.25058907\n",
            "Val acc and loss are 0.8802 and 0.33306098\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.91068 and 0.25033575\n",
            "Val acc and loss are 0.8802 and 0.33321983\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.9107 and 0.25016114\n",
            "Val acc and loss are 0.8797 and 0.3334458\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.91102 and 0.24970382\n",
            "Val acc and loss are 0.8799 and 0.33323368\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.91122 and 0.24913098\n",
            "Val acc and loss are 0.88 and 0.33272105\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.9113 and 0.24862367\n",
            "Val acc and loss are 0.8798 and 0.33242345\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.91122 and 0.24822414\n",
            "Val acc and loss are 0.8802 and 0.3325381\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.91152 and 0.24786691\n",
            "Val acc and loss are 0.8799 and 0.33269748\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.91166 and 0.24716933\n",
            "Val acc and loss are 0.8796 and 0.33224237\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.9121 and 0.24652696\n",
            "Val acc and loss are 0.8795 and 0.33180618\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.91234 and 0.24603055\n",
            "Val acc and loss are 0.8798 and 0.3316993\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.91256 and 0.24568449\n",
            "Val acc and loss are 0.8793 and 0.33167842\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.91248 and 0.24508223\n",
            "Val acc and loss are 0.8795 and 0.331307\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.9124 and 0.24468902\n",
            "Val acc and loss are 0.8801 and 0.33107254\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.91298 and 0.24435687\n",
            "Val acc and loss are 0.8798 and 0.33094323\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.91276 and 0.24408834\n",
            "Val acc and loss are 0.8799 and 0.33081666\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.91274 and 0.24378008\n",
            "Val acc and loss are 0.8796 and 0.33067375\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.91286 and 0.24331988\n",
            "Val acc and loss are 0.8804 and 0.33048043\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.91314 and 0.24298233\n",
            "Val acc and loss are 0.8807 and 0.3305658\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.91312 and 0.24255903\n",
            "Val acc and loss are 0.881 and 0.33058548\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.91356 and 0.24232627\n",
            "Val acc and loss are 0.8806 and 0.33092862\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.9136 and 0.24185885\n",
            "Val acc and loss are 0.8805 and 0.330837\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.91384 and 0.24110457\n",
            "Val acc and loss are 0.8806 and 0.33007348\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.91396 and 0.24061434\n",
            "Val acc and loss are 0.8805 and 0.3298167\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.91424 and 0.24025133\n",
            "Val acc and loss are 0.8808 and 0.32993862\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.91428 and 0.23999651\n",
            "Val acc and loss are 0.8808 and 0.3302075\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.91464 and 0.23951206\n",
            "Val acc and loss are 0.8809 and 0.33015448\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.91476 and 0.23877707\n",
            "Val acc and loss are 0.8816 and 0.32969984\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.91504 and 0.23821712\n",
            "Val acc and loss are 0.8817 and 0.32964826\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.91514 and 0.23787312\n",
            "Val acc and loss are 0.8818 and 0.32993105\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.9152 and 0.2376169\n",
            "Val acc and loss are 0.8813 and 0.33011475\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.91494 and 0.23730086\n",
            "Val acc and loss are 0.8809 and 0.33002672\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.91518 and 0.23678395\n",
            "Val acc and loss are 0.8817 and 0.32948482\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.9153 and 0.23637067\n",
            "Val acc and loss are 0.8816 and 0.32920527\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.91572 and 0.2361063\n",
            "Val acc and loss are 0.8811 and 0.32941836\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.9159 and 0.23602231\n",
            "Val acc and loss are 0.8812 and 0.32990578\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.91608 and 0.23548019\n",
            "Val acc and loss are 0.8814 and 0.32964286\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.91656 and 0.234707\n",
            "Val acc and loss are 0.8825 and 0.328907\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.91668 and 0.23404996\n",
            "Val acc and loss are 0.8827 and 0.32839838\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.9166 and 0.23359337\n",
            "Val acc and loss are 0.8825 and 0.3283106\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.9168 and 0.23313542\n",
            "Val acc and loss are 0.882 and 0.32819125\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.91694 and 0.23258758\n",
            "Val acc and loss are 0.8823 and 0.32795113\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.91712 and 0.23223989\n",
            "Val acc and loss are 0.8829 and 0.3278982\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.91704 and 0.23197378\n",
            "Val acc and loss are 0.8828 and 0.3279105\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.9169 and 0.23178558\n",
            "Val acc and loss are 0.8817 and 0.32804877\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.91706 and 0.2315477\n",
            "Val acc and loss are 0.8823 and 0.32817823\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.91726 and 0.23106375\n",
            "Val acc and loss are 0.8826 and 0.3277835\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.91734 and 0.23067476\n",
            "Val acc and loss are 0.8824 and 0.3275138\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.91746 and 0.23019941\n",
            "Val acc and loss are 0.8825 and 0.32733488\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.91752 and 0.22992879\n",
            "Val acc and loss are 0.8823 and 0.3275047\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.91804 and 0.22961994\n",
            "Val acc and loss are 0.8828 and 0.32767007\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.91792 and 0.22894968\n",
            "Val acc and loss are 0.8825 and 0.32729727\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.91828 and 0.22840066\n",
            "Val acc and loss are 0.8826 and 0.32700297\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.91836 and 0.22784328\n",
            "Val acc and loss are 0.8825 and 0.32654572\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.91832 and 0.22746898\n",
            "Val acc and loss are 0.8825 and 0.32662672\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.91846 and 0.22751243\n",
            "Val acc and loss are 0.8829 and 0.32728988\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.91864 and 0.22727743\n",
            "Val acc and loss are 0.8823 and 0.32748783\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.91888 and 0.226585\n",
            "Val acc and loss are 0.883 and 0.32676807\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.91918 and 0.22614047\n",
            "Val acc and loss are 0.8828 and 0.32638806\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.91946 and 0.22577484\n",
            "Val acc and loss are 0.8827 and 0.32653797\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.91938 and 0.2256431\n",
            "Val acc and loss are 0.8826 and 0.32708746\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.91944 and 0.2253985\n",
            "Val acc and loss are 0.8826 and 0.32721347\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.91978 and 0.22469176\n",
            "Val acc and loss are 0.8827 and 0.32654312\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.91964 and 0.22426735\n",
            "Val acc and loss are 0.8822 and 0.32612956\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.91992 and 0.22408323\n",
            "Val acc and loss are 0.8824 and 0.3266005\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.9199 and 0.22410545\n",
            "Val acc and loss are 0.8832 and 0.32726005\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.9201 and 0.22362648\n",
            "Val acc and loss are 0.8832 and 0.3270841\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.92022 and 0.22288811\n",
            "Val acc and loss are 0.8836 and 0.3262521\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.9205 and 0.222365\n",
            "Val acc and loss are 0.8834 and 0.32590562\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.9205 and 0.2219694\n",
            "Val acc and loss are 0.883 and 0.3262251\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.92072 and 0.22184004\n",
            "Val acc and loss are 0.883 and 0.3267673\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.92086 and 0.22126324\n",
            "Val acc and loss are 0.8824 and 0.3264816\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.92104 and 0.22074218\n",
            "Val acc and loss are 0.8824 and 0.32606366\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.92166 and 0.22014864\n",
            "Val acc and loss are 0.8825 and 0.32545605\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.92162 and 0.2198528\n",
            "Val acc and loss are 0.8826 and 0.32547534\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.92174 and 0.21954793\n",
            "Val acc and loss are 0.8832 and 0.32557046\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.92172 and 0.2192701\n",
            "Val acc and loss are 0.8823 and 0.32574952\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.92198 and 0.21899915\n",
            "Val acc and loss are 0.8831 and 0.32587636\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.92212 and 0.21830107\n",
            "Val acc and loss are 0.8831 and 0.3254331\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.92238 and 0.21806914\n",
            "Val acc and loss are 0.8829 and 0.32562104\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.92256 and 0.21800955\n",
            "Val acc and loss are 0.8825 and 0.32601422\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.9228 and 0.21749398\n",
            "Val acc and loss are 0.883 and 0.32567206\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.92292 and 0.21732864\n",
            "Val acc and loss are 0.8834 and 0.32555133\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.92268 and 0.21687122\n",
            "Val acc and loss are 0.8835 and 0.32558438\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.92292 and 0.21669003\n",
            "Val acc and loss are 0.8839 and 0.32582706\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.92288 and 0.21627127\n",
            "Val acc and loss are 0.8839 and 0.32578346\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.92288 and 0.21547262\n",
            "Val acc and loss are 0.8833 and 0.32520074\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.92326 and 0.21496941\n",
            "Val acc and loss are 0.8831 and 0.324956\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.92346 and 0.21490295\n",
            "Val acc and loss are 0.8834 and 0.32547224\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.92378 and 0.21460795\n",
            "Val acc and loss are 0.8838 and 0.3255102\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.92372 and 0.21392353\n",
            "Val acc and loss are 0.8845 and 0.32485577\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.9235 and 0.21368887\n",
            "Val acc and loss are 0.8845 and 0.32477868\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.92372 and 0.21340553\n",
            "Val acc and loss are 0.8841 and 0.32489753\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.92412 and 0.21324582\n",
            "Val acc and loss are 0.8844 and 0.32523754\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.92436 and 0.21270421\n",
            "Val acc and loss are 0.8846 and 0.32494584\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.92418 and 0.21231334\n",
            "Val acc and loss are 0.884 and 0.32491416\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.92434 and 0.21222578\n",
            "Val acc and loss are 0.8844 and 0.32537445\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.92454 and 0.21191041\n",
            "Val acc and loss are 0.8845 and 0.32551244\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.92472 and 0.21151562\n",
            "Val acc and loss are 0.884 and 0.3252642\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.92492 and 0.21103394\n",
            "Val acc and loss are 0.8844 and 0.3248191\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.92504 and 0.21062157\n",
            "Val acc and loss are 0.8843 and 0.3245267\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.92522 and 0.2100576\n",
            "Val acc and loss are 0.8835 and 0.32399824\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.92558 and 0.20973432\n",
            "Val acc and loss are 0.8839 and 0.32397652\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.926 and 0.20934114\n",
            "Val acc and loss are 0.8844 and 0.32384184\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.92588 and 0.20899087\n",
            "Val acc and loss are 0.8845 and 0.32380524\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.9259 and 0.20855454\n",
            "Val acc and loss are 0.8843 and 0.32354072\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.92578 and 0.20831293\n",
            "Val acc and loss are 0.885 and 0.32359764\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.92576 and 0.20808989\n",
            "Val acc and loss are 0.8842 and 0.3236903\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.92608 and 0.2076343\n",
            "Val acc and loss are 0.8846 and 0.32324985\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.92622 and 0.20733172\n",
            "Val acc and loss are 0.8845 and 0.323143\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.92644 and 0.20709318\n",
            "Val acc and loss are 0.8844 and 0.3233414\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.92624 and 0.20672318\n",
            "Val acc and loss are 0.8843 and 0.32363632\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.92644 and 0.20625967\n",
            "Val acc and loss are 0.8846 and 0.32362217\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.9262 and 0.20600876\n",
            "Val acc and loss are 0.8846 and 0.3238182\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.92676 and 0.20560618\n",
            "Val acc and loss are 0.8847 and 0.3236374\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.92716 and 0.20508415\n",
            "Val acc and loss are 0.8851 and 0.32317215\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.92714 and 0.20475304\n",
            "Val acc and loss are 0.8849 and 0.32298422\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.92738 and 0.20439683\n",
            "Val acc and loss are 0.885 and 0.32271454\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.92774 and 0.20399968\n",
            "Val acc and loss are 0.8846 and 0.32252455\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.92784 and 0.20374128\n",
            "Val acc and loss are 0.8854 and 0.32267502\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.92782 and 0.20381568\n",
            "Val acc and loss are 0.8853 and 0.3233879\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.92782 and 0.20358898\n",
            "Val acc and loss are 0.8852 and 0.32333165\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.9281 and 0.20327945\n",
            "Val acc and loss are 0.8849 and 0.32322583\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.9282 and 0.20306572\n",
            "Val acc and loss are 0.8843 and 0.32339755\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.9283 and 0.2027952\n",
            "Val acc and loss are 0.8845 and 0.32355526\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.92886 and 0.20219684\n",
            "Val acc and loss are 0.8844 and 0.3233296\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.92864 and 0.20173812\n",
            "Val acc and loss are 0.8851 and 0.3232771\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.92852 and 0.20133032\n",
            "Val acc and loss are 0.8858 and 0.32324478\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.929 and 0.20089328\n",
            "Val acc and loss are 0.8853 and 0.32312796\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.9292 and 0.20064257\n",
            "Val acc and loss are 0.8856 and 0.3231596\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.92926 and 0.2002778\n",
            "Val acc and loss are 0.8854 and 0.32302633\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.9294 and 0.19979712\n",
            "Val acc and loss are 0.8857 and 0.3228782\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.9296 and 0.19956611\n",
            "Val acc and loss are 0.8863 and 0.32315272\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.9294 and 0.19942941\n",
            "Val acc and loss are 0.8863 and 0.32381982\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.9291 and 0.19942258\n",
            "Val acc and loss are 0.8862 and 0.32441983\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.9292 and 0.19878377\n",
            "Val acc and loss are 0.8861 and 0.32371065\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.92958 and 0.1980978\n",
            "Val acc and loss are 0.8857 and 0.3227714\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.93002 and 0.19769216\n",
            "Val acc and loss are 0.8854 and 0.32279798\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.93022 and 0.19751601\n",
            "Val acc and loss are 0.8855 and 0.32326674\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.93028 and 0.19735344\n",
            "Val acc and loss are 0.8856 and 0.32365125\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.93078 and 0.19679637\n",
            "Val acc and loss are 0.8859 and 0.3232051\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.9311 and 0.1961809\n",
            "Val acc and loss are 0.8859 and 0.32235652\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.93058 and 0.19602129\n",
            "Val acc and loss are 0.8854 and 0.32238418\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.93038 and 0.19595976\n",
            "Val acc and loss are 0.8855 and 0.32304922\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.93046 and 0.19580083\n",
            "Val acc and loss are 0.8857 and 0.32346645\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.9308 and 0.19528913\n",
            "Val acc and loss are 0.8855 and 0.323264\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.93136 and 0.19478984\n",
            "Val acc and loss are 0.8855 and 0.32315445\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.93128 and 0.19482179\n",
            "Val acc and loss are 0.8862 and 0.32368538\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.93152 and 0.19430996\n",
            "Val acc and loss are 0.8858 and 0.3229663\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.93158 and 0.1937785\n",
            "Val acc and loss are 0.8858 and 0.32255352\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.93156 and 0.19345905\n",
            "Val acc and loss are 0.8857 and 0.3224033\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.93108 and 0.19331428\n",
            "Val acc and loss are 0.8861 and 0.32262826\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.9312 and 0.19311555\n",
            "Val acc and loss are 0.886 and 0.32285914\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.93162 and 0.19272493\n",
            "Val acc and loss are 0.8868 and 0.32272607\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.93198 and 0.19239192\n",
            "Val acc and loss are 0.8863 and 0.32262823\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.93202 and 0.19226396\n",
            "Val acc and loss are 0.8862 and 0.3229784\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.93222 and 0.19173071\n",
            "Val acc and loss are 0.8861 and 0.32272193\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.9322 and 0.1914131\n",
            "Val acc and loss are 0.8854 and 0.32277006\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.93226 and 0.19100018\n",
            "Val acc and loss are 0.886 and 0.32253778\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.9321 and 0.190871\n",
            "Val acc and loss are 0.8867 and 0.32248843\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.9326 and 0.1906514\n",
            "Val acc and loss are 0.8868 and 0.32234073\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.93288 and 0.19029683\n",
            "Val acc and loss are 0.8861 and 0.32199547\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.93314 and 0.19019082\n",
            "Val acc and loss are 0.8861 and 0.32234335\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.93278 and 0.19015884\n",
            "Val acc and loss are 0.8869 and 0.32291913\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.93314 and 0.1896886\n",
            "Val acc and loss are 0.8874 and 0.32265767\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.93294 and 0.18902518\n",
            "Val acc and loss are 0.8873 and 0.32214794\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.9333 and 0.18851712\n",
            "Val acc and loss are 0.8869 and 0.32205528\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.9334 and 0.18860754\n",
            "Val acc and loss are 0.886 and 0.32287902\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.93346 and 0.1882897\n",
            "Val acc and loss are 0.8861 and 0.32301742\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.93374 and 0.18792586\n",
            "Val acc and loss are 0.8872 and 0.32296753\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.93394 and 0.18752998\n",
            "Val acc and loss are 0.8878 and 0.32258436\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.93392 and 0.18711177\n",
            "Val acc and loss are 0.8872 and 0.32230943\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.93382 and 0.18707648\n",
            "Val acc and loss are 0.8862 and 0.32262808\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.93384 and 0.18687578\n",
            "Val acc and loss are 0.8865 and 0.3227726\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.93428 and 0.18622525\n",
            "Val acc and loss are 0.8873 and 0.32237035\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.93468 and 0.18576418\n",
            "Val acc and loss are 0.8873 and 0.32189205\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.93488 and 0.18545353\n",
            "Val acc and loss are 0.8878 and 0.32198894\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.93474 and 0.1855348\n",
            "Val acc and loss are 0.8875 and 0.32267785\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.9347 and 0.18526693\n",
            "Val acc and loss are 0.8877 and 0.3226245\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.93484 and 0.18491402\n",
            "Val acc and loss are 0.8868 and 0.3221813\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.935 and 0.18470992\n",
            "Val acc and loss are 0.8876 and 0.32209766\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.93482 and 0.18449676\n",
            "Val acc and loss are 0.8875 and 0.32244623\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.93452 and 0.18470195\n",
            "Val acc and loss are 0.8868 and 0.32353437\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.93454 and 0.18402559\n",
            "Val acc and loss are 0.8876 and 0.32330844\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.93516 and 0.18348934\n",
            "Val acc and loss are 0.8874 and 0.3230116\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.93528 and 0.18289559\n",
            "Val acc and loss are 0.8872 and 0.32281366\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.93534 and 0.18277629\n",
            "Val acc and loss are 0.8868 and 0.3232845\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.93526 and 0.18264805\n",
            "Val acc and loss are 0.8862 and 0.32336304\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.93562 and 0.18209934\n",
            "Val acc and loss are 0.8874 and 0.32268453\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.93602 and 0.18178836\n",
            "Val acc and loss are 0.8883 and 0.32243475\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.93558 and 0.18167914\n",
            "Val acc and loss are 0.8892 and 0.32279047\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.93594 and 0.18136153\n",
            "Val acc and loss are 0.8881 and 0.32273957\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.9363 and 0.18103893\n",
            "Val acc and loss are 0.8876 and 0.32276914\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.93622 and 0.18052815\n",
            "Val acc and loss are 0.8877 and 0.32222167\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.9365 and 0.18008734\n",
            "Val acc and loss are 0.8874 and 0.32209778\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.93624 and 0.1800294\n",
            "Val acc and loss are 0.8876 and 0.32274345\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.93596 and 0.18007797\n",
            "Val acc and loss are 0.8873 and 0.3234731\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.93644 and 0.17940016\n",
            "Val acc and loss are 0.8872 and 0.32237554\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.93696 and 0.1792546\n",
            "Val acc and loss are 0.8882 and 0.32205033\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.93676 and 0.1792383\n",
            "Val acc and loss are 0.8874 and 0.32286444\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.93654 and 0.17929105\n",
            "Val acc and loss are 0.888 and 0.32354385\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.9371 and 0.17848843\n",
            "Val acc and loss are 0.8873 and 0.3226735\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.93718 and 0.17796938\n",
            "Val acc and loss are 0.8874 and 0.32177123\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.9372 and 0.17775613\n",
            "Val acc and loss are 0.8871 and 0.32196215\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.93708 and 0.17773172\n",
            "Val acc and loss are 0.8868 and 0.32266203\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.93704 and 0.17756051\n",
            "Val acc and loss are 0.887 and 0.3229228\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.93776 and 0.17688622\n",
            "Val acc and loss are 0.8886 and 0.3222484\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.93804 and 0.1764207\n",
            "Val acc and loss are 0.888 and 0.32205525\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.93836 and 0.17604885\n",
            "Val acc and loss are 0.8878 and 0.3220757\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.9385 and 0.17578162\n",
            "Val acc and loss are 0.8877 and 0.3221332\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.93856 and 0.17562191\n",
            "Val acc and loss are 0.8874 and 0.32235828\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.9385 and 0.17521271\n",
            "Val acc and loss are 0.888 and 0.32212842\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.93842 and 0.17495282\n",
            "Val acc and loss are 0.8886 and 0.3222589\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.93854 and 0.17461114\n",
            "Val acc and loss are 0.888 and 0.3222114\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.93874 and 0.1743491\n",
            "Val acc and loss are 0.888 and 0.32208702\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.93842 and 0.17429188\n",
            "Val acc and loss are 0.8881 and 0.32228276\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.93876 and 0.17414807\n",
            "Val acc and loss are 0.8886 and 0.32250944\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.93904 and 0.17374672\n",
            "Val acc and loss are 0.8882 and 0.3221989\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.93876 and 0.17317446\n",
            "Val acc and loss are 0.8884 and 0.32143292\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.93862 and 0.1729825\n",
            "Val acc and loss are 0.8894 and 0.3215013\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.9389 and 0.17301711\n",
            "Val acc and loss are 0.8885 and 0.32241315\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.939 and 0.17268805\n",
            "Val acc and loss are 0.8883 and 0.3224913\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.93948 and 0.17212366\n",
            "Val acc and loss are 0.889 and 0.3217764\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.9396 and 0.17189686\n",
            "Val acc and loss are 0.8894 and 0.32171732\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.93952 and 0.17185444\n",
            "Val acc and loss are 0.8888 and 0.32245934\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.93966 and 0.17155582\n",
            "Val acc and loss are 0.8882 and 0.3228067\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.93988 and 0.17104696\n",
            "Val acc and loss are 0.8876 and 0.32249305\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.94004 and 0.17045836\n",
            "Val acc and loss are 0.8876 and 0.32215923\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.93992 and 0.17039338\n",
            "Val acc and loss are 0.8889 and 0.3224718\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.94014 and 0.170415\n",
            "Val acc and loss are 0.8879 and 0.3232411\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.94042 and 0.17026551\n",
            "Val acc and loss are 0.8884 and 0.32329413\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.94028 and 0.16986136\n",
            "Val acc and loss are 0.8882 and 0.32290468\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.94006 and 0.16955233\n",
            "Val acc and loss are 0.8883 and 0.3233041\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.94018 and 0.16944233\n",
            "Val acc and loss are 0.8877 and 0.3240985\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.94076 and 0.16901903\n",
            "Val acc and loss are 0.8875 and 0.3242345\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.94068 and 0.1687891\n",
            "Val acc and loss are 0.8883 and 0.32398197\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.94108 and 0.16821438\n",
            "Val acc and loss are 0.8889 and 0.32335535\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.9406 and 0.16834322\n",
            "Val acc and loss are 0.8888 and 0.32361197\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.94106 and 0.1678926\n",
            "Val acc and loss are 0.8892 and 0.32354155\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.94118 and 0.16755079\n",
            "Val acc and loss are 0.8885 and 0.32309413\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.9412 and 0.16711056\n",
            "Val acc and loss are 0.8887 and 0.3225463\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.94106 and 0.16696882\n",
            "Val acc and loss are 0.8891 and 0.3227353\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.94102 and 0.16691688\n",
            "Val acc and loss are 0.8888 and 0.3232523\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.94142 and 0.1668307\n",
            "Val acc and loss are 0.8891 and 0.3235951\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.94182 and 0.16657668\n",
            "Val acc and loss are 0.8887 and 0.32369176\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.94174 and 0.16608351\n",
            "Val acc and loss are 0.8889 and 0.32329443\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.94162 and 0.16598125\n",
            "Val acc and loss are 0.8878 and 0.3237752\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.94198 and 0.16559045\n",
            "Val acc and loss are 0.8875 and 0.32372016\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.942 and 0.16507742\n",
            "Val acc and loss are 0.8881 and 0.32324004\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.9419 and 0.16489123\n",
            "Val acc and loss are 0.8885 and 0.32296053\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.94204 and 0.1647264\n",
            "Val acc and loss are 0.8892 and 0.3229284\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.94222 and 0.16453846\n",
            "Val acc and loss are 0.8892 and 0.32296082\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.9423 and 0.16424447\n",
            "Val acc and loss are 0.889 and 0.32322267\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.94268 and 0.1637926\n",
            "Val acc and loss are 0.889 and 0.3231446\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.94292 and 0.16339214\n",
            "Val acc and loss are 0.8896 and 0.3230772\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.94312 and 0.16311714\n",
            "Val acc and loss are 0.8896 and 0.32288316\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.94294 and 0.16328685\n",
            "Val acc and loss are 0.8891 and 0.3234531\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.94282 and 0.16291943\n",
            "Val acc and loss are 0.8893 and 0.32308197\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.94324 and 0.1624553\n",
            "Val acc and loss are 0.8892 and 0.32237777\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.94326 and 0.16222559\n",
            "Val acc and loss are 0.8889 and 0.3226248\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.94304 and 0.16236538\n",
            "Val acc and loss are 0.8899 and 0.3233377\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.94292 and 0.16182144\n",
            "Val acc and loss are 0.8896 and 0.32279092\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.9434 and 0.16106375\n",
            "Val acc and loss are 0.8901 and 0.32158133\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.94366 and 0.16067292\n",
            "Val acc and loss are 0.8898 and 0.32134667\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.94338 and 0.16103654\n",
            "Val acc and loss are 0.8896 and 0.32292074\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.94344 and 0.16105002\n",
            "Val acc and loss are 0.8893 and 0.3233865\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.94396 and 0.16028757\n",
            "Val acc and loss are 0.89 and 0.32203004\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.94366 and 0.16018493\n",
            "Val acc and loss are 0.8903 and 0.32205883\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.94364 and 0.16044414\n",
            "Val acc and loss are 0.8901 and 0.323902\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.94344 and 0.16076392\n",
            "Val acc and loss are 0.8885 and 0.32503664\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.9436 and 0.1596918\n",
            "Val acc and loss are 0.8887 and 0.32327175\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.94338 and 0.15947308\n",
            "Val acc and loss are 0.8892 and 0.32270697\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.94374 and 0.15910837\n",
            "Val acc and loss are 0.8894 and 0.3233855\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.94362 and 0.15964639\n",
            "Val acc and loss are 0.8891 and 0.32506156\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.9442 and 0.15867153\n",
            "Val acc and loss are 0.8892 and 0.32358688\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.94442 and 0.15797104\n",
            "Val acc and loss are 0.8897 and 0.32257003\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.9443 and 0.15791185\n",
            "Val acc and loss are 0.8899 and 0.32327256\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.94392 and 0.15824015\n",
            "Val acc and loss are 0.8896 and 0.3243669\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.94454 and 0.1577065\n",
            "Val acc and loss are 0.89 and 0.323646\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.9447 and 0.15757039\n",
            "Val acc and loss are 0.8893 and 0.3229346\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.94506 and 0.15703203\n",
            "Val acc and loss are 0.8896 and 0.32281846\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.94508 and 0.15777428\n",
            "Val acc and loss are 0.8892 and 0.32494178\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.945 and 0.15716647\n",
            "Val acc and loss are 0.8893 and 0.32490283\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.9453 and 0.15633519\n",
            "Val acc and loss are 0.8892 and 0.3232084\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.94548 and 0.15575574\n",
            "Val acc and loss are 0.89 and 0.32199252\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.9451 and 0.15605085\n",
            "Val acc and loss are 0.89 and 0.32307464\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.9451 and 0.15599878\n",
            "Val acc and loss are 0.8896 and 0.3237739\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.94542 and 0.15542202\n",
            "Val acc and loss are 0.8898 and 0.32317016\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.94576 and 0.15495105\n",
            "Val acc and loss are 0.8898 and 0.3227002\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.9455 and 0.1548788\n",
            "Val acc and loss are 0.8899 and 0.323462\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.94566 and 0.1550333\n",
            "Val acc and loss are 0.8893 and 0.32433328\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.94606 and 0.15388781\n",
            "Val acc and loss are 0.8902 and 0.32312617\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.9461 and 0.15360421\n",
            "Val acc and loss are 0.8905 and 0.32277846\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.9462 and 0.15347125\n",
            "Val acc and loss are 0.8906 and 0.32311964\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.9462 and 0.15419757\n",
            "Val acc and loss are 0.8896 and 0.3247309\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.94618 and 0.15337059\n",
            "Val acc and loss are 0.8899 and 0.32387218\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.94686 and 0.1528504\n",
            "Val acc and loss are 0.8912 and 0.32281202\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.94726 and 0.15237708\n",
            "Val acc and loss are 0.8909 and 0.3226056\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.94698 and 0.15252736\n",
            "Val acc and loss are 0.8903 and 0.3236563\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.94696 and 0.15214983\n",
            "Val acc and loss are 0.8904 and 0.3234978\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.94642 and 0.1519928\n",
            "Val acc and loss are 0.8905 and 0.32333922\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.94662 and 0.15175748\n",
            "Val acc and loss are 0.8904 and 0.32322735\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.94648 and 0.1517336\n",
            "Val acc and loss are 0.8895 and 0.3236468\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.94668 and 0.15154043\n",
            "Val acc and loss are 0.8899 and 0.32404664\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.94724 and 0.15095371\n",
            "Val acc and loss are 0.8906 and 0.3235416\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.94724 and 0.15051624\n",
            "Val acc and loss are 0.8901 and 0.32297924\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.94754 and 0.15024318\n",
            "Val acc and loss are 0.8901 and 0.32294858\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.94742 and 0.15028138\n",
            "Val acc and loss are 0.8906 and 0.32352915\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.9475 and 0.15000059\n",
            "Val acc and loss are 0.8902 and 0.3231577\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.94756 and 0.14984922\n",
            "Val acc and loss are 0.8906 and 0.32295683\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.94726 and 0.14992423\n",
            "Val acc and loss are 0.8903 and 0.32379386\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.94728 and 0.15006195\n",
            "Val acc and loss are 0.8901 and 0.32462215\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.94748 and 0.1493433\n",
            "Val acc and loss are 0.8912 and 0.32370082\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.94828 and 0.14885017\n",
            "Val acc and loss are 0.8909 and 0.32305592\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.94828 and 0.14860182\n",
            "Val acc and loss are 0.8901 and 0.3236409\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.94786 and 0.14868641\n",
            "Val acc and loss are 0.8901 and 0.3245453\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.948 and 0.1483485\n",
            "Val acc and loss are 0.8902 and 0.32421649\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.94828 and 0.1477935\n",
            "Val acc and loss are 0.8907 and 0.3233122\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.94842 and 0.14766634\n",
            "Val acc and loss are 0.89 and 0.32313907\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.94848 and 0.14760076\n",
            "Val acc and loss are 0.8903 and 0.32350677\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.94828 and 0.14790258\n",
            "Val acc and loss are 0.8907 and 0.3243709\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.9485 and 0.147297\n",
            "Val acc and loss are 0.8905 and 0.32397795\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.94858 and 0.1473371\n",
            "Val acc and loss are 0.8904 and 0.32403275\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.94862 and 0.1472601\n",
            "Val acc and loss are 0.8907 and 0.3246676\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.9484 and 0.1471897\n",
            "Val acc and loss are 0.8904 and 0.3252802\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.94852 and 0.14651117\n",
            "Val acc and loss are 0.8907 and 0.32459617\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.94898 and 0.14588304\n",
            "Val acc and loss are 0.8901 and 0.3235955\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.94912 and 0.14563388\n",
            "Val acc and loss are 0.8908 and 0.32367003\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.9494 and 0.1456192\n",
            "Val acc and loss are 0.8907 and 0.32418823\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.94902 and 0.1456178\n",
            "Val acc and loss are 0.8905 and 0.32420328\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.94922 and 0.1451237\n",
            "Val acc and loss are 0.8903 and 0.32336366\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.94986 and 0.14459842\n",
            "Val acc and loss are 0.891 and 0.3231414\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.9498 and 0.14460753\n",
            "Val acc and loss are 0.8909 and 0.324131\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.94964 and 0.14453724\n",
            "Val acc and loss are 0.8907 and 0.3243741\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.94996 and 0.1441363\n",
            "Val acc and loss are 0.8904 and 0.32391915\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.9499 and 0.14395481\n",
            "Val acc and loss are 0.8901 and 0.32395\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.94996 and 0.14390567\n",
            "Val acc and loss are 0.8903 and 0.32482123\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.94996 and 0.14390306\n",
            "Val acc and loss are 0.8897 and 0.32576668\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.94998 and 0.14342277\n",
            "Val acc and loss are 0.8899 and 0.32532915\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.95016 and 0.14287935\n",
            "Val acc and loss are 0.8897 and 0.3247877\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.95016 and 0.1426114\n",
            "Val acc and loss are 0.891 and 0.32504836\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.9502 and 0.14253837\n",
            "Val acc and loss are 0.8904 and 0.32538095\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.95022 and 0.14210011\n",
            "Val acc and loss are 0.8904 and 0.3246358\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.95014 and 0.1419564\n",
            "Val acc and loss are 0.8902 and 0.32466757\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.95016 and 0.14186345\n",
            "Val acc and loss are 0.891 and 0.3252048\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.9496 and 0.14218402\n",
            "Val acc and loss are 0.8897 and 0.32651806\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.95018 and 0.14180197\n",
            "Val acc and loss are 0.8892 and 0.32645243\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.95102 and 0.14090951\n",
            "Val acc and loss are 0.8902 and 0.32524118\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.95116 and 0.14059375\n",
            "Val acc and loss are 0.8908 and 0.32509497\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.95124 and 0.14086208\n",
            "Val acc and loss are 0.8902 and 0.32614982\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.95092 and 0.14092058\n",
            "Val acc and loss are 0.8904 and 0.3267118\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.9513 and 0.1402313\n",
            "Val acc and loss are 0.8912 and 0.32610106\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.95154 and 0.1400101\n",
            "Val acc and loss are 0.8907 and 0.32600686\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.95118 and 0.14006174\n",
            "Val acc and loss are 0.8905 and 0.32676232\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.95098 and 0.14060304\n",
            "Val acc and loss are 0.8895 and 0.3279159\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.9513 and 0.13963473\n",
            "Val acc and loss are 0.8904 and 0.32648653\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.95172 and 0.13926654\n",
            "Val acc and loss are 0.8911 and 0.3255304\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.95168 and 0.1389985\n",
            "Val acc and loss are 0.8906 and 0.32592523\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.95164 and 0.13922594\n",
            "Val acc and loss are 0.8904 and 0.32718933\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.95176 and 0.13871753\n",
            "Val acc and loss are 0.8908 and 0.32681862\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.95196 and 0.13854963\n",
            "Val acc and loss are 0.8914 and 0.32665446\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.9519 and 0.13838759\n",
            "Val acc and loss are 0.8917 and 0.32631075\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.95176 and 0.13861115\n",
            "Val acc and loss are 0.8911 and 0.32664484\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.95186 and 0.13837238\n",
            "Val acc and loss are 0.8916 and 0.3264622\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.9522 and 0.13765539\n",
            "Val acc and loss are 0.8919 and 0.32573295\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.95198 and 0.13724788\n",
            "Val acc and loss are 0.8912 and 0.3260098\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.95206 and 0.1373093\n",
            "Val acc and loss are 0.8909 and 0.32702735\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.95206 and 0.1370356\n",
            "Val acc and loss are 0.8907 and 0.32691568\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.95246 and 0.13679022\n",
            "Val acc and loss are 0.8913 and 0.3263444\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.95216 and 0.13666913\n",
            "Val acc and loss are 0.8907 and 0.32645944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWQgyYPzmkru"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "TLaI9qBhmkrv",
        "outputId": "a8b2cbf0-081e-46ca-d1f8-ad70d07f04ef"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.95246 achieved at epoch: 598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "mfwt67iOmkrv",
        "outputId": "c518cf00-c487-4219-abdd-ef504d27e034"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "cmatrix = confusion_matrix(y_train, pred_train)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f9015ea58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "f-DOklRSmkrv",
        "outputId": "0b35bef7-7bc0-4cb4-82af-99dcfc0633bf"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8919 achieved at epoch: 594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "6eY9yq_amkrv",
        "outputId": "aa98159b-def3-4c2b-aa5f-2ddc728ab8e1"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "cmatrix = confusion_matrix(y_val, pred_val)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f9ca0cac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4CdEZGrmkrv",
        "outputId": "1b6ca21d-18ae-4f48-e85a-cea8e8411cb4"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3526301\n",
            "0.8817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "LRLEyzjXmkrv",
        "outputId": "731d3166-abed-4522-a830-5d909ba487df"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True, fmt = 'd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f9d08fa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY602k9QYtQ"
      },
      "source": [
        "# **Test 9** *Using 2 layer MLP with 300 nodes in the hidden layer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaaZ4-4AQ07M"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 2; nodes_per_layer = [dim, 300, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"tanh\", optimizer_name = 'Adam', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzAnNJDRQ07W",
        "outputId": "f5d8020b-1075-4c25-cbb1-b75a107dbdfc"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.46408 and 1.5716621\n",
            "Val acc and loss are 0.4641 and 1.5664607\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.5985 and 1.17361\n",
            "Val acc and loss are 0.6027 and 1.1630449\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.67166 and 0.962812\n",
            "Val acc and loss are 0.6758 and 0.9510094\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.70678 and 0.8440525\n",
            "Val acc and loss are 0.7091 and 0.8343626\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.73068 and 0.76935476\n",
            "Val acc and loss are 0.7313 and 0.7632954\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.74916 and 0.7187739\n",
            "Val acc and loss are 0.7477 and 0.71685046\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.76358 and 0.68545586\n",
            "Val acc and loss are 0.7617 and 0.6869826\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.77104 and 0.6621622\n",
            "Val acc and loss are 0.7697 and 0.6660245\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.77758 and 0.6427036\n",
            "Val acc and loss are 0.7731 and 0.6480041\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.78412 and 0.6240856\n",
            "Val acc and loss are 0.7788 and 0.6303588\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.79038 and 0.6059229\n",
            "Val acc and loss are 0.7822 and 0.61295605\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.79596 and 0.58896476\n",
            "Val acc and loss are 0.7885 and 0.5966482\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.80098 and 0.5737124\n",
            "Val acc and loss are 0.7918 and 0.58190197\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.80726 and 0.5603156\n",
            "Val acc and loss are 0.7966 and 0.5688109\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.81158 and 0.5485645\n",
            "Val acc and loss are 0.8011 and 0.5571636\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.81466 and 0.53816015\n",
            "Val acc and loss are 0.8072 and 0.5466997\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.81772 and 0.5288916\n",
            "Val acc and loss are 0.8119 and 0.5373526\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.82042 and 0.52054524\n",
            "Val acc and loss are 0.8152 and 0.52899456\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.82356 and 0.5127903\n",
            "Val acc and loss are 0.8166 and 0.5214041\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.82566 and 0.50534314\n",
            "Val acc and loss are 0.8198 and 0.5143554\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.82836 and 0.49826926\n",
            "Val acc and loss are 0.8209 and 0.5078292\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.83004 and 0.49168882\n",
            "Val acc and loss are 0.8224 and 0.5019668\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.83176 and 0.48569593\n",
            "Val acc and loss are 0.8238 and 0.49676844\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.83338 and 0.4800896\n",
            "Val acc and loss are 0.8265 and 0.49202862\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.83548 and 0.4747398\n",
            "Val acc and loss are 0.8283 and 0.48760605\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.83798 and 0.46959305\n",
            "Val acc and loss are 0.8303 and 0.4833817\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.83964 and 0.46476093\n",
            "Val acc and loss are 0.8332 and 0.47944728\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.84118 and 0.46035212\n",
            "Val acc and loss are 0.8347 and 0.47590625\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.84248 and 0.4562947\n",
            "Val acc and loss are 0.8357 and 0.47267368\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.84388 and 0.45240846\n",
            "Val acc and loss are 0.8365 and 0.4695807\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.84536 and 0.44859132\n",
            "Val acc and loss are 0.8378 and 0.46654332\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.8473 and 0.4448311\n",
            "Val acc and loss are 0.8386 and 0.46355575\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.84832 and 0.44117394\n",
            "Val acc and loss are 0.8396 and 0.46068242\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.84958 and 0.4376603\n",
            "Val acc and loss are 0.839 and 0.45792687\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.85056 and 0.4342853\n",
            "Val acc and loss are 0.8392 and 0.45527577\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.85136 and 0.4310548\n",
            "Val acc and loss are 0.8399 and 0.4527253\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.85242 and 0.4280049\n",
            "Val acc and loss are 0.8408 and 0.45029813\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.85328 and 0.4250229\n",
            "Val acc and loss are 0.841 and 0.44792372\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.8544 and 0.42211008\n",
            "Val acc and loss are 0.8412 and 0.445592\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.85496 and 0.41926944\n",
            "Val acc and loss are 0.8418 and 0.44322446\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.85554 and 0.4164944\n",
            "Val acc and loss are 0.8427 and 0.440862\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.85608 and 0.41388723\n",
            "Val acc and loss are 0.8424 and 0.43855616\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.85666 and 0.41132164\n",
            "Val acc and loss are 0.8437 and 0.43625933\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.85718 and 0.40872696\n",
            "Val acc and loss are 0.8448 and 0.4339145\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.85832 and 0.4061943\n",
            "Val acc and loss are 0.8451 and 0.43160114\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.85924 and 0.40376508\n",
            "Val acc and loss are 0.8465 and 0.42938164\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.85994 and 0.40145862\n",
            "Val acc and loss are 0.8472 and 0.4272799\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.8611 and 0.39920023\n",
            "Val acc and loss are 0.8474 and 0.4252829\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.86178 and 0.39696547\n",
            "Val acc and loss are 0.8479 and 0.4233373\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.8624 and 0.39475638\n",
            "Val acc and loss are 0.8488 and 0.421445\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.86298 and 0.392639\n",
            "Val acc and loss are 0.8497 and 0.4196492\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.86378 and 0.39056352\n",
            "Val acc and loss are 0.8501 and 0.4179471\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.86418 and 0.38857493\n",
            "Val acc and loss are 0.8512 and 0.41635332\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.86468 and 0.3867044\n",
            "Val acc and loss are 0.8524 and 0.4148665\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.86528 and 0.3849425\n",
            "Val acc and loss are 0.8537 and 0.41348243\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.86604 and 0.38326663\n",
            "Val acc and loss are 0.854 and 0.4121636\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.86634 and 0.38166136\n",
            "Val acc and loss are 0.854 and 0.41088653\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.8665 and 0.38007092\n",
            "Val acc and loss are 0.8541 and 0.4096853\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.86704 and 0.37848625\n",
            "Val acc and loss are 0.8548 and 0.40855312\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.86754 and 0.37696242\n",
            "Val acc and loss are 0.8553 and 0.4074555\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.86796 and 0.375506\n",
            "Val acc and loss are 0.8558 and 0.40637875\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.86834 and 0.37408093\n",
            "Val acc and loss are 0.856 and 0.40526202\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.8691 and 0.37269714\n",
            "Val acc and loss are 0.8566 and 0.40416223\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.86972 and 0.37139323\n",
            "Val acc and loss are 0.8569 and 0.40314913\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.87026 and 0.3700335\n",
            "Val acc and loss are 0.8566 and 0.40214306\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.87074 and 0.36864874\n",
            "Val acc and loss are 0.8572 and 0.4011346\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.87132 and 0.36731917\n",
            "Val acc and loss are 0.8577 and 0.4001452\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.87216 and 0.36602572\n",
            "Val acc and loss are 0.8576 and 0.39918098\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.87266 and 0.3647668\n",
            "Val acc and loss are 0.8579 and 0.3981964\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.87284 and 0.3635507\n",
            "Val acc and loss are 0.8577 and 0.39722908\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.87304 and 0.36240757\n",
            "Val acc and loss are 0.8586 and 0.3963156\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.87348 and 0.36130032\n",
            "Val acc and loss are 0.8591 and 0.39548817\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.874 and 0.36014917\n",
            "Val acc and loss are 0.8594 and 0.3946847\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.8745 and 0.35900405\n",
            "Val acc and loss are 0.8599 and 0.39389297\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.875 and 0.3579012\n",
            "Val acc and loss are 0.8603 and 0.3931003\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.87518 and 0.35684726\n",
            "Val acc and loss are 0.8602 and 0.39228833\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.87566 and 0.35587648\n",
            "Val acc and loss are 0.8606 and 0.39151052\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.87586 and 0.35494655\n",
            "Val acc and loss are 0.861 and 0.39076763\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.87638 and 0.35391906\n",
            "Val acc and loss are 0.8608 and 0.39001113\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.87664 and 0.35282004\n",
            "Val acc and loss are 0.8615 and 0.38926446\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.8771 and 0.35179242\n",
            "Val acc and loss are 0.8621 and 0.38859096\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.87768 and 0.35082343\n",
            "Val acc and loss are 0.8621 and 0.3879659\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.87764 and 0.34988844\n",
            "Val acc and loss are 0.8621 and 0.38730043\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.87802 and 0.3490203\n",
            "Val acc and loss are 0.8615 and 0.38666138\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.87834 and 0.34813648\n",
            "Val acc and loss are 0.862 and 0.3860215\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.87874 and 0.34719953\n",
            "Val acc and loss are 0.8615 and 0.38533857\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.87906 and 0.34617102\n",
            "Val acc and loss are 0.8625 and 0.3846047\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.87952 and 0.34520227\n",
            "Val acc and loss are 0.8631 and 0.38391456\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.87986 and 0.3443042\n",
            "Val acc and loss are 0.8628 and 0.38324812\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.8804 and 0.3434725\n",
            "Val acc and loss are 0.8623 and 0.38262826\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.88038 and 0.34264\n",
            "Val acc and loss are 0.8629 and 0.38205647\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.88066 and 0.34173116\n",
            "Val acc and loss are 0.8632 and 0.38145232\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.88086 and 0.34084022\n",
            "Val acc and loss are 0.8639 and 0.38090748\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.88106 and 0.33998853\n",
            "Val acc and loss are 0.8637 and 0.38042918\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.88128 and 0.3391547\n",
            "Val acc and loss are 0.8642 and 0.3799057\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.88182 and 0.33830702\n",
            "Val acc and loss are 0.8639 and 0.3792895\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.88176 and 0.33754265\n",
            "Val acc and loss are 0.8637 and 0.37872598\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.88188 and 0.33683273\n",
            "Val acc and loss are 0.8639 and 0.37823355\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.88196 and 0.336074\n",
            "Val acc and loss are 0.8646 and 0.37774134\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.88244 and 0.33521688\n",
            "Val acc and loss are 0.8653 and 0.37720764\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.88298 and 0.33438978\n",
            "Val acc and loss are 0.8655 and 0.3767438\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.88318 and 0.333589\n",
            "Val acc and loss are 0.8656 and 0.3762435\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.88336 and 0.33284423\n",
            "Val acc and loss are 0.866 and 0.37568104\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.88334 and 0.3321589\n",
            "Val acc and loss are 0.8659 and 0.37514487\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.88374 and 0.33143687\n",
            "Val acc and loss are 0.8666 and 0.37458766\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.88406 and 0.3306274\n",
            "Val acc and loss are 0.8672 and 0.373998\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.88436 and 0.32981881\n",
            "Val acc and loss are 0.8668 and 0.37345892\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.88478 and 0.32909554\n",
            "Val acc and loss are 0.867 and 0.37297282\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.88506 and 0.32840562\n",
            "Val acc and loss are 0.8673 and 0.37252083\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.88504 and 0.32774827\n",
            "Val acc and loss are 0.8671 and 0.37216425\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.8851 and 0.32708657\n",
            "Val acc and loss are 0.8676 and 0.37176332\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.88536 and 0.32640022\n",
            "Val acc and loss are 0.8673 and 0.37129575\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.88572 and 0.32566884\n",
            "Val acc and loss are 0.8679 and 0.3707657\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.88588 and 0.32496244\n",
            "Val acc and loss are 0.8677 and 0.37030026\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.8861 and 0.32427675\n",
            "Val acc and loss are 0.8678 and 0.3698446\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.88648 and 0.32360348\n",
            "Val acc and loss are 0.8677 and 0.3694096\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.8866 and 0.32293302\n",
            "Val acc and loss are 0.8677 and 0.36906466\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.8867 and 0.32228762\n",
            "Val acc and loss are 0.868 and 0.36870345\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.88688 and 0.32160532\n",
            "Val acc and loss are 0.8685 and 0.36833453\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.88724 and 0.32090205\n",
            "Val acc and loss are 0.8682 and 0.3679275\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.88756 and 0.3202299\n",
            "Val acc and loss are 0.8679 and 0.36753476\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.88788 and 0.31960317\n",
            "Val acc and loss are 0.8674 and 0.3671036\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.88808 and 0.31897315\n",
            "Val acc and loss are 0.8679 and 0.36665767\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.8881 and 0.31835526\n",
            "Val acc and loss are 0.8688 and 0.36623704\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.88854 and 0.31770557\n",
            "Val acc and loss are 0.8694 and 0.3658486\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.88878 and 0.31707552\n",
            "Val acc and loss are 0.8696 and 0.36545473\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.88896 and 0.31640375\n",
            "Val acc and loss are 0.87 and 0.36502662\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.88902 and 0.3157209\n",
            "Val acc and loss are 0.8698 and 0.3646119\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.8895 and 0.31506142\n",
            "Val acc and loss are 0.8694 and 0.36421368\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.88966 and 0.31439024\n",
            "Val acc and loss are 0.8699 and 0.36381724\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.8901 and 0.31374806\n",
            "Val acc and loss are 0.8702 and 0.36345902\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.89034 and 0.3131503\n",
            "Val acc and loss are 0.8703 and 0.36307457\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.89056 and 0.3125601\n",
            "Val acc and loss are 0.8707 and 0.36265138\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.89082 and 0.31198272\n",
            "Val acc and loss are 0.8708 and 0.36221653\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.89102 and 0.31142718\n",
            "Val acc and loss are 0.8707 and 0.36183473\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.89124 and 0.31081113\n",
            "Val acc and loss are 0.8705 and 0.3615091\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.89138 and 0.31020027\n",
            "Val acc and loss are 0.8706 and 0.36117783\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.89132 and 0.30961883\n",
            "Val acc and loss are 0.8707 and 0.3608472\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.89146 and 0.30905747\n",
            "Val acc and loss are 0.8701 and 0.36052287\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.89164 and 0.3085031\n",
            "Val acc and loss are 0.8704 and 0.36019477\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.89186 and 0.30791217\n",
            "Val acc and loss are 0.8707 and 0.35979223\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.8921 and 0.30728406\n",
            "Val acc and loss are 0.8707 and 0.3593429\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.89242 and 0.30671477\n",
            "Val acc and loss are 0.8707 and 0.35896218\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.89258 and 0.30619827\n",
            "Val acc and loss are 0.8712 and 0.35856733\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.89304 and 0.3057462\n",
            "Val acc and loss are 0.8713 and 0.35828912\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.89324 and 0.30517393\n",
            "Val acc and loss are 0.8714 and 0.3579939\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.89326 and 0.30450737\n",
            "Val acc and loss are 0.8711 and 0.35767552\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.89368 and 0.30389792\n",
            "Val acc and loss are 0.8716 and 0.35742006\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.8939 and 0.30330497\n",
            "Val acc and loss are 0.8721 and 0.35709664\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.89372 and 0.3027434\n",
            "Val acc and loss are 0.8721 and 0.3567218\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.89362 and 0.30223534\n",
            "Val acc and loss are 0.8724 and 0.3563855\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.8939 and 0.30169365\n",
            "Val acc and loss are 0.8728 and 0.35606593\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.89406 and 0.30111557\n",
            "Val acc and loss are 0.8724 and 0.35577032\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.89422 and 0.30058128\n",
            "Val acc and loss are 0.8723 and 0.35550565\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.89476 and 0.30006284\n",
            "Val acc and loss are 0.8728 and 0.35521036\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.89512 and 0.29956394\n",
            "Val acc and loss are 0.8735 and 0.3548925\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.89528 and 0.2990718\n",
            "Val acc and loss are 0.8733 and 0.35462344\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.89566 and 0.29854435\n",
            "Val acc and loss are 0.8729 and 0.3544044\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.89574 and 0.29802626\n",
            "Val acc and loss are 0.8732 and 0.3541987\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.89584 and 0.29750025\n",
            "Val acc and loss are 0.8737 and 0.35391307\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.89598 and 0.29695415\n",
            "Val acc and loss are 0.8736 and 0.3535223\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.8961 and 0.29641384\n",
            "Val acc and loss are 0.8742 and 0.35309514\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.89636 and 0.29587963\n",
            "Val acc and loss are 0.8744 and 0.35271254\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.89658 and 0.29536554\n",
            "Val acc and loss are 0.8746 and 0.3524213\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.89678 and 0.29485178\n",
            "Val acc and loss are 0.8743 and 0.35216036\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.89674 and 0.2943599\n",
            "Val acc and loss are 0.8739 and 0.35197297\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.89692 and 0.2938784\n",
            "Val acc and loss are 0.8734 and 0.35179457\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.89706 and 0.29339036\n",
            "Val acc and loss are 0.8738 and 0.35157198\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.89724 and 0.29287323\n",
            "Val acc and loss are 0.8736 and 0.35127172\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.89758 and 0.29235524\n",
            "Val acc and loss are 0.8742 and 0.35094556\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.89748 and 0.29186198\n",
            "Val acc and loss are 0.8738 and 0.35063273\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.89774 and 0.2913532\n",
            "Val acc and loss are 0.8744 and 0.35041478\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.89784 and 0.2907768\n",
            "Val acc and loss are 0.8741 and 0.3501312\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.89776 and 0.2901643\n",
            "Val acc and loss are 0.8745 and 0.34979674\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.8983 and 0.28966227\n",
            "Val acc and loss are 0.8738 and 0.34948152\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.89838 and 0.2892241\n",
            "Val acc and loss are 0.8743 and 0.34917432\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.89862 and 0.28881887\n",
            "Val acc and loss are 0.8742 and 0.348954\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.89882 and 0.28843054\n",
            "Val acc and loss are 0.8743 and 0.34878564\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.89872 and 0.2879739\n",
            "Val acc and loss are 0.8749 and 0.3485447\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.89904 and 0.28743148\n",
            "Val acc and loss are 0.875 and 0.34828526\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.89922 and 0.28690696\n",
            "Val acc and loss are 0.8748 and 0.34802213\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.89952 and 0.28642008\n",
            "Val acc and loss are 0.8758 and 0.3478067\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.89952 and 0.28595203\n",
            "Val acc and loss are 0.8749 and 0.34759578\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.89946 and 0.28550753\n",
            "Val acc and loss are 0.8755 and 0.34741023\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.89956 and 0.2850158\n",
            "Val acc and loss are 0.8756 and 0.3471636\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.90018 and 0.28450164\n",
            "Val acc and loss are 0.8758 and 0.34684905\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.90066 and 0.283982\n",
            "Val acc and loss are 0.8758 and 0.34650162\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.90082 and 0.2834972\n",
            "Val acc and loss are 0.8756 and 0.3461488\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.90078 and 0.2830157\n",
            "Val acc and loss are 0.8758 and 0.34582135\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.90088 and 0.28255376\n",
            "Val acc and loss are 0.8756 and 0.34555396\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.90094 and 0.28212962\n",
            "Val acc and loss are 0.8762 and 0.34532556\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.90112 and 0.28170016\n",
            "Val acc and loss are 0.8762 and 0.34516427\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.9013 and 0.28121263\n",
            "Val acc and loss are 0.8764 and 0.34497344\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.9015 and 0.28070578\n",
            "Val acc and loss are 0.8764 and 0.34474906\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.90174 and 0.28029436\n",
            "Val acc and loss are 0.8763 and 0.3445707\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.90182 and 0.27987885\n",
            "Val acc and loss are 0.8765 and 0.3442964\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.90196 and 0.27946812\n",
            "Val acc and loss are 0.877 and 0.34403405\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.90214 and 0.2790253\n",
            "Val acc and loss are 0.8765 and 0.34380412\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.90238 and 0.27856672\n",
            "Val acc and loss are 0.8768 and 0.34366503\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.90232 and 0.2781275\n",
            "Val acc and loss are 0.8766 and 0.34356546\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.90236 and 0.27773356\n",
            "Val acc and loss are 0.8767 and 0.34345856\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.90246 and 0.277355\n",
            "Val acc and loss are 0.8772 and 0.3433139\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.90272 and 0.27686027\n",
            "Val acc and loss are 0.8769 and 0.34303388\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.90294 and 0.27630934\n",
            "Val acc and loss are 0.8766 and 0.34266302\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.90302 and 0.27581766\n",
            "Val acc and loss are 0.8768 and 0.34234834\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.90318 and 0.27539054\n",
            "Val acc and loss are 0.8771 and 0.34215206\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.90294 and 0.27502865\n",
            "Val acc and loss are 0.8776 and 0.3419959\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.9032 and 0.2745907\n",
            "Val acc and loss are 0.8776 and 0.34182113\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.90336 and 0.2740621\n",
            "Val acc and loss are 0.8775 and 0.34161276\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.90372 and 0.27358434\n",
            "Val acc and loss are 0.8776 and 0.34142628\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.90378 and 0.27317157\n",
            "Val acc and loss are 0.877 and 0.34126368\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.90396 and 0.27279422\n",
            "Val acc and loss are 0.8772 and 0.34112057\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.90434 and 0.27244112\n",
            "Val acc and loss are 0.8772 and 0.34100905\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.90444 and 0.27202725\n",
            "Val acc and loss are 0.8778 and 0.34085917\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.90456 and 0.27151835\n",
            "Val acc and loss are 0.8781 and 0.34065247\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.90472 and 0.27101395\n",
            "Val acc and loss are 0.8782 and 0.34043068\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.90492 and 0.27054638\n",
            "Val acc and loss are 0.8782 and 0.34022474\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.9049 and 0.27013358\n",
            "Val acc and loss are 0.8778 and 0.3400781\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.90492 and 0.2697768\n",
            "Val acc and loss are 0.8781 and 0.33993316\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.90536 and 0.26932046\n",
            "Val acc and loss are 0.8779 and 0.33974308\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.9055 and 0.26886928\n",
            "Val acc and loss are 0.878 and 0.3394949\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.9059 and 0.26844358\n",
            "Val acc and loss are 0.8783 and 0.33923754\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.90618 and 0.26804897\n",
            "Val acc and loss are 0.8783 and 0.33894125\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.90628 and 0.2676807\n",
            "Val acc and loss are 0.8778 and 0.3387066\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.90642 and 0.2673021\n",
            "Val acc and loss are 0.878 and 0.3385012\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.9064 and 0.2668963\n",
            "Val acc and loss are 0.8783 and 0.3382712\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.90672 and 0.26640436\n",
            "Val acc and loss are 0.878 and 0.33804458\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.90706 and 0.26595822\n",
            "Val acc and loss are 0.8782 and 0.33794907\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.90686 and 0.26560384\n",
            "Val acc and loss are 0.8783 and 0.33796522\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.90668 and 0.26520652\n",
            "Val acc and loss are 0.8785 and 0.33786568\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.9069 and 0.2648359\n",
            "Val acc and loss are 0.879 and 0.33779034\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.90724 and 0.26441643\n",
            "Val acc and loss are 0.8795 and 0.33761215\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.90736 and 0.26394132\n",
            "Val acc and loss are 0.879 and 0.33731958\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.90764 and 0.26355273\n",
            "Val acc and loss are 0.8792 and 0.33709985\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.90788 and 0.263187\n",
            "Val acc and loss are 0.8794 and 0.3369907\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.90814 and 0.2627241\n",
            "Val acc and loss are 0.8791 and 0.33682\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.9082 and 0.2623257\n",
            "Val acc and loss are 0.879 and 0.33672634\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.90826 and 0.2619393\n",
            "Val acc and loss are 0.8791 and 0.336646\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.90826 and 0.26149815\n",
            "Val acc and loss are 0.8792 and 0.33649677\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.90856 and 0.26112324\n",
            "Val acc and loss are 0.88 and 0.33633775\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.90854 and 0.26081875\n",
            "Val acc and loss are 0.8803 and 0.33619657\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.90854 and 0.26041555\n",
            "Val acc and loss are 0.8796 and 0.33595034\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.90872 and 0.2599081\n",
            "Val acc and loss are 0.8807 and 0.33567232\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.90898 and 0.25946093\n",
            "Val acc and loss are 0.8805 and 0.33545738\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.90924 and 0.2590079\n",
            "Val acc and loss are 0.8803 and 0.33526015\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.90974 and 0.2586457\n",
            "Val acc and loss are 0.8803 and 0.33514762\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.9098 and 0.25830156\n",
            "Val acc and loss are 0.8802 and 0.33503345\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.90982 and 0.25791097\n",
            "Val acc and loss are 0.8803 and 0.33489507\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.90996 and 0.25751302\n",
            "Val acc and loss are 0.8794 and 0.33469933\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.9098 and 0.25717837\n",
            "Val acc and loss are 0.8802 and 0.33453944\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.91012 and 0.25675678\n",
            "Val acc and loss are 0.8804 and 0.3343378\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.91048 and 0.25640234\n",
            "Val acc and loss are 0.8802 and 0.33429354\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.9106 and 0.25599802\n",
            "Val acc and loss are 0.8801 and 0.33421278\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.91098 and 0.25556695\n",
            "Val acc and loss are 0.8805 and 0.3341095\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.91086 and 0.2552375\n",
            "Val acc and loss are 0.8808 and 0.3340168\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.91108 and 0.25488245\n",
            "Val acc and loss are 0.8807 and 0.3338709\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.91106 and 0.25450492\n",
            "Val acc and loss are 0.8805 and 0.33371025\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.9111 and 0.2541648\n",
            "Val acc and loss are 0.8801 and 0.3335639\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.91118 and 0.25377953\n",
            "Val acc and loss are 0.8801 and 0.33339036\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.9115 and 0.25337157\n",
            "Val acc and loss are 0.8806 and 0.33323374\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.9118 and 0.2529483\n",
            "Val acc and loss are 0.8803 and 0.3331109\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.91204 and 0.25252345\n",
            "Val acc and loss are 0.8809 and 0.33306524\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.91198 and 0.25218096\n",
            "Val acc and loss are 0.881 and 0.33308676\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.91212 and 0.2518272\n",
            "Val acc and loss are 0.8806 and 0.33303124\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.91248 and 0.2514691\n",
            "Val acc and loss are 0.8808 and 0.33292198\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.91232 and 0.25107142\n",
            "Val acc and loss are 0.8808 and 0.33273062\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.91248 and 0.25063956\n",
            "Val acc and loss are 0.881 and 0.33252516\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.91276 and 0.25026882\n",
            "Val acc and loss are 0.8814 and 0.33238828\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.91296 and 0.24991432\n",
            "Val acc and loss are 0.881 and 0.3322601\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.9128 and 0.24957699\n",
            "Val acc and loss are 0.8805 and 0.33212706\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.91322 and 0.24913166\n",
            "Val acc and loss are 0.8812 and 0.33182693\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.91334 and 0.24870887\n",
            "Val acc and loss are 0.8812 and 0.3315667\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.91346 and 0.2483449\n",
            "Val acc and loss are 0.8817 and 0.331403\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.91338 and 0.24802524\n",
            "Val acc and loss are 0.8817 and 0.33132207\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.91358 and 0.24769224\n",
            "Val acc and loss are 0.8816 and 0.33121973\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.91358 and 0.24736683\n",
            "Val acc and loss are 0.8812 and 0.3311432\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.9138 and 0.24705102\n",
            "Val acc and loss are 0.8816 and 0.3310599\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.91396 and 0.24671313\n",
            "Val acc and loss are 0.8814 and 0.3309189\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.91426 and 0.24629311\n",
            "Val acc and loss are 0.8811 and 0.33070782\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.91448 and 0.24588539\n",
            "Val acc and loss are 0.8816 and 0.3305162\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.91448 and 0.24551031\n",
            "Val acc and loss are 0.8817 and 0.33038697\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.91468 and 0.24520469\n",
            "Val acc and loss are 0.8822 and 0.330408\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.9148 and 0.24483243\n",
            "Val acc and loss are 0.8821 and 0.33040592\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.91486 and 0.24437368\n",
            "Val acc and loss are 0.8822 and 0.33028626\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.91506 and 0.24398932\n",
            "Val acc and loss are 0.8824 and 0.3301503\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.91534 and 0.24368086\n",
            "Val acc and loss are 0.8823 and 0.33012074\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.9154 and 0.24338433\n",
            "Val acc and loss are 0.8818 and 0.3300828\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.91534 and 0.24309173\n",
            "Val acc and loss are 0.8814 and 0.32996637\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.91554 and 0.24273439\n",
            "Val acc and loss are 0.8811 and 0.3297957\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.91574 and 0.24231352\n",
            "Val acc and loss are 0.882 and 0.32957897\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.91594 and 0.24189058\n",
            "Val acc and loss are 0.8825 and 0.3294479\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.91618 and 0.24153127\n",
            "Val acc and loss are 0.882 and 0.32940474\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.91604 and 0.2411907\n",
            "Val acc and loss are 0.8824 and 0.32933787\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.91608 and 0.24086624\n",
            "Val acc and loss are 0.8821 and 0.32922852\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.91626 and 0.2404999\n",
            "Val acc and loss are 0.8825 and 0.32903412\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.91636 and 0.24016756\n",
            "Val acc and loss are 0.8822 and 0.32892495\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.91658 and 0.23987453\n",
            "Val acc and loss are 0.8824 and 0.32884404\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.91688 and 0.23955564\n",
            "Val acc and loss are 0.8821 and 0.32870105\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.91686 and 0.23917055\n",
            "Val acc and loss are 0.8826 and 0.32843906\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.91678 and 0.23875566\n",
            "Val acc and loss are 0.8831 and 0.3282123\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.91708 and 0.23839328\n",
            "Val acc and loss are 0.8831 and 0.32806215\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.91708 and 0.23799604\n",
            "Val acc and loss are 0.8829 and 0.32808042\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.91726 and 0.23765093\n",
            "Val acc and loss are 0.8825 and 0.3281267\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.91746 and 0.23736471\n",
            "Val acc and loss are 0.8824 and 0.32821634\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.91746 and 0.23702477\n",
            "Val acc and loss are 0.8827 and 0.32815436\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.9177 and 0.23668417\n",
            "Val acc and loss are 0.883 and 0.32805765\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.91776 and 0.23635316\n",
            "Val acc and loss are 0.8832 and 0.32790497\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.91786 and 0.23605148\n",
            "Val acc and loss are 0.8831 and 0.32781503\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.91804 and 0.23569985\n",
            "Val acc and loss are 0.8828 and 0.32770574\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.91824 and 0.2353513\n",
            "Val acc and loss are 0.8832 and 0.32772797\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.91826 and 0.23508184\n",
            "Val acc and loss are 0.883 and 0.32782876\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.9182 and 0.2346881\n",
            "Val acc and loss are 0.883 and 0.32770357\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.91856 and 0.2342784\n",
            "Val acc and loss are 0.8832 and 0.32744238\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.9188 and 0.23396035\n",
            "Val acc and loss are 0.8833 and 0.32727513\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.91888 and 0.23360275\n",
            "Val acc and loss are 0.8839 and 0.3270599\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.91902 and 0.23325695\n",
            "Val acc and loss are 0.8839 and 0.32693797\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.91882 and 0.23296815\n",
            "Val acc and loss are 0.8842 and 0.3269392\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.91924 and 0.2326121\n",
            "Val acc and loss are 0.8845 and 0.32681137\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.91952 and 0.23219481\n",
            "Val acc and loss are 0.8843 and 0.3264622\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.91956 and 0.23192766\n",
            "Val acc and loss are 0.8843 and 0.32625157\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.91972 and 0.23156744\n",
            "Val acc and loss are 0.8838 and 0.32604\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.91964 and 0.23116903\n",
            "Val acc and loss are 0.8843 and 0.32590756\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.92016 and 0.23080857\n",
            "Val acc and loss are 0.8843 and 0.32580483\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.92008 and 0.23048176\n",
            "Val acc and loss are 0.8845 and 0.3257222\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.91994 and 0.23016848\n",
            "Val acc and loss are 0.8839 and 0.32564855\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.91978 and 0.22988625\n",
            "Val acc and loss are 0.8837 and 0.32560337\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.92018 and 0.22953762\n",
            "Val acc and loss are 0.8838 and 0.32543385\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.92028 and 0.22920625\n",
            "Val acc and loss are 0.8843 and 0.32532486\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.92042 and 0.22885363\n",
            "Val acc and loss are 0.8846 and 0.32521114\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.92076 and 0.2284712\n",
            "Val acc and loss are 0.8845 and 0.32513052\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.92086 and 0.2282229\n",
            "Val acc and loss are 0.8844 and 0.32520804\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.9207 and 0.22795196\n",
            "Val acc and loss are 0.884 and 0.3251853\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.92096 and 0.22767207\n",
            "Val acc and loss are 0.8841 and 0.3251392\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.92084 and 0.22734387\n",
            "Val acc and loss are 0.8844 and 0.32504994\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.92116 and 0.22696152\n",
            "Val acc and loss are 0.8851 and 0.32486764\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.92142 and 0.22662418\n",
            "Val acc and loss are 0.8851 and 0.32474017\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.92154 and 0.22628096\n",
            "Val acc and loss are 0.8851 and 0.32466394\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.92168 and 0.22596863\n",
            "Val acc and loss are 0.8845 and 0.32454517\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.92182 and 0.22562979\n",
            "Val acc and loss are 0.8851 and 0.32439208\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.9218 and 0.22532518\n",
            "Val acc and loss are 0.8845 and 0.32435405\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.92172 and 0.22493128\n",
            "Val acc and loss are 0.8847 and 0.32426187\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.92214 and 0.22464104\n",
            "Val acc and loss are 0.8841 and 0.32428065\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.92214 and 0.22442168\n",
            "Val acc and loss are 0.8839 and 0.32438314\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.9222 and 0.22409292\n",
            "Val acc and loss are 0.8842 and 0.32430002\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.92238 and 0.22368874\n",
            "Val acc and loss are 0.885 and 0.324026\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.92252 and 0.2232791\n",
            "Val acc and loss are 0.8848 and 0.323775\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.9228 and 0.22289248\n",
            "Val acc and loss are 0.8847 and 0.32357198\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.92278 and 0.22251597\n",
            "Val acc and loss are 0.8854 and 0.32344174\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.92296 and 0.22218125\n",
            "Val acc and loss are 0.8855 and 0.3233974\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.9231 and 0.22200029\n",
            "Val acc and loss are 0.8849 and 0.32349873\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.92326 and 0.2218073\n",
            "Val acc and loss are 0.8844 and 0.32353732\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.92318 and 0.2214656\n",
            "Val acc and loss are 0.8847 and 0.32339343\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.92392 and 0.2210904\n",
            "Val acc and loss are 0.8855 and 0.32322702\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.92384 and 0.22080992\n",
            "Val acc and loss are 0.8855 and 0.3231705\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.92366 and 0.22057855\n",
            "Val acc and loss are 0.8852 and 0.3231125\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.92376 and 0.22040154\n",
            "Val acc and loss are 0.885 and 0.3230763\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.92386 and 0.21997644\n",
            "Val acc and loss are 0.8856 and 0.32284558\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.9242 and 0.21950683\n",
            "Val acc and loss are 0.886 and 0.32263842\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.9244 and 0.21923652\n",
            "Val acc and loss are 0.8858 and 0.32271212\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.92442 and 0.21891731\n",
            "Val acc and loss are 0.8856 and 0.32274872\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.9245 and 0.21872033\n",
            "Val acc and loss are 0.8852 and 0.32278746\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.92444 and 0.2184102\n",
            "Val acc and loss are 0.8857 and 0.32266244\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.92452 and 0.21799326\n",
            "Val acc and loss are 0.8852 and 0.3225348\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.92498 and 0.21761468\n",
            "Val acc and loss are 0.8857 and 0.32248074\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.92506 and 0.21737398\n",
            "Val acc and loss are 0.8855 and 0.32257056\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.92496 and 0.21714528\n",
            "Val acc and loss are 0.8859 and 0.3225211\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.92518 and 0.21674396\n",
            "Val acc and loss are 0.886 and 0.3223076\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.92508 and 0.21637213\n",
            "Val acc and loss are 0.8861 and 0.32210478\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.92546 and 0.21597777\n",
            "Val acc and loss are 0.8859 and 0.32204327\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.92536 and 0.21566014\n",
            "Val acc and loss are 0.8857 and 0.32217535\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.92566 and 0.21534322\n",
            "Val acc and loss are 0.8857 and 0.3222175\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.92564 and 0.21507794\n",
            "Val acc and loss are 0.8857 and 0.3220441\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.92582 and 0.21484742\n",
            "Val acc and loss are 0.8866 and 0.32184815\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.92578 and 0.2145043\n",
            "Val acc and loss are 0.8863 and 0.32162583\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.92588 and 0.2142278\n",
            "Val acc and loss are 0.8857 and 0.32163134\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.9263 and 0.21398173\n",
            "Val acc and loss are 0.8857 and 0.32169148\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.92628 and 0.2137104\n",
            "Val acc and loss are 0.8856 and 0.3217557\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.92616 and 0.21338964\n",
            "Val acc and loss are 0.8857 and 0.32166797\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.92636 and 0.21301515\n",
            "Val acc and loss are 0.8865 and 0.3214518\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.92628 and 0.21270257\n",
            "Val acc and loss are 0.8866 and 0.3213014\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.92658 and 0.21244894\n",
            "Val acc and loss are 0.8865 and 0.321279\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.92658 and 0.21211073\n",
            "Val acc and loss are 0.8862 and 0.3213098\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.92672 and 0.21177594\n",
            "Val acc and loss are 0.886 and 0.32133818\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.9268 and 0.21146204\n",
            "Val acc and loss are 0.8865 and 0.3211748\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.92686 and 0.21116817\n",
            "Val acc and loss are 0.8865 and 0.32097575\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.92686 and 0.21102098\n",
            "Val acc and loss are 0.8859 and 0.32094145\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.92704 and 0.21062942\n",
            "Val acc and loss are 0.8867 and 0.32069355\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.92744 and 0.21025266\n",
            "Val acc and loss are 0.8872 and 0.3205571\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.92762 and 0.20998302\n",
            "Val acc and loss are 0.8871 and 0.3206026\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.92742 and 0.20977281\n",
            "Val acc and loss are 0.8865 and 0.3207005\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.92726 and 0.20961455\n",
            "Val acc and loss are 0.887 and 0.32075682\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.9274 and 0.20920205\n",
            "Val acc and loss are 0.887 and 0.32047036\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.92766 and 0.20868757\n",
            "Val acc and loss are 0.8871 and 0.3202088\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.9281 and 0.20835014\n",
            "Val acc and loss are 0.887 and 0.32022378\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.92814 and 0.2081777\n",
            "Val acc and loss are 0.8861 and 0.3205289\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.9282 and 0.2079885\n",
            "Val acc and loss are 0.8863 and 0.32063517\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.92828 and 0.2076408\n",
            "Val acc and loss are 0.8871 and 0.32042402\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.92844 and 0.20729512\n",
            "Val acc and loss are 0.8869 and 0.32015014\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.9285 and 0.2071055\n",
            "Val acc and loss are 0.8866 and 0.32007354\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.92856 and 0.20689699\n",
            "Val acc and loss are 0.8865 and 0.32011676\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.92878 and 0.20669748\n",
            "Val acc and loss are 0.8867 and 0.32041103\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.92856 and 0.20643535\n",
            "Val acc and loss are 0.8859 and 0.32065555\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.9287 and 0.20593484\n",
            "Val acc and loss are 0.8867 and 0.3204766\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.92864 and 0.2055693\n",
            "Val acc and loss are 0.8867 and 0.32030827\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.92898 and 0.20524621\n",
            "Val acc and loss are 0.8868 and 0.32011235\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.92898 and 0.20494077\n",
            "Val acc and loss are 0.8869 and 0.3198468\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.9288 and 0.20485625\n",
            "Val acc and loss are 0.8869 and 0.31979072\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.92912 and 0.2045567\n",
            "Val acc and loss are 0.8868 and 0.31974214\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.92946 and 0.20421231\n",
            "Val acc and loss are 0.887 and 0.31976312\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.92938 and 0.20389363\n",
            "Val acc and loss are 0.8866 and 0.3197101\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.92952 and 0.20354094\n",
            "Val acc and loss are 0.8868 and 0.3196927\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.92946 and 0.2033711\n",
            "Val acc and loss are 0.8876 and 0.31984937\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.92974 and 0.20303732\n",
            "Val acc and loss are 0.8871 and 0.3197912\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.93004 and 0.20266524\n",
            "Val acc and loss are 0.887 and 0.31962338\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.93006 and 0.20247756\n",
            "Val acc and loss are 0.8876 and 0.3195432\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.93038 and 0.20220688\n",
            "Val acc and loss are 0.8874 and 0.31940547\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.93034 and 0.20187241\n",
            "Val acc and loss are 0.8873 and 0.31927636\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.93048 and 0.20160738\n",
            "Val acc and loss are 0.8876 and 0.31926146\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.9304 and 0.20124039\n",
            "Val acc and loss are 0.8876 and 0.31922\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.9306 and 0.20090632\n",
            "Val acc and loss are 0.8871 and 0.31922424\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.93066 and 0.20069566\n",
            "Val acc and loss are 0.8872 and 0.31923327\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.93086 and 0.20032652\n",
            "Val acc and loss are 0.8875 and 0.31904012\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.9308 and 0.2000346\n",
            "Val acc and loss are 0.8881 and 0.3189377\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.9309 and 0.19969542\n",
            "Val acc and loss are 0.8882 and 0.31886092\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.93106 and 0.19950646\n",
            "Val acc and loss are 0.8873 and 0.31895223\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.93134 and 0.19921641\n",
            "Val acc and loss are 0.8874 and 0.31887335\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.9317 and 0.1987714\n",
            "Val acc and loss are 0.8876 and 0.31857815\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.93176 and 0.19852813\n",
            "Val acc and loss are 0.8877 and 0.31855452\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.93174 and 0.1982262\n",
            "Val acc and loss are 0.8877 and 0.31861913\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.93164 and 0.1980621\n",
            "Val acc and loss are 0.8873 and 0.31902394\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.93184 and 0.19784434\n",
            "Val acc and loss are 0.887 and 0.31913325\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.93198 and 0.19744274\n",
            "Val acc and loss are 0.8878 and 0.31882676\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.93186 and 0.19720201\n",
            "Val acc and loss are 0.8878 and 0.3185424\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.93194 and 0.19688328\n",
            "Val acc and loss are 0.8877 and 0.31828424\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.93264 and 0.19656815\n",
            "Val acc and loss are 0.8877 and 0.31812724\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.9326 and 0.1962118\n",
            "Val acc and loss are 0.8881 and 0.31809977\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.9329 and 0.19593771\n",
            "Val acc and loss are 0.8874 and 0.31810966\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.93292 and 0.19572778\n",
            "Val acc and loss are 0.8872 and 0.31811556\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.93296 and 0.19549817\n",
            "Val acc and loss are 0.8868 and 0.31800342\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.9331 and 0.19521147\n",
            "Val acc and loss are 0.8877 and 0.3178851\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.93328 and 0.19494574\n",
            "Val acc and loss are 0.888 and 0.31777868\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.93322 and 0.1947202\n",
            "Val acc and loss are 0.8878 and 0.31769156\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.93312 and 0.19454685\n",
            "Val acc and loss are 0.8877 and 0.3177279\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.9335 and 0.19411139\n",
            "Val acc and loss are 0.8878 and 0.3174895\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.93376 and 0.19366816\n",
            "Val acc and loss are 0.8892 and 0.31735677\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.93396 and 0.19344129\n",
            "Val acc and loss are 0.8887 and 0.317516\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.9337 and 0.19328871\n",
            "Val acc and loss are 0.8879 and 0.31783134\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.93372 and 0.19319406\n",
            "Val acc and loss are 0.8877 and 0.31808886\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.93366 and 0.19284149\n",
            "Val acc and loss are 0.8883 and 0.3179576\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.93402 and 0.19246583\n",
            "Val acc and loss are 0.8885 and 0.31774497\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.93408 and 0.19214036\n",
            "Val acc and loss are 0.8885 and 0.31766644\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.93418 and 0.19190624\n",
            "Val acc and loss are 0.8876 and 0.31781408\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.93414 and 0.19176348\n",
            "Val acc and loss are 0.8877 and 0.31798148\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.93434 and 0.19148387\n",
            "Val acc and loss are 0.8878 and 0.31781924\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.93454 and 0.1911498\n",
            "Val acc and loss are 0.8878 and 0.31753683\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.93462 and 0.19091082\n",
            "Val acc and loss are 0.8879 and 0.31739098\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.9349 and 0.19064222\n",
            "Val acc and loss are 0.8879 and 0.31737697\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.93466 and 0.19045289\n",
            "Val acc and loss are 0.8881 and 0.3175403\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.93474 and 0.19020015\n",
            "Val acc and loss are 0.8879 and 0.3176291\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.9351 and 0.18978082\n",
            "Val acc and loss are 0.8882 and 0.31748214\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.93512 and 0.18939562\n",
            "Val acc and loss are 0.8877 and 0.3174002\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.93538 and 0.18914531\n",
            "Val acc and loss are 0.8878 and 0.3173413\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.93554 and 0.1889713\n",
            "Val acc and loss are 0.8878 and 0.31725225\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.93554 and 0.18886742\n",
            "Val acc and loss are 0.8879 and 0.31728134\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.93576 and 0.1884328\n",
            "Val acc and loss are 0.8884 and 0.31704506\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.93588 and 0.18812442\n",
            "Val acc and loss are 0.8886 and 0.31709254\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.93578 and 0.18789476\n",
            "Val acc and loss are 0.8883 and 0.31709364\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.93536 and 0.18773848\n",
            "Val acc and loss are 0.8882 and 0.31728306\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.9358 and 0.1874144\n",
            "Val acc and loss are 0.8879 and 0.31737608\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.93632 and 0.18700676\n",
            "Val acc and loss are 0.8884 and 0.31717992\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.93674 and 0.1866343\n",
            "Val acc and loss are 0.8886 and 0.3168767\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.93676 and 0.18644555\n",
            "Val acc and loss are 0.8883 and 0.31672364\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.9367 and 0.18631314\n",
            "Val acc and loss are 0.8883 and 0.31678313\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.93676 and 0.18610027\n",
            "Val acc and loss are 0.8883 and 0.31694376\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.93684 and 0.18573958\n",
            "Val acc and loss are 0.8883 and 0.31692335\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.93712 and 0.18542933\n",
            "Val acc and loss are 0.8886 and 0.31694922\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.93724 and 0.18523076\n",
            "Val acc and loss are 0.8885 and 0.31705126\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.9368 and 0.18508847\n",
            "Val acc and loss are 0.8882 and 0.31703007\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.93708 and 0.18495949\n",
            "Val acc and loss are 0.8887 and 0.3169369\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.93744 and 0.1846587\n",
            "Val acc and loss are 0.8892 and 0.31661972\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.93782 and 0.18424472\n",
            "Val acc and loss are 0.8898 and 0.31622973\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.93776 and 0.18392324\n",
            "Val acc and loss are 0.8893 and 0.31610414\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.93768 and 0.18363687\n",
            "Val acc and loss are 0.8886 and 0.31623185\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.9377 and 0.18343097\n",
            "Val acc and loss are 0.8886 and 0.31655905\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.938 and 0.18312368\n",
            "Val acc and loss are 0.889 and 0.31668058\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.93798 and 0.18280719\n",
            "Val acc and loss are 0.889 and 0.31651202\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.93836 and 0.18255429\n",
            "Val acc and loss are 0.8891 and 0.31621525\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.93814 and 0.18245679\n",
            "Val acc and loss are 0.8893 and 0.31608292\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.9377 and 0.18241492\n",
            "Val acc and loss are 0.8886 and 0.31633744\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.93804 and 0.18201399\n",
            "Val acc and loss are 0.8885 and 0.31653482\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.93856 and 0.1817329\n",
            "Val acc and loss are 0.8881 and 0.3167618\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.93856 and 0.18156615\n",
            "Val acc and loss are 0.8884 and 0.31690738\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.93818 and 0.18152617\n",
            "Val acc and loss are 0.888 and 0.3169327\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.93826 and 0.18121503\n",
            "Val acc and loss are 0.8878 and 0.31665564\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.93896 and 0.18054298\n",
            "Val acc and loss are 0.8884 and 0.3160726\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.93936 and 0.18011934\n",
            "Val acc and loss are 0.8889 and 0.31597817\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.93928 and 0.17984547\n",
            "Val acc and loss are 0.8886 and 0.31602743\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.93926 and 0.17979938\n",
            "Val acc and loss are 0.8884 and 0.3163203\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.93952 and 0.17950138\n",
            "Val acc and loss are 0.8882 and 0.3162628\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.93964 and 0.17913535\n",
            "Val acc and loss are 0.8879 and 0.31604335\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.93992 and 0.17890109\n",
            "Val acc and loss are 0.8892 and 0.3159412\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.93994 and 0.17871712\n",
            "Val acc and loss are 0.8881 and 0.3158361\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.93948 and 0.17868932\n",
            "Val acc and loss are 0.8888 and 0.3160686\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.93986 and 0.17821643\n",
            "Val acc and loss are 0.8884 and 0.31605977\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.94018 and 0.1778404\n",
            "Val acc and loss are 0.8877 and 0.31605095\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.9402 and 0.1776398\n",
            "Val acc and loss are 0.8874 and 0.31608063\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.93992 and 0.17750764\n",
            "Val acc and loss are 0.8885 and 0.31609297\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.93974 and 0.17744781\n",
            "Val acc and loss are 0.8882 and 0.3160773\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.94 and 0.17705525\n",
            "Val acc and loss are 0.8883 and 0.31585795\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.94062 and 0.17670129\n",
            "Val acc and loss are 0.8883 and 0.31573302\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.94036 and 0.17654993\n",
            "Val acc and loss are 0.8888 and 0.3160269\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.9403 and 0.17651457\n",
            "Val acc and loss are 0.8886 and 0.31637877\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.94026 and 0.17616059\n",
            "Val acc and loss are 0.8883 and 0.31628653\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.94046 and 0.17582254\n",
            "Val acc and loss are 0.8884 and 0.31584892\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.94072 and 0.17548153\n",
            "Val acc and loss are 0.8893 and 0.31561783\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.9407 and 0.17520446\n",
            "Val acc and loss are 0.8885 and 0.31549835\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.94098 and 0.17526224\n",
            "Val acc and loss are 0.8888 and 0.31572077\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.94076 and 0.17502137\n",
            "Val acc and loss are 0.8887 and 0.31579784\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.94098 and 0.17475332\n",
            "Val acc and loss are 0.8881 and 0.31585672\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.94104 and 0.17443602\n",
            "Val acc and loss are 0.8883 and 0.3158934\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.94122 and 0.17420125\n",
            "Val acc and loss are 0.8891 and 0.31581226\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.9411 and 0.17413281\n",
            "Val acc and loss are 0.8886 and 0.31580368\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.94114 and 0.17376453\n",
            "Val acc and loss are 0.889 and 0.3156164\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.94172 and 0.17331252\n",
            "Val acc and loss are 0.889 and 0.31543002\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.9416 and 0.17312828\n",
            "Val acc and loss are 0.8884 and 0.31564307\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.94162 and 0.17293012\n",
            "Val acc and loss are 0.8882 and 0.31575018\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.94172 and 0.17259097\n",
            "Val acc and loss are 0.8891 and 0.31548417\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.94192 and 0.17227428\n",
            "Val acc and loss are 0.8891 and 0.31519732\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.9418 and 0.17206827\n",
            "Val acc and loss are 0.8897 and 0.31497172\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.94186 and 0.17192239\n",
            "Val acc and loss are 0.8895 and 0.31498334\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.94196 and 0.17167717\n",
            "Val acc and loss are 0.8896 and 0.315025\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.94218 and 0.17138453\n",
            "Val acc and loss are 0.8895 and 0.31521857\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.94238 and 0.17106348\n",
            "Val acc and loss are 0.8888 and 0.31537578\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.94214 and 0.17088586\n",
            "Val acc and loss are 0.8896 and 0.31551948\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.94212 and 0.17067254\n",
            "Val acc and loss are 0.8893 and 0.31545705\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.94226 and 0.17025253\n",
            "Val acc and loss are 0.8897 and 0.31515628\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.9428 and 0.16992867\n",
            "Val acc and loss are 0.889 and 0.31495997\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.94274 and 0.16964895\n",
            "Val acc and loss are 0.8888 and 0.31486005\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.94292 and 0.16950235\n",
            "Val acc and loss are 0.8896 and 0.31498834\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.94288 and 0.16941321\n",
            "Val acc and loss are 0.8895 and 0.31517535\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.94322 and 0.1691972\n",
            "Val acc and loss are 0.8895 and 0.31520838\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.94328 and 0.16888644\n",
            "Val acc and loss are 0.8896 and 0.3152748\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.94314 and 0.1686368\n",
            "Val acc and loss are 0.8893 and 0.31547275\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.94328 and 0.1684273\n",
            "Val acc and loss are 0.8885 and 0.31566876\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.94348 and 0.16830033\n",
            "Val acc and loss are 0.8881 and 0.31592584\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.9436 and 0.16787979\n",
            "Val acc and loss are 0.8883 and 0.31568116\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.94354 and 0.16771221\n",
            "Val acc and loss are 0.889 and 0.3156011\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.9436 and 0.16757055\n",
            "Val acc and loss are 0.8894 and 0.31553617\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.94378 and 0.16735017\n",
            "Val acc and loss are 0.8892 and 0.31559145\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.94384 and 0.1670934\n",
            "Val acc and loss are 0.8882 and 0.3156559\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.94414 and 0.16661781\n",
            "Val acc and loss are 0.8886 and 0.31558004\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.94398 and 0.16649638\n",
            "Val acc and loss are 0.8889 and 0.31578118\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.9437 and 0.16647416\n",
            "Val acc and loss are 0.8889 and 0.3156813\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.94388 and 0.16622257\n",
            "Val acc and loss are 0.8895 and 0.31521142\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.94426 and 0.16584806\n",
            "Val acc and loss are 0.8896 and 0.314843\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.94446 and 0.16556688\n",
            "Val acc and loss are 0.8891 and 0.31493413\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.94454 and 0.16532955\n",
            "Val acc and loss are 0.8893 and 0.31532735\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.94408 and 0.16535762\n",
            "Val acc and loss are 0.8889 and 0.3159406\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.94444 and 0.16505441\n",
            "Val acc and loss are 0.8888 and 0.31581274\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.94486 and 0.16448987\n",
            "Val acc and loss are 0.889 and 0.31511283\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.94492 and 0.16424046\n",
            "Val acc and loss are 0.8895 and 0.31468564\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.9449 and 0.1641397\n",
            "Val acc and loss are 0.8899 and 0.31480035\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.94466 and 0.16411327\n",
            "Val acc and loss are 0.8895 and 0.31544665\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.94518 and 0.16362669\n",
            "Val acc and loss are 0.8889 and 0.31563345\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.94534 and 0.16326629\n",
            "Val acc and loss are 0.8888 and 0.31561166\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.9454 and 0.16311151\n",
            "Val acc and loss are 0.889 and 0.3155756\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.94528 and 0.16309014\n",
            "Val acc and loss are 0.8897 and 0.3155769\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.94522 and 0.16315137\n",
            "Val acc and loss are 0.89 and 0.31570643\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.9453 and 0.1626423\n",
            "Val acc and loss are 0.89 and 0.31547427\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.94558 and 0.16227908\n",
            "Val acc and loss are 0.8894 and 0.3154386\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.94578 and 0.16193703\n",
            "Val acc and loss are 0.8898 and 0.31542113\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.94564 and 0.16213912\n",
            "Val acc and loss are 0.8885 and 0.315914\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.94584 and 0.16176811\n",
            "Val acc and loss are 0.8885 and 0.3158555\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.94614 and 0.16120344\n",
            "Val acc and loss are 0.8894 and 0.31551147\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.94646 and 0.16103248\n",
            "Val acc and loss are 0.889 and 0.31536686\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.94624 and 0.16083612\n",
            "Val acc and loss are 0.8891 and 0.31532714\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.946 and 0.16080005\n",
            "Val acc and loss are 0.8896 and 0.31561488\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.9465 and 0.16027911\n",
            "Val acc and loss are 0.8896 and 0.3153545\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.9468 and 0.15992665\n",
            "Val acc and loss are 0.8891 and 0.31520274\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.94676 and 0.15978037\n",
            "Val acc and loss are 0.8895 and 0.3151951\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.94628 and 0.15984115\n",
            "Val acc and loss are 0.8899 and 0.31535795\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.94654 and 0.15964052\n",
            "Val acc and loss are 0.8895 and 0.31523865\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.94686 and 0.15927392\n",
            "Val acc and loss are 0.8892 and 0.31503415\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.94698 and 0.15904129\n",
            "Val acc and loss are 0.8892 and 0.31504855\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.94684 and 0.15892601\n",
            "Val acc and loss are 0.8887 and 0.3151532\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.94692 and 0.15877764\n",
            "Val acc and loss are 0.8895 and 0.31528115\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.94708 and 0.1584181\n",
            "Val acc and loss are 0.89 and 0.31507573\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.94712 and 0.15812248\n",
            "Val acc and loss are 0.89 and 0.31485122\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.94734 and 0.15791522\n",
            "Val acc and loss are 0.8899 and 0.31468827\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.94752 and 0.15778123\n",
            "Val acc and loss are 0.8895 and 0.31489262\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.9472 and 0.15755713\n",
            "Val acc and loss are 0.889 and 0.3149795\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.9477 and 0.1571909\n",
            "Val acc and loss are 0.8894 and 0.31490213\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.94756 and 0.15703669\n",
            "Val acc and loss are 0.8895 and 0.31507468\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.9477 and 0.1568196\n",
            "Val acc and loss are 0.8896 and 0.31513596\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.94802 and 0.15676312\n",
            "Val acc and loss are 0.8892 and 0.31539002\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.94802 and 0.15638764\n",
            "Val acc and loss are 0.8888 and 0.31535676\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.94818 and 0.15592363\n",
            "Val acc and loss are 0.8887 and 0.315227\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.94816 and 0.15568495\n",
            "Val acc and loss are 0.8893 and 0.31499785\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.94814 and 0.15563008\n",
            "Val acc and loss are 0.8901 and 0.31499508\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.94794 and 0.15569152\n",
            "Val acc and loss are 0.8905 and 0.31530678\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.94804 and 0.15542755\n",
            "Val acc and loss are 0.8903 and 0.3154613\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.94846 and 0.15487336\n",
            "Val acc and loss are 0.8895 and 0.31523237\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.9487 and 0.15464759\n",
            "Val acc and loss are 0.8888 and 0.31543618\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.94884 and 0.15449758\n",
            "Val acc and loss are 0.8885 and 0.31551394\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.94856 and 0.15430632\n",
            "Val acc and loss are 0.8894 and 0.31541646\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.94852 and 0.15405168\n",
            "Val acc and loss are 0.8899 and 0.31519994\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.94858 and 0.15396619\n",
            "Val acc and loss are 0.8898 and 0.31521583\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.94856 and 0.15380391\n",
            "Val acc and loss are 0.89 and 0.31523517\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.94898 and 0.15351605\n",
            "Val acc and loss are 0.8901 and 0.31528327\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.94922 and 0.15325162\n",
            "Val acc and loss are 0.8898 and 0.31537947\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.9492 and 0.15297568\n",
            "Val acc and loss are 0.8895 and 0.3151388\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.94918 and 0.15276384\n",
            "Val acc and loss are 0.8893 and 0.31486997\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.94934 and 0.15249768\n",
            "Val acc and loss are 0.8892 and 0.31472304\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.94932 and 0.1522805\n",
            "Val acc and loss are 0.8893 and 0.315077\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.94922 and 0.15220489\n",
            "Val acc and loss are 0.889 and 0.31568637\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.94936 and 0.15200716\n",
            "Val acc and loss are 0.8891 and 0.3161321\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.94972 and 0.15182792\n",
            "Val acc and loss are 0.8891 and 0.3161041\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.94972 and 0.15152715\n",
            "Val acc and loss are 0.8894 and 0.31570283\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.9498 and 0.15133709\n",
            "Val acc and loss are 0.889 and 0.31553677\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.94982 and 0.15114701\n",
            "Val acc and loss are 0.8893 and 0.31558755\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.94996 and 0.15090397\n",
            "Val acc and loss are 0.8894 and 0.3156079\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.9502 and 0.15062428\n",
            "Val acc and loss are 0.8894 and 0.31562865\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.95018 and 0.15025108\n",
            "Val acc and loss are 0.8898 and 0.31518546\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.9501 and 0.15015207\n",
            "Val acc and loss are 0.8898 and 0.31483892\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.95026 and 0.1498853\n",
            "Val acc and loss are 0.8901 and 0.3145673\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.95014 and 0.14974257\n",
            "Val acc and loss are 0.8906 and 0.31471813\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.95038 and 0.14962333\n",
            "Val acc and loss are 0.8909 and 0.3150416\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.95066 and 0.149371\n",
            "Val acc and loss are 0.89 and 0.3151561\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.9508 and 0.14889757\n",
            "Val acc and loss are 0.8894 and 0.31470296\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.9508 and 0.14886424\n",
            "Val acc and loss are 0.8894 and 0.31470972\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.95084 and 0.1486767\n",
            "Val acc and loss are 0.8899 and 0.31487447\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.95072 and 0.1485311\n",
            "Val acc and loss are 0.8896 and 0.31518587\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.95084 and 0.14849971\n",
            "Val acc and loss are 0.8896 and 0.31559956\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.95066 and 0.1480471\n",
            "Val acc and loss are 0.8898 and 0.31535804\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.95108 and 0.14782996\n",
            "Val acc and loss are 0.889 and 0.31532714\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.95086 and 0.14773223\n",
            "Val acc and loss are 0.8895 and 0.3155298\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.95104 and 0.14747983\n",
            "Val acc and loss are 0.8887 and 0.31570467\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.95076 and 0.1474808\n",
            "Val acc and loss are 0.8894 and 0.3159628\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.95124 and 0.14703618\n",
            "Val acc and loss are 0.8892 and 0.3154854\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.9509 and 0.14699985\n",
            "Val acc and loss are 0.8892 and 0.31559473\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.95096 and 0.14671268\n",
            "Val acc and loss are 0.8893 and 0.31584468\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.95102 and 0.14660174\n",
            "Val acc and loss are 0.8886 and 0.31626213\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.9513 and 0.14614657\n",
            "Val acc and loss are 0.8891 and 0.31606105\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.95166 and 0.14587498\n",
            "Val acc and loss are 0.8898 and 0.315655\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.95178 and 0.14580517\n",
            "Val acc and loss are 0.8891 and 0.31564593\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.95174 and 0.14571933\n",
            "Val acc and loss are 0.8897 and 0.31577548\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.95186 and 0.14540341\n",
            "Val acc and loss are 0.8897 and 0.3156462\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.95232 and 0.14494233\n",
            "Val acc and loss are 0.8892 and 0.31545788\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.9524 and 0.14472237\n",
            "Val acc and loss are 0.8894 and 0.3156794\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.95242 and 0.14463939\n",
            "Val acc and loss are 0.8902 and 0.31600338\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.95248 and 0.14448874\n",
            "Val acc and loss are 0.8903 and 0.3161216\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.95242 and 0.14403757\n",
            "Val acc and loss are 0.8898 and 0.3156009\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.95296 and 0.14370026\n",
            "Val acc and loss are 0.8902 and 0.3153185\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.95286 and 0.14354138\n",
            "Val acc and loss are 0.8901 and 0.31528497\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.95276 and 0.14366448\n",
            "Val acc and loss are 0.8902 and 0.31560975\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.95302 and 0.14318262\n",
            "Val acc and loss are 0.8894 and 0.31534684\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.9528 and 0.14299011\n",
            "Val acc and loss are 0.8886 and 0.3154076\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.9528 and 0.14278251\n",
            "Val acc and loss are 0.8901 and 0.31532753\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.95258 and 0.14303496\n",
            "Val acc and loss are 0.8901 and 0.31577325\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.95286 and 0.1428265\n",
            "Val acc and loss are 0.8909 and 0.3157979\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.95324 and 0.14238966\n",
            "Val acc and loss are 0.8906 and 0.31553105\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.95322 and 0.14207095\n",
            "Val acc and loss are 0.89 and 0.31546128\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.95318 and 0.14210105\n",
            "Val acc and loss are 0.8897 and 0.31587753\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.95322 and 0.14197044\n",
            "Val acc and loss are 0.8893 and 0.31599367\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.9533 and 0.14147963\n",
            "Val acc and loss are 0.8899 and 0.3158376\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.95312 and 0.14128965\n",
            "Val acc and loss are 0.8907 and 0.3157344\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.95324 and 0.1412948\n",
            "Val acc and loss are 0.8906 and 0.31556875\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.95336 and 0.14108792\n",
            "Val acc and loss are 0.8908 and 0.31557447\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.95362 and 0.14070173\n",
            "Val acc and loss are 0.8894 and 0.31590956\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.95348 and 0.14048603\n",
            "Val acc and loss are 0.8895 and 0.3164902\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.9536 and 0.1403095\n",
            "Val acc and loss are 0.8898 and 0.31654635\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.95412 and 0.14012313\n",
            "Val acc and loss are 0.8898 and 0.3161455\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.95426 and 0.13977256\n",
            "Val acc and loss are 0.8901 and 0.3154894\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.95442 and 0.13945098\n",
            "Val acc and loss are 0.8897 and 0.3152954\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.95436 and 0.1393442\n",
            "Val acc and loss are 0.8893 and 0.3158583\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.9544 and 0.13938186\n",
            "Val acc and loss are 0.8894 and 0.31654868\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.95442 and 0.1390466\n",
            "Val acc and loss are 0.8897 and 0.31622162\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.95446 and 0.13884054\n",
            "Val acc and loss are 0.8895 and 0.31572714\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.95476 and 0.13890612\n",
            "Val acc and loss are 0.8904 and 0.31550217\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.95482 and 0.13894217\n",
            "Val acc and loss are 0.8899 and 0.31569365\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.95472 and 0.13837159\n",
            "Val acc and loss are 0.8904 and 0.3154887\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.95502 and 0.13790819\n",
            "Val acc and loss are 0.8893 and 0.31551307\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.95484 and 0.13770963\n",
            "Val acc and loss are 0.8899 and 0.31604984\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.95516 and 0.13776538\n",
            "Val acc and loss are 0.8895 and 0.31643265\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.95536 and 0.13750537\n",
            "Val acc and loss are 0.8902 and 0.31623766\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.9554 and 0.13725078\n",
            "Val acc and loss are 0.8901 and 0.31602007\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.95524 and 0.13713875\n",
            "Val acc and loss are 0.8901 and 0.31630126\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.95534 and 0.13709137\n",
            "Val acc and loss are 0.8898 and 0.31660908\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.95504 and 0.13716847\n",
            "Val acc and loss are 0.8896 and 0.31690487\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.95524 and 0.13661894\n",
            "Val acc and loss are 0.8897 and 0.3166618\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.95526 and 0.13622132\n",
            "Val acc and loss are 0.8895 and 0.31658006\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.95526 and 0.1360765\n",
            "Val acc and loss are 0.8903 and 0.31670907\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.9555 and 0.13627452\n",
            "Val acc and loss are 0.8903 and 0.31697622\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.95548 and 0.13588002\n",
            "Val acc and loss are 0.8902 and 0.31659052\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.95586 and 0.13548127\n",
            "Val acc and loss are 0.8899 and 0.31620213\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.95558 and 0.13532838\n",
            "Val acc and loss are 0.8902 and 0.3164274\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.95554 and 0.1355333\n",
            "Val acc and loss are 0.8901 and 0.31710342\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.95568 and 0.13527556\n",
            "Val acc and loss are 0.8905 and 0.31700298\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.95606 and 0.13464834\n",
            "Val acc and loss are 0.8897 and 0.3165418\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.95638 and 0.13443993\n",
            "Val acc and loss are 0.8894 and 0.316623\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.95628 and 0.13442165\n",
            "Val acc and loss are 0.89 and 0.31699204\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.95634 and 0.1343042\n",
            "Val acc and loss are 0.8903 and 0.31722406\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.95644 and 0.13381962\n",
            "Val acc and loss are 0.8898 and 0.31677368\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.95636 and 0.13363537\n",
            "Val acc and loss are 0.8901 and 0.31640998\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.95632 and 0.13351473\n",
            "Val acc and loss are 0.8904 and 0.31624576\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.95648 and 0.13336052\n",
            "Val acc and loss are 0.8906 and 0.3161842\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.95664 and 0.13334446\n",
            "Val acc and loss are 0.8901 and 0.31659335\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.95646 and 0.13306394\n",
            "Val acc and loss are 0.8902 and 0.31686407\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.95638 and 0.1326725\n",
            "Val acc and loss are 0.8893 and 0.3166679\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.95688 and 0.13261533\n",
            "Val acc and loss are 0.8896 and 0.31659156\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.9567 and 0.13260978\n",
            "Val acc and loss are 0.8901 and 0.31654933\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.95682 and 0.13228399\n",
            "Val acc and loss are 0.8901 and 0.31630343\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.95696 and 0.13215104\n",
            "Val acc and loss are 0.8896 and 0.31638682\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.957 and 0.13194409\n",
            "Val acc and loss are 0.8896 and 0.316646\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.95684 and 0.1320553\n",
            "Val acc and loss are 0.8888 and 0.3172414\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.95704 and 0.13175955\n",
            "Val acc and loss are 0.8894 and 0.31726554\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.95708 and 0.13148397\n",
            "Val acc and loss are 0.8892 and 0.31697673\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.95706 and 0.13120477\n",
            "Val acc and loss are 0.8902 and 0.31680757\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.95724 and 0.13134304\n",
            "Val acc and loss are 0.8898 and 0.31750533\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.95714 and 0.13110088\n",
            "Val acc and loss are 0.8898 and 0.31773326\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.95746 and 0.13061471\n",
            "Val acc and loss are 0.8907 and 0.31715977\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.9575 and 0.13052118\n",
            "Val acc and loss are 0.8908 and 0.31715825\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.95774 and 0.13003777\n",
            "Val acc and loss are 0.8906 and 0.31677213\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.95778 and 0.13018367\n",
            "Val acc and loss are 0.8901 and 0.31717855\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.95806 and 0.12970683\n",
            "Val acc and loss are 0.8903 and 0.31678158\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.95782 and 0.12964208\n",
            "Val acc and loss are 0.8895 and 0.3168902\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.95764 and 0.1295074\n",
            "Val acc and loss are 0.8898 and 0.31728315\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.95766 and 0.12966509\n",
            "Val acc and loss are 0.8907 and 0.31794107\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.95754 and 0.12954482\n",
            "Val acc and loss are 0.8911 and 0.31795675\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.9579 and 0.12884401\n",
            "Val acc and loss are 0.891 and 0.3168727\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.95822 and 0.12863049\n",
            "Val acc and loss are 0.8905 and 0.31659755\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.95824 and 0.128369\n",
            "Val acc and loss are 0.8901 and 0.31696886\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.95818 and 0.12849294\n",
            "Val acc and loss are 0.8904 and 0.31776035\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.95846 and 0.12805423\n",
            "Val acc and loss are 0.8902 and 0.3175227\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.95842 and 0.12785903\n",
            "Val acc and loss are 0.8907 and 0.31677645\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.95874 and 0.12797944\n",
            "Val acc and loss are 0.8917 and 0.3166245\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.95842 and 0.12779701\n",
            "Val acc and loss are 0.8913 and 0.31670037\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.95866 and 0.12730433\n",
            "Val acc and loss are 0.8902 and 0.31694648\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.9588 and 0.12703198\n",
            "Val acc and loss are 0.8898 and 0.3174037\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.95906 and 0.12686044\n",
            "Val acc and loss are 0.8904 and 0.31747353\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.95926 and 0.12657997\n",
            "Val acc and loss are 0.8907 and 0.31697628\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.95908 and 0.12658511\n",
            "Val acc and loss are 0.891 and 0.31684706\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.9591 and 0.12647256\n",
            "Val acc and loss are 0.8909 and 0.31703767\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.95904 and 0.12647265\n",
            "Val acc and loss are 0.8909 and 0.3177129\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.9589 and 0.12595494\n",
            "Val acc and loss are 0.8905 and 0.31767213\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.95946 and 0.12564553\n",
            "Val acc and loss are 0.8896 and 0.31759965\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.95938 and 0.12545052\n",
            "Val acc and loss are 0.8905 and 0.3173116\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.959 and 0.1255377\n",
            "Val acc and loss are 0.8908 and 0.3176479\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.95926 and 0.12532568\n",
            "Val acc and loss are 0.8909 and 0.3176857\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.95948 and 0.12500747\n",
            "Val acc and loss are 0.8909 and 0.3176238\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.9594 and 0.124934725\n",
            "Val acc and loss are 0.8907 and 0.31767118\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.95948 and 0.12500069\n",
            "Val acc and loss are 0.8904 and 0.31779215\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.9595 and 0.12466513\n",
            "Val acc and loss are 0.8903 and 0.3175727\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.95966 and 0.124455236\n",
            "Val acc and loss are 0.8901 and 0.31754565\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.95972 and 0.12436441\n",
            "Val acc and loss are 0.8904 and 0.31775516\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.95982 and 0.12414721\n",
            "Val acc and loss are 0.8904 and 0.3177646\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.9601 and 0.12380586\n",
            "Val acc and loss are 0.89 and 0.31766835\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.96038 and 0.12356703\n",
            "Val acc and loss are 0.8903 and 0.31781283\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.9602 and 0.12345078\n",
            "Val acc and loss are 0.891 and 0.31826285\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.9599 and 0.123407386\n",
            "Val acc and loss are 0.8901 and 0.31843844\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.95998 and 0.123205915\n",
            "Val acc and loss are 0.8902 and 0.317895\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.9598 and 0.123287596\n",
            "Val acc and loss are 0.8911 and 0.3177517\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.95998 and 0.12301579\n",
            "Val acc and loss are 0.8912 and 0.31785688\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.95996 and 0.12275821\n",
            "Val acc and loss are 0.8902 and 0.31818753\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.96024 and 0.12237944\n",
            "Val acc and loss are 0.891 and 0.31810543\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.9605 and 0.12230008\n",
            "Val acc and loss are 0.8919 and 0.31803346\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.9607 and 0.122269355\n",
            "Val acc and loss are 0.8915 and 0.31781363\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.96044 and 0.12201135\n",
            "Val acc and loss are 0.891 and 0.317193\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.96042 and 0.1217855\n",
            "Val acc and loss are 0.891 and 0.31689122\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.96056 and 0.1216435\n",
            "Val acc and loss are 0.8911 and 0.31727445\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.96032 and 0.12172553\n",
            "Val acc and loss are 0.8916 and 0.31801453\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.96066 and 0.121398605\n",
            "Val acc and loss are 0.8912 and 0.31810737\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.96082 and 0.12092718\n",
            "Val acc and loss are 0.8909 and 0.3176028\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.96078 and 0.120737284\n",
            "Val acc and loss are 0.8908 and 0.31723353\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.96094 and 0.12051037\n",
            "Val acc and loss are 0.8906 and 0.31732216\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.961 and 0.12053264\n",
            "Val acc and loss are 0.8916 and 0.31795716\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.96104 and 0.12068642\n",
            "Val acc and loss are 0.8914 and 0.3187118\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.96116 and 0.120224394\n",
            "Val acc and loss are 0.8913 and 0.3184432\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.96172 and 0.11990391\n",
            "Val acc and loss are 0.891 and 0.318062\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.96146 and 0.11966004\n",
            "Val acc and loss are 0.8905 and 0.31789997\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.96154 and 0.11980187\n",
            "Val acc and loss are 0.8914 and 0.3186627\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.96126 and 0.11957401\n",
            "Val acc and loss are 0.8918 and 0.31903276\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.96198 and 0.118998915\n",
            "Val acc and loss are 0.8906 and 0.31860322\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.9619 and 0.1188994\n",
            "Val acc and loss are 0.8915 and 0.31830674\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.9616 and 0.11914093\n",
            "Val acc and loss are 0.8919 and 0.31850603\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.96144 and 0.11933832\n",
            "Val acc and loss are 0.8916 and 0.3188419\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.96176 and 0.11882332\n",
            "Val acc and loss are 0.8916 and 0.3187804\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.96192 and 0.118566886\n",
            "Val acc and loss are 0.8914 and 0.31906646\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.96184 and 0.118286856\n",
            "Val acc and loss are 0.8911 and 0.3192535\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.96176 and 0.11831435\n",
            "Val acc and loss are 0.8914 and 0.31925762\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.9621 and 0.1177943\n",
            "Val acc and loss are 0.8908 and 0.3188823\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.9623 and 0.1177067\n",
            "Val acc and loss are 0.8904 and 0.31880465\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.9622 and 0.117647715\n",
            "Val acc and loss are 0.8913 and 0.31896177\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.96226 and 0.11744032\n",
            "Val acc and loss are 0.8915 and 0.31902796\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.96244 and 0.11704265\n",
            "Val acc and loss are 0.8913 and 0.31881845\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.9629 and 0.11685856\n",
            "Val acc and loss are 0.8916 and 0.31878507\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.96292 and 0.116636954\n",
            "Val acc and loss are 0.8909 and 0.3186683\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.96288 and 0.11645914\n",
            "Val acc and loss are 0.8911 and 0.31865942\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.9628 and 0.11628323\n",
            "Val acc and loss are 0.8915 and 0.31865054\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.96268 and 0.116301835\n",
            "Val acc and loss are 0.8919 and 0.3187432\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.96248 and 0.11638348\n",
            "Val acc and loss are 0.8911 and 0.31892067\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.9628 and 0.11613318\n",
            "Val acc and loss are 0.8914 and 0.3187097\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.96304 and 0.115975216\n",
            "Val acc and loss are 0.8916 and 0.3188023\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.9631 and 0.11569867\n",
            "Val acc and loss are 0.8915 and 0.31905752\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.96304 and 0.115529075\n",
            "Val acc and loss are 0.8911 and 0.31928915\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.96286 and 0.11536911\n",
            "Val acc and loss are 0.8913 and 0.31965077\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.96314 and 0.115187414\n",
            "Val acc and loss are 0.8908 and 0.32002568\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.9632 and 0.11523906\n",
            "Val acc and loss are 0.8912 and 0.32035017\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.96324 and 0.11504908\n",
            "Val acc and loss are 0.8919 and 0.32000658\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.96296 and 0.11485209\n",
            "Val acc and loss are 0.8911 and 0.3198282\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.96316 and 0.11455801\n",
            "Val acc and loss are 0.8912 and 0.31959742\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.96336 and 0.114241\n",
            "Val acc and loss are 0.8911 and 0.31952477\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.96342 and 0.11398981\n",
            "Val acc and loss are 0.8914 and 0.3195332\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.96338 and 0.11381703\n",
            "Val acc and loss are 0.8915 and 0.31967905\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.96392 and 0.113550745\n",
            "Val acc and loss are 0.891 and 0.3198097\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.96372 and 0.1134392\n",
            "Val acc and loss are 0.8906 and 0.32005492\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.96354 and 0.113596536\n",
            "Val acc and loss are 0.8911 and 0.3204125\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.9636 and 0.11333621\n",
            "Val acc and loss are 0.8913 and 0.31997553\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.96392 and 0.11304152\n",
            "Val acc and loss are 0.8917 and 0.31960952\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.96412 and 0.11277861\n",
            "Val acc and loss are 0.8909 and 0.3201323\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.96362 and 0.11307103\n",
            "Val acc and loss are 0.8904 and 0.32134107\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.96372 and 0.11306772\n",
            "Val acc and loss are 0.8904 and 0.32161015\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.964 and 0.112658225\n",
            "Val acc and loss are 0.8919 and 0.3207517\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.96432 and 0.112442985\n",
            "Val acc and loss are 0.8914 and 0.3203273\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.96398 and 0.11241941\n",
            "Val acc and loss are 0.8913 and 0.32072636\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.96394 and 0.11245643\n",
            "Val acc and loss are 0.8913 and 0.32110104\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.9644 and 0.11170924\n",
            "Val acc and loss are 0.8915 and 0.3204101\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.96472 and 0.11170713\n",
            "Val acc and loss are 0.8909 and 0.3206724\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.96444 and 0.11170536\n",
            "Val acc and loss are 0.8917 and 0.32066205\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.96424 and 0.11198188\n",
            "Val acc and loss are 0.892 and 0.3205883\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.96462 and 0.111490466\n",
            "Val acc and loss are 0.8921 and 0.31966278\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.96498 and 0.11106914\n",
            "Val acc and loss are 0.8914 and 0.31974578\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.96486 and 0.11079582\n",
            "Val acc and loss are 0.8912 and 0.32056382\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.96402 and 0.11136279\n",
            "Val acc and loss are 0.8909 and 0.32191852\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.96482 and 0.11055633\n",
            "Val acc and loss are 0.8915 and 0.32091072\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.96472 and 0.11040748\n",
            "Val acc and loss are 0.8919 and 0.3202707\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.96458 and 0.11041606\n",
            "Val acc and loss are 0.8918 and 0.3204784\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.9647 and 0.11057172\n",
            "Val acc and loss are 0.8912 and 0.3214941\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.9649 and 0.110241994\n",
            "Val acc and loss are 0.8911 and 0.32208502\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.9649 and 0.109957345\n",
            "Val acc and loss are 0.8905 and 0.32206878\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.96512 and 0.10982632\n",
            "Val acc and loss are 0.8908 and 0.32168585\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.96522 and 0.10953209\n",
            "Val acc and loss are 0.8908 and 0.32101256\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.9653 and 0.10959012\n",
            "Val acc and loss are 0.8908 and 0.32092842\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.96542 and 0.108979926\n",
            "Val acc and loss are 0.8915 and 0.3206522\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.96524 and 0.109115794\n",
            "Val acc and loss are 0.8908 and 0.32122222\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.96508 and 0.10899715\n",
            "Val acc and loss are 0.8915 and 0.32152346\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.9656 and 0.10868707\n",
            "Val acc and loss are 0.8917 and 0.32127643\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.96618 and 0.108276404\n",
            "Val acc and loss are 0.8915 and 0.3207093\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.96594 and 0.108152926\n",
            "Val acc and loss are 0.8911 and 0.3206109\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.96522 and 0.1083804\n",
            "Val acc and loss are 0.8909 and 0.32155818\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.96512 and 0.108484305\n",
            "Val acc and loss are 0.8909 and 0.32219225\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.96568 and 0.10804791\n",
            "Val acc and loss are 0.8912 and 0.32196036\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.96672 and 0.10757658\n",
            "Val acc and loss are 0.891 and 0.32127064\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.96614 and 0.10754887\n",
            "Val acc and loss are 0.8911 and 0.3211329\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.96542 and 0.107593246\n",
            "Val acc and loss are 0.8918 and 0.3214137\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.96588 and 0.107389025\n",
            "Val acc and loss are 0.8917 and 0.32148546\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.96628 and 0.107293025\n",
            "Val acc and loss are 0.8914 and 0.32169917\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.96644 and 0.10700833\n",
            "Val acc and loss are 0.8912 and 0.3219916\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.96582 and 0.10708745\n",
            "Val acc and loss are 0.8908 and 0.32210377\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.96592 and 0.107014135\n",
            "Val acc and loss are 0.8908 and 0.32173556\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.96652 and 0.10638849\n",
            "Val acc and loss are 0.8906 and 0.32117236\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.96646 and 0.10637672\n",
            "Val acc and loss are 0.8911 and 0.32189614\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.96572 and 0.10666248\n",
            "Val acc and loss are 0.8909 and 0.3227306\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.96586 and 0.10640084\n",
            "Val acc and loss are 0.8915 and 0.32236907\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.96634 and 0.1057601\n",
            "Val acc and loss are 0.8915 and 0.3215889\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.96688 and 0.10554031\n",
            "Val acc and loss are 0.8913 and 0.32149938\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.96648 and 0.10555766\n",
            "Val acc and loss are 0.8914 and 0.3218388\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.96638 and 0.105472244\n",
            "Val acc and loss are 0.8917 and 0.32204953\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.96642 and 0.105268456\n",
            "Val acc and loss are 0.8919 and 0.32185826\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.96672 and 0.10515465\n",
            "Val acc and loss are 0.8916 and 0.32147646\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.96656 and 0.10516643\n",
            "Val acc and loss are 0.8914 and 0.32175684\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.96622 and 0.10509839\n",
            "Val acc and loss are 0.8914 and 0.32226577\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.96678 and 0.10482583\n",
            "Val acc and loss are 0.8914 and 0.32267627\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.96698 and 0.10456306\n",
            "Val acc and loss are 0.8912 and 0.32298487\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.96726 and 0.10422922\n",
            "Val acc and loss are 0.8913 and 0.32254994\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.96696 and 0.10422255\n",
            "Val acc and loss are 0.8914 and 0.32216585\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.96714 and 0.10430154\n",
            "Val acc and loss are 0.8916 and 0.32191852\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.96692 and 0.104182854\n",
            "Val acc and loss are 0.8912 and 0.3219942\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.96736 and 0.10375369\n",
            "Val acc and loss are 0.8916 and 0.32215294\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.96774 and 0.10350894\n",
            "Val acc and loss are 0.8911 and 0.32254758\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.96782 and 0.103386775\n",
            "Val acc and loss are 0.8908 and 0.32261118\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.96742 and 0.1033025\n",
            "Val acc and loss are 0.8909 and 0.32231435\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.96754 and 0.10316586\n",
            "Val acc and loss are 0.8919 and 0.322277\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.96728 and 0.10306423\n",
            "Val acc and loss are 0.8915 and 0.32226333\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.96764 and 0.103001215\n",
            "Val acc and loss are 0.8915 and 0.32251507\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.96742 and 0.102815956\n",
            "Val acc and loss are 0.8916 and 0.32250494\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.96774 and 0.102682985\n",
            "Val acc and loss are 0.8919 and 0.32240942\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.9678 and 0.10244513\n",
            "Val acc and loss are 0.8915 and 0.32232052\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.96786 and 0.102287635\n",
            "Val acc and loss are 0.8918 and 0.32252187\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.96812 and 0.102000095\n",
            "Val acc and loss are 0.8917 and 0.32262814\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.96828 and 0.10186441\n",
            "Val acc and loss are 0.8916 and 0.32273057\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.96774 and 0.10209007\n",
            "Val acc and loss are 0.8915 and 0.32292452\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.96792 and 0.101943985\n",
            "Val acc and loss are 0.8917 and 0.3227267\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.9685 and 0.101508185\n",
            "Val acc and loss are 0.8925 and 0.32228458\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.96848 and 0.10128793\n",
            "Val acc and loss are 0.892 and 0.32263973\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.96872 and 0.10104709\n",
            "Val acc and loss are 0.891 and 0.3228515\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.96848 and 0.10092889\n",
            "Val acc and loss are 0.8913 and 0.3227023\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.96824 and 0.10089189\n",
            "Val acc and loss are 0.8915 and 0.32233766\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.96848 and 0.10080821\n",
            "Val acc and loss are 0.8921 and 0.32209432\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.96878 and 0.100616336\n",
            "Val acc and loss are 0.8919 and 0.32240507\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.9685 and 0.10073765\n",
            "Val acc and loss are 0.8915 and 0.323087\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.96846 and 0.1003874\n",
            "Val acc and loss are 0.8919 and 0.3229153\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.96902 and 0.10010181\n",
            "Val acc and loss are 0.8924 and 0.3228833\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.96842 and 0.09999421\n",
            "Val acc and loss are 0.8916 and 0.32300246\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.96876 and 0.09975892\n",
            "Val acc and loss are 0.8914 and 0.32316864\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.9691 and 0.09966883\n",
            "Val acc and loss are 0.8906 and 0.3235012\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.96904 and 0.09947373\n",
            "Val acc and loss are 0.8911 and 0.32370508\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.9687 and 0.099565595\n",
            "Val acc and loss are 0.891 and 0.32398173\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.96892 and 0.09950502\n",
            "Val acc and loss are 0.8912 and 0.32409242\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.9691 and 0.09933219\n",
            "Val acc and loss are 0.8909 and 0.32425985\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.96952 and 0.098966315\n",
            "Val acc and loss are 0.8914 and 0.32418087\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.9696 and 0.09878766\n",
            "Val acc and loss are 0.8918 and 0.32409397\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.96954 and 0.09873823\n",
            "Val acc and loss are 0.8918 and 0.32405135\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.9697 and 0.09863429\n",
            "Val acc and loss are 0.891 and 0.3241917\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.96958 and 0.09852805\n",
            "Val acc and loss are 0.8907 and 0.32428047\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.97012 and 0.09827895\n",
            "Val acc and loss are 0.8912 and 0.32439178\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.97022 and 0.098136075\n",
            "Val acc and loss are 0.8917 and 0.32433012\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.96964 and 0.09821637\n",
            "Val acc and loss are 0.8919 and 0.3246125\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.9698 and 0.09802756\n",
            "Val acc and loss are 0.8918 and 0.32408446\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.96964 and 0.09779582\n",
            "Val acc and loss are 0.8915 and 0.3237281\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.96962 and 0.09764367\n",
            "Val acc and loss are 0.8922 and 0.32389\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.96966 and 0.09766731\n",
            "Val acc and loss are 0.8922 and 0.32427526\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.9696 and 0.097520895\n",
            "Val acc and loss are 0.892 and 0.32447758\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.96968 and 0.09719311\n",
            "Val acc and loss are 0.8925 and 0.32435328\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.97036 and 0.09678709\n",
            "Val acc and loss are 0.8918 and 0.3241922\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.97054 and 0.09670451\n",
            "Val acc and loss are 0.8916 and 0.32445163\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.96984 and 0.09687383\n",
            "Val acc and loss are 0.8921 and 0.3249683\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.9697 and 0.09707125\n",
            "Val acc and loss are 0.8914 and 0.32526085\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.9701 and 0.09658123\n",
            "Val acc and loss are 0.8918 and 0.32480752\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.97032 and 0.096107654\n",
            "Val acc and loss are 0.8917 and 0.32465857\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.97044 and 0.09588094\n",
            "Val acc and loss are 0.8918 and 0.32479972\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.97014 and 0.09570507\n",
            "Val acc and loss are 0.8928 and 0.32473403\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.97012 and 0.095785804\n",
            "Val acc and loss are 0.8925 and 0.32459024\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.97086 and 0.09561516\n",
            "Val acc and loss are 0.8928 and 0.32433257\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.97058 and 0.09554336\n",
            "Val acc and loss are 0.8926 and 0.32475597\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.97036 and 0.09544465\n",
            "Val acc and loss are 0.8922 and 0.32527614\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.97042 and 0.09534124\n",
            "Val acc and loss are 0.8919 and 0.32549688\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.97064 and 0.09510923\n",
            "Val acc and loss are 0.8917 and 0.32517558\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.97128 and 0.09471682\n",
            "Val acc and loss are 0.8922 and 0.32439733\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.97108 and 0.094684355\n",
            "Val acc and loss are 0.8918 and 0.32441944\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.97054 and 0.095262416\n",
            "Val acc and loss are 0.8925 and 0.32563725\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.97064 and 0.0949934\n",
            "Val acc and loss are 0.8926 and 0.32579136\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.97112 and 0.09446128\n",
            "Val acc and loss are 0.892 and 0.3253594\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.97116 and 0.09428762\n",
            "Val acc and loss are 0.8919 and 0.32497948\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.97014 and 0.094793215\n",
            "Val acc and loss are 0.8916 and 0.32535234\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.97002 and 0.09525041\n",
            "Val acc and loss are 0.892 and 0.32582328\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.97108 and 0.09421882\n",
            "Val acc and loss are 0.893 and 0.3252372\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.97156 and 0.09391828\n",
            "Val acc and loss are 0.892 and 0.32568488\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.97132 and 0.09365125\n",
            "Val acc and loss are 0.8917 and 0.32595968\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.97088 and 0.09390461\n",
            "Val acc and loss are 0.8916 and 0.32613626\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.97164 and 0.093374394\n",
            "Val acc and loss are 0.892 and 0.32492894\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.97162 and 0.0935955\n",
            "Val acc and loss are 0.8935 and 0.32466704\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.97112 and 0.093472615\n",
            "Val acc and loss are 0.8932 and 0.32545412\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.97068 and 0.09397292\n",
            "Val acc and loss are 0.8919 and 0.32717645\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.97132 and 0.09328502\n",
            "Val acc and loss are 0.8909 and 0.3270185\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.97226 and 0.09263647\n",
            "Val acc and loss are 0.8927 and 0.3261687\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.97178 and 0.09244091\n",
            "Val acc and loss are 0.8925 and 0.325906\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.9711 and 0.09288279\n",
            "Val acc and loss are 0.8914 and 0.32667533\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.97136 and 0.09243214\n",
            "Val acc and loss are 0.8929 and 0.32603773\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.97202 and 0.09222859\n",
            "Val acc and loss are 0.8929 and 0.32587433\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.97152 and 0.09225246\n",
            "Val acc and loss are 0.8923 and 0.32625163\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.97164 and 0.09224578\n",
            "Val acc and loss are 0.8929 and 0.32651266\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.97114 and 0.0922073\n",
            "Val acc and loss are 0.8931 and 0.32654512\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.97164 and 0.091701135\n",
            "Val acc and loss are 0.8924 and 0.32639757\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.972 and 0.091407664\n",
            "Val acc and loss are 0.8919 and 0.3269225\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.97222 and 0.09127908\n",
            "Val acc and loss are 0.8914 and 0.32738715\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.97152 and 0.091495186\n",
            "Val acc and loss are 0.892 and 0.3277791\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.97174 and 0.09143835\n",
            "Val acc and loss are 0.893 and 0.32714975\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.97216 and 0.09139\n",
            "Val acc and loss are 0.8928 and 0.32674176\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.97234 and 0.091244355\n",
            "Val acc and loss are 0.8929 and 0.32694325\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.97214 and 0.091086835\n",
            "Val acc and loss are 0.8925 and 0.32769942\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.97204 and 0.09096108\n",
            "Val acc and loss are 0.8912 and 0.32833266\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.97264 and 0.09030982\n",
            "Val acc and loss are 0.8913 and 0.32744622\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.97278 and 0.09015701\n",
            "Val acc and loss are 0.892 and 0.32688555\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.97208 and 0.090374276\n",
            "Val acc and loss are 0.8925 and 0.32711187\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.97172 and 0.09072912\n",
            "Val acc and loss are 0.8926 and 0.32765758\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.97248 and 0.09012252\n",
            "Val acc and loss are 0.8924 and 0.327305\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.97338 and 0.08991852\n",
            "Val acc and loss are 0.8919 and 0.32694498\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.97294 and 0.08945033\n",
            "Val acc and loss are 0.8922 and 0.32646826\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.9722 and 0.08982333\n",
            "Val acc and loss are 0.8915 and 0.32712832\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.97218 and 0.089688025\n",
            "Val acc and loss are 0.8927 and 0.3271365\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.97308 and 0.08912811\n",
            "Val acc and loss are 0.892 and 0.32645562\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.9736 and 0.0889906\n",
            "Val acc and loss are 0.8916 and 0.32631168\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.97276 and 0.08929499\n",
            "Val acc and loss are 0.8915 and 0.32700264\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.97248 and 0.08961222\n",
            "Val acc and loss are 0.8922 and 0.32742438\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.97322 and 0.08884958\n",
            "Val acc and loss are 0.8927 and 0.32648632\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.97352 and 0.089090765\n",
            "Val acc and loss are 0.8926 and 0.32721674\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.97224 and 0.08896732\n",
            "Val acc and loss are 0.8919 and 0.3280453\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.97224 and 0.08950095\n",
            "Val acc and loss are 0.8904 and 0.32890326\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.97356 and 0.08838563\n",
            "Val acc and loss are 0.8911 and 0.3274248\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.97406 and 0.088438846\n",
            "Val acc and loss are 0.8922 and 0.32716912\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.97296 and 0.088461824\n",
            "Val acc and loss are 0.8923 and 0.32769644\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.97298 and 0.08867758\n",
            "Val acc and loss are 0.8915 and 0.32844898\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.97368 and 0.08786734\n",
            "Val acc and loss are 0.8928 and 0.3276351\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.97422 and 0.08765418\n",
            "Val acc and loss are 0.8915 and 0.32715702\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.97368 and 0.08771496\n",
            "Val acc and loss are 0.8918 and 0.32688484\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.97298 and 0.08843913\n",
            "Val acc and loss are 0.8913 and 0.32795107\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.97332 and 0.08764155\n",
            "Val acc and loss are 0.8925 and 0.32767236\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.97414 and 0.087589376\n",
            "Val acc and loss are 0.8916 and 0.32834005\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.97416 and 0.08719957\n",
            "Val acc and loss are 0.8918 and 0.3283874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtVhOZUWQ07W"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "jJldzUvwQ07X",
        "outputId": "2ab1e358-4517-402b-fcc5-4ec463e1f7d7"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.97422 achieved at epoch: 994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOTEyRmfQ07X",
        "outputId": "e9c83dd9-2814-4b03-c6b0-3a1fc3f14755"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "confusion_matrix(y_train, pred_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4614,    3,   37,   84,    6,    1,  228,    0,    4,    0],\n",
              "       [   5, 4974,    4,   25,    2,    0,    1,    0,    1,    0],\n",
              "       [  40,    1, 4537,   26,  221,    1,  162,    0,    3,    1],\n",
              "       [  67,   19,   31, 4692,   99,    0,   64,    0,    6,    1],\n",
              "       [  13,    4,  255,   92, 4393,    0,  183,    0,   10,    0],\n",
              "       [   1,    0,    0,    0,    0, 4940,    1,   55,    2,    5],\n",
              "       [ 307,    4,  210,   70,  177,    1, 4250,    0,   11,    0],\n",
              "       [   0,    0,    0,    0,    0,   19,    0, 4981,    3,   42],\n",
              "       [   5,    1,    5,    9,   11,    2,    8,    3, 4987,    1],\n",
              "       [   0,    1,    0,    0,    0,   13,    0,   83,    1, 4881]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "GBlFh_SHQ07X",
        "outputId": "a29ca16b-3d9a-4be9-aec1-9f858fa14a0a"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8935 achieved at epoch: 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9adk080Q07Y",
        "outputId": "fa52c4b1-8fdc-4731-fa37-78b10c7f51b8"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "confusion_matrix(y_val, pred_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[859,   0,  15,  32,   2,   2, 105,   0,   7,   1],\n",
              "       [  2, 966,   0,  12,   2,   0,   4,   0,   2,   0],\n",
              "       [ 14,   0, 842,   9,  80,   0,  57,   0,   6,   0],\n",
              "       [ 24,   5,   9, 923,  33,   0,  24,   0,   2,   1],\n",
              "       [  3,   3,  82,  25, 875,   1,  56,   0,   5,   0],\n",
              "       [  1,   0,   0,   0,   0, 946,   0,  31,   7,  11],\n",
              "       [108,   2,  73,  16,  75,   0, 682,   0,  13,   1],\n",
              "       [  0,   0,   1,   0,   0,  14,   0, 917,   1,  22],\n",
              "       [  3,   1,   4,   4,   5,   2,   6,   4, 937,   2],\n",
              "       [  0,   0,   0,   0,   0,   9,   0,  41,   0, 971]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzHxKr9Q07Y",
        "outputId": "82a039c5-20ba-4d34-be11-5a57a2063a6e"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3545027\n",
            "0.8851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FldhaWwQ07Y",
        "outputId": "3d3b9dc6-4fa2-4f66-9ad4-ab221e8f132a"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[841,   2,  17,  24,   5,   1,  99,   0,  11,   0],\n",
              "       [  1, 967,   2,  22,   2,   0,   5,   0,   1,   0],\n",
              "       [ 17,   1, 828,   8,  81,   1,  62,   1,   1,   0],\n",
              "       [ 21,   6,  16, 891,  32,   1,  28,   0,   5,   0],\n",
              "       [  1,   1,  96,  29, 819,   1,  50,   0,   3,   0],\n",
              "       [  1,   0,   0,   1,   0, 942,   0,  32,   3,  21],\n",
              "       [113,   0,  98,  26,  66,   0, 684,   0,  12,   1],\n",
              "       [  0,   0,   0,   0,   0,  19,   0, 959,   1,  21],\n",
              "       [  6,   1,   4,   5,   4,   4,   7,   5, 963,   1],\n",
              "       [  0,   0,   0,   0,   0,   7,   1,  35,   0, 957]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKNybY3XQ7kI"
      },
      "source": [
        "# **Test 10** *Using 2-layer MLP with 128 nodes in the hidden layer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0zxRJUTQ_ns"
      },
      "source": [
        "nData, dim = x_train.shape; num_labels = 10\n",
        "layers = 2; nodes_per_layer = [dim, 128, num_labels]; learn_rate = 0.001; batch_size = nData; reg_coeff = 2e-06; drop_prob = 0.4\n",
        "\n",
        "# Reset everytime we build a new model.\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "MLP = build_model(layers, nodes_per_layer, learn_rate, num_labels = num_labels, hlactivation = \"tanh\", optimizer_name = 'Adam', reg_coeff = reg_coeff, drop_prob = drop_prob)\n",
        "\n",
        "# Initialize all variables in the constructed graph (resulting from model construction)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNLZdTsXQ_nz",
        "outputId": "a7efd0ed-f17d-435a-9a54-508ec59c0ad1"
      },
      "source": [
        "train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = train_model(MLP, 1000, batch_size, x_train, y_train, y_train_oh, x_val, y_val, y_val_oh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Epoch 1\n",
            "Training acc and loss are 0.3029 and 2.0005422\n",
            "Val acc and loss are 0.304 and 1.999875\n",
            "Processing Epoch 2\n",
            "Training acc and loss are 0.51474 and 1.5167835\n",
            "Val acc and loss are 0.5129 and 1.5135896\n",
            "Processing Epoch 3\n",
            "Training acc and loss are 0.58918 and 1.2454394\n",
            "Val acc and loss are 0.5877 and 1.239143\n",
            "Processing Epoch 4\n",
            "Training acc and loss are 0.63698 and 1.0837244\n",
            "Val acc and loss are 0.6371 and 1.074692\n",
            "Processing Epoch 5\n",
            "Training acc and loss are 0.66218 and 0.98522985\n",
            "Val acc and loss are 0.6617 and 0.97477746\n",
            "Processing Epoch 6\n",
            "Training acc and loss are 0.67694 and 0.91806173\n",
            "Val acc and loss are 0.6789 and 0.90736604\n",
            "Processing Epoch 7\n",
            "Training acc and loss are 0.69482 and 0.8626134\n",
            "Val acc and loss are 0.6967 and 0.85229003\n",
            "Processing Epoch 8\n",
            "Training acc and loss are 0.71198 and 0.8142814\n",
            "Val acc and loss are 0.7119 and 0.8046647\n",
            "Processing Epoch 9\n",
            "Training acc and loss are 0.7281 and 0.77342904\n",
            "Val acc and loss are 0.7281 and 0.7648806\n",
            "Processing Epoch 10\n",
            "Training acc and loss are 0.74186 and 0.73902774\n",
            "Val acc and loss are 0.7426 and 0.7319544\n",
            "Processing Epoch 11\n",
            "Training acc and loss are 0.75268 and 0.70926017\n",
            "Val acc and loss are 0.752 and 0.7040119\n",
            "Processing Epoch 12\n",
            "Training acc and loss are 0.76282 and 0.6835097\n",
            "Val acc and loss are 0.7609 and 0.6803506\n",
            "Processing Epoch 13\n",
            "Training acc and loss are 0.77118 and 0.66229844\n",
            "Val acc and loss are 0.7691 and 0.6613371\n",
            "Processing Epoch 14\n",
            "Training acc and loss are 0.7771 and 0.645589\n",
            "Val acc and loss are 0.7751 and 0.6467236\n",
            "Processing Epoch 15\n",
            "Training acc and loss are 0.78192 and 0.63207257\n",
            "Val acc and loss are 0.7782 and 0.6349385\n",
            "Processing Epoch 16\n",
            "Training acc and loss are 0.78576 and 0.6200637\n",
            "Val acc and loss are 0.7818 and 0.62415916\n",
            "Processing Epoch 17\n",
            "Training acc and loss are 0.78924 and 0.6085971\n",
            "Val acc and loss are 0.7837 and 0.6133882\n",
            "Processing Epoch 18\n",
            "Training acc and loss are 0.79252 and 0.5977903\n",
            "Val acc and loss are 0.7862 and 0.6028503\n",
            "Processing Epoch 19\n",
            "Training acc and loss are 0.7955 and 0.58821326\n",
            "Val acc and loss are 0.7892 and 0.5933043\n",
            "Processing Epoch 20\n",
            "Training acc and loss are 0.79714 and 0.5799301\n",
            "Val acc and loss are 0.793 and 0.58502764\n",
            "Processing Epoch 21\n",
            "Training acc and loss are 0.7988 and 0.57239306\n",
            "Val acc and loss are 0.7943 and 0.57758826\n",
            "Processing Epoch 22\n",
            "Training acc and loss are 0.80136 and 0.5649786\n",
            "Val acc and loss are 0.7959 and 0.57040876\n",
            "Processing Epoch 23\n",
            "Training acc and loss are 0.8037 and 0.55747026\n",
            "Val acc and loss are 0.7991 and 0.5632825\n",
            "Processing Epoch 24\n",
            "Training acc and loss are 0.8062 and 0.55001384\n",
            "Val acc and loss are 0.801 and 0.5563396\n",
            "Processing Epoch 25\n",
            "Training acc and loss are 0.80984 and 0.5430328\n",
            "Val acc and loss are 0.804 and 0.5499173\n",
            "Processing Epoch 26\n",
            "Training acc and loss are 0.81276 and 0.5366194\n",
            "Val acc and loss are 0.8055 and 0.5440327\n",
            "Processing Epoch 27\n",
            "Training acc and loss are 0.81494 and 0.5306337\n",
            "Val acc and loss are 0.8072 and 0.538464\n",
            "Processing Epoch 28\n",
            "Training acc and loss are 0.81642 and 0.52491915\n",
            "Val acc and loss are 0.8087 and 0.53303283\n",
            "Processing Epoch 29\n",
            "Training acc and loss are 0.8182 and 0.5194252\n",
            "Val acc and loss are 0.8122 and 0.527725\n",
            "Processing Epoch 30\n",
            "Training acc and loss are 0.8203 and 0.5142091\n",
            "Val acc and loss are 0.8147 and 0.52263546\n",
            "Processing Epoch 31\n",
            "Training acc and loss are 0.822 and 0.50933486\n",
            "Val acc and loss are 0.8155 and 0.5179046\n",
            "Processing Epoch 32\n",
            "Training acc and loss are 0.82342 and 0.50480074\n",
            "Val acc and loss are 0.8168 and 0.51356584\n",
            "Processing Epoch 33\n",
            "Training acc and loss are 0.82504 and 0.5005456\n",
            "Val acc and loss are 0.8182 and 0.5095966\n",
            "Processing Epoch 34\n",
            "Training acc and loss are 0.82632 and 0.4964525\n",
            "Val acc and loss are 0.8191 and 0.5058993\n",
            "Processing Epoch 35\n",
            "Training acc and loss are 0.8274 and 0.49249548\n",
            "Val acc and loss are 0.8202 and 0.502417\n",
            "Processing Epoch 36\n",
            "Training acc and loss are 0.82888 and 0.48863673\n",
            "Val acc and loss are 0.8211 and 0.49910802\n",
            "Processing Epoch 37\n",
            "Training acc and loss are 0.83022 and 0.4848332\n",
            "Val acc and loss are 0.8226 and 0.49588615\n",
            "Processing Epoch 38\n",
            "Training acc and loss are 0.83176 and 0.48110944\n",
            "Val acc and loss are 0.824 and 0.49273446\n",
            "Processing Epoch 39\n",
            "Training acc and loss are 0.83306 and 0.47749913\n",
            "Val acc and loss are 0.8248 and 0.48966134\n",
            "Processing Epoch 40\n",
            "Training acc and loss are 0.83492 and 0.4740738\n",
            "Val acc and loss are 0.8253 and 0.48670304\n",
            "Processing Epoch 41\n",
            "Training acc and loss are 0.83614 and 0.47091946\n",
            "Val acc and loss are 0.827 and 0.48395792\n",
            "Processing Epoch 42\n",
            "Training acc and loss are 0.83732 and 0.46804142\n",
            "Val acc and loss are 0.8272 and 0.48143902\n",
            "Processing Epoch 43\n",
            "Training acc and loss are 0.83818 and 0.465357\n",
            "Val acc and loss are 0.8275 and 0.47909248\n",
            "Processing Epoch 44\n",
            "Training acc and loss are 0.83908 and 0.46276814\n",
            "Val acc and loss are 0.8288 and 0.4768374\n",
            "Processing Epoch 45\n",
            "Training acc and loss are 0.83982 and 0.4601793\n",
            "Val acc and loss are 0.8287 and 0.47460485\n",
            "Processing Epoch 46\n",
            "Training acc and loss are 0.84044 and 0.45764023\n",
            "Val acc and loss are 0.8296 and 0.47242466\n",
            "Processing Epoch 47\n",
            "Training acc and loss are 0.84112 and 0.45517087\n",
            "Val acc and loss are 0.8305 and 0.47030967\n",
            "Processing Epoch 48\n",
            "Training acc and loss are 0.84188 and 0.45278642\n",
            "Val acc and loss are 0.8306 and 0.46826944\n",
            "Processing Epoch 49\n",
            "Training acc and loss are 0.84304 and 0.45047557\n",
            "Val acc and loss are 0.8314 and 0.4663007\n",
            "Processing Epoch 50\n",
            "Training acc and loss are 0.8439 and 0.4481996\n",
            "Val acc and loss are 0.8322 and 0.46433893\n",
            "Processing Epoch 51\n",
            "Training acc and loss are 0.84476 and 0.44597015\n",
            "Val acc and loss are 0.8324 and 0.46240458\n",
            "Processing Epoch 52\n",
            "Training acc and loss are 0.84556 and 0.44381726\n",
            "Val acc and loss are 0.8334 and 0.46052617\n",
            "Processing Epoch 53\n",
            "Training acc and loss are 0.84676 and 0.4417468\n",
            "Val acc and loss are 0.8339 and 0.45871922\n",
            "Processing Epoch 54\n",
            "Training acc and loss are 0.8474 and 0.43973547\n",
            "Val acc and loss are 0.8347 and 0.45697123\n",
            "Processing Epoch 55\n",
            "Training acc and loss are 0.84802 and 0.43776762\n",
            "Val acc and loss are 0.8354 and 0.45526832\n",
            "Processing Epoch 56\n",
            "Training acc and loss are 0.84888 and 0.4358289\n",
            "Val acc and loss are 0.837 and 0.45359913\n",
            "Processing Epoch 57\n",
            "Training acc and loss are 0.84946 and 0.43395478\n",
            "Val acc and loss are 0.8382 and 0.45195785\n",
            "Processing Epoch 58\n",
            "Training acc and loss are 0.84992 and 0.4321486\n",
            "Val acc and loss are 0.8386 and 0.4503789\n",
            "Processing Epoch 59\n",
            "Training acc and loss are 0.85066 and 0.430411\n",
            "Val acc and loss are 0.8391 and 0.44885245\n",
            "Processing Epoch 60\n",
            "Training acc and loss are 0.85126 and 0.4287282\n",
            "Val acc and loss are 0.839 and 0.44736743\n",
            "Processing Epoch 61\n",
            "Training acc and loss are 0.85192 and 0.42709306\n",
            "Val acc and loss are 0.8397 and 0.4459208\n",
            "Processing Epoch 62\n",
            "Training acc and loss are 0.8527 and 0.42549524\n",
            "Val acc and loss are 0.8397 and 0.44452617\n",
            "Processing Epoch 63\n",
            "Training acc and loss are 0.8534 and 0.4239429\n",
            "Val acc and loss are 0.8399 and 0.44320428\n",
            "Processing Epoch 64\n",
            "Training acc and loss are 0.85416 and 0.42240977\n",
            "Val acc and loss are 0.8401 and 0.44194424\n",
            "Processing Epoch 65\n",
            "Training acc and loss are 0.85458 and 0.42084813\n",
            "Val acc and loss are 0.8404 and 0.4407022\n",
            "Processing Epoch 66\n",
            "Training acc and loss are 0.85506 and 0.41928935\n",
            "Val acc and loss are 0.8415 and 0.43948966\n",
            "Processing Epoch 67\n",
            "Training acc and loss are 0.85518 and 0.41776085\n",
            "Val acc and loss are 0.8419 and 0.43830362\n",
            "Processing Epoch 68\n",
            "Training acc and loss are 0.85562 and 0.41629413\n",
            "Val acc and loss are 0.8422 and 0.43716392\n",
            "Processing Epoch 69\n",
            "Training acc and loss are 0.856 and 0.41489515\n",
            "Val acc and loss are 0.8432 and 0.4360802\n",
            "Processing Epoch 70\n",
            "Training acc and loss are 0.85614 and 0.4135611\n",
            "Val acc and loss are 0.8437 and 0.43500635\n",
            "Processing Epoch 71\n",
            "Training acc and loss are 0.85674 and 0.41228038\n",
            "Val acc and loss are 0.8438 and 0.43396845\n",
            "Processing Epoch 72\n",
            "Training acc and loss are 0.85754 and 0.41103032\n",
            "Val acc and loss are 0.8447 and 0.43296236\n",
            "Processing Epoch 73\n",
            "Training acc and loss are 0.85784 and 0.4098132\n",
            "Val acc and loss are 0.8455 and 0.4319877\n",
            "Processing Epoch 74\n",
            "Training acc and loss are 0.85842 and 0.40860954\n",
            "Val acc and loss are 0.846 and 0.4310167\n",
            "Processing Epoch 75\n",
            "Training acc and loss are 0.85892 and 0.40740833\n",
            "Val acc and loss are 0.8465 and 0.4300586\n",
            "Processing Epoch 76\n",
            "Training acc and loss are 0.85904 and 0.4061802\n",
            "Val acc and loss are 0.847 and 0.4290874\n",
            "Processing Epoch 77\n",
            "Training acc and loss are 0.85938 and 0.40494317\n",
            "Val acc and loss are 0.8471 and 0.4281357\n",
            "Processing Epoch 78\n",
            "Training acc and loss are 0.8598 and 0.40375656\n",
            "Val acc and loss are 0.8476 and 0.4272343\n",
            "Processing Epoch 79\n",
            "Training acc and loss are 0.86022 and 0.40259993\n",
            "Val acc and loss are 0.8475 and 0.4263625\n",
            "Processing Epoch 80\n",
            "Training acc and loss are 0.86038 and 0.4014792\n",
            "Val acc and loss are 0.8478 and 0.42550126\n",
            "Processing Epoch 81\n",
            "Training acc and loss are 0.86088 and 0.40038157\n",
            "Val acc and loss are 0.8483 and 0.42465478\n",
            "Processing Epoch 82\n",
            "Training acc and loss are 0.8613 and 0.39930603\n",
            "Val acc and loss are 0.8482 and 0.4238081\n",
            "Processing Epoch 83\n",
            "Training acc and loss are 0.86168 and 0.3982342\n",
            "Val acc and loss are 0.849 and 0.42297262\n",
            "Processing Epoch 84\n",
            "Training acc and loss are 0.862 and 0.3971814\n",
            "Val acc and loss are 0.8491 and 0.42214715\n",
            "Processing Epoch 85\n",
            "Training acc and loss are 0.86226 and 0.39612538\n",
            "Val acc and loss are 0.849 and 0.42133072\n",
            "Processing Epoch 86\n",
            "Training acc and loss are 0.86246 and 0.3950869\n",
            "Val acc and loss are 0.8493 and 0.42053467\n",
            "Processing Epoch 87\n",
            "Training acc and loss are 0.86294 and 0.39404172\n",
            "Val acc and loss are 0.8493 and 0.4197461\n",
            "Processing Epoch 88\n",
            "Training acc and loss are 0.86328 and 0.39298823\n",
            "Val acc and loss are 0.8498 and 0.4189558\n",
            "Processing Epoch 89\n",
            "Training acc and loss are 0.86342 and 0.39193633\n",
            "Val acc and loss are 0.8499 and 0.41818786\n",
            "Processing Epoch 90\n",
            "Training acc and loss are 0.86396 and 0.39090458\n",
            "Val acc and loss are 0.8503 and 0.41742325\n",
            "Processing Epoch 91\n",
            "Training acc and loss are 0.86426 and 0.38990602\n",
            "Val acc and loss are 0.8505 and 0.41666767\n",
            "Processing Epoch 92\n",
            "Training acc and loss are 0.86458 and 0.38893473\n",
            "Val acc and loss are 0.8507 and 0.4159198\n",
            "Processing Epoch 93\n",
            "Training acc and loss are 0.86496 and 0.38798922\n",
            "Val acc and loss are 0.8511 and 0.41517362\n",
            "Processing Epoch 94\n",
            "Training acc and loss are 0.86538 and 0.3870643\n",
            "Val acc and loss are 0.8515 and 0.41443267\n",
            "Processing Epoch 95\n",
            "Training acc and loss are 0.86578 and 0.3861538\n",
            "Val acc and loss are 0.8517 and 0.41371903\n",
            "Processing Epoch 96\n",
            "Training acc and loss are 0.86608 and 0.3852593\n",
            "Val acc and loss are 0.8522 and 0.4130077\n",
            "Processing Epoch 97\n",
            "Training acc and loss are 0.86624 and 0.38436422\n",
            "Val acc and loss are 0.8528 and 0.41229945\n",
            "Processing Epoch 98\n",
            "Training acc and loss are 0.86642 and 0.38348323\n",
            "Val acc and loss are 0.8528 and 0.41160214\n",
            "Processing Epoch 99\n",
            "Training acc and loss are 0.86672 and 0.3826038\n",
            "Val acc and loss are 0.8528 and 0.410889\n",
            "Processing Epoch 100\n",
            "Training acc and loss are 0.86674 and 0.38173988\n",
            "Val acc and loss are 0.8525 and 0.41017997\n",
            "Processing Epoch 101\n",
            "Training acc and loss are 0.8671 and 0.38090527\n",
            "Val acc and loss are 0.8527 and 0.40950245\n",
            "Processing Epoch 102\n",
            "Training acc and loss are 0.86728 and 0.38005117\n",
            "Val acc and loss are 0.853 and 0.40882543\n",
            "Processing Epoch 103\n",
            "Training acc and loss are 0.86762 and 0.37922063\n",
            "Val acc and loss are 0.8533 and 0.4081797\n",
            "Processing Epoch 104\n",
            "Training acc and loss are 0.86804 and 0.37837422\n",
            "Val acc and loss are 0.8537 and 0.40754077\n",
            "Processing Epoch 105\n",
            "Training acc and loss are 0.86836 and 0.37753478\n",
            "Val acc and loss are 0.8541 and 0.4069152\n",
            "Processing Epoch 106\n",
            "Training acc and loss are 0.86896 and 0.3767043\n",
            "Val acc and loss are 0.8543 and 0.4062875\n",
            "Processing Epoch 107\n",
            "Training acc and loss are 0.86894 and 0.37589946\n",
            "Val acc and loss are 0.8548 and 0.40567967\n",
            "Processing Epoch 108\n",
            "Training acc and loss are 0.86926 and 0.37511608\n",
            "Val acc and loss are 0.8548 and 0.40507248\n",
            "Processing Epoch 109\n",
            "Training acc and loss are 0.86956 and 0.37435368\n",
            "Val acc and loss are 0.8548 and 0.40448827\n",
            "Processing Epoch 110\n",
            "Training acc and loss are 0.86974 and 0.37359235\n",
            "Val acc and loss are 0.8551 and 0.4039005\n",
            "Processing Epoch 111\n",
            "Training acc and loss are 0.86996 and 0.37281457\n",
            "Val acc and loss are 0.855 and 0.4033037\n",
            "Processing Epoch 112\n",
            "Training acc and loss are 0.8704 and 0.3720321\n",
            "Val acc and loss are 0.8559 and 0.40273216\n",
            "Processing Epoch 113\n",
            "Training acc and loss are 0.8707 and 0.37127516\n",
            "Val acc and loss are 0.8564 and 0.40218464\n",
            "Processing Epoch 114\n",
            "Training acc and loss are 0.87086 and 0.37056196\n",
            "Val acc and loss are 0.8567 and 0.4016636\n",
            "Processing Epoch 115\n",
            "Training acc and loss are 0.87114 and 0.36986378\n",
            "Val acc and loss are 0.8569 and 0.40113956\n",
            "Processing Epoch 116\n",
            "Training acc and loss are 0.87124 and 0.36915267\n",
            "Val acc and loss are 0.8566 and 0.40060362\n",
            "Processing Epoch 117\n",
            "Training acc and loss are 0.87168 and 0.36843774\n",
            "Val acc and loss are 0.857 and 0.40008065\n",
            "Processing Epoch 118\n",
            "Training acc and loss are 0.87182 and 0.36769527\n",
            "Val acc and loss are 0.8575 and 0.39954767\n",
            "Processing Epoch 119\n",
            "Training acc and loss are 0.87192 and 0.36694884\n",
            "Val acc and loss are 0.858 and 0.3990161\n",
            "Processing Epoch 120\n",
            "Training acc and loss are 0.87226 and 0.36622506\n",
            "Val acc and loss are 0.8581 and 0.3985007\n",
            "Processing Epoch 121\n",
            "Training acc and loss are 0.8724 and 0.36552063\n",
            "Val acc and loss are 0.8584 and 0.39799926\n",
            "Processing Epoch 122\n",
            "Training acc and loss are 0.87268 and 0.36485147\n",
            "Val acc and loss are 0.858 and 0.39751634\n",
            "Processing Epoch 123\n",
            "Training acc and loss are 0.87282 and 0.36420313\n",
            "Val acc and loss are 0.8584 and 0.39702922\n",
            "Processing Epoch 124\n",
            "Training acc and loss are 0.87312 and 0.3635625\n",
            "Val acc and loss are 0.8591 and 0.39654106\n",
            "Processing Epoch 125\n",
            "Training acc and loss are 0.87344 and 0.36291954\n",
            "Val acc and loss are 0.8594 and 0.39606598\n",
            "Processing Epoch 126\n",
            "Training acc and loss are 0.87368 and 0.36224157\n",
            "Val acc and loss are 0.8594 and 0.3955708\n",
            "Processing Epoch 127\n",
            "Training acc and loss are 0.8739 and 0.36156547\n",
            "Val acc and loss are 0.8599 and 0.3950856\n",
            "Processing Epoch 128\n",
            "Training acc and loss are 0.87414 and 0.36092848\n",
            "Val acc and loss are 0.8602 and 0.39464206\n",
            "Processing Epoch 129\n",
            "Training acc and loss are 0.87436 and 0.360295\n",
            "Val acc and loss are 0.8601 and 0.39420047\n",
            "Processing Epoch 130\n",
            "Training acc and loss are 0.87428 and 0.3596815\n",
            "Val acc and loss are 0.8605 and 0.39377147\n",
            "Processing Epoch 131\n",
            "Training acc and loss are 0.87436 and 0.35905805\n",
            "Val acc and loss are 0.8604 and 0.39332858\n",
            "Processing Epoch 132\n",
            "Training acc and loss are 0.87452 and 0.35841653\n",
            "Val acc and loss are 0.8605 and 0.39288133\n",
            "Processing Epoch 133\n",
            "Training acc and loss are 0.8748 and 0.35775954\n",
            "Val acc and loss are 0.8607 and 0.39243352\n",
            "Processing Epoch 134\n",
            "Training acc and loss are 0.87514 and 0.35711563\n",
            "Val acc and loss are 0.8607 and 0.3919904\n",
            "Processing Epoch 135\n",
            "Training acc and loss are 0.87532 and 0.35649186\n",
            "Val acc and loss are 0.8609 and 0.3915198\n",
            "Processing Epoch 136\n",
            "Training acc and loss are 0.87558 and 0.35588378\n",
            "Val acc and loss are 0.8608 and 0.39106542\n",
            "Processing Epoch 137\n",
            "Training acc and loss are 0.87588 and 0.35529447\n",
            "Val acc and loss are 0.8617 and 0.39061025\n",
            "Processing Epoch 138\n",
            "Training acc and loss are 0.87614 and 0.35473415\n",
            "Val acc and loss are 0.862 and 0.39016196\n",
            "Processing Epoch 139\n",
            "Training acc and loss are 0.87626 and 0.3541804\n",
            "Val acc and loss are 0.8617 and 0.3897231\n",
            "Processing Epoch 140\n",
            "Training acc and loss are 0.87658 and 0.35360423\n",
            "Val acc and loss are 0.8615 and 0.38928834\n",
            "Processing Epoch 141\n",
            "Training acc and loss are 0.8768 and 0.35300457\n",
            "Val acc and loss are 0.8624 and 0.3888711\n",
            "Processing Epoch 142\n",
            "Training acc and loss are 0.87688 and 0.3524105\n",
            "Val acc and loss are 0.8629 and 0.38848466\n",
            "Processing Epoch 143\n",
            "Training acc and loss are 0.87688 and 0.35181445\n",
            "Val acc and loss are 0.8629 and 0.38812762\n",
            "Processing Epoch 144\n",
            "Training acc and loss are 0.87708 and 0.3512486\n",
            "Val acc and loss are 0.8628 and 0.3877775\n",
            "Processing Epoch 145\n",
            "Training acc and loss are 0.87716 and 0.3507127\n",
            "Val acc and loss are 0.8627 and 0.38743934\n",
            "Processing Epoch 146\n",
            "Training acc and loss are 0.87722 and 0.35017914\n",
            "Val acc and loss are 0.8623 and 0.38708052\n",
            "Processing Epoch 147\n",
            "Training acc and loss are 0.87736 and 0.3496286\n",
            "Val acc and loss are 0.8628 and 0.38668883\n",
            "Processing Epoch 148\n",
            "Training acc and loss are 0.87762 and 0.3490703\n",
            "Val acc and loss are 0.8626 and 0.3862821\n",
            "Processing Epoch 149\n",
            "Training acc and loss are 0.87806 and 0.34852204\n",
            "Val acc and loss are 0.863 and 0.38588446\n",
            "Processing Epoch 150\n",
            "Training acc and loss are 0.87824 and 0.34796384\n",
            "Val acc and loss are 0.8634 and 0.38550177\n",
            "Processing Epoch 151\n",
            "Training acc and loss are 0.87832 and 0.3474082\n",
            "Val acc and loss are 0.8636 and 0.3850878\n",
            "Processing Epoch 152\n",
            "Training acc and loss are 0.8785 and 0.34684664\n",
            "Val acc and loss are 0.8637 and 0.3846996\n",
            "Processing Epoch 153\n",
            "Training acc and loss are 0.87868 and 0.34630132\n",
            "Val acc and loss are 0.8638 and 0.3843165\n",
            "Processing Epoch 154\n",
            "Training acc and loss are 0.8788 and 0.34575891\n",
            "Val acc and loss are 0.8639 and 0.38394716\n",
            "Processing Epoch 155\n",
            "Training acc and loss are 0.87888 and 0.34522867\n",
            "Val acc and loss are 0.8643 and 0.383602\n",
            "Processing Epoch 156\n",
            "Training acc and loss are 0.87918 and 0.34470206\n",
            "Val acc and loss are 0.8645 and 0.38325906\n",
            "Processing Epoch 157\n",
            "Training acc and loss are 0.87936 and 0.3441703\n",
            "Val acc and loss are 0.8645 and 0.3829137\n",
            "Processing Epoch 158\n",
            "Training acc and loss are 0.8796 and 0.34366786\n",
            "Val acc and loss are 0.8645 and 0.38254088\n",
            "Processing Epoch 159\n",
            "Training acc and loss are 0.87974 and 0.34317645\n",
            "Val acc and loss are 0.8643 and 0.382164\n",
            "Processing Epoch 160\n",
            "Training acc and loss are 0.87982 and 0.34268454\n",
            "Val acc and loss are 0.8645 and 0.38179794\n",
            "Processing Epoch 161\n",
            "Training acc and loss are 0.8799 and 0.34220508\n",
            "Val acc and loss are 0.864 and 0.38146332\n",
            "Processing Epoch 162\n",
            "Training acc and loss are 0.88016 and 0.34171596\n",
            "Val acc and loss are 0.864 and 0.38112953\n",
            "Processing Epoch 163\n",
            "Training acc and loss are 0.88034 and 0.34120977\n",
            "Val acc and loss are 0.864 and 0.3807905\n",
            "Processing Epoch 164\n",
            "Training acc and loss are 0.88058 and 0.34070298\n",
            "Val acc and loss are 0.8641 and 0.38044998\n",
            "Processing Epoch 165\n",
            "Training acc and loss are 0.8808 and 0.3402046\n",
            "Val acc and loss are 0.8649 and 0.3801062\n",
            "Processing Epoch 166\n",
            "Training acc and loss are 0.88086 and 0.33971053\n",
            "Val acc and loss are 0.8651 and 0.37977785\n",
            "Processing Epoch 167\n",
            "Training acc and loss are 0.88104 and 0.33923298\n",
            "Val acc and loss are 0.8652 and 0.37942764\n",
            "Processing Epoch 168\n",
            "Training acc and loss are 0.8812 and 0.33874777\n",
            "Val acc and loss are 0.8651 and 0.37907228\n",
            "Processing Epoch 169\n",
            "Training acc and loss are 0.88114 and 0.33827582\n",
            "Val acc and loss are 0.8654 and 0.3787197\n",
            "Processing Epoch 170\n",
            "Training acc and loss are 0.88106 and 0.337807\n",
            "Val acc and loss are 0.8655 and 0.37836924\n",
            "Processing Epoch 171\n",
            "Training acc and loss are 0.88144 and 0.3373421\n",
            "Val acc and loss are 0.8655 and 0.37803534\n",
            "Processing Epoch 172\n",
            "Training acc and loss are 0.8816 and 0.33685425\n",
            "Val acc and loss are 0.8654 and 0.37768653\n",
            "Processing Epoch 173\n",
            "Training acc and loss are 0.88204 and 0.3363803\n",
            "Val acc and loss are 0.8653 and 0.37737828\n",
            "Processing Epoch 174\n",
            "Training acc and loss are 0.88228 and 0.3359189\n",
            "Val acc and loss are 0.8654 and 0.37710604\n",
            "Processing Epoch 175\n",
            "Training acc and loss are 0.8824 and 0.33547387\n",
            "Val acc and loss are 0.8656 and 0.37685958\n",
            "Processing Epoch 176\n",
            "Training acc and loss are 0.88228 and 0.33504277\n",
            "Val acc and loss are 0.8659 and 0.37662342\n",
            "Processing Epoch 177\n",
            "Training acc and loss are 0.88216 and 0.33459124\n",
            "Val acc and loss are 0.8656 and 0.37639254\n",
            "Processing Epoch 178\n",
            "Training acc and loss are 0.88248 and 0.3341264\n",
            "Val acc and loss are 0.8661 and 0.3761504\n",
            "Processing Epoch 179\n",
            "Training acc and loss are 0.88294 and 0.333629\n",
            "Val acc and loss are 0.866 and 0.37588933\n",
            "Processing Epoch 180\n",
            "Training acc and loss are 0.88294 and 0.33314225\n",
            "Val acc and loss are 0.8661 and 0.37564182\n",
            "Processing Epoch 181\n",
            "Training acc and loss are 0.88298 and 0.33266884\n",
            "Val acc and loss are 0.8657 and 0.3753835\n",
            "Processing Epoch 182\n",
            "Training acc and loss are 0.88316 and 0.33221364\n",
            "Val acc and loss are 0.8661 and 0.37510797\n",
            "Processing Epoch 183\n",
            "Training acc and loss are 0.8832 and 0.3317694\n",
            "Val acc and loss are 0.8661 and 0.37481043\n",
            "Processing Epoch 184\n",
            "Training acc and loss are 0.88346 and 0.33134055\n",
            "Val acc and loss are 0.8662 and 0.37450856\n",
            "Processing Epoch 185\n",
            "Training acc and loss are 0.88368 and 0.33090824\n",
            "Val acc and loss are 0.8668 and 0.37420383\n",
            "Processing Epoch 186\n",
            "Training acc and loss are 0.8837 and 0.3304696\n",
            "Val acc and loss are 0.8668 and 0.37392944\n",
            "Processing Epoch 187\n",
            "Training acc and loss are 0.88394 and 0.3300449\n",
            "Val acc and loss are 0.867 and 0.37363365\n",
            "Processing Epoch 188\n",
            "Training acc and loss are 0.88408 and 0.3296155\n",
            "Val acc and loss are 0.8668 and 0.37335986\n",
            "Processing Epoch 189\n",
            "Training acc and loss are 0.88418 and 0.3291996\n",
            "Val acc and loss are 0.867 and 0.37309137\n",
            "Processing Epoch 190\n",
            "Training acc and loss are 0.88416 and 0.32877934\n",
            "Val acc and loss are 0.8671 and 0.372821\n",
            "Processing Epoch 191\n",
            "Training acc and loss are 0.8842 and 0.328358\n",
            "Val acc and loss are 0.867 and 0.37255758\n",
            "Processing Epoch 192\n",
            "Training acc and loss are 0.88458 and 0.3279215\n",
            "Val acc and loss are 0.8677 and 0.3722919\n",
            "Processing Epoch 193\n",
            "Training acc and loss are 0.8846 and 0.32748362\n",
            "Val acc and loss are 0.8676 and 0.372013\n",
            "Processing Epoch 194\n",
            "Training acc and loss are 0.88488 and 0.3270535\n",
            "Val acc and loss are 0.8674 and 0.37175965\n",
            "Processing Epoch 195\n",
            "Training acc and loss are 0.88504 and 0.32663348\n",
            "Val acc and loss are 0.8675 and 0.37154165\n",
            "Processing Epoch 196\n",
            "Training acc and loss are 0.8853 and 0.3262234\n",
            "Val acc and loss are 0.868 and 0.37132972\n",
            "Processing Epoch 197\n",
            "Training acc and loss are 0.88522 and 0.32581353\n",
            "Val acc and loss are 0.8684 and 0.37110063\n",
            "Processing Epoch 198\n",
            "Training acc and loss are 0.88544 and 0.3253986\n",
            "Val acc and loss are 0.8689 and 0.37085602\n",
            "Processing Epoch 199\n",
            "Training acc and loss are 0.88572 and 0.32499087\n",
            "Val acc and loss are 0.869 and 0.37060878\n",
            "Processing Epoch 200\n",
            "Training acc and loss are 0.88602 and 0.32458743\n",
            "Val acc and loss are 0.8688 and 0.3703727\n",
            "Processing Epoch 201\n",
            "Training acc and loss are 0.88632 and 0.3242106\n",
            "Val acc and loss are 0.8691 and 0.37014183\n",
            "Processing Epoch 202\n",
            "Training acc and loss are 0.88642 and 0.3238486\n",
            "Val acc and loss are 0.8692 and 0.36990666\n",
            "Processing Epoch 203\n",
            "Training acc and loss are 0.88634 and 0.3234924\n",
            "Val acc and loss are 0.8687 and 0.36968085\n",
            "Processing Epoch 204\n",
            "Training acc and loss are 0.88636 and 0.32312286\n",
            "Val acc and loss are 0.8688 and 0.36947638\n",
            "Processing Epoch 205\n",
            "Training acc and loss are 0.8868 and 0.32269567\n",
            "Val acc and loss are 0.869 and 0.3692525\n",
            "Processing Epoch 206\n",
            "Training acc and loss are 0.887 and 0.32223913\n",
            "Val acc and loss are 0.8692 and 0.36900845\n",
            "Processing Epoch 207\n",
            "Training acc and loss are 0.8872 and 0.32179108\n",
            "Val acc and loss are 0.8701 and 0.3687451\n",
            "Processing Epoch 208\n",
            "Training acc and loss are 0.88764 and 0.32135692\n",
            "Val acc and loss are 0.8698 and 0.36843482\n",
            "Processing Epoch 209\n",
            "Training acc and loss are 0.88774 and 0.3209461\n",
            "Val acc and loss are 0.8697 and 0.36813295\n",
            "Processing Epoch 210\n",
            "Training acc and loss are 0.88766 and 0.3205629\n",
            "Val acc and loss are 0.8702 and 0.3678321\n",
            "Processing Epoch 211\n",
            "Training acc and loss are 0.88762 and 0.32021052\n",
            "Val acc and loss are 0.8703 and 0.36757553\n",
            "Processing Epoch 212\n",
            "Training acc and loss are 0.88756 and 0.31985152\n",
            "Val acc and loss are 0.8701 and 0.36731684\n",
            "Processing Epoch 213\n",
            "Training acc and loss are 0.88792 and 0.3194676\n",
            "Val acc and loss are 0.8703 and 0.36704856\n",
            "Processing Epoch 214\n",
            "Training acc and loss are 0.88822 and 0.3190352\n",
            "Val acc and loss are 0.8703 and 0.3667361\n",
            "Processing Epoch 215\n",
            "Training acc and loss are 0.88854 and 0.3186014\n",
            "Val acc and loss are 0.8701 and 0.3664253\n",
            "Processing Epoch 216\n",
            "Training acc and loss are 0.88862 and 0.31817898\n",
            "Val acc and loss are 0.8701 and 0.3661594\n",
            "Processing Epoch 217\n",
            "Training acc and loss are 0.88874 and 0.3177681\n",
            "Val acc and loss are 0.8702 and 0.36591464\n",
            "Processing Epoch 218\n",
            "Training acc and loss are 0.88894 and 0.31739098\n",
            "Val acc and loss are 0.87 and 0.3656966\n",
            "Processing Epoch 219\n",
            "Training acc and loss are 0.8891 and 0.31704623\n",
            "Val acc and loss are 0.8705 and 0.36549518\n",
            "Processing Epoch 220\n",
            "Training acc and loss are 0.88918 and 0.31670007\n",
            "Val acc and loss are 0.8706 and 0.36527354\n",
            "Processing Epoch 221\n",
            "Training acc and loss are 0.88928 and 0.31633413\n",
            "Val acc and loss are 0.8705 and 0.3650446\n",
            "Processing Epoch 222\n",
            "Training acc and loss are 0.88948 and 0.31592667\n",
            "Val acc and loss are 0.8705 and 0.36480743\n",
            "Processing Epoch 223\n",
            "Training acc and loss are 0.88958 and 0.31555587\n",
            "Val acc and loss are 0.8708 and 0.36456177\n",
            "Processing Epoch 224\n",
            "Training acc and loss are 0.88972 and 0.31521976\n",
            "Val acc and loss are 0.871 and 0.3643258\n",
            "Processing Epoch 225\n",
            "Training acc and loss are 0.8898 and 0.31488913\n",
            "Val acc and loss are 0.871 and 0.36411944\n",
            "Processing Epoch 226\n",
            "Training acc and loss are 0.89002 and 0.31455243\n",
            "Val acc and loss are 0.8711 and 0.36395365\n",
            "Processing Epoch 227\n",
            "Training acc and loss are 0.89 and 0.31421065\n",
            "Val acc and loss are 0.871 and 0.36381978\n",
            "Processing Epoch 228\n",
            "Training acc and loss are 0.89026 and 0.31388065\n",
            "Val acc and loss are 0.871 and 0.3636975\n",
            "Processing Epoch 229\n",
            "Training acc and loss are 0.89036 and 0.31354883\n",
            "Val acc and loss are 0.8714 and 0.36354434\n",
            "Processing Epoch 230\n",
            "Training acc and loss are 0.89044 and 0.31321773\n",
            "Val acc and loss are 0.872 and 0.36335742\n",
            "Processing Epoch 231\n",
            "Training acc and loss are 0.8904 and 0.3128549\n",
            "Val acc and loss are 0.8727 and 0.3631308\n",
            "Processing Epoch 232\n",
            "Training acc and loss are 0.89054 and 0.3124528\n",
            "Val acc and loss are 0.8722 and 0.362909\n",
            "Processing Epoch 233\n",
            "Training acc and loss are 0.89062 and 0.3120582\n",
            "Val acc and loss are 0.8722 and 0.3626755\n",
            "Processing Epoch 234\n",
            "Training acc and loss are 0.8908 and 0.3116743\n",
            "Val acc and loss are 0.8724 and 0.36246112\n",
            "Processing Epoch 235\n",
            "Training acc and loss are 0.89096 and 0.31131643\n",
            "Val acc and loss are 0.8724 and 0.36226252\n",
            "Processing Epoch 236\n",
            "Training acc and loss are 0.89102 and 0.31099093\n",
            "Val acc and loss are 0.8726 and 0.3620893\n",
            "Processing Epoch 237\n",
            "Training acc and loss are 0.89112 and 0.3106239\n",
            "Val acc and loss are 0.8726 and 0.36189017\n",
            "Processing Epoch 238\n",
            "Training acc and loss are 0.89122 and 0.31025708\n",
            "Val acc and loss are 0.8728 and 0.3616964\n",
            "Processing Epoch 239\n",
            "Training acc and loss are 0.89162 and 0.30988953\n",
            "Val acc and loss are 0.8729 and 0.361505\n",
            "Processing Epoch 240\n",
            "Training acc and loss are 0.8918 and 0.30952942\n",
            "Val acc and loss are 0.873 and 0.36131766\n",
            "Processing Epoch 241\n",
            "Training acc and loss are 0.8918 and 0.30915716\n",
            "Val acc and loss are 0.8727 and 0.36110634\n",
            "Processing Epoch 242\n",
            "Training acc and loss are 0.8922 and 0.3087863\n",
            "Val acc and loss are 0.8726 and 0.36086506\n",
            "Processing Epoch 243\n",
            "Training acc and loss are 0.8922 and 0.30844754\n",
            "Val acc and loss are 0.8728 and 0.36065435\n",
            "Processing Epoch 244\n",
            "Training acc and loss are 0.89226 and 0.30812258\n",
            "Val acc and loss are 0.8729 and 0.36047027\n",
            "Processing Epoch 245\n",
            "Training acc and loss are 0.8921 and 0.30780396\n",
            "Val acc and loss are 0.8733 and 0.36029315\n",
            "Processing Epoch 246\n",
            "Training acc and loss are 0.89234 and 0.30749515\n",
            "Val acc and loss are 0.8726 and 0.36013287\n",
            "Processing Epoch 247\n",
            "Training acc and loss are 0.89254 and 0.30715027\n",
            "Val acc and loss are 0.8725 and 0.3599494\n",
            "Processing Epoch 248\n",
            "Training acc and loss are 0.8925 and 0.30681258\n",
            "Val acc and loss are 0.8727 and 0.359781\n",
            "Processing Epoch 249\n",
            "Training acc and loss are 0.89276 and 0.30647698\n",
            "Val acc and loss are 0.8727 and 0.35962692\n",
            "Processing Epoch 250\n",
            "Training acc and loss are 0.89288 and 0.30613652\n",
            "Val acc and loss are 0.8731 and 0.3594872\n",
            "Processing Epoch 251\n",
            "Training acc and loss are 0.89292 and 0.30581212\n",
            "Val acc and loss are 0.8737 and 0.35936043\n",
            "Processing Epoch 252\n",
            "Training acc and loss are 0.89306 and 0.30550203\n",
            "Val acc and loss are 0.8734 and 0.35921714\n",
            "Processing Epoch 253\n",
            "Training acc and loss are 0.8931 and 0.3052024\n",
            "Val acc and loss are 0.8731 and 0.35908726\n",
            "Processing Epoch 254\n",
            "Training acc and loss are 0.8933 and 0.30490956\n",
            "Val acc and loss are 0.873 and 0.35891655\n",
            "Processing Epoch 255\n",
            "Training acc and loss are 0.8932 and 0.30460966\n",
            "Val acc and loss are 0.873 and 0.35872564\n",
            "Processing Epoch 256\n",
            "Training acc and loss are 0.8932 and 0.30430794\n",
            "Val acc and loss are 0.8732 and 0.35853934\n",
            "Processing Epoch 257\n",
            "Training acc and loss are 0.89318 and 0.30399218\n",
            "Val acc and loss are 0.8735 and 0.35835078\n",
            "Processing Epoch 258\n",
            "Training acc and loss are 0.8933 and 0.3036692\n",
            "Val acc and loss are 0.8738 and 0.35817584\n",
            "Processing Epoch 259\n",
            "Training acc and loss are 0.89362 and 0.30331638\n",
            "Val acc and loss are 0.874 and 0.35799637\n",
            "Processing Epoch 260\n",
            "Training acc and loss are 0.8937 and 0.30294192\n",
            "Val acc and loss are 0.8739 and 0.3577951\n",
            "Processing Epoch 261\n",
            "Training acc and loss are 0.89386 and 0.30258366\n",
            "Val acc and loss are 0.8741 and 0.35760018\n",
            "Processing Epoch 262\n",
            "Training acc and loss are 0.8941 and 0.30222964\n",
            "Val acc and loss are 0.8737 and 0.35742027\n",
            "Processing Epoch 263\n",
            "Training acc and loss are 0.89426 and 0.30191383\n",
            "Val acc and loss are 0.8739 and 0.35726997\n",
            "Processing Epoch 264\n",
            "Training acc and loss are 0.89442 and 0.30160207\n",
            "Val acc and loss are 0.8737 and 0.35713014\n",
            "Processing Epoch 265\n",
            "Training acc and loss are 0.89464 and 0.30127665\n",
            "Val acc and loss are 0.8736 and 0.35697782\n",
            "Processing Epoch 266\n",
            "Training acc and loss are 0.89466 and 0.300946\n",
            "Val acc and loss are 0.8738 and 0.35677487\n",
            "Processing Epoch 267\n",
            "Training acc and loss are 0.89458 and 0.3006366\n",
            "Val acc and loss are 0.8735 and 0.35654667\n",
            "Processing Epoch 268\n",
            "Training acc and loss are 0.8945 and 0.30035365\n",
            "Val acc and loss are 0.8737 and 0.3563123\n",
            "Processing Epoch 269\n",
            "Training acc and loss are 0.89464 and 0.30007526\n",
            "Val acc and loss are 0.8732 and 0.3560951\n",
            "Processing Epoch 270\n",
            "Training acc and loss are 0.89478 and 0.29979384\n",
            "Val acc and loss are 0.8733 and 0.3559086\n",
            "Processing Epoch 271\n",
            "Training acc and loss are 0.89486 and 0.29948035\n",
            "Val acc and loss are 0.8738 and 0.35573632\n",
            "Processing Epoch 272\n",
            "Training acc and loss are 0.89504 and 0.29910615\n",
            "Val acc and loss are 0.8739 and 0.35551715\n",
            "Processing Epoch 273\n",
            "Training acc and loss are 0.8952 and 0.29875118\n",
            "Val acc and loss are 0.874 and 0.35531065\n",
            "Processing Epoch 274\n",
            "Training acc and loss are 0.89548 and 0.29839683\n",
            "Val acc and loss are 0.8739 and 0.35509717\n",
            "Processing Epoch 275\n",
            "Training acc and loss are 0.89548 and 0.29805627\n",
            "Val acc and loss are 0.874 and 0.35489964\n",
            "Processing Epoch 276\n",
            "Training acc and loss are 0.89562 and 0.29772356\n",
            "Val acc and loss are 0.8741 and 0.3547167\n",
            "Processing Epoch 277\n",
            "Training acc and loss are 0.89568 and 0.2974022\n",
            "Val acc and loss are 0.8742 and 0.35453585\n",
            "Processing Epoch 278\n",
            "Training acc and loss are 0.8957 and 0.29709813\n",
            "Val acc and loss are 0.8751 and 0.3543602\n",
            "Processing Epoch 279\n",
            "Training acc and loss are 0.8959 and 0.29680738\n",
            "Val acc and loss are 0.8748 and 0.35423136\n",
            "Processing Epoch 280\n",
            "Training acc and loss are 0.89596 and 0.29651874\n",
            "Val acc and loss are 0.8748 and 0.35410115\n",
            "Processing Epoch 281\n",
            "Training acc and loss are 0.89604 and 0.29622206\n",
            "Val acc and loss are 0.8747 and 0.35395995\n",
            "Processing Epoch 282\n",
            "Training acc and loss are 0.89638 and 0.29594353\n",
            "Val acc and loss are 0.8748 and 0.35385334\n",
            "Processing Epoch 283\n",
            "Training acc and loss are 0.8965 and 0.29564774\n",
            "Val acc and loss are 0.8747 and 0.35374326\n",
            "Processing Epoch 284\n",
            "Training acc and loss are 0.89658 and 0.29535073\n",
            "Val acc and loss are 0.8749 and 0.35362396\n",
            "Processing Epoch 285\n",
            "Training acc and loss are 0.8967 and 0.2950651\n",
            "Val acc and loss are 0.8754 and 0.35349455\n",
            "Processing Epoch 286\n",
            "Training acc and loss are 0.89678 and 0.29478744\n",
            "Val acc and loss are 0.8751 and 0.35336277\n",
            "Processing Epoch 287\n",
            "Training acc and loss are 0.89692 and 0.2944919\n",
            "Val acc and loss are 0.8748 and 0.35321227\n",
            "Processing Epoch 288\n",
            "Training acc and loss are 0.89716 and 0.29419625\n",
            "Val acc and loss are 0.875 and 0.35306975\n",
            "Processing Epoch 289\n",
            "Training acc and loss are 0.89714 and 0.29390186\n",
            "Val acc and loss are 0.875 and 0.3529333\n",
            "Processing Epoch 290\n",
            "Training acc and loss are 0.89714 and 0.29358995\n",
            "Val acc and loss are 0.8751 and 0.35279393\n",
            "Processing Epoch 291\n",
            "Training acc and loss are 0.89756 and 0.29328272\n",
            "Val acc and loss are 0.8752 and 0.35268623\n",
            "Processing Epoch 292\n",
            "Training acc and loss are 0.89782 and 0.29295832\n",
            "Val acc and loss are 0.8752 and 0.3524868\n",
            "Processing Epoch 293\n",
            "Training acc and loss are 0.89794 and 0.2926271\n",
            "Val acc and loss are 0.8756 and 0.35228324\n",
            "Processing Epoch 294\n",
            "Training acc and loss are 0.8979 and 0.29233077\n",
            "Val acc and loss are 0.8755 and 0.35210437\n",
            "Processing Epoch 295\n",
            "Training acc and loss are 0.89782 and 0.29207835\n",
            "Val acc and loss are 0.8757 and 0.35197186\n",
            "Processing Epoch 296\n",
            "Training acc and loss are 0.89782 and 0.2918363\n",
            "Val acc and loss are 0.8757 and 0.3518791\n",
            "Processing Epoch 297\n",
            "Training acc and loss are 0.89798 and 0.2915835\n",
            "Val acc and loss are 0.8755 and 0.35180178\n",
            "Processing Epoch 298\n",
            "Training acc and loss are 0.89812 and 0.29131392\n",
            "Val acc and loss are 0.8752 and 0.35171714\n",
            "Processing Epoch 299\n",
            "Training acc and loss are 0.89832 and 0.2909866\n",
            "Val acc and loss are 0.8748 and 0.35155946\n",
            "Processing Epoch 300\n",
            "Training acc and loss are 0.89832 and 0.2906224\n",
            "Val acc and loss are 0.8754 and 0.3513689\n",
            "Processing Epoch 301\n",
            "Training acc and loss are 0.8983 and 0.29030988\n",
            "Val acc and loss are 0.8751 and 0.35120127\n",
            "Processing Epoch 302\n",
            "Training acc and loss are 0.8983 and 0.2900209\n",
            "Val acc and loss are 0.8748 and 0.35100538\n",
            "Processing Epoch 303\n",
            "Training acc and loss are 0.89858 and 0.28973258\n",
            "Val acc and loss are 0.8748 and 0.35081196\n",
            "Processing Epoch 304\n",
            "Training acc and loss are 0.89852 and 0.2895027\n",
            "Val acc and loss are 0.8742 and 0.35068384\n",
            "Processing Epoch 305\n",
            "Training acc and loss are 0.89854 and 0.28927767\n",
            "Val acc and loss are 0.8752 and 0.35059458\n",
            "Processing Epoch 306\n",
            "Training acc and loss are 0.8987 and 0.288968\n",
            "Val acc and loss are 0.8756 and 0.35047576\n",
            "Processing Epoch 307\n",
            "Training acc and loss are 0.89898 and 0.28863573\n",
            "Val acc and loss are 0.876 and 0.3503791\n",
            "Processing Epoch 308\n",
            "Training acc and loss are 0.89908 and 0.28835016\n",
            "Val acc and loss are 0.8758 and 0.3503454\n",
            "Processing Epoch 309\n",
            "Training acc and loss are 0.89924 and 0.2880696\n",
            "Val acc and loss are 0.8757 and 0.35025513\n",
            "Processing Epoch 310\n",
            "Training acc and loss are 0.89944 and 0.28774816\n",
            "Val acc and loss are 0.8757 and 0.35008705\n",
            "Processing Epoch 311\n",
            "Training acc and loss are 0.89958 and 0.2874819\n",
            "Val acc and loss are 0.8752 and 0.34989855\n",
            "Processing Epoch 312\n",
            "Training acc and loss are 0.8996 and 0.28725946\n",
            "Val acc and loss are 0.8753 and 0.34972697\n",
            "Processing Epoch 313\n",
            "Training acc and loss are 0.89958 and 0.28698143\n",
            "Val acc and loss are 0.875 and 0.34949508\n",
            "Processing Epoch 314\n",
            "Training acc and loss are 0.89988 and 0.28664267\n",
            "Val acc and loss are 0.8749 and 0.3492707\n",
            "Processing Epoch 315\n",
            "Training acc and loss are 0.89976 and 0.28635344\n",
            "Val acc and loss are 0.8754 and 0.34909633\n",
            "Processing Epoch 316\n",
            "Training acc and loss are 0.89984 and 0.28610313\n",
            "Val acc and loss are 0.8754 and 0.34894747\n",
            "Processing Epoch 317\n",
            "Training acc and loss are 0.89986 and 0.2858218\n",
            "Val acc and loss are 0.8749 and 0.34878057\n",
            "Processing Epoch 318\n",
            "Training acc and loss are 0.90006 and 0.28553602\n",
            "Val acc and loss are 0.8748 and 0.34858128\n",
            "Processing Epoch 319\n",
            "Training acc and loss are 0.9002 and 0.28526533\n",
            "Val acc and loss are 0.8749 and 0.34843668\n",
            "Processing Epoch 320\n",
            "Training acc and loss are 0.90032 and 0.28497952\n",
            "Val acc and loss are 0.8754 and 0.34833255\n",
            "Processing Epoch 321\n",
            "Training acc and loss are 0.90028 and 0.28464895\n",
            "Val acc and loss are 0.875 and 0.3482282\n",
            "Processing Epoch 322\n",
            "Training acc and loss are 0.90044 and 0.28431827\n",
            "Val acc and loss are 0.8753 and 0.34817442\n",
            "Processing Epoch 323\n",
            "Training acc and loss are 0.90064 and 0.28403372\n",
            "Val acc and loss are 0.8754 and 0.34816396\n",
            "Processing Epoch 324\n",
            "Training acc and loss are 0.90078 and 0.2837807\n",
            "Val acc and loss are 0.8757 and 0.34812042\n",
            "Processing Epoch 325\n",
            "Training acc and loss are 0.901 and 0.28353828\n",
            "Val acc and loss are 0.8753 and 0.34797877\n",
            "Processing Epoch 326\n",
            "Training acc and loss are 0.90102 and 0.28327838\n",
            "Val acc and loss are 0.8751 and 0.34782055\n",
            "Processing Epoch 327\n",
            "Training acc and loss are 0.90094 and 0.28300533\n",
            "Val acc and loss are 0.8752 and 0.34768793\n",
            "Processing Epoch 328\n",
            "Training acc and loss are 0.90102 and 0.28272325\n",
            "Val acc and loss are 0.8752 and 0.34760317\n",
            "Processing Epoch 329\n",
            "Training acc and loss are 0.90124 and 0.28244737\n",
            "Val acc and loss are 0.8757 and 0.34754324\n",
            "Processing Epoch 330\n",
            "Training acc and loss are 0.90124 and 0.28219134\n",
            "Val acc and loss are 0.8755 and 0.3475112\n",
            "Processing Epoch 331\n",
            "Training acc and loss are 0.90146 and 0.28193504\n",
            "Val acc and loss are 0.8758 and 0.34745416\n",
            "Processing Epoch 332\n",
            "Training acc and loss are 0.90156 and 0.28168905\n",
            "Val acc and loss are 0.8757 and 0.3473424\n",
            "Processing Epoch 333\n",
            "Training acc and loss are 0.90132 and 0.28149167\n",
            "Val acc and loss are 0.876 and 0.3472488\n",
            "Processing Epoch 334\n",
            "Training acc and loss are 0.90136 and 0.28128937\n",
            "Val acc and loss are 0.8757 and 0.34712037\n",
            "Processing Epoch 335\n",
            "Training acc and loss are 0.90154 and 0.2810202\n",
            "Val acc and loss are 0.8754 and 0.3469246\n",
            "Processing Epoch 336\n",
            "Training acc and loss are 0.90174 and 0.28070855\n",
            "Val acc and loss are 0.8751 and 0.3467198\n",
            "Processing Epoch 337\n",
            "Training acc and loss are 0.9019 and 0.28039843\n",
            "Val acc and loss are 0.8753 and 0.34654114\n",
            "Processing Epoch 338\n",
            "Training acc and loss are 0.9021 and 0.2800959\n",
            "Val acc and loss are 0.8756 and 0.34642062\n",
            "Processing Epoch 339\n",
            "Training acc and loss are 0.90192 and 0.27983856\n",
            "Val acc and loss are 0.876 and 0.34638345\n",
            "Processing Epoch 340\n",
            "Training acc and loss are 0.90226 and 0.27960706\n",
            "Val acc and loss are 0.8765 and 0.34634247\n",
            "Processing Epoch 341\n",
            "Training acc and loss are 0.90214 and 0.27940038\n",
            "Val acc and loss are 0.8766 and 0.34630936\n",
            "Processing Epoch 342\n",
            "Training acc and loss are 0.902 and 0.27912563\n",
            "Val acc and loss are 0.8768 and 0.3461579\n",
            "Processing Epoch 343\n",
            "Training acc and loss are 0.90202 and 0.27882376\n",
            "Val acc and loss are 0.8772 and 0.345915\n",
            "Processing Epoch 344\n",
            "Training acc and loss are 0.90226 and 0.27854344\n",
            "Val acc and loss are 0.8769 and 0.34571582\n",
            "Processing Epoch 345\n",
            "Training acc and loss are 0.9024 and 0.27827245\n",
            "Val acc and loss are 0.877 and 0.34555957\n",
            "Processing Epoch 346\n",
            "Training acc and loss are 0.90254 and 0.27800488\n",
            "Val acc and loss are 0.8764 and 0.34542045\n",
            "Processing Epoch 347\n",
            "Training acc and loss are 0.90252 and 0.27778035\n",
            "Val acc and loss are 0.8759 and 0.34532943\n",
            "Processing Epoch 348\n",
            "Training acc and loss are 0.9027 and 0.27755153\n",
            "Val acc and loss are 0.8754 and 0.34524265\n",
            "Processing Epoch 349\n",
            "Training acc and loss are 0.90292 and 0.27730954\n",
            "Val acc and loss are 0.8755 and 0.34518683\n",
            "Processing Epoch 350\n",
            "Training acc and loss are 0.90302 and 0.27706534\n",
            "Val acc and loss are 0.8759 and 0.3451451\n",
            "Processing Epoch 351\n",
            "Training acc and loss are 0.90304 and 0.27683583\n",
            "Val acc and loss are 0.8763 and 0.3451108\n",
            "Processing Epoch 352\n",
            "Training acc and loss are 0.90284 and 0.27660638\n",
            "Val acc and loss are 0.8765 and 0.34505108\n",
            "Processing Epoch 353\n",
            "Training acc and loss are 0.90302 and 0.2763546\n",
            "Val acc and loss are 0.8763 and 0.3449769\n",
            "Processing Epoch 354\n",
            "Training acc and loss are 0.90314 and 0.27607846\n",
            "Val acc and loss are 0.8764 and 0.34485504\n",
            "Processing Epoch 355\n",
            "Training acc and loss are 0.90348 and 0.27578598\n",
            "Val acc and loss are 0.8764 and 0.34477982\n",
            "Processing Epoch 356\n",
            "Training acc and loss are 0.90354 and 0.27549613\n",
            "Val acc and loss are 0.8763 and 0.34470043\n",
            "Processing Epoch 357\n",
            "Training acc and loss are 0.9036 and 0.2752278\n",
            "Val acc and loss are 0.8766 and 0.34464\n",
            "Processing Epoch 358\n",
            "Training acc and loss are 0.90372 and 0.27495575\n",
            "Val acc and loss are 0.8766 and 0.3445232\n",
            "Processing Epoch 359\n",
            "Training acc and loss are 0.90376 and 0.2747261\n",
            "Val acc and loss are 0.8772 and 0.34440497\n",
            "Processing Epoch 360\n",
            "Training acc and loss are 0.9038 and 0.2744904\n",
            "Val acc and loss are 0.8771 and 0.34431922\n",
            "Processing Epoch 361\n",
            "Training acc and loss are 0.90386 and 0.27423123\n",
            "Val acc and loss are 0.8772 and 0.344228\n",
            "Processing Epoch 362\n",
            "Training acc and loss are 0.90382 and 0.27391964\n",
            "Val acc and loss are 0.8767 and 0.34408385\n",
            "Processing Epoch 363\n",
            "Training acc and loss are 0.90396 and 0.2736803\n",
            "Val acc and loss are 0.8771 and 0.34397188\n",
            "Processing Epoch 364\n",
            "Training acc and loss are 0.90398 and 0.27347487\n",
            "Val acc and loss are 0.8765 and 0.34387758\n",
            "Processing Epoch 365\n",
            "Training acc and loss are 0.90416 and 0.2732643\n",
            "Val acc and loss are 0.8763 and 0.343803\n",
            "Processing Epoch 366\n",
            "Training acc and loss are 0.90416 and 0.27303442\n",
            "Val acc and loss are 0.8764 and 0.34373057\n",
            "Processing Epoch 367\n",
            "Training acc and loss are 0.9042 and 0.2727935\n",
            "Val acc and loss are 0.8765 and 0.34365967\n",
            "Processing Epoch 368\n",
            "Training acc and loss are 0.90438 and 0.27253512\n",
            "Val acc and loss are 0.8764 and 0.34353647\n",
            "Processing Epoch 369\n",
            "Training acc and loss are 0.90458 and 0.27223942\n",
            "Val acc and loss are 0.8766 and 0.3433317\n",
            "Processing Epoch 370\n",
            "Training acc and loss are 0.90462 and 0.27196893\n",
            "Val acc and loss are 0.8761 and 0.34312177\n",
            "Processing Epoch 371\n",
            "Training acc and loss are 0.90484 and 0.2716909\n",
            "Val acc and loss are 0.8764 and 0.34292832\n",
            "Processing Epoch 372\n",
            "Training acc and loss are 0.90488 and 0.27143005\n",
            "Val acc and loss are 0.8764 and 0.3427579\n",
            "Processing Epoch 373\n",
            "Training acc and loss are 0.90494 and 0.2711933\n",
            "Val acc and loss are 0.8771 and 0.34269208\n",
            "Processing Epoch 374\n",
            "Training acc and loss are 0.9051 and 0.2709644\n",
            "Val acc and loss are 0.877 and 0.34270087\n",
            "Processing Epoch 375\n",
            "Training acc and loss are 0.90544 and 0.27074137\n",
            "Val acc and loss are 0.8774 and 0.34268335\n",
            "Processing Epoch 376\n",
            "Training acc and loss are 0.90542 and 0.27052104\n",
            "Val acc and loss are 0.8774 and 0.34266695\n",
            "Processing Epoch 377\n",
            "Training acc and loss are 0.90552 and 0.2702742\n",
            "Val acc and loss are 0.8776 and 0.342586\n",
            "Processing Epoch 378\n",
            "Training acc and loss are 0.90566 and 0.27000114\n",
            "Val acc and loss are 0.8778 and 0.34246096\n",
            "Processing Epoch 379\n",
            "Training acc and loss are 0.90586 and 0.26977628\n",
            "Val acc and loss are 0.8774 and 0.34239164\n",
            "Processing Epoch 380\n",
            "Training acc and loss are 0.9059 and 0.26959643\n",
            "Val acc and loss are 0.8772 and 0.3423811\n",
            "Processing Epoch 381\n",
            "Training acc and loss are 0.90596 and 0.2693962\n",
            "Val acc and loss are 0.8777 and 0.34232622\n",
            "Processing Epoch 382\n",
            "Training acc and loss are 0.90606 and 0.2691684\n",
            "Val acc and loss are 0.8779 and 0.3422644\n",
            "Processing Epoch 383\n",
            "Training acc and loss are 0.906 and 0.2688576\n",
            "Val acc and loss are 0.8776 and 0.34210142\n",
            "Processing Epoch 384\n",
            "Training acc and loss are 0.90584 and 0.2685668\n",
            "Val acc and loss are 0.8787 and 0.34195942\n",
            "Processing Epoch 385\n",
            "Training acc and loss are 0.90608 and 0.26832312\n",
            "Val acc and loss are 0.8786 and 0.34187418\n",
            "Processing Epoch 386\n",
            "Training acc and loss are 0.90628 and 0.2680821\n",
            "Val acc and loss are 0.8784 and 0.34177116\n",
            "Processing Epoch 387\n",
            "Training acc and loss are 0.90614 and 0.26781794\n",
            "Val acc and loss are 0.8785 and 0.3416042\n",
            "Processing Epoch 388\n",
            "Training acc and loss are 0.90638 and 0.2676078\n",
            "Val acc and loss are 0.8785 and 0.3415005\n",
            "Processing Epoch 389\n",
            "Training acc and loss are 0.90646 and 0.26742095\n",
            "Val acc and loss are 0.8779 and 0.34142762\n",
            "Processing Epoch 390\n",
            "Training acc and loss are 0.90666 and 0.26719707\n",
            "Val acc and loss are 0.8779 and 0.341362\n",
            "Processing Epoch 391\n",
            "Training acc and loss are 0.90668 and 0.26693186\n",
            "Val acc and loss are 0.8786 and 0.34129593\n",
            "Processing Epoch 392\n",
            "Training acc and loss are 0.90666 and 0.26671547\n",
            "Val acc and loss are 0.8779 and 0.34128228\n",
            "Processing Epoch 393\n",
            "Training acc and loss are 0.9066 and 0.26650184\n",
            "Val acc and loss are 0.8778 and 0.34128338\n",
            "Processing Epoch 394\n",
            "Training acc and loss are 0.90674 and 0.2662627\n",
            "Val acc and loss are 0.8779 and 0.3412036\n",
            "Processing Epoch 395\n",
            "Training acc and loss are 0.90704 and 0.26604939\n",
            "Val acc and loss are 0.8777 and 0.34113574\n",
            "Processing Epoch 396\n",
            "Training acc and loss are 0.9073 and 0.26586798\n",
            "Val acc and loss are 0.8777 and 0.3410479\n",
            "Processing Epoch 397\n",
            "Training acc and loss are 0.90748 and 0.26564202\n",
            "Val acc and loss are 0.8775 and 0.34089848\n",
            "Processing Epoch 398\n",
            "Training acc and loss are 0.90738 and 0.2654076\n",
            "Val acc and loss are 0.8779 and 0.3407752\n",
            "Processing Epoch 399\n",
            "Training acc and loss are 0.90732 and 0.26511744\n",
            "Val acc and loss are 0.878 and 0.34055477\n",
            "Processing Epoch 400\n",
            "Training acc and loss are 0.90752 and 0.2648295\n",
            "Val acc and loss are 0.8777 and 0.34033722\n",
            "Processing Epoch 401\n",
            "Training acc and loss are 0.90758 and 0.2645726\n",
            "Val acc and loss are 0.8778 and 0.3401706\n",
            "Processing Epoch 402\n",
            "Training acc and loss are 0.90752 and 0.26434922\n",
            "Val acc and loss are 0.8782 and 0.34010795\n",
            "Processing Epoch 403\n",
            "Training acc and loss are 0.90784 and 0.26416978\n",
            "Val acc and loss are 0.8779 and 0.34014833\n",
            "Processing Epoch 404\n",
            "Training acc and loss are 0.90788 and 0.263978\n",
            "Val acc and loss are 0.8789 and 0.34020826\n",
            "Processing Epoch 405\n",
            "Training acc and loss are 0.90808 and 0.26369607\n",
            "Val acc and loss are 0.8792 and 0.34021187\n",
            "Processing Epoch 406\n",
            "Training acc and loss are 0.90828 and 0.26342696\n",
            "Val acc and loss are 0.8787 and 0.34015507\n",
            "Processing Epoch 407\n",
            "Training acc and loss are 0.90834 and 0.26316062\n",
            "Val acc and loss are 0.8783 and 0.33998755\n",
            "Processing Epoch 408\n",
            "Training acc and loss are 0.90864 and 0.26293257\n",
            "Val acc and loss are 0.878 and 0.33979413\n",
            "Processing Epoch 409\n",
            "Training acc and loss are 0.9084 and 0.26273686\n",
            "Val acc and loss are 0.8782 and 0.33965963\n",
            "Processing Epoch 410\n",
            "Training acc and loss are 0.9084 and 0.2625489\n",
            "Val acc and loss are 0.8783 and 0.33958018\n",
            "Processing Epoch 411\n",
            "Training acc and loss are 0.9085 and 0.26238078\n",
            "Val acc and loss are 0.8784 and 0.33960193\n",
            "Processing Epoch 412\n",
            "Training acc and loss are 0.90834 and 0.26217538\n",
            "Val acc and loss are 0.8784 and 0.33965757\n",
            "Processing Epoch 413\n",
            "Training acc and loss are 0.90876 and 0.26192617\n",
            "Val acc and loss are 0.8786 and 0.33964336\n",
            "Processing Epoch 414\n",
            "Training acc and loss are 0.90894 and 0.26167765\n",
            "Val acc and loss are 0.878 and 0.33957267\n",
            "Processing Epoch 415\n",
            "Training acc and loss are 0.90884 and 0.2614425\n",
            "Val acc and loss are 0.8784 and 0.33946747\n",
            "Processing Epoch 416\n",
            "Training acc and loss are 0.90882 and 0.26122767\n",
            "Val acc and loss are 0.8785 and 0.339367\n",
            "Processing Epoch 417\n",
            "Training acc and loss are 0.90914 and 0.26103765\n",
            "Val acc and loss are 0.8788 and 0.33927056\n",
            "Processing Epoch 418\n",
            "Training acc and loss are 0.90908 and 0.2608942\n",
            "Val acc and loss are 0.8783 and 0.33925128\n",
            "Processing Epoch 419\n",
            "Training acc and loss are 0.90906 and 0.26067692\n",
            "Val acc and loss are 0.8787 and 0.33920652\n",
            "Processing Epoch 420\n",
            "Training acc and loss are 0.90912 and 0.2603569\n",
            "Val acc and loss are 0.8789 and 0.33910125\n",
            "Processing Epoch 421\n",
            "Training acc and loss are 0.90948 and 0.2600755\n",
            "Val acc and loss are 0.8788 and 0.3390406\n",
            "Processing Epoch 422\n",
            "Training acc and loss are 0.90936 and 0.2598125\n",
            "Val acc and loss are 0.8782 and 0.3389819\n",
            "Processing Epoch 423\n",
            "Training acc and loss are 0.90948 and 0.25955632\n",
            "Val acc and loss are 0.8784 and 0.33880746\n",
            "Processing Epoch 424\n",
            "Training acc and loss are 0.90948 and 0.25935853\n",
            "Val acc and loss are 0.8785 and 0.3386983\n",
            "Processing Epoch 425\n",
            "Training acc and loss are 0.90976 and 0.25918806\n",
            "Val acc and loss are 0.8788 and 0.3386053\n",
            "Processing Epoch 426\n",
            "Training acc and loss are 0.90988 and 0.258934\n",
            "Val acc and loss are 0.879 and 0.33848965\n",
            "Processing Epoch 427\n",
            "Training acc and loss are 0.90988 and 0.25867847\n",
            "Val acc and loss are 0.879 and 0.33836493\n",
            "Processing Epoch 428\n",
            "Training acc and loss are 0.90998 and 0.2584406\n",
            "Val acc and loss are 0.8787 and 0.33830875\n",
            "Processing Epoch 429\n",
            "Training acc and loss are 0.9101 and 0.25820342\n",
            "Val acc and loss are 0.8789 and 0.3382828\n",
            "Processing Epoch 430\n",
            "Training acc and loss are 0.91028 and 0.2579614\n",
            "Val acc and loss are 0.8791 and 0.33825257\n",
            "Processing Epoch 431\n",
            "Training acc and loss are 0.9102 and 0.25777456\n",
            "Val acc and loss are 0.8792 and 0.3382819\n",
            "Processing Epoch 432\n",
            "Training acc and loss are 0.91006 and 0.25760272\n",
            "Val acc and loss are 0.8793 and 0.3383214\n",
            "Processing Epoch 433\n",
            "Training acc and loss are 0.91026 and 0.25737172\n",
            "Val acc and loss are 0.8792 and 0.33826745\n",
            "Processing Epoch 434\n",
            "Training acc and loss are 0.91038 and 0.2571089\n",
            "Val acc and loss are 0.8791 and 0.33816704\n",
            "Processing Epoch 435\n",
            "Training acc and loss are 0.91056 and 0.25687733\n",
            "Val acc and loss are 0.8791 and 0.33808774\n",
            "Processing Epoch 436\n",
            "Training acc and loss are 0.91044 and 0.25667045\n",
            "Val acc and loss are 0.879 and 0.33800963\n",
            "Processing Epoch 437\n",
            "Training acc and loss are 0.91058 and 0.25647694\n",
            "Val acc and loss are 0.879 and 0.33799922\n",
            "Processing Epoch 438\n",
            "Training acc and loss are 0.9107 and 0.2562843\n",
            "Val acc and loss are 0.8794 and 0.33799416\n",
            "Processing Epoch 439\n",
            "Training acc and loss are 0.91072 and 0.25605702\n",
            "Val acc and loss are 0.8798 and 0.33799428\n",
            "Processing Epoch 440\n",
            "Training acc and loss are 0.91064 and 0.25579447\n",
            "Val acc and loss are 0.8797 and 0.33793873\n",
            "Processing Epoch 441\n",
            "Training acc and loss are 0.9111 and 0.25554186\n",
            "Val acc and loss are 0.8793 and 0.3378452\n",
            "Processing Epoch 442\n",
            "Training acc and loss are 0.91132 and 0.2552597\n",
            "Val acc and loss are 0.879 and 0.33766085\n",
            "Processing Epoch 443\n",
            "Training acc and loss are 0.91134 and 0.25502574\n",
            "Val acc and loss are 0.8794 and 0.33748853\n",
            "Processing Epoch 444\n",
            "Training acc and loss are 0.91168 and 0.25487733\n",
            "Val acc and loss are 0.8794 and 0.33733848\n",
            "Processing Epoch 445\n",
            "Training acc and loss are 0.91184 and 0.25475505\n",
            "Val acc and loss are 0.8795 and 0.33730322\n",
            "Processing Epoch 446\n",
            "Training acc and loss are 0.91162 and 0.25460237\n",
            "Val acc and loss are 0.8795 and 0.3372791\n",
            "Processing Epoch 447\n",
            "Training acc and loss are 0.91166 and 0.25437564\n",
            "Val acc and loss are 0.8795 and 0.3372244\n",
            "Processing Epoch 448\n",
            "Training acc and loss are 0.91146 and 0.2540968\n",
            "Val acc and loss are 0.8786 and 0.3371692\n",
            "Processing Epoch 449\n",
            "Training acc and loss are 0.91174 and 0.2538767\n",
            "Val acc and loss are 0.8792 and 0.337099\n",
            "Processing Epoch 450\n",
            "Training acc and loss are 0.91178 and 0.25366524\n",
            "Val acc and loss are 0.8797 and 0.336978\n",
            "Processing Epoch 451\n",
            "Training acc and loss are 0.9118 and 0.2535199\n",
            "Val acc and loss are 0.88 and 0.3369495\n",
            "Processing Epoch 452\n",
            "Training acc and loss are 0.9119 and 0.25340176\n",
            "Val acc and loss are 0.8797 and 0.33695787\n",
            "Processing Epoch 453\n",
            "Training acc and loss are 0.91198 and 0.2531971\n",
            "Val acc and loss are 0.8796 and 0.33689207\n",
            "Processing Epoch 454\n",
            "Training acc and loss are 0.91208 and 0.2529404\n",
            "Val acc and loss are 0.8795 and 0.33680332\n",
            "Processing Epoch 455\n",
            "Training acc and loss are 0.91202 and 0.25266972\n",
            "Val acc and loss are 0.8795 and 0.3367664\n",
            "Processing Epoch 456\n",
            "Training acc and loss are 0.91226 and 0.25245222\n",
            "Val acc and loss are 0.8796 and 0.336807\n",
            "Processing Epoch 457\n",
            "Training acc and loss are 0.91252 and 0.25231567\n",
            "Val acc and loss are 0.8799 and 0.3368962\n",
            "Processing Epoch 458\n",
            "Training acc and loss are 0.91246 and 0.25212675\n",
            "Val acc and loss are 0.8797 and 0.33686334\n",
            "Processing Epoch 459\n",
            "Training acc and loss are 0.91268 and 0.25190192\n",
            "Val acc and loss are 0.8796 and 0.33673885\n",
            "Processing Epoch 460\n",
            "Training acc and loss are 0.91264 and 0.25165343\n",
            "Val acc and loss are 0.8801 and 0.3365791\n",
            "Processing Epoch 461\n",
            "Training acc and loss are 0.91298 and 0.25139645\n",
            "Val acc and loss are 0.8796 and 0.33647484\n",
            "Processing Epoch 462\n",
            "Training acc and loss are 0.91314 and 0.25114676\n",
            "Val acc and loss are 0.8798 and 0.33634868\n",
            "Processing Epoch 463\n",
            "Training acc and loss are 0.91312 and 0.25092453\n",
            "Val acc and loss are 0.88 and 0.33620635\n",
            "Processing Epoch 464\n",
            "Training acc and loss are 0.91336 and 0.25072712\n",
            "Val acc and loss are 0.8801 and 0.33610713\n",
            "Processing Epoch 465\n",
            "Training acc and loss are 0.91352 and 0.25051907\n",
            "Val acc and loss are 0.8802 and 0.33601326\n",
            "Processing Epoch 466\n",
            "Training acc and loss are 0.91344 and 0.2503193\n",
            "Val acc and loss are 0.8797 and 0.33599445\n",
            "Processing Epoch 467\n",
            "Training acc and loss are 0.91326 and 0.25015807\n",
            "Val acc and loss are 0.8799 and 0.33598226\n",
            "Processing Epoch 468\n",
            "Training acc and loss are 0.9134 and 0.24998197\n",
            "Val acc and loss are 0.8803 and 0.33593106\n",
            "Processing Epoch 469\n",
            "Training acc and loss are 0.91362 and 0.24977918\n",
            "Val acc and loss are 0.8799 and 0.33583638\n",
            "Processing Epoch 470\n",
            "Training acc and loss are 0.91352 and 0.24961573\n",
            "Val acc and loss are 0.8803 and 0.33582237\n",
            "Processing Epoch 471\n",
            "Training acc and loss are 0.91368 and 0.24941327\n",
            "Val acc and loss are 0.8799 and 0.3357554\n",
            "Processing Epoch 472\n",
            "Training acc and loss are 0.91376 and 0.24916033\n",
            "Val acc and loss are 0.8802 and 0.33563346\n",
            "Processing Epoch 473\n",
            "Training acc and loss are 0.91396 and 0.2489013\n",
            "Val acc and loss are 0.8804 and 0.33552954\n",
            "Processing Epoch 474\n",
            "Training acc and loss are 0.91382 and 0.2486915\n",
            "Val acc and loss are 0.8807 and 0.3355234\n",
            "Processing Epoch 475\n",
            "Training acc and loss are 0.91384 and 0.24846238\n",
            "Val acc and loss are 0.8802 and 0.33547348\n",
            "Processing Epoch 476\n",
            "Training acc and loss are 0.91416 and 0.24823213\n",
            "Val acc and loss are 0.8803 and 0.33541352\n",
            "Processing Epoch 477\n",
            "Training acc and loss are 0.91428 and 0.24805574\n",
            "Val acc and loss are 0.8803 and 0.33543986\n",
            "Processing Epoch 478\n",
            "Training acc and loss are 0.91426 and 0.2478679\n",
            "Val acc and loss are 0.8801 and 0.33549583\n",
            "Processing Epoch 479\n",
            "Training acc and loss are 0.91436 and 0.24763043\n",
            "Val acc and loss are 0.8803 and 0.33550498\n",
            "Processing Epoch 480\n",
            "Training acc and loss are 0.91438 and 0.2474095\n",
            "Val acc and loss are 0.8801 and 0.33545658\n",
            "Processing Epoch 481\n",
            "Training acc and loss are 0.9145 and 0.24718828\n",
            "Val acc and loss are 0.8805 and 0.335304\n",
            "Processing Epoch 482\n",
            "Training acc and loss are 0.9146 and 0.24703158\n",
            "Val acc and loss are 0.8801 and 0.33515552\n",
            "Processing Epoch 483\n",
            "Training acc and loss are 0.91456 and 0.24687897\n",
            "Val acc and loss are 0.8801 and 0.33510092\n",
            "Processing Epoch 484\n",
            "Training acc and loss are 0.9146 and 0.24671578\n",
            "Val acc and loss are 0.8806 and 0.33515567\n",
            "Processing Epoch 485\n",
            "Training acc and loss are 0.91462 and 0.2465677\n",
            "Val acc and loss are 0.8805 and 0.3352599\n",
            "Processing Epoch 486\n",
            "Training acc and loss are 0.91452 and 0.24643509\n",
            "Val acc and loss are 0.8804 and 0.3352922\n",
            "Processing Epoch 487\n",
            "Training acc and loss are 0.9147 and 0.24618937\n",
            "Val acc and loss are 0.8802 and 0.33516842\n",
            "Processing Epoch 488\n",
            "Training acc and loss are 0.9147 and 0.2459068\n",
            "Val acc and loss are 0.8803 and 0.33495197\n",
            "Processing Epoch 489\n",
            "Training acc and loss are 0.91496 and 0.24568316\n",
            "Val acc and loss are 0.8805 and 0.33481944\n",
            "Processing Epoch 490\n",
            "Training acc and loss are 0.91506 and 0.24546553\n",
            "Val acc and loss are 0.8805 and 0.33479297\n",
            "Processing Epoch 491\n",
            "Training acc and loss are 0.91524 and 0.24533246\n",
            "Val acc and loss are 0.8807 and 0.33489493\n",
            "Processing Epoch 492\n",
            "Training acc and loss are 0.9153 and 0.24519904\n",
            "Val acc and loss are 0.8809 and 0.33500996\n",
            "Processing Epoch 493\n",
            "Training acc and loss are 0.91532 and 0.24492574\n",
            "Val acc and loss are 0.8808 and 0.33499545\n",
            "Processing Epoch 494\n",
            "Training acc and loss are 0.91536 and 0.24460517\n",
            "Val acc and loss are 0.8805 and 0.3348732\n",
            "Processing Epoch 495\n",
            "Training acc and loss are 0.91524 and 0.24438018\n",
            "Val acc and loss are 0.8806 and 0.33477843\n",
            "Processing Epoch 496\n",
            "Training acc and loss are 0.91554 and 0.24420987\n",
            "Val acc and loss are 0.8806 and 0.3346572\n",
            "Processing Epoch 497\n",
            "Training acc and loss are 0.91566 and 0.24405931\n",
            "Val acc and loss are 0.8805 and 0.33450893\n",
            "Processing Epoch 498\n",
            "Training acc and loss are 0.91556 and 0.24389952\n",
            "Val acc and loss are 0.8805 and 0.3344019\n",
            "Processing Epoch 499\n",
            "Training acc and loss are 0.91578 and 0.24368489\n",
            "Val acc and loss are 0.8808 and 0.33423987\n",
            "Processing Epoch 500\n",
            "Training acc and loss are 0.9158 and 0.24347238\n",
            "Val acc and loss are 0.8805 and 0.33418226\n",
            "Processing Epoch 501\n",
            "Training acc and loss are 0.9158 and 0.2433143\n",
            "Val acc and loss are 0.8806 and 0.33418232\n",
            "Processing Epoch 502\n",
            "Training acc and loss are 0.91596 and 0.24313761\n",
            "Val acc and loss are 0.8805 and 0.3342179\n",
            "Processing Epoch 503\n",
            "Training acc and loss are 0.91606 and 0.24291262\n",
            "Val acc and loss are 0.8802 and 0.33414385\n",
            "Processing Epoch 504\n",
            "Training acc and loss are 0.91606 and 0.24276091\n",
            "Val acc and loss are 0.8803 and 0.33415148\n",
            "Processing Epoch 505\n",
            "Training acc and loss are 0.91618 and 0.2425519\n",
            "Val acc and loss are 0.8803 and 0.33416244\n",
            "Processing Epoch 506\n",
            "Training acc and loss are 0.9164 and 0.24229674\n",
            "Val acc and loss are 0.8798 and 0.33415636\n",
            "Processing Epoch 507\n",
            "Training acc and loss are 0.91626 and 0.24207245\n",
            "Val acc and loss are 0.88 and 0.33412835\n",
            "Processing Epoch 508\n",
            "Training acc and loss are 0.91644 and 0.24186729\n",
            "Val acc and loss are 0.8797 and 0.33409083\n",
            "Processing Epoch 509\n",
            "Training acc and loss are 0.91662 and 0.24163352\n",
            "Val acc and loss are 0.8798 and 0.33392408\n",
            "Processing Epoch 510\n",
            "Training acc and loss are 0.91674 and 0.24141417\n",
            "Val acc and loss are 0.8796 and 0.33375517\n",
            "Processing Epoch 511\n",
            "Training acc and loss are 0.91658 and 0.2412629\n",
            "Val acc and loss are 0.8803 and 0.33360648\n",
            "Processing Epoch 512\n",
            "Training acc and loss are 0.91676 and 0.24105336\n",
            "Val acc and loss are 0.8806 and 0.3335377\n",
            "Processing Epoch 513\n",
            "Training acc and loss are 0.9169 and 0.24083254\n",
            "Val acc and loss are 0.8805 and 0.33356628\n",
            "Processing Epoch 514\n",
            "Training acc and loss are 0.917 and 0.24063723\n",
            "Val acc and loss are 0.8802 and 0.33362797\n",
            "Processing Epoch 515\n",
            "Training acc and loss are 0.91704 and 0.24042284\n",
            "Val acc and loss are 0.8803 and 0.3336198\n",
            "Processing Epoch 516\n",
            "Training acc and loss are 0.91712 and 0.24018574\n",
            "Val acc and loss are 0.8807 and 0.33347392\n",
            "Processing Epoch 517\n",
            "Training acc and loss are 0.917 and 0.24001382\n",
            "Val acc and loss are 0.8811 and 0.33332968\n",
            "Processing Epoch 518\n",
            "Training acc and loss are 0.91702 and 0.23984241\n",
            "Val acc and loss are 0.8809 and 0.33319902\n",
            "Processing Epoch 519\n",
            "Training acc and loss are 0.9171 and 0.23971553\n",
            "Val acc and loss are 0.8809 and 0.33317208\n",
            "Processing Epoch 520\n",
            "Training acc and loss are 0.91738 and 0.23957473\n",
            "Val acc and loss are 0.8808 and 0.3331517\n",
            "Processing Epoch 521\n",
            "Training acc and loss are 0.91744 and 0.23935312\n",
            "Val acc and loss are 0.881 and 0.3330126\n",
            "Processing Epoch 522\n",
            "Training acc and loss are 0.91766 and 0.23910885\n",
            "Val acc and loss are 0.8809 and 0.3328787\n",
            "Processing Epoch 523\n",
            "Training acc and loss are 0.91768 and 0.2388486\n",
            "Val acc and loss are 0.8805 and 0.3328133\n",
            "Processing Epoch 524\n",
            "Training acc and loss are 0.91764 and 0.23864941\n",
            "Val acc and loss are 0.8808 and 0.33287647\n",
            "Processing Epoch 525\n",
            "Training acc and loss are 0.9176 and 0.23851699\n",
            "Val acc and loss are 0.8808 and 0.33301476\n",
            "Processing Epoch 526\n",
            "Training acc and loss are 0.91756 and 0.23835689\n",
            "Val acc and loss are 0.8805 and 0.33308217\n",
            "Processing Epoch 527\n",
            "Training acc and loss are 0.91792 and 0.23811136\n",
            "Val acc and loss are 0.8809 and 0.33302286\n",
            "Processing Epoch 528\n",
            "Training acc and loss are 0.9179 and 0.23788598\n",
            "Val acc and loss are 0.8808 and 0.3329389\n",
            "Processing Epoch 529\n",
            "Training acc and loss are 0.91812 and 0.23772107\n",
            "Val acc and loss are 0.8812 and 0.33291763\n",
            "Processing Epoch 530\n",
            "Training acc and loss are 0.91822 and 0.23758869\n",
            "Val acc and loss are 0.8812 and 0.33283433\n",
            "Processing Epoch 531\n",
            "Training acc and loss are 0.9184 and 0.23750864\n",
            "Val acc and loss are 0.8808 and 0.3328425\n",
            "Processing Epoch 532\n",
            "Training acc and loss are 0.91832 and 0.23735993\n",
            "Val acc and loss are 0.8808 and 0.33282906\n",
            "Processing Epoch 533\n",
            "Training acc and loss are 0.9181 and 0.23708414\n",
            "Val acc and loss are 0.8808 and 0.3327183\n",
            "Processing Epoch 534\n",
            "Training acc and loss are 0.91792 and 0.23687556\n",
            "Val acc and loss are 0.8801 and 0.33268097\n",
            "Processing Epoch 535\n",
            "Training acc and loss are 0.91784 and 0.23671547\n",
            "Val acc and loss are 0.8805 and 0.33270416\n",
            "Processing Epoch 536\n",
            "Training acc and loss are 0.91804 and 0.23650166\n",
            "Val acc and loss are 0.8807 and 0.33267143\n",
            "Processing Epoch 537\n",
            "Training acc and loss are 0.91832 and 0.23634009\n",
            "Val acc and loss are 0.8811 and 0.3326676\n",
            "Processing Epoch 538\n",
            "Training acc and loss are 0.9185 and 0.23616864\n",
            "Val acc and loss are 0.8807 and 0.33267465\n",
            "Processing Epoch 539\n",
            "Training acc and loss are 0.9185 and 0.2359098\n",
            "Val acc and loss are 0.8807 and 0.33261284\n",
            "Processing Epoch 540\n",
            "Training acc and loss are 0.91866 and 0.23566097\n",
            "Val acc and loss are 0.8808 and 0.33249262\n",
            "Processing Epoch 541\n",
            "Training acc and loss are 0.9186 and 0.23550254\n",
            "Val acc and loss are 0.8805 and 0.33240077\n",
            "Processing Epoch 542\n",
            "Training acc and loss are 0.91874 and 0.23537469\n",
            "Val acc and loss are 0.881 and 0.3323131\n",
            "Processing Epoch 543\n",
            "Training acc and loss are 0.9185 and 0.23528199\n",
            "Val acc and loss are 0.8814 and 0.332242\n",
            "Processing Epoch 544\n",
            "Training acc and loss are 0.91864 and 0.23509498\n",
            "Val acc and loss are 0.8808 and 0.33206278\n",
            "Processing Epoch 545\n",
            "Training acc and loss are 0.91876 and 0.23484625\n",
            "Val acc and loss are 0.8807 and 0.3319125\n",
            "Processing Epoch 546\n",
            "Training acc and loss are 0.91888 and 0.23457047\n",
            "Val acc and loss are 0.8808 and 0.33184144\n",
            "Processing Epoch 547\n",
            "Training acc and loss are 0.91892 and 0.23431075\n",
            "Val acc and loss are 0.8808 and 0.33185807\n",
            "Processing Epoch 548\n",
            "Training acc and loss are 0.9189 and 0.23411441\n",
            "Val acc and loss are 0.8814 and 0.3319562\n",
            "Processing Epoch 549\n",
            "Training acc and loss are 0.9188 and 0.2339534\n",
            "Val acc and loss are 0.8809 and 0.33210406\n",
            "Processing Epoch 550\n",
            "Training acc and loss are 0.91908 and 0.23378235\n",
            "Val acc and loss are 0.8809 and 0.3321314\n",
            "Processing Epoch 551\n",
            "Training acc and loss are 0.91916 and 0.2336788\n",
            "Val acc and loss are 0.8811 and 0.33218446\n",
            "Processing Epoch 552\n",
            "Training acc and loss are 0.91922 and 0.23355322\n",
            "Val acc and loss are 0.881 and 0.33223096\n",
            "Processing Epoch 553\n",
            "Training acc and loss are 0.91928 and 0.2333486\n",
            "Val acc and loss are 0.881 and 0.33212057\n",
            "Processing Epoch 554\n",
            "Training acc and loss are 0.91942 and 0.23314826\n",
            "Val acc and loss are 0.8815 and 0.33202282\n",
            "Processing Epoch 555\n",
            "Training acc and loss are 0.91954 and 0.2329204\n",
            "Val acc and loss are 0.8814 and 0.33191407\n",
            "Processing Epoch 556\n",
            "Training acc and loss are 0.91958 and 0.23274118\n",
            "Val acc and loss are 0.8814 and 0.3318754\n",
            "Processing Epoch 557\n",
            "Training acc and loss are 0.91958 and 0.23261541\n",
            "Val acc and loss are 0.8802 and 0.3319063\n",
            "Processing Epoch 558\n",
            "Training acc and loss are 0.91936 and 0.23245734\n",
            "Val acc and loss are 0.8803 and 0.33189583\n",
            "Processing Epoch 559\n",
            "Training acc and loss are 0.9195 and 0.23226023\n",
            "Val acc and loss are 0.8797 and 0.33181414\n",
            "Processing Epoch 560\n",
            "Training acc and loss are 0.9196 and 0.23202558\n",
            "Val acc and loss are 0.8798 and 0.33166143\n",
            "Processing Epoch 561\n",
            "Training acc and loss are 0.9196 and 0.23180419\n",
            "Val acc and loss are 0.88 and 0.33146566\n",
            "Processing Epoch 562\n",
            "Training acc and loss are 0.91984 and 0.23164488\n",
            "Val acc and loss are 0.8805 and 0.3313972\n",
            "Processing Epoch 563\n",
            "Training acc and loss are 0.92016 and 0.23156461\n",
            "Val acc and loss are 0.8806 and 0.3314719\n",
            "Processing Epoch 564\n",
            "Training acc and loss are 0.92012 and 0.23145363\n",
            "Val acc and loss are 0.8808 and 0.3315585\n",
            "Processing Epoch 565\n",
            "Training acc and loss are 0.92028 and 0.23126788\n",
            "Val acc and loss are 0.8806 and 0.331569\n",
            "Processing Epoch 566\n",
            "Training acc and loss are 0.92004 and 0.23105755\n",
            "Val acc and loss are 0.8809 and 0.33153233\n",
            "Processing Epoch 567\n",
            "Training acc and loss are 0.92022 and 0.23073734\n",
            "Val acc and loss are 0.881 and 0.3314037\n",
            "Processing Epoch 568\n",
            "Training acc and loss are 0.92054 and 0.23047715\n",
            "Val acc and loss are 0.8813 and 0.3313647\n",
            "Processing Epoch 569\n",
            "Training acc and loss are 0.9206 and 0.23031028\n",
            "Val acc and loss are 0.8814 and 0.33146632\n",
            "Processing Epoch 570\n",
            "Training acc and loss are 0.92062 and 0.23014434\n",
            "Val acc and loss are 0.8812 and 0.33149347\n",
            "Processing Epoch 571\n",
            "Training acc and loss are 0.9207 and 0.23000197\n",
            "Val acc and loss are 0.8808 and 0.33145195\n",
            "Processing Epoch 572\n",
            "Training acc and loss are 0.92092 and 0.22985084\n",
            "Val acc and loss are 0.8811 and 0.33135805\n",
            "Processing Epoch 573\n",
            "Training acc and loss are 0.92088 and 0.2296299\n",
            "Val acc and loss are 0.8807 and 0.33116007\n",
            "Processing Epoch 574\n",
            "Training acc and loss are 0.92092 and 0.22944711\n",
            "Val acc and loss are 0.881 and 0.3310112\n",
            "Processing Epoch 575\n",
            "Training acc and loss are 0.9209 and 0.22929393\n",
            "Val acc and loss are 0.8812 and 0.3310256\n",
            "Processing Epoch 576\n",
            "Training acc and loss are 0.92082 and 0.22917974\n",
            "Val acc and loss are 0.881 and 0.331158\n",
            "Processing Epoch 577\n",
            "Training acc and loss are 0.92104 and 0.22908607\n",
            "Val acc and loss are 0.881 and 0.33129188\n",
            "Processing Epoch 578\n",
            "Training acc and loss are 0.92094 and 0.22890534\n",
            "Val acc and loss are 0.8807 and 0.3313155\n",
            "Processing Epoch 579\n",
            "Training acc and loss are 0.9208 and 0.22862144\n",
            "Val acc and loss are 0.8813 and 0.3311776\n",
            "Processing Epoch 580\n",
            "Training acc and loss are 0.92122 and 0.22834112\n",
            "Val acc and loss are 0.8809 and 0.33105576\n",
            "Processing Epoch 581\n",
            "Training acc and loss are 0.92124 and 0.22815464\n",
            "Val acc and loss are 0.8808 and 0.3310442\n",
            "Processing Epoch 582\n",
            "Training acc and loss are 0.92142 and 0.22800234\n",
            "Val acc and loss are 0.8807 and 0.33108205\n",
            "Processing Epoch 583\n",
            "Training acc and loss are 0.92132 and 0.22787239\n",
            "Val acc and loss are 0.8812 and 0.33114135\n",
            "Processing Epoch 584\n",
            "Training acc and loss are 0.9213 and 0.22775015\n",
            "Val acc and loss are 0.8816 and 0.33113888\n",
            "Processing Epoch 585\n",
            "Training acc and loss are 0.92154 and 0.2275839\n",
            "Val acc and loss are 0.8817 and 0.3310163\n",
            "Processing Epoch 586\n",
            "Training acc and loss are 0.92148 and 0.22740285\n",
            "Val acc and loss are 0.8815 and 0.33089468\n",
            "Processing Epoch 587\n",
            "Training acc and loss are 0.9218 and 0.22725067\n",
            "Val acc and loss are 0.8817 and 0.33089176\n",
            "Processing Epoch 588\n",
            "Training acc and loss are 0.92168 and 0.22705731\n",
            "Val acc and loss are 0.8817 and 0.3308888\n",
            "Processing Epoch 589\n",
            "Training acc and loss are 0.92168 and 0.22687699\n",
            "Val acc and loss are 0.8812 and 0.3309771\n",
            "Processing Epoch 590\n",
            "Training acc and loss are 0.92192 and 0.22666167\n",
            "Val acc and loss are 0.8812 and 0.3311242\n",
            "Processing Epoch 591\n",
            "Training acc and loss are 0.922 and 0.22649372\n",
            "Val acc and loss are 0.8814 and 0.3312257\n",
            "Processing Epoch 592\n",
            "Training acc and loss are 0.92206 and 0.22631246\n",
            "Val acc and loss are 0.8812 and 0.33113456\n",
            "Processing Epoch 593\n",
            "Training acc and loss are 0.92214 and 0.22614835\n",
            "Val acc and loss are 0.8812 and 0.33091575\n",
            "Processing Epoch 594\n",
            "Training acc and loss are 0.92222 and 0.22605242\n",
            "Val acc and loss are 0.8817 and 0.33079764\n",
            "Processing Epoch 595\n",
            "Training acc and loss are 0.92218 and 0.22586016\n",
            "Val acc and loss are 0.882 and 0.330656\n",
            "Processing Epoch 596\n",
            "Training acc and loss are 0.92244 and 0.22565028\n",
            "Val acc and loss are 0.8819 and 0.33061334\n",
            "Processing Epoch 597\n",
            "Training acc and loss are 0.92254 and 0.22544135\n",
            "Val acc and loss are 0.8818 and 0.33053434\n",
            "Processing Epoch 598\n",
            "Training acc and loss are 0.92266 and 0.22522715\n",
            "Val acc and loss are 0.8815 and 0.33051395\n",
            "Processing Epoch 599\n",
            "Training acc and loss are 0.92248 and 0.22510864\n",
            "Val acc and loss are 0.8813 and 0.33056307\n",
            "Processing Epoch 600\n",
            "Training acc and loss are 0.9226 and 0.22503114\n",
            "Val acc and loss are 0.8812 and 0.33062264\n",
            "Processing Epoch 601\n",
            "Training acc and loss are 0.9226 and 0.22489859\n",
            "Val acc and loss are 0.8812 and 0.33063594\n",
            "Processing Epoch 602\n",
            "Training acc and loss are 0.92266 and 0.2247365\n",
            "Val acc and loss are 0.8816 and 0.33059672\n",
            "Processing Epoch 603\n",
            "Training acc and loss are 0.9225 and 0.22459422\n",
            "Val acc and loss are 0.8819 and 0.33065128\n",
            "Processing Epoch 604\n",
            "Training acc and loss are 0.92262 and 0.22442518\n",
            "Val acc and loss are 0.8817 and 0.33072457\n",
            "Processing Epoch 605\n",
            "Training acc and loss are 0.92272 and 0.22425044\n",
            "Val acc and loss are 0.8816 and 0.33072683\n",
            "Processing Epoch 606\n",
            "Training acc and loss are 0.92266 and 0.224058\n",
            "Val acc and loss are 0.8819 and 0.33062407\n",
            "Processing Epoch 607\n",
            "Training acc and loss are 0.92274 and 0.22383258\n",
            "Val acc and loss are 0.8819 and 0.33048907\n",
            "Processing Epoch 608\n",
            "Training acc and loss are 0.923 and 0.22361356\n",
            "Val acc and loss are 0.882 and 0.33035937\n",
            "Processing Epoch 609\n",
            "Training acc and loss are 0.92294 and 0.22342221\n",
            "Val acc and loss are 0.8818 and 0.33032748\n",
            "Processing Epoch 610\n",
            "Training acc and loss are 0.9229 and 0.22318444\n",
            "Val acc and loss are 0.882 and 0.3302825\n",
            "Processing Epoch 611\n",
            "Training acc and loss are 0.92304 and 0.222973\n",
            "Val acc and loss are 0.8816 and 0.3302022\n",
            "Processing Epoch 612\n",
            "Training acc and loss are 0.92308 and 0.22276664\n",
            "Val acc and loss are 0.8815 and 0.3300802\n",
            "Processing Epoch 613\n",
            "Training acc and loss are 0.9231 and 0.2225954\n",
            "Val acc and loss are 0.8823 and 0.3300545\n",
            "Processing Epoch 614\n",
            "Training acc and loss are 0.92362 and 0.22247817\n",
            "Val acc and loss are 0.8825 and 0.33013922\n",
            "Processing Epoch 615\n",
            "Training acc and loss are 0.92344 and 0.22244015\n",
            "Val acc and loss are 0.8826 and 0.33032975\n",
            "Processing Epoch 616\n",
            "Training acc and loss are 0.92364 and 0.22228962\n",
            "Val acc and loss are 0.8822 and 0.33039263\n",
            "Processing Epoch 617\n",
            "Training acc and loss are 0.9237 and 0.22211127\n",
            "Val acc and loss are 0.8822 and 0.3304211\n",
            "Processing Epoch 618\n",
            "Training acc and loss are 0.92364 and 0.22191654\n",
            "Val acc and loss are 0.8819 and 0.33032563\n",
            "Processing Epoch 619\n",
            "Training acc and loss are 0.92386 and 0.22175984\n",
            "Val acc and loss are 0.8814 and 0.33023906\n",
            "Processing Epoch 620\n",
            "Training acc and loss are 0.9238 and 0.22158739\n",
            "Val acc and loss are 0.8814 and 0.3301756\n",
            "Processing Epoch 621\n",
            "Training acc and loss are 0.9237 and 0.22141941\n",
            "Val acc and loss are 0.8819 and 0.3300396\n",
            "Processing Epoch 622\n",
            "Training acc and loss are 0.92396 and 0.22117971\n",
            "Val acc and loss are 0.8814 and 0.3300199\n",
            "Processing Epoch 623\n",
            "Training acc and loss are 0.9243 and 0.22091602\n",
            "Val acc and loss are 0.8813 and 0.33003926\n",
            "Processing Epoch 624\n",
            "Training acc and loss are 0.92416 and 0.22070707\n",
            "Val acc and loss are 0.8815 and 0.3300649\n",
            "Processing Epoch 625\n",
            "Training acc and loss are 0.924 and 0.2205975\n",
            "Val acc and loss are 0.882 and 0.33009467\n",
            "Processing Epoch 626\n",
            "Training acc and loss are 0.92394 and 0.22043304\n",
            "Val acc and loss are 0.8823 and 0.32997295\n",
            "Processing Epoch 627\n",
            "Training acc and loss are 0.9241 and 0.22032923\n",
            "Val acc and loss are 0.8824 and 0.32990295\n",
            "Processing Epoch 628\n",
            "Training acc and loss are 0.9242 and 0.22022282\n",
            "Val acc and loss are 0.8829 and 0.32977393\n",
            "Processing Epoch 629\n",
            "Training acc and loss are 0.92422 and 0.22003727\n",
            "Val acc and loss are 0.8823 and 0.32960615\n",
            "Processing Epoch 630\n",
            "Training acc and loss are 0.92442 and 0.21985403\n",
            "Val acc and loss are 0.8822 and 0.32945797\n",
            "Processing Epoch 631\n",
            "Training acc and loss are 0.92448 and 0.21972555\n",
            "Val acc and loss are 0.8818 and 0.32951573\n",
            "Processing Epoch 632\n",
            "Training acc and loss are 0.92432 and 0.21960442\n",
            "Val acc and loss are 0.8823 and 0.32966417\n",
            "Processing Epoch 633\n",
            "Training acc and loss are 0.92422 and 0.21955973\n",
            "Val acc and loss are 0.8832 and 0.3299028\n",
            "Processing Epoch 634\n",
            "Training acc and loss are 0.92444 and 0.21938413\n",
            "Val acc and loss are 0.8831 and 0.3298965\n",
            "Processing Epoch 635\n",
            "Training acc and loss are 0.92468 and 0.21906126\n",
            "Val acc and loss are 0.8828 and 0.32971823\n",
            "Processing Epoch 636\n",
            "Training acc and loss are 0.92472 and 0.21883477\n",
            "Val acc and loss are 0.8824 and 0.32962358\n",
            "Processing Epoch 637\n",
            "Training acc and loss are 0.92478 and 0.21869844\n",
            "Val acc and loss are 0.8829 and 0.32959205\n",
            "Processing Epoch 638\n",
            "Training acc and loss are 0.92478 and 0.21860126\n",
            "Val acc and loss are 0.8827 and 0.32965934\n",
            "Processing Epoch 639\n",
            "Training acc and loss are 0.9249 and 0.21851707\n",
            "Val acc and loss are 0.8825 and 0.32972157\n",
            "Processing Epoch 640\n",
            "Training acc and loss are 0.9248 and 0.21842586\n",
            "Val acc and loss are 0.8824 and 0.32977232\n",
            "Processing Epoch 641\n",
            "Training acc and loss are 0.92482 and 0.21826944\n",
            "Val acc and loss are 0.8828 and 0.32976463\n",
            "Processing Epoch 642\n",
            "Training acc and loss are 0.92496 and 0.21806376\n",
            "Val acc and loss are 0.8823 and 0.32970172\n",
            "Processing Epoch 643\n",
            "Training acc and loss are 0.92534 and 0.21781544\n",
            "Val acc and loss are 0.8822 and 0.3296979\n",
            "Processing Epoch 644\n",
            "Training acc and loss are 0.92536 and 0.21760066\n",
            "Val acc and loss are 0.8816 and 0.32972765\n",
            "Processing Epoch 645\n",
            "Training acc and loss are 0.92532 and 0.21743754\n",
            "Val acc and loss are 0.8816 and 0.3298369\n",
            "Processing Epoch 646\n",
            "Training acc and loss are 0.92518 and 0.21731211\n",
            "Val acc and loss are 0.8815 and 0.32986954\n",
            "Processing Epoch 647\n",
            "Training acc and loss are 0.92524 and 0.21710809\n",
            "Val acc and loss are 0.8823 and 0.32973158\n",
            "Processing Epoch 648\n",
            "Training acc and loss are 0.92548 and 0.21686213\n",
            "Val acc and loss are 0.8825 and 0.32958937\n",
            "Processing Epoch 649\n",
            "Training acc and loss are 0.9256 and 0.2167102\n",
            "Val acc and loss are 0.8823 and 0.3296095\n",
            "Processing Epoch 650\n",
            "Training acc and loss are 0.92544 and 0.21660453\n",
            "Val acc and loss are 0.8825 and 0.32967702\n",
            "Processing Epoch 651\n",
            "Training acc and loss are 0.92546 and 0.21643615\n",
            "Val acc and loss are 0.8827 and 0.32962412\n",
            "Processing Epoch 652\n",
            "Training acc and loss are 0.92562 and 0.21632923\n",
            "Val acc and loss are 0.8817 and 0.32958242\n",
            "Processing Epoch 653\n",
            "Training acc and loss are 0.9255 and 0.21615265\n",
            "Val acc and loss are 0.8816 and 0.32954043\n",
            "Processing Epoch 654\n",
            "Training acc and loss are 0.92558 and 0.21592401\n",
            "Val acc and loss are 0.8819 and 0.32945663\n",
            "Processing Epoch 655\n",
            "Training acc and loss are 0.92548 and 0.21574065\n",
            "Val acc and loss are 0.8824 and 0.32936022\n",
            "Processing Epoch 656\n",
            "Training acc and loss are 0.92572 and 0.21554844\n",
            "Val acc and loss are 0.8826 and 0.329294\n",
            "Processing Epoch 657\n",
            "Training acc and loss are 0.92582 and 0.21536008\n",
            "Val acc and loss are 0.8824 and 0.32915998\n",
            "Processing Epoch 658\n",
            "Training acc and loss are 0.926 and 0.21520305\n",
            "Val acc and loss are 0.8829 and 0.3291148\n",
            "Processing Epoch 659\n",
            "Training acc and loss are 0.92626 and 0.21503422\n",
            "Val acc and loss are 0.8829 and 0.32903793\n",
            "Processing Epoch 660\n",
            "Training acc and loss are 0.92618 and 0.21488482\n",
            "Val acc and loss are 0.883 and 0.32902804\n",
            "Processing Epoch 661\n",
            "Training acc and loss are 0.92624 and 0.2147362\n",
            "Val acc and loss are 0.8827 and 0.32901508\n",
            "Processing Epoch 662\n",
            "Training acc and loss are 0.92628 and 0.21458429\n",
            "Val acc and loss are 0.8823 and 0.32897523\n",
            "Processing Epoch 663\n",
            "Training acc and loss are 0.92652 and 0.21442065\n",
            "Val acc and loss are 0.8829 and 0.32888678\n",
            "Processing Epoch 664\n",
            "Training acc and loss are 0.92656 and 0.2142341\n",
            "Val acc and loss are 0.883 and 0.32883212\n",
            "Processing Epoch 665\n",
            "Training acc and loss are 0.9267 and 0.21405533\n",
            "Val acc and loss are 0.8835 and 0.32877994\n",
            "Processing Epoch 666\n",
            "Training acc and loss are 0.92662 and 0.21392302\n",
            "Val acc and loss are 0.8831 and 0.32888082\n",
            "Processing Epoch 667\n",
            "Training acc and loss are 0.92662 and 0.21373774\n",
            "Val acc and loss are 0.8834 and 0.32890987\n",
            "Processing Epoch 668\n",
            "Training acc and loss are 0.92644 and 0.21357508\n",
            "Val acc and loss are 0.8831 and 0.3288672\n",
            "Processing Epoch 669\n",
            "Training acc and loss are 0.92652 and 0.21349637\n",
            "Val acc and loss are 0.8831 and 0.32889917\n",
            "Processing Epoch 670\n",
            "Training acc and loss are 0.92664 and 0.21333402\n",
            "Val acc and loss are 0.8829 and 0.32885316\n",
            "Processing Epoch 671\n",
            "Training acc and loss are 0.92692 and 0.21316098\n",
            "Val acc and loss are 0.8832 and 0.32885912\n",
            "Processing Epoch 672\n",
            "Training acc and loss are 0.9268 and 0.21298297\n",
            "Val acc and loss are 0.8827 and 0.32891572\n",
            "Processing Epoch 673\n",
            "Training acc and loss are 0.92688 and 0.21277156\n",
            "Val acc and loss are 0.8829 and 0.32892254\n",
            "Processing Epoch 674\n",
            "Training acc and loss are 0.927 and 0.21259074\n",
            "Val acc and loss are 0.8832 and 0.32887545\n",
            "Processing Epoch 675\n",
            "Training acc and loss are 0.92724 and 0.21238379\n",
            "Val acc and loss are 0.8826 and 0.3287208\n",
            "Processing Epoch 676\n",
            "Training acc and loss are 0.92712 and 0.21221985\n",
            "Val acc and loss are 0.8824 and 0.32853615\n",
            "Processing Epoch 677\n",
            "Training acc and loss are 0.9272 and 0.21208282\n",
            "Val acc and loss are 0.8826 and 0.32841793\n",
            "Processing Epoch 678\n",
            "Training acc and loss are 0.92728 and 0.21185876\n",
            "Val acc and loss are 0.8823 and 0.32836565\n",
            "Processing Epoch 679\n",
            "Training acc and loss are 0.9274 and 0.21170919\n",
            "Val acc and loss are 0.8826 and 0.3284379\n",
            "Processing Epoch 680\n",
            "Training acc and loss are 0.9272 and 0.21172625\n",
            "Val acc and loss are 0.883 and 0.3286656\n",
            "Processing Epoch 681\n",
            "Training acc and loss are 0.92712 and 0.21167004\n",
            "Val acc and loss are 0.8828 and 0.32874078\n",
            "Processing Epoch 682\n",
            "Training acc and loss are 0.92726 and 0.21148223\n",
            "Val acc and loss are 0.8825 and 0.32863593\n",
            "Processing Epoch 683\n",
            "Training acc and loss are 0.92752 and 0.21135922\n",
            "Val acc and loss are 0.8821 and 0.32859737\n",
            "Processing Epoch 684\n",
            "Training acc and loss are 0.92756 and 0.21111886\n",
            "Val acc and loss are 0.8818 and 0.328594\n",
            "Processing Epoch 685\n",
            "Training acc and loss are 0.92778 and 0.2108427\n",
            "Val acc and loss are 0.882 and 0.3287033\n",
            "Processing Epoch 686\n",
            "Training acc and loss are 0.9278 and 0.21079087\n",
            "Val acc and loss are 0.8826 and 0.3290722\n",
            "Processing Epoch 687\n",
            "Training acc and loss are 0.9276 and 0.21078314\n",
            "Val acc and loss are 0.883 and 0.3292938\n",
            "Processing Epoch 688\n",
            "Training acc and loss are 0.92742 and 0.21049635\n",
            "Val acc and loss are 0.8831 and 0.32902762\n",
            "Processing Epoch 689\n",
            "Training acc and loss are 0.9279 and 0.21023743\n",
            "Val acc and loss are 0.8827 and 0.32859284\n",
            "Processing Epoch 690\n",
            "Training acc and loss are 0.92822 and 0.21016039\n",
            "Val acc and loss are 0.8831 and 0.32837862\n",
            "Processing Epoch 691\n",
            "Training acc and loss are 0.92816 and 0.20990053\n",
            "Val acc and loss are 0.8828 and 0.32833046\n",
            "Processing Epoch 692\n",
            "Training acc and loss are 0.928 and 0.20979513\n",
            "Val acc and loss are 0.883 and 0.32857412\n",
            "Processing Epoch 693\n",
            "Training acc and loss are 0.92804 and 0.20985594\n",
            "Val acc and loss are 0.8829 and 0.32902312\n",
            "Processing Epoch 694\n",
            "Training acc and loss are 0.92796 and 0.20961422\n",
            "Val acc and loss are 0.8823 and 0.3289246\n",
            "Processing Epoch 695\n",
            "Training acc and loss are 0.9281 and 0.2092767\n",
            "Val acc and loss are 0.882 and 0.3285102\n",
            "Processing Epoch 696\n",
            "Training acc and loss are 0.92826 and 0.20916697\n",
            "Val acc and loss are 0.8826 and 0.3282428\n",
            "Processing Epoch 697\n",
            "Training acc and loss are 0.92828 and 0.20910965\n",
            "Val acc and loss are 0.8836 and 0.32811758\n",
            "Processing Epoch 698\n",
            "Training acc and loss are 0.92828 and 0.20894225\n",
            "Val acc and loss are 0.8829 and 0.3282618\n",
            "Processing Epoch 699\n",
            "Training acc and loss are 0.92802 and 0.20883305\n",
            "Val acc and loss are 0.8828 and 0.32852876\n",
            "Processing Epoch 700\n",
            "Training acc and loss are 0.92806 and 0.20873559\n",
            "Val acc and loss are 0.883 and 0.32881293\n",
            "Processing Epoch 701\n",
            "Training acc and loss are 0.92836 and 0.2084607\n",
            "Val acc and loss are 0.8833 and 0.32875678\n",
            "Processing Epoch 702\n",
            "Training acc and loss are 0.92882 and 0.20819424\n",
            "Val acc and loss are 0.8816 and 0.32858658\n",
            "Processing Epoch 703\n",
            "Training acc and loss are 0.92894 and 0.20807728\n",
            "Val acc and loss are 0.882 and 0.3285445\n",
            "Processing Epoch 704\n",
            "Training acc and loss are 0.92872 and 0.2078953\n",
            "Val acc and loss are 0.8818 and 0.32849568\n",
            "Processing Epoch 705\n",
            "Training acc and loss are 0.92884 and 0.20773463\n",
            "Val acc and loss are 0.8824 and 0.32852855\n",
            "Processing Epoch 706\n",
            "Training acc and loss are 0.92868 and 0.20771836\n",
            "Val acc and loss are 0.8835 and 0.3287016\n",
            "Processing Epoch 707\n",
            "Training acc and loss are 0.92878 and 0.20752391\n",
            "Val acc and loss are 0.8837 and 0.32850963\n",
            "Processing Epoch 708\n",
            "Training acc and loss are 0.92888 and 0.20726314\n",
            "Val acc and loss are 0.8841 and 0.32821524\n",
            "Processing Epoch 709\n",
            "Training acc and loss are 0.92894 and 0.20705895\n",
            "Val acc and loss are 0.8836 and 0.3281143\n",
            "Processing Epoch 710\n",
            "Training acc and loss are 0.92904 and 0.20691985\n",
            "Val acc and loss are 0.8831 and 0.32824218\n",
            "Processing Epoch 711\n",
            "Training acc and loss are 0.92904 and 0.20683877\n",
            "Val acc and loss are 0.8831 and 0.32842684\n",
            "Processing Epoch 712\n",
            "Training acc and loss are 0.9288 and 0.20674579\n",
            "Val acc and loss are 0.883 and 0.32847416\n",
            "Processing Epoch 713\n",
            "Training acc and loss are 0.92902 and 0.2065652\n",
            "Val acc and loss are 0.8831 and 0.32837805\n",
            "Processing Epoch 714\n",
            "Training acc and loss are 0.92914 and 0.20632723\n",
            "Val acc and loss are 0.8833 and 0.32814375\n",
            "Processing Epoch 715\n",
            "Training acc and loss are 0.9294 and 0.20615596\n",
            "Val acc and loss are 0.8834 and 0.32799205\n",
            "Processing Epoch 716\n",
            "Training acc and loss are 0.92954 and 0.2060379\n",
            "Val acc and loss are 0.8829 and 0.3280607\n",
            "Processing Epoch 717\n",
            "Training acc and loss are 0.92958 and 0.20588398\n",
            "Val acc and loss are 0.8836 and 0.32816556\n",
            "Processing Epoch 718\n",
            "Training acc and loss are 0.92946 and 0.20580658\n",
            "Val acc and loss are 0.8838 and 0.32836962\n",
            "Processing Epoch 719\n",
            "Training acc and loss are 0.92946 and 0.20573507\n",
            "Val acc and loss are 0.8835 and 0.32857075\n",
            "Processing Epoch 720\n",
            "Training acc and loss are 0.9297 and 0.20562027\n",
            "Val acc and loss are 0.883 and 0.32869422\n",
            "Processing Epoch 721\n",
            "Training acc and loss are 0.9299 and 0.20552467\n",
            "Val acc and loss are 0.8828 and 0.32872638\n",
            "Processing Epoch 722\n",
            "Training acc and loss are 0.92974 and 0.20539254\n",
            "Val acc and loss are 0.8827 and 0.3286618\n",
            "Processing Epoch 723\n",
            "Training acc and loss are 0.92974 and 0.20521039\n",
            "Val acc and loss are 0.8831 and 0.32851422\n",
            "Processing Epoch 724\n",
            "Training acc and loss are 0.92984 and 0.2050621\n",
            "Val acc and loss are 0.8834 and 0.32839972\n",
            "Processing Epoch 725\n",
            "Training acc and loss are 0.92988 and 0.20493187\n",
            "Val acc and loss are 0.8842 and 0.32839945\n",
            "Processing Epoch 726\n",
            "Training acc and loss are 0.92974 and 0.20480533\n",
            "Val acc and loss are 0.8841 and 0.32838285\n",
            "Processing Epoch 727\n",
            "Training acc and loss are 0.93002 and 0.20458758\n",
            "Val acc and loss are 0.884 and 0.32825056\n",
            "Processing Epoch 728\n",
            "Training acc and loss are 0.9301 and 0.20434\n",
            "Val acc and loss are 0.8838 and 0.32811967\n",
            "Processing Epoch 729\n",
            "Training acc and loss are 0.93026 and 0.20411094\n",
            "Val acc and loss are 0.8832 and 0.32802492\n",
            "Processing Epoch 730\n",
            "Training acc and loss are 0.93052 and 0.20392291\n",
            "Val acc and loss are 0.8835 and 0.32804492\n",
            "Processing Epoch 731\n",
            "Training acc and loss are 0.93026 and 0.2037971\n",
            "Val acc and loss are 0.8836 and 0.32816708\n",
            "Processing Epoch 732\n",
            "Training acc and loss are 0.9302 and 0.20373265\n",
            "Val acc and loss are 0.8834 and 0.3282145\n",
            "Processing Epoch 733\n",
            "Training acc and loss are 0.93046 and 0.20355965\n",
            "Val acc and loss are 0.8838 and 0.32799205\n",
            "Processing Epoch 734\n",
            "Training acc and loss are 0.93034 and 0.20334813\n",
            "Val acc and loss are 0.8838 and 0.32767907\n",
            "Processing Epoch 735\n",
            "Training acc and loss are 0.9304 and 0.20327078\n",
            "Val acc and loss are 0.884 and 0.32765564\n",
            "Processing Epoch 736\n",
            "Training acc and loss are 0.93078 and 0.20315795\n",
            "Val acc and loss are 0.8834 and 0.3278519\n",
            "Processing Epoch 737\n",
            "Training acc and loss are 0.9307 and 0.20304719\n",
            "Val acc and loss are 0.8833 and 0.32807183\n",
            "Processing Epoch 738\n",
            "Training acc and loss are 0.93078 and 0.20294598\n",
            "Val acc and loss are 0.883 and 0.32823813\n",
            "Processing Epoch 739\n",
            "Training acc and loss are 0.93094 and 0.20274684\n",
            "Val acc and loss are 0.8825 and 0.32816255\n",
            "Processing Epoch 740\n",
            "Training acc and loss are 0.9311 and 0.20255835\n",
            "Val acc and loss are 0.8824 and 0.32807404\n",
            "Processing Epoch 741\n",
            "Training acc and loss are 0.9311 and 0.20239043\n",
            "Val acc and loss are 0.8834 and 0.32799536\n",
            "Processing Epoch 742\n",
            "Training acc and loss are 0.93106 and 0.20218667\n",
            "Val acc and loss are 0.8834 and 0.32802647\n",
            "Processing Epoch 743\n",
            "Training acc and loss are 0.93104 and 0.20197424\n",
            "Val acc and loss are 0.8829 and 0.3281923\n",
            "Processing Epoch 744\n",
            "Training acc and loss are 0.93122 and 0.20185664\n",
            "Val acc and loss are 0.8825 and 0.3283435\n",
            "Processing Epoch 745\n",
            "Training acc and loss are 0.93128 and 0.20170473\n",
            "Val acc and loss are 0.883 and 0.32831484\n",
            "Processing Epoch 746\n",
            "Training acc and loss are 0.93146 and 0.2015468\n",
            "Val acc and loss are 0.8831 and 0.3281449\n",
            "Processing Epoch 747\n",
            "Training acc and loss are 0.93138 and 0.20145355\n",
            "Val acc and loss are 0.8831 and 0.3281794\n",
            "Processing Epoch 748\n",
            "Training acc and loss are 0.9314 and 0.20124348\n",
            "Val acc and loss are 0.8836 and 0.32806972\n",
            "Processing Epoch 749\n",
            "Training acc and loss are 0.9313 and 0.20100008\n",
            "Val acc and loss are 0.8841 and 0.3278489\n",
            "Processing Epoch 750\n",
            "Training acc and loss are 0.9316 and 0.20083155\n",
            "Val acc and loss are 0.8836 and 0.32768756\n",
            "Processing Epoch 751\n",
            "Training acc and loss are 0.93168 and 0.2007086\n",
            "Val acc and loss are 0.8835 and 0.32769457\n",
            "Processing Epoch 752\n",
            "Training acc and loss are 0.93176 and 0.20070244\n",
            "Val acc and loss are 0.8834 and 0.32788613\n",
            "Processing Epoch 753\n",
            "Training acc and loss are 0.9318 and 0.20072192\n",
            "Val acc and loss are 0.8832 and 0.32808188\n",
            "Processing Epoch 754\n",
            "Training acc and loss are 0.9317 and 0.20051266\n",
            "Val acc and loss are 0.883 and 0.32799706\n",
            "Processing Epoch 755\n",
            "Training acc and loss are 0.93174 and 0.20027795\n",
            "Val acc and loss are 0.8827 and 0.32786778\n",
            "Processing Epoch 756\n",
            "Training acc and loss are 0.93198 and 0.20009309\n",
            "Val acc and loss are 0.883 and 0.32770273\n",
            "Processing Epoch 757\n",
            "Training acc and loss are 0.93168 and 0.1999592\n",
            "Val acc and loss are 0.8838 and 0.32769\n",
            "Processing Epoch 758\n",
            "Training acc and loss are 0.93158 and 0.19987833\n",
            "Val acc and loss are 0.8838 and 0.32777485\n",
            "Processing Epoch 759\n",
            "Training acc and loss are 0.93174 and 0.19978066\n",
            "Val acc and loss are 0.8838 and 0.32785165\n",
            "Processing Epoch 760\n",
            "Training acc and loss are 0.93204 and 0.1995779\n",
            "Val acc and loss are 0.8841 and 0.3277232\n",
            "Processing Epoch 761\n",
            "Training acc and loss are 0.93194 and 0.19934191\n",
            "Val acc and loss are 0.8838 and 0.3275376\n",
            "Processing Epoch 762\n",
            "Training acc and loss are 0.932 and 0.19924428\n",
            "Val acc and loss are 0.8836 and 0.32751882\n",
            "Processing Epoch 763\n",
            "Training acc and loss are 0.93206 and 0.19917454\n",
            "Val acc and loss are 0.8835 and 0.32755902\n",
            "Processing Epoch 764\n",
            "Training acc and loss are 0.93206 and 0.19915351\n",
            "Val acc and loss are 0.8831 and 0.32766488\n",
            "Processing Epoch 765\n",
            "Training acc and loss are 0.93216 and 0.19904633\n",
            "Val acc and loss are 0.8832 and 0.32770082\n",
            "Processing Epoch 766\n",
            "Training acc and loss are 0.9322 and 0.19880286\n",
            "Val acc and loss are 0.8833 and 0.32760236\n",
            "Processing Epoch 767\n",
            "Training acc and loss are 0.93214 and 0.19860248\n",
            "Val acc and loss are 0.8832 and 0.32760033\n",
            "Processing Epoch 768\n",
            "Training acc and loss are 0.93206 and 0.1984188\n",
            "Val acc and loss are 0.8829 and 0.32768622\n",
            "Processing Epoch 769\n",
            "Training acc and loss are 0.93214 and 0.19831064\n",
            "Val acc and loss are 0.8832 and 0.32783702\n",
            "Processing Epoch 770\n",
            "Training acc and loss are 0.9323 and 0.19825065\n",
            "Val acc and loss are 0.8832 and 0.3280232\n",
            "Processing Epoch 771\n",
            "Training acc and loss are 0.93244 and 0.19807023\n",
            "Val acc and loss are 0.8836 and 0.32808378\n",
            "Processing Epoch 772\n",
            "Training acc and loss are 0.93244 and 0.19789508\n",
            "Val acc and loss are 0.8839 and 0.3280374\n",
            "Processing Epoch 773\n",
            "Training acc and loss are 0.93236 and 0.19768292\n",
            "Val acc and loss are 0.8844 and 0.32790497\n",
            "Processing Epoch 774\n",
            "Training acc and loss are 0.9325 and 0.19746992\n",
            "Val acc and loss are 0.8842 and 0.32780972\n",
            "Processing Epoch 775\n",
            "Training acc and loss are 0.93272 and 0.19727156\n",
            "Val acc and loss are 0.8846 and 0.32773468\n",
            "Processing Epoch 776\n",
            "Training acc and loss are 0.93306 and 0.19715576\n",
            "Val acc and loss are 0.8842 and 0.3277774\n",
            "Processing Epoch 777\n",
            "Training acc and loss are 0.93292 and 0.1969867\n",
            "Val acc and loss are 0.8841 and 0.32773578\n",
            "Processing Epoch 778\n",
            "Training acc and loss are 0.93312 and 0.19680047\n",
            "Val acc and loss are 0.8843 and 0.32779628\n",
            "Processing Epoch 779\n",
            "Training acc and loss are 0.93328 and 0.19664246\n",
            "Val acc and loss are 0.8839 and 0.32784715\n",
            "Processing Epoch 780\n",
            "Training acc and loss are 0.93328 and 0.19655792\n",
            "Val acc and loss are 0.8836 and 0.3278587\n",
            "Processing Epoch 781\n",
            "Training acc and loss are 0.9333 and 0.19643566\n",
            "Val acc and loss are 0.8834 and 0.32779682\n",
            "Processing Epoch 782\n",
            "Training acc and loss are 0.93318 and 0.19628252\n",
            "Val acc and loss are 0.8839 and 0.3276295\n",
            "Processing Epoch 783\n",
            "Training acc and loss are 0.93282 and 0.19615456\n",
            "Val acc and loss are 0.8838 and 0.32740328\n",
            "Processing Epoch 784\n",
            "Training acc and loss are 0.93312 and 0.19609874\n",
            "Val acc and loss are 0.8844 and 0.3273244\n",
            "Processing Epoch 785\n",
            "Training acc and loss are 0.93308 and 0.19605207\n",
            "Val acc and loss are 0.8841 and 0.32734632\n",
            "Processing Epoch 786\n",
            "Training acc and loss are 0.93322 and 0.19607933\n",
            "Val acc and loss are 0.8834 and 0.32751167\n",
            "Processing Epoch 787\n",
            "Training acc and loss are 0.93322 and 0.19593957\n",
            "Val acc and loss are 0.8838 and 0.32763886\n",
            "Processing Epoch 788\n",
            "Training acc and loss are 0.93336 and 0.19564347\n",
            "Val acc and loss are 0.8836 and 0.3275782\n",
            "Processing Epoch 789\n",
            "Training acc and loss are 0.93348 and 0.19548978\n",
            "Val acc and loss are 0.8836 and 0.32767227\n",
            "Processing Epoch 790\n",
            "Training acc and loss are 0.93332 and 0.19538999\n",
            "Val acc and loss are 0.8841 and 0.32778004\n",
            "Processing Epoch 791\n",
            "Training acc and loss are 0.93322 and 0.19534907\n",
            "Val acc and loss are 0.8841 and 0.32784826\n",
            "Processing Epoch 792\n",
            "Training acc and loss are 0.93328 and 0.19521788\n",
            "Val acc and loss are 0.8839 and 0.327713\n",
            "Processing Epoch 793\n",
            "Training acc and loss are 0.93346 and 0.19494727\n",
            "Val acc and loss are 0.8839 and 0.32746658\n",
            "Processing Epoch 794\n",
            "Training acc and loss are 0.93388 and 0.19472219\n",
            "Val acc and loss are 0.8834 and 0.32738015\n",
            "Processing Epoch 795\n",
            "Training acc and loss are 0.9338 and 0.19458318\n",
            "Val acc and loss are 0.8835 and 0.32740685\n",
            "Processing Epoch 796\n",
            "Training acc and loss are 0.93394 and 0.19447567\n",
            "Val acc and loss are 0.8836 and 0.32749805\n",
            "Processing Epoch 797\n",
            "Training acc and loss are 0.93392 and 0.19430129\n",
            "Val acc and loss are 0.8836 and 0.32761192\n",
            "Processing Epoch 798\n",
            "Training acc and loss are 0.93388 and 0.19405828\n",
            "Val acc and loss are 0.8838 and 0.3275501\n",
            "Processing Epoch 799\n",
            "Training acc and loss are 0.93386 and 0.19393145\n",
            "Val acc and loss are 0.8842 and 0.32755604\n",
            "Processing Epoch 800\n",
            "Training acc and loss are 0.93384 and 0.19386016\n",
            "Val acc and loss are 0.884 and 0.32762086\n",
            "Processing Epoch 801\n",
            "Training acc and loss are 0.93406 and 0.19394605\n",
            "Val acc and loss are 0.8839 and 0.32786235\n",
            "Processing Epoch 802\n",
            "Training acc and loss are 0.93402 and 0.19377172\n",
            "Val acc and loss are 0.884 and 0.32777578\n",
            "Processing Epoch 803\n",
            "Training acc and loss are 0.93416 and 0.19348253\n",
            "Val acc and loss are 0.8842 and 0.32752174\n",
            "Processing Epoch 804\n",
            "Training acc and loss are 0.9339 and 0.19331545\n",
            "Val acc and loss are 0.885 and 0.32740748\n",
            "Processing Epoch 805\n",
            "Training acc and loss are 0.9342 and 0.19310355\n",
            "Val acc and loss are 0.8844 and 0.32731244\n",
            "Processing Epoch 806\n",
            "Training acc and loss are 0.93434 and 0.19310656\n",
            "Val acc and loss are 0.8839 and 0.32748014\n",
            "Processing Epoch 807\n",
            "Training acc and loss are 0.93452 and 0.19301338\n",
            "Val acc and loss are 0.8838 and 0.3276072\n",
            "Processing Epoch 808\n",
            "Training acc and loss are 0.93442 and 0.19284229\n",
            "Val acc and loss are 0.8841 and 0.32763922\n",
            "Processing Epoch 809\n",
            "Training acc and loss are 0.9344 and 0.19265\n",
            "Val acc and loss are 0.884 and 0.32749677\n",
            "Processing Epoch 810\n",
            "Training acc and loss are 0.93446 and 0.19250387\n",
            "Val acc and loss are 0.884 and 0.3274146\n",
            "Processing Epoch 811\n",
            "Training acc and loss are 0.93442 and 0.19232386\n",
            "Val acc and loss are 0.8836 and 0.32725155\n",
            "Processing Epoch 812\n",
            "Training acc and loss are 0.93452 and 0.19216627\n",
            "Val acc and loss are 0.8837 and 0.32727885\n",
            "Processing Epoch 813\n",
            "Training acc and loss are 0.93476 and 0.19200511\n",
            "Val acc and loss are 0.8839 and 0.32744235\n",
            "Processing Epoch 814\n",
            "Training acc and loss are 0.93474 and 0.19182262\n",
            "Val acc and loss are 0.884 and 0.32760963\n",
            "Processing Epoch 815\n",
            "Training acc and loss are 0.93474 and 0.19163258\n",
            "Val acc and loss are 0.8841 and 0.3275297\n",
            "Processing Epoch 816\n",
            "Training acc and loss are 0.93498 and 0.19142374\n",
            "Val acc and loss are 0.884 and 0.32722136\n",
            "Processing Epoch 817\n",
            "Training acc and loss are 0.93512 and 0.19130488\n",
            "Val acc and loss are 0.884 and 0.32707128\n",
            "Processing Epoch 818\n",
            "Training acc and loss are 0.9351 and 0.19129689\n",
            "Val acc and loss are 0.884 and 0.32697335\n",
            "Processing Epoch 819\n",
            "Training acc and loss are 0.93516 and 0.19119059\n",
            "Val acc and loss are 0.8841 and 0.32689214\n",
            "Processing Epoch 820\n",
            "Training acc and loss are 0.9352 and 0.19107568\n",
            "Val acc and loss are 0.8843 and 0.3268679\n",
            "Processing Epoch 821\n",
            "Training acc and loss are 0.93498 and 0.19097723\n",
            "Val acc and loss are 0.8844 and 0.32699504\n",
            "Processing Epoch 822\n",
            "Training acc and loss are 0.93512 and 0.19076441\n",
            "Val acc and loss are 0.8841 and 0.32705495\n",
            "Processing Epoch 823\n",
            "Training acc and loss are 0.9351 and 0.190603\n",
            "Val acc and loss are 0.884 and 0.3271546\n",
            "Processing Epoch 824\n",
            "Training acc and loss are 0.93522 and 0.19046283\n",
            "Val acc and loss are 0.8837 and 0.3271622\n",
            "Processing Epoch 825\n",
            "Training acc and loss are 0.93542 and 0.19025587\n",
            "Val acc and loss are 0.8837 and 0.32712084\n",
            "Processing Epoch 826\n",
            "Training acc and loss are 0.93528 and 0.19015124\n",
            "Val acc and loss are 0.8838 and 0.32718414\n",
            "Processing Epoch 827\n",
            "Training acc and loss are 0.93536 and 0.19004774\n",
            "Val acc and loss are 0.8838 and 0.327145\n",
            "Processing Epoch 828\n",
            "Training acc and loss are 0.93544 and 0.18989313\n",
            "Val acc and loss are 0.884 and 0.32712567\n",
            "Processing Epoch 829\n",
            "Training acc and loss are 0.93554 and 0.18973523\n",
            "Val acc and loss are 0.8841 and 0.3271296\n",
            "Processing Epoch 830\n",
            "Training acc and loss are 0.93564 and 0.18963055\n",
            "Val acc and loss are 0.8845 and 0.32719392\n",
            "Processing Epoch 831\n",
            "Training acc and loss are 0.93572 and 0.18958049\n",
            "Val acc and loss are 0.8848 and 0.327308\n",
            "Processing Epoch 832\n",
            "Training acc and loss are 0.93556 and 0.1895236\n",
            "Val acc and loss are 0.8848 and 0.3275001\n",
            "Processing Epoch 833\n",
            "Training acc and loss are 0.93566 and 0.18945551\n",
            "Val acc and loss are 0.8845 and 0.32772803\n",
            "Processing Epoch 834\n",
            "Training acc and loss are 0.93556 and 0.18938394\n",
            "Val acc and loss are 0.8843 and 0.3278885\n",
            "Processing Epoch 835\n",
            "Training acc and loss are 0.93568 and 0.18921702\n",
            "Val acc and loss are 0.8843 and 0.3279822\n",
            "Processing Epoch 836\n",
            "Training acc and loss are 0.9355 and 0.18907107\n",
            "Val acc and loss are 0.884 and 0.32800913\n",
            "Processing Epoch 837\n",
            "Training acc and loss are 0.9355 and 0.18885128\n",
            "Val acc and loss are 0.8837 and 0.32786697\n",
            "Processing Epoch 838\n",
            "Training acc and loss are 0.93568 and 0.18865877\n",
            "Val acc and loss are 0.8844 and 0.32775062\n",
            "Processing Epoch 839\n",
            "Training acc and loss are 0.93546 and 0.18845014\n",
            "Val acc and loss are 0.8844 and 0.32764184\n",
            "Processing Epoch 840\n",
            "Training acc and loss are 0.93594 and 0.18830697\n",
            "Val acc and loss are 0.8846 and 0.32764477\n",
            "Processing Epoch 841\n",
            "Training acc and loss are 0.93606 and 0.18818586\n",
            "Val acc and loss are 0.8838 and 0.3275352\n",
            "Processing Epoch 842\n",
            "Training acc and loss are 0.93616 and 0.18802652\n",
            "Val acc and loss are 0.8842 and 0.32731557\n",
            "Processing Epoch 843\n",
            "Training acc and loss are 0.93614 and 0.1878154\n",
            "Val acc and loss are 0.8843 and 0.32709816\n",
            "Processing Epoch 844\n",
            "Training acc and loss are 0.93596 and 0.18777388\n",
            "Val acc and loss are 0.8844 and 0.3270968\n",
            "Processing Epoch 845\n",
            "Training acc and loss are 0.93588 and 0.18779664\n",
            "Val acc and loss are 0.8846 and 0.32718682\n",
            "Processing Epoch 846\n",
            "Training acc and loss are 0.93582 and 0.18767022\n",
            "Val acc and loss are 0.8845 and 0.32718226\n",
            "Processing Epoch 847\n",
            "Training acc and loss are 0.93608 and 0.18753546\n",
            "Val acc and loss are 0.8843 and 0.32720265\n",
            "Processing Epoch 848\n",
            "Training acc and loss are 0.936 and 0.1873587\n",
            "Val acc and loss are 0.8852 and 0.32722393\n",
            "Processing Epoch 849\n",
            "Training acc and loss are 0.9361 and 0.1871975\n",
            "Val acc and loss are 0.8845 and 0.32733643\n",
            "Processing Epoch 850\n",
            "Training acc and loss are 0.93624 and 0.18707715\n",
            "Val acc and loss are 0.8847 and 0.32752338\n",
            "Processing Epoch 851\n",
            "Training acc and loss are 0.93622 and 0.18699309\n",
            "Val acc and loss are 0.8837 and 0.327681\n",
            "Processing Epoch 852\n",
            "Training acc and loss are 0.93642 and 0.18691166\n",
            "Val acc and loss are 0.8839 and 0.32771295\n",
            "Processing Epoch 853\n",
            "Training acc and loss are 0.9368 and 0.18670228\n",
            "Val acc and loss are 0.8844 and 0.32744956\n",
            "Processing Epoch 854\n",
            "Training acc and loss are 0.93676 and 0.18652652\n",
            "Val acc and loss are 0.8844 and 0.32717943\n",
            "Processing Epoch 855\n",
            "Training acc and loss are 0.93672 and 0.18640192\n",
            "Val acc and loss are 0.8843 and 0.3269772\n",
            "Processing Epoch 856\n",
            "Training acc and loss are 0.93674 and 0.18625244\n",
            "Val acc and loss are 0.8845 and 0.32686597\n",
            "Processing Epoch 857\n",
            "Training acc and loss are 0.93688 and 0.18608782\n",
            "Val acc and loss are 0.8844 and 0.32693154\n",
            "Processing Epoch 858\n",
            "Training acc and loss are 0.93696 and 0.18591726\n",
            "Val acc and loss are 0.8845 and 0.32702124\n",
            "Processing Epoch 859\n",
            "Training acc and loss are 0.93708 and 0.18581906\n",
            "Val acc and loss are 0.8845 and 0.32720146\n",
            "Processing Epoch 860\n",
            "Training acc and loss are 0.93706 and 0.18569039\n",
            "Val acc and loss are 0.8842 and 0.32724503\n",
            "Processing Epoch 861\n",
            "Training acc and loss are 0.93716 and 0.18560441\n",
            "Val acc and loss are 0.8841 and 0.32712397\n",
            "Processing Epoch 862\n",
            "Training acc and loss are 0.93712 and 0.18547387\n",
            "Val acc and loss are 0.8845 and 0.32690787\n",
            "Processing Epoch 863\n",
            "Training acc and loss are 0.93706 and 0.18541348\n",
            "Val acc and loss are 0.8844 and 0.3269206\n",
            "Processing Epoch 864\n",
            "Training acc and loss are 0.9371 and 0.18523446\n",
            "Val acc and loss are 0.8849 and 0.32702228\n",
            "Processing Epoch 865\n",
            "Training acc and loss are 0.93726 and 0.18502137\n",
            "Val acc and loss are 0.8847 and 0.3272351\n",
            "Processing Epoch 866\n",
            "Training acc and loss are 0.93712 and 0.18483216\n",
            "Val acc and loss are 0.8846 and 0.32728806\n",
            "Processing Epoch 867\n",
            "Training acc and loss are 0.93742 and 0.18481585\n",
            "Val acc and loss are 0.8841 and 0.32740578\n",
            "Processing Epoch 868\n",
            "Training acc and loss are 0.93744 and 0.18464398\n",
            "Val acc and loss are 0.8839 and 0.32732368\n",
            "Processing Epoch 869\n",
            "Training acc and loss are 0.93746 and 0.1845151\n",
            "Val acc and loss are 0.8848 and 0.32737386\n",
            "Processing Epoch 870\n",
            "Training acc and loss are 0.9374 and 0.18447672\n",
            "Val acc and loss are 0.8843 and 0.32751426\n",
            "Processing Epoch 871\n",
            "Training acc and loss are 0.93736 and 0.18438207\n",
            "Val acc and loss are 0.8845 and 0.32757223\n",
            "Processing Epoch 872\n",
            "Training acc and loss are 0.93734 and 0.18421863\n",
            "Val acc and loss are 0.884 and 0.32740912\n",
            "Processing Epoch 873\n",
            "Training acc and loss are 0.93734 and 0.18415275\n",
            "Val acc and loss are 0.8838 and 0.3273861\n",
            "Processing Epoch 874\n",
            "Training acc and loss are 0.93742 and 0.18400805\n",
            "Val acc and loss are 0.8838 and 0.32738826\n",
            "Processing Epoch 875\n",
            "Training acc and loss are 0.9373 and 0.183902\n",
            "Val acc and loss are 0.8838 and 0.32754532\n",
            "Processing Epoch 876\n",
            "Training acc and loss are 0.93758 and 0.18377428\n",
            "Val acc and loss are 0.884 and 0.32761025\n",
            "Processing Epoch 877\n",
            "Training acc and loss are 0.93772 and 0.18346357\n",
            "Val acc and loss are 0.8838 and 0.3273264\n",
            "Processing Epoch 878\n",
            "Training acc and loss are 0.9378 and 0.18317719\n",
            "Val acc and loss are 0.8842 and 0.32702643\n",
            "Processing Epoch 879\n",
            "Training acc and loss are 0.9381 and 0.18303515\n",
            "Val acc and loss are 0.8843 and 0.32701075\n",
            "Processing Epoch 880\n",
            "Training acc and loss are 0.93826 and 0.18293813\n",
            "Val acc and loss are 0.8837 and 0.327023\n",
            "Processing Epoch 881\n",
            "Training acc and loss are 0.93814 and 0.18287899\n",
            "Val acc and loss are 0.8837 and 0.32711694\n",
            "Processing Epoch 882\n",
            "Training acc and loss are 0.93806 and 0.1828203\n",
            "Val acc and loss are 0.8841 and 0.32727802\n",
            "Processing Epoch 883\n",
            "Training acc and loss are 0.93808 and 0.18273845\n",
            "Val acc and loss are 0.8841 and 0.3273889\n",
            "Processing Epoch 884\n",
            "Training acc and loss are 0.938 and 0.18247671\n",
            "Val acc and loss are 0.8841 and 0.32717857\n",
            "Processing Epoch 885\n",
            "Training acc and loss are 0.93832 and 0.18235855\n",
            "Val acc and loss are 0.8835 and 0.32701966\n",
            "Processing Epoch 886\n",
            "Training acc and loss are 0.9384 and 0.18225859\n",
            "Val acc and loss are 0.8833 and 0.32682362\n",
            "Processing Epoch 887\n",
            "Training acc and loss are 0.93832 and 0.18219537\n",
            "Val acc and loss are 0.884 and 0.32693705\n",
            "Processing Epoch 888\n",
            "Training acc and loss are 0.93822 and 0.18222106\n",
            "Val acc and loss are 0.8844 and 0.32737222\n",
            "Processing Epoch 889\n",
            "Training acc and loss are 0.9383 and 0.18206364\n",
            "Val acc and loss are 0.8841 and 0.32759145\n",
            "Processing Epoch 890\n",
            "Training acc and loss are 0.93842 and 0.18188356\n",
            "Val acc and loss are 0.8845 and 0.32754126\n",
            "Processing Epoch 891\n",
            "Training acc and loss are 0.93828 and 0.1817174\n",
            "Val acc and loss are 0.8843 and 0.3273899\n",
            "Processing Epoch 892\n",
            "Training acc and loss are 0.93844 and 0.18164968\n",
            "Val acc and loss are 0.8843 and 0.32733768\n",
            "Processing Epoch 893\n",
            "Training acc and loss are 0.93852 and 0.18154162\n",
            "Val acc and loss are 0.8843 and 0.32740358\n",
            "Processing Epoch 894\n",
            "Training acc and loss are 0.93844 and 0.1815396\n",
            "Val acc and loss are 0.8843 and 0.3276371\n",
            "Processing Epoch 895\n",
            "Training acc and loss are 0.93842 and 0.18145266\n",
            "Val acc and loss are 0.8837 and 0.32785887\n",
            "Processing Epoch 896\n",
            "Training acc and loss are 0.93836 and 0.18124992\n",
            "Val acc and loss are 0.8835 and 0.32784912\n",
            "Processing Epoch 897\n",
            "Training acc and loss are 0.93846 and 0.18117543\n",
            "Val acc and loss are 0.8836 and 0.32774135\n",
            "Processing Epoch 898\n",
            "Training acc and loss are 0.93836 and 0.18107378\n",
            "Val acc and loss are 0.8841 and 0.32742882\n",
            "Processing Epoch 899\n",
            "Training acc and loss are 0.9384 and 0.18094788\n",
            "Val acc and loss are 0.8844 and 0.32729244\n",
            "Processing Epoch 900\n",
            "Training acc and loss are 0.93836 and 0.18086118\n",
            "Val acc and loss are 0.885 and 0.32744542\n",
            "Processing Epoch 901\n",
            "Training acc and loss are 0.93874 and 0.18065223\n",
            "Val acc and loss are 0.8849 and 0.32770142\n",
            "Processing Epoch 902\n",
            "Training acc and loss are 0.93888 and 0.18042219\n",
            "Val acc and loss are 0.8846 and 0.32788175\n",
            "Processing Epoch 903\n",
            "Training acc and loss are 0.93906 and 0.18025133\n",
            "Val acc and loss are 0.8843 and 0.3279837\n",
            "Processing Epoch 904\n",
            "Training acc and loss are 0.9389 and 0.18008007\n",
            "Val acc and loss are 0.8837 and 0.32785258\n",
            "Processing Epoch 905\n",
            "Training acc and loss are 0.93902 and 0.17980418\n",
            "Val acc and loss are 0.884 and 0.3274842\n",
            "Processing Epoch 906\n",
            "Training acc and loss are 0.9391 and 0.17972125\n",
            "Val acc and loss are 0.8838 and 0.3273509\n",
            "Processing Epoch 907\n",
            "Training acc and loss are 0.9389 and 0.17967246\n",
            "Val acc and loss are 0.8837 and 0.32746133\n",
            "Processing Epoch 908\n",
            "Training acc and loss are 0.9391 and 0.17963822\n",
            "Val acc and loss are 0.8838 and 0.3277233\n",
            "Processing Epoch 909\n",
            "Training acc and loss are 0.9391 and 0.17956088\n",
            "Val acc and loss are 0.8835 and 0.32773927\n",
            "Processing Epoch 910\n",
            "Training acc and loss are 0.93908 and 0.17939629\n",
            "Val acc and loss are 0.8839 and 0.32757252\n",
            "Processing Epoch 911\n",
            "Training acc and loss are 0.939 and 0.1791566\n",
            "Val acc and loss are 0.8835 and 0.32727724\n",
            "Processing Epoch 912\n",
            "Training acc and loss are 0.9393 and 0.17903908\n",
            "Val acc and loss are 0.8836 and 0.32717675\n",
            "Processing Epoch 913\n",
            "Training acc and loss are 0.93922 and 0.17892094\n",
            "Val acc and loss are 0.8833 and 0.32733858\n",
            "Processing Epoch 914\n",
            "Training acc and loss are 0.9393 and 0.17893137\n",
            "Val acc and loss are 0.8841 and 0.32769746\n",
            "Processing Epoch 915\n",
            "Training acc and loss are 0.93936 and 0.17890453\n",
            "Val acc and loss are 0.8835 and 0.3278825\n",
            "Processing Epoch 916\n",
            "Training acc and loss are 0.93936 and 0.17864512\n",
            "Val acc and loss are 0.8829 and 0.32770157\n",
            "Processing Epoch 917\n",
            "Training acc and loss are 0.9394 and 0.17843342\n",
            "Val acc and loss are 0.8835 and 0.32749483\n",
            "Processing Epoch 918\n",
            "Training acc and loss are 0.93958 and 0.17837004\n",
            "Val acc and loss are 0.8835 and 0.32751015\n",
            "Processing Epoch 919\n",
            "Training acc and loss are 0.93976 and 0.17834653\n",
            "Val acc and loss are 0.8837 and 0.32762086\n",
            "Processing Epoch 920\n",
            "Training acc and loss are 0.9398 and 0.17833875\n",
            "Val acc and loss are 0.883 and 0.32771686\n",
            "Processing Epoch 921\n",
            "Training acc and loss are 0.93976 and 0.17817445\n",
            "Val acc and loss are 0.8835 and 0.32765406\n",
            "Processing Epoch 922\n",
            "Training acc and loss are 0.9397 and 0.1778362\n",
            "Val acc and loss are 0.8844 and 0.32739154\n",
            "Processing Epoch 923\n",
            "Training acc and loss are 0.93972 and 0.17764011\n",
            "Val acc and loss are 0.8842 and 0.32719767\n",
            "Processing Epoch 924\n",
            "Training acc and loss are 0.9396 and 0.17748864\n",
            "Val acc and loss are 0.8839 and 0.32710844\n",
            "Processing Epoch 925\n",
            "Training acc and loss are 0.93964 and 0.17742236\n",
            "Val acc and loss are 0.8839 and 0.32722133\n",
            "Processing Epoch 926\n",
            "Training acc and loss are 0.9399 and 0.17725874\n",
            "Val acc and loss are 0.8839 and 0.32721528\n",
            "Processing Epoch 927\n",
            "Training acc and loss are 0.93996 and 0.17710628\n",
            "Val acc and loss are 0.8844 and 0.32712635\n",
            "Processing Epoch 928\n",
            "Training acc and loss are 0.93992 and 0.17703816\n",
            "Val acc and loss are 0.8842 and 0.3271713\n",
            "Processing Epoch 929\n",
            "Training acc and loss are 0.93978 and 0.17702883\n",
            "Val acc and loss are 0.8838 and 0.32736185\n",
            "Processing Epoch 930\n",
            "Training acc and loss are 0.94014 and 0.17699383\n",
            "Val acc and loss are 0.8841 and 0.32752132\n",
            "Processing Epoch 931\n",
            "Training acc and loss are 0.94014 and 0.17680226\n",
            "Val acc and loss are 0.8832 and 0.32759115\n",
            "Processing Epoch 932\n",
            "Training acc and loss are 0.93996 and 0.17655134\n",
            "Val acc and loss are 0.8835 and 0.32746598\n",
            "Processing Epoch 933\n",
            "Training acc and loss are 0.94008 and 0.17637861\n",
            "Val acc and loss are 0.8834 and 0.32749635\n",
            "Processing Epoch 934\n",
            "Training acc and loss are 0.94026 and 0.17622691\n",
            "Val acc and loss are 0.8836 and 0.3274758\n",
            "Processing Epoch 935\n",
            "Training acc and loss are 0.9405 and 0.1760793\n",
            "Val acc and loss are 0.8837 and 0.32741612\n",
            "Processing Epoch 936\n",
            "Training acc and loss are 0.94044 and 0.1759511\n",
            "Val acc and loss are 0.8837 and 0.32731014\n",
            "Processing Epoch 937\n",
            "Training acc and loss are 0.94054 and 0.17581789\n",
            "Val acc and loss are 0.884 and 0.3271881\n",
            "Processing Epoch 938\n",
            "Training acc and loss are 0.94072 and 0.17564216\n",
            "Val acc and loss are 0.8847 and 0.32711306\n",
            "Processing Epoch 939\n",
            "Training acc and loss are 0.94072 and 0.1754857\n",
            "Val acc and loss are 0.8849 and 0.32712176\n",
            "Processing Epoch 940\n",
            "Training acc and loss are 0.9407 and 0.17544667\n",
            "Val acc and loss are 0.8847 and 0.32724613\n",
            "Processing Epoch 941\n",
            "Training acc and loss are 0.94078 and 0.17534135\n",
            "Val acc and loss are 0.8842 and 0.32735363\n",
            "Processing Epoch 942\n",
            "Training acc and loss are 0.94074 and 0.17520785\n",
            "Val acc and loss are 0.8841 and 0.32748717\n",
            "Processing Epoch 943\n",
            "Training acc and loss are 0.94076 and 0.17512484\n",
            "Val acc and loss are 0.884 and 0.32757577\n",
            "Processing Epoch 944\n",
            "Training acc and loss are 0.94074 and 0.17511274\n",
            "Val acc and loss are 0.8839 and 0.32765707\n",
            "Processing Epoch 945\n",
            "Training acc and loss are 0.94058 and 0.1751029\n",
            "Val acc and loss are 0.8847 and 0.3276663\n",
            "Processing Epoch 946\n",
            "Training acc and loss are 0.94066 and 0.17511113\n",
            "Val acc and loss are 0.8835 and 0.32758853\n",
            "Processing Epoch 947\n",
            "Training acc and loss are 0.94092 and 0.1749956\n",
            "Val acc and loss are 0.8836 and 0.32751027\n",
            "Processing Epoch 948\n",
            "Training acc and loss are 0.94104 and 0.17477563\n",
            "Val acc and loss are 0.8843 and 0.32753655\n",
            "Processing Epoch 949\n",
            "Training acc and loss are 0.94096 and 0.17467782\n",
            "Val acc and loss are 0.8843 and 0.32788688\n",
            "Processing Epoch 950\n",
            "Training acc and loss are 0.94096 and 0.17452945\n",
            "Val acc and loss are 0.8843 and 0.32804716\n",
            "Processing Epoch 951\n",
            "Training acc and loss are 0.9411 and 0.17432968\n",
            "Val acc and loss are 0.8833 and 0.328019\n",
            "Processing Epoch 952\n",
            "Training acc and loss are 0.94124 and 0.17417634\n",
            "Val acc and loss are 0.8837 and 0.32792518\n",
            "Processing Epoch 953\n",
            "Training acc and loss are 0.94134 and 0.17400324\n",
            "Val acc and loss are 0.8834 and 0.32778507\n",
            "Processing Epoch 954\n",
            "Training acc and loss are 0.94144 and 0.17393683\n",
            "Val acc and loss are 0.8838 and 0.32779872\n",
            "Processing Epoch 955\n",
            "Training acc and loss are 0.9414 and 0.1738605\n",
            "Val acc and loss are 0.8833 and 0.32782826\n",
            "Processing Epoch 956\n",
            "Training acc and loss are 0.94164 and 0.17364334\n",
            "Val acc and loss are 0.8835 and 0.32779908\n",
            "Processing Epoch 957\n",
            "Training acc and loss are 0.9417 and 0.17346957\n",
            "Val acc and loss are 0.8835 and 0.32781962\n",
            "Processing Epoch 958\n",
            "Training acc and loss are 0.94168 and 0.1733188\n",
            "Val acc and loss are 0.8836 and 0.32775185\n",
            "Processing Epoch 959\n",
            "Training acc and loss are 0.94158 and 0.17319648\n",
            "Val acc and loss are 0.8834 and 0.32757753\n",
            "Processing Epoch 960\n",
            "Training acc and loss are 0.9415 and 0.17313713\n",
            "Val acc and loss are 0.8837 and 0.32751036\n",
            "Processing Epoch 961\n",
            "Training acc and loss are 0.9414 and 0.17315494\n",
            "Val acc and loss are 0.8843 and 0.32765606\n",
            "Processing Epoch 962\n",
            "Training acc and loss are 0.94152 and 0.17307664\n",
            "Val acc and loss are 0.8839 and 0.32776704\n",
            "Processing Epoch 963\n",
            "Training acc and loss are 0.94152 and 0.1728182\n",
            "Val acc and loss are 0.8843 and 0.32754967\n",
            "Processing Epoch 964\n",
            "Training acc and loss are 0.94178 and 0.17255156\n",
            "Val acc and loss are 0.8844 and 0.32727626\n",
            "Processing Epoch 965\n",
            "Training acc and loss are 0.94188 and 0.1724034\n",
            "Val acc and loss are 0.8842 and 0.32720873\n",
            "Processing Epoch 966\n",
            "Training acc and loss are 0.94202 and 0.17239511\n",
            "Val acc and loss are 0.8837 and 0.32750165\n",
            "Processing Epoch 967\n",
            "Training acc and loss are 0.9421 and 0.17245796\n",
            "Val acc and loss are 0.8835 and 0.3277936\n",
            "Processing Epoch 968\n",
            "Training acc and loss are 0.94226 and 0.17227475\n",
            "Val acc and loss are 0.8836 and 0.32774925\n",
            "Processing Epoch 969\n",
            "Training acc and loss are 0.94208 and 0.17212258\n",
            "Val acc and loss are 0.884 and 0.3276627\n",
            "Processing Epoch 970\n",
            "Training acc and loss are 0.94196 and 0.17193761\n",
            "Val acc and loss are 0.8842 and 0.32757735\n",
            "Processing Epoch 971\n",
            "Training acc and loss are 0.94194 and 0.17176141\n",
            "Val acc and loss are 0.8846 and 0.3275921\n",
            "Processing Epoch 972\n",
            "Training acc and loss are 0.94174 and 0.17170002\n",
            "Val acc and loss are 0.8838 and 0.3278473\n",
            "Processing Epoch 973\n",
            "Training acc and loss are 0.94168 and 0.17161232\n",
            "Val acc and loss are 0.8837 and 0.32805705\n",
            "Processing Epoch 974\n",
            "Training acc and loss are 0.94186 and 0.17149305\n",
            "Val acc and loss are 0.8836 and 0.32801777\n",
            "Processing Epoch 975\n",
            "Training acc and loss are 0.94186 and 0.171455\n",
            "Val acc and loss are 0.8836 and 0.32804653\n",
            "Processing Epoch 976\n",
            "Training acc and loss are 0.9418 and 0.17159075\n",
            "Val acc and loss are 0.8838 and 0.32823163\n",
            "Processing Epoch 977\n",
            "Training acc and loss are 0.94206 and 0.17162608\n",
            "Val acc and loss are 0.8835 and 0.32834017\n",
            "Processing Epoch 978\n",
            "Training acc and loss are 0.94204 and 0.17135371\n",
            "Val acc and loss are 0.8839 and 0.3281984\n",
            "Processing Epoch 979\n",
            "Training acc and loss are 0.94232 and 0.17100412\n",
            "Val acc and loss are 0.8841 and 0.3280423\n",
            "Processing Epoch 980\n",
            "Training acc and loss are 0.9423 and 0.17082484\n",
            "Val acc and loss are 0.8833 and 0.3281669\n",
            "Processing Epoch 981\n",
            "Training acc and loss are 0.94218 and 0.17077301\n",
            "Val acc and loss are 0.8839 and 0.32832193\n",
            "Processing Epoch 982\n",
            "Training acc and loss are 0.9425 and 0.17067364\n",
            "Val acc and loss are 0.8837 and 0.32833344\n",
            "Processing Epoch 983\n",
            "Training acc and loss are 0.94222 and 0.17056505\n",
            "Val acc and loss are 0.8841 and 0.32820874\n",
            "Processing Epoch 984\n",
            "Training acc and loss are 0.9424 and 0.17044309\n",
            "Val acc and loss are 0.8843 and 0.32803977\n",
            "Processing Epoch 985\n",
            "Training acc and loss are 0.94254 and 0.17020902\n",
            "Val acc and loss are 0.8843 and 0.3277705\n",
            "Processing Epoch 986\n",
            "Training acc and loss are 0.94272 and 0.17008957\n",
            "Val acc and loss are 0.8841 and 0.32768753\n",
            "Processing Epoch 987\n",
            "Training acc and loss are 0.94274 and 0.17011404\n",
            "Val acc and loss are 0.8839 and 0.32799107\n",
            "Processing Epoch 988\n",
            "Training acc and loss are 0.94244 and 0.16991031\n",
            "Val acc and loss are 0.8835 and 0.32814717\n",
            "Processing Epoch 989\n",
            "Training acc and loss are 0.94248 and 0.16977996\n",
            "Val acc and loss are 0.8828 and 0.3283025\n",
            "Processing Epoch 990\n",
            "Training acc and loss are 0.9426 and 0.16972551\n",
            "Val acc and loss are 0.8837 and 0.32823834\n",
            "Processing Epoch 991\n",
            "Training acc and loss are 0.9428 and 0.16970201\n",
            "Val acc and loss are 0.8837 and 0.32805791\n",
            "Processing Epoch 992\n",
            "Training acc and loss are 0.94262 and 0.16970173\n",
            "Val acc and loss are 0.8839 and 0.32796025\n",
            "Processing Epoch 993\n",
            "Training acc and loss are 0.94238 and 0.16964547\n",
            "Val acc and loss are 0.8838 and 0.32792088\n",
            "Processing Epoch 994\n",
            "Training acc and loss are 0.94276 and 0.16939513\n",
            "Val acc and loss are 0.8846 and 0.3277625\n",
            "Processing Epoch 995\n",
            "Training acc and loss are 0.94296 and 0.16912739\n",
            "Val acc and loss are 0.8848 and 0.32770258\n",
            "Processing Epoch 996\n",
            "Training acc and loss are 0.94292 and 0.16902438\n",
            "Val acc and loss are 0.8843 and 0.32783568\n",
            "Processing Epoch 997\n",
            "Training acc and loss are 0.9427 and 0.1690013\n",
            "Val acc and loss are 0.8836 and 0.3280006\n",
            "Processing Epoch 998\n",
            "Training acc and loss are 0.94278 and 0.16905558\n",
            "Val acc and loss are 0.8843 and 0.32820055\n",
            "Processing Epoch 999\n",
            "Training acc and loss are 0.9429 and 0.16897471\n",
            "Val acc and loss are 0.8842 and 0.32820728\n",
            "Processing Epoch 1000\n",
            "Training acc and loss are 0.94304 and 0.16878477\n",
            "Val acc and loss are 0.8844 and 0.328081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3AYcgHFQ_n0"
      },
      "source": [
        "feed_dictionary = {MLP['input'] : x_train\n",
        "                  ,MLP['targets'] : y_train_oh\n",
        "                  ,MLP['isTrain']: True}\n",
        "feed_dictionary_val = {MLP['input'] : x_val,\n",
        "                       MLP['targets'] : y_val_oh,\n",
        "                      MLP['isTrain']: False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "JuEwAo6vQ_n1",
        "outputId": "51f8c858-47f4-419b-ad17-2d9dd5143bdb"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the training data\n",
        "plot_loss_acc(train_loss_arr, train_acc_arr, title='Training Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.94304 achieved at epoch: 999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1h7smAcQ_n1",
        "outputId": "a1cab324-64af-4414-c13e-793b5a71a4f8"
      },
      "source": [
        "pred_train = sess.run(MLP['predictions'], feed_dict = feed_dictionary)\n",
        "# Display the confusion matrix of the training data\n",
        "confusion_matrix(y_train, pred_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4431,   10,   45,  106,   14,    2,  348,    0,   21,    0],\n",
              "       [   5, 4941,    6,   47,    4,    1,    6,    0,    2,    0],\n",
              "       [  56,    3, 4209,   37,  394,    4,  277,    0,   12,    0],\n",
              "       [ 102,   30,   33, 4608,  121,    1,   74,    0,    9,    1],\n",
              "       [   9,    7,  301,  123, 4239,    1,  263,    1,    6,    0],\n",
              "       [   0,    2,    0,    3,    0, 4888,    0,   78,    8,   25],\n",
              "       [ 489,    5,  285,   97,  285,    3, 3846,    0,   18,    2],\n",
              "       [   0,    0,    0,    1,    0,   72,    0, 4866,    4,  102],\n",
              "       [   6,    2,    9,   14,    9,    3,   22,    7, 4957,    3],\n",
              "       [   0,    0,    0,    0,    0,   21,    0,  103,    1, 4854]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "US-EBcB6Q_n2",
        "outputId": "1547640b-5dcb-4270-e850-861277adddd5"
      },
      "source": [
        "#  Plot the cost function and the accuracy of the validation data\n",
        "plot_loss_acc(val_loss_arr, val_acc_arr, title='Validation Data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x230.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "best_accuracy: 0.8852 achieved at epoch: 847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9KHVdkYQ_n2",
        "outputId": "f66ffbd9-4e43-427b-fb7f-b0508ea58e68"
      },
      "source": [
        "pred_val = sess.run(MLP['predictions'], feed_dict = feed_dictionary_val)\n",
        "# Display the confusion matrix of the validation data\n",
        "confusion_matrix(y_val, pred_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[858,   0,  14,  36,   4,   2, 100,   0,   9,   0],\n",
              "       [  3, 959,   0,  18,   2,   0,   5,   0,   1,   0],\n",
              "       [ 15,   1, 819,  10,  90,   0,  62,   0,  11,   0],\n",
              "       [ 26,   2,   6, 928,  36,   0,  20,   0,   3,   0],\n",
              "       [  2,   3,  75,  27, 874,   1,  64,   0,   4,   0],\n",
              "       [  0,   0,   0,   0,   0, 936,   0,  35,   8,  17],\n",
              "       [124,   1,  81,  19,  69,   0, 658,   1,  17,   0],\n",
              "       [  0,   0,   1,   0,   0,  17,   0, 908,   1,  28],\n",
              "       [  3,   0,   4,   3,   5,   3,   9,   4, 935,   2],\n",
              "       [  0,   0,   0,   0,   0,  10,   0,  42,   0, 969]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVvmprMkQ_n2",
        "outputId": "b022a008-600e-43d6-fb1b-3e43377e8c0c"
      },
      "source": [
        "feed_test = {MLP['input']: x_test,\n",
        "                MLP['targets']: y_test_oh,\n",
        "                MLP['isTrain']: False}\n",
        "# Note: Change here to print L2 included loss as well\n",
        "test_pred, test_loss = sess.run([MLP['predictions'], MLP['loss']], feed_dict = feed_test)\n",
        "test_acc = np.mean(test_pred == y_test)\n",
        "\n",
        "# Display the cost and accuracy of the test data\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35848033\n",
            "0.8767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0OOc8F4Q_n2",
        "outputId": "3a7dcc40-ddfb-4d22-c256-ba95e8ae7a66"
      },
      "source": [
        "# Display the confusion matrix of the test data\n",
        "cmatrix = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.title(\"Confusion Matrix of Test as Heatmap\")\n",
        "sb.heatmap(cmatrix, cmap = 'Oranges', annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[832,   3,  10,  37,   6,   0, 100,   0,  11,   1],\n",
              "       [  2, 961,   3,  25,   4,   0,   4,   0,   1,   0],\n",
              "       [ 26,   1, 791,  11,  96,   1,  70,   1,   3,   0],\n",
              "       [ 23,   6,   8, 893,  32,   1,  32,   0,   5,   0],\n",
              "       [  0,   1,  89,  35, 816,   1,  51,   0,   7,   0],\n",
              "       [  1,   0,   0,   1,   0, 938,   0,  33,   4,  23],\n",
              "       [125,   1,  98,  34,  66,   0, 661,   0,  15,   0],\n",
              "       [  0,   0,   0,   0,   0,  20,   0, 956,   0,  24],\n",
              "       [  4,   1,   3,   5,   4,   5,  13,   5, 960,   0],\n",
              "       [  0,   0,   0,   0,   0,   6,   1,  34,   0, 959]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    }
  ]
}